{"url": "https://datastax.github.io/graph-rag/", "title": "Graph RAG", "content": ["Graph RAG provides retrievers that combineunstructuredsimilarity-search on vectors andstructuredtraversal of metadata properties.These retrievers are implemented using the metadata search functionality of existing vector stores,allowing you to traverse your existing vector store!", "Link based on existing metadataUse existing metadata fields without additional processing.Retrieve more from your existing vector store!Get started", "Change links on demandEdges can be specified on-the-fly, allowing different relationships to be traversed based on the question.Edges", "Pluggable Traversal StrategiesUse built-in traversal strategies like Eager or MMR, or define your own logic to select which nodes to explore.Strategies", "Broad compatibilityAdapters are available for a variety of vector stores with support foradditional stores easily added.Adapters", "Link based on existing metadata", "Use existing metadata fields without additional processing.Retrieve more from your existing vector store!", "Get started", "Change links on demand", "Edges can be specified on-the-fly, allowing different relationships to be traversed based on the question.", "Edges", "Pluggable Traversal Strategies", "Use built-in traversal strategies like Eager or MMR, or define your own logic to select which nodes to explore.", "Strategies", "Broad compatibility", "Adapters are available for a variety of vector stores with support foradditional stores easily added.", "Adapters"]}
{"url": "https://datastax.github.io/graph-rag/", "title": "Example: LangChain Retriever combining Vector and Graph traversal", "content": ["fromlangchain_graph_retrieverimportGraphRetrieverretriever=GraphRetriever(store=store,edges=[(\"mentions\",\"$id\"),(\"entities\",\"entities\")],# (1)!)retriever.invoke(\"where is Santa Clara?\")", "fromlangchain_graph_retrieverimportGraphRetrieverretriever=GraphRetriever(store=store,edges=[(\"mentions\",\"$id\"),(\"entities\",\"entities\")],# (1)!)retriever.invoke(\"where is Santa Clara?\")", "edgesconfigures traversing from a node to other nodes listed in themetadata[\"mentions\"]field (to the correspondingid) and to other nodes with overlappingmetadata[\"entities\"].", "edges", "metadata[\"mentions\"]", "id", "metadata[\"entities\"]", "SeeExamplesfor more complete examples."]}
{"url": "https://datastax.github.io/graph-rag/guide/", "title": "Guide", "content": ["Graph RAG provides the ability to traverse content in a vector store based on relationships in the metadata.The traversal may start from specific nodes or use vector search to find the best starting places (or both).Each traversal may define a different way to use the metadata to relate information, allowing different calls to traverse to focus on different properties.", "A variety of traversal strategies are supported, allowing the specific nodes selected during traversal to be controlled depending on the goals.In some cases, it is important to find deeper supporting content, while in others finding a broader set of relevant perspectives is more appropriate.", "This guide provides an overview of the key concepts and the components provided by the project.", "In a hurry to get started?", "Go to theGetting Started Guideto jump in!"]}
{"url": "https://datastax.github.io/graph-rag/guide/", "title": "Packages and Components", "content": ["", {"image": {"alt": "Component diagram", "src": "components.svg"}}, "The Graph RAG project primarily consists of two Python packages:", "graph_retrieverprovides the coretraversal functions in a framework independent way.Traversal: The primary methodstraverseandatraversefor performing the graph traversal.Strategies: Configurable and customizable strategies for selecting nodes to visit.Edges: Configurable and customizable specification of relationships between nodes.Adapters: Interface used to interact with a Vector Store.", "Traversal: The primary methodstraverseandatraversefor performing the graph traversal.", "Strategies: Configurable and customizable strategies for selecting nodes to visit.", "Edges: Configurable and customizable specification of relationships between nodes.", "Adapters: Interface used to interact with a Vector Store.", "langchain_graph_retrieveris built on it and integrates with LangChain to allow graph retrieval on LangChainsupported Vector Stores.GraphRetriever: A LangChain Retriever for performing the traversal. Usestraverseandatraverseunder the hood.Transformers: A variety of LangChain document transformers adding metadata that may be useful for traversing.Adapters: Adapter implementations for LangChain Vector Stores.", "GraphRetriever: A LangChain Retriever for performing the traversal. Usestraverseandatraverseunder the hood.", "Transformers: A variety of LangChain document transformers adding metadata that may be useful for traversing.", "Adapters: Adapter implementations for LangChain Vector Stores.", "graph_retrieverprovides the coretraversal functions in a framework independent way.", "graph_retriever", "Traversal: The primary methodstraverseandatraversefor performing the graph traversal.", "Strategies: Configurable and customizable strategies for selecting nodes to visit.", "Edges: Configurable and customizable specification of relationships between nodes.", "Adapters: Interface used to interact with a Vector Store.", "traverse", "atraverse", "langchain_graph_retrieveris built on it and integrates with LangChain to allow graph retrieval on LangChainsupported Vector Stores.", "langchain_graph_retriever", "GraphRetriever: A LangChain Retriever for performing the traversal. Usestraverseandatraverseunder the hood.", "Transformers: A variety of LangChain document transformers adding metadata that may be useful for traversing.", "Adapters: Adapter implementations for LangChain Vector Stores.", "traverse", "atraverse"]}
{"url": "https://datastax.github.io/graph-rag/reference/", "title": "API Reference", "content": ["graph-retrievercontains the core graph traversal logic.", "langchain-graph-retrievercontains aGraphRetrieverand store adapters for use with LangChain.", "graph-rag-example-helperscontains utilities used in some examples, such as recoverably loading large datasets.", "graph-retriever", "langchain-graph-retriever", "GraphRetriever", "graph-rag-example-helpers"]}
{"url": "https://datastax.github.io/graph-rag/faqs/", "title": "FAQs", "content": []}
{"url": "https://datastax.github.io/graph-rag/faqs/", "title": "Is Graph RAG a Knowledge Graph?", "content": ["Yes, for the recent usage of the term.Graph RAG implements graph traversal of structured metadata during retrieval.The structured metadata provides edges connecting unstructured content (\"knowledge\").Graph RAG traverses a graph of knowledge.", "However, prior to the recent surge of Graph RAG, there was a more academic definition of knowledge graphs where nodes specifically represented entities and knowledge about the relationships appeared as edges. Graph RAG isnotthis version of a knowledge graph.", "We have found that adding edges to unstructured content is much easier and efficient to use.Seethe Lazy Graph RAG examplefor more details.", "We previously wrote about this distinction as\"content-centric\" (nodes are content) vs. \"entity-centric\" (nodes are entities).", "We've also demonstrated thatbuilding the content-centric knowledge graph is significinatly cheaper.", "In many ways this mirrors the difference between Microsoft's GraphRAG and LazyGraphRAG."]}
{"url": "https://datastax.github.io/graph-rag/faqs/", "title": "Does Graph RAG need a Graph DB?", "content": ["No.", "Graph databases are excellent for operating on academic knowledge graphs, where you way be looking for specific relationships between multiple nodes -- eg., finding people who live in Seattle (have a \"lives in\" edge pointing at Seattle) and work at a company in Santa Clara (has a \"works at\" edge to a company node with a \"headquartered in\" edge pointing at Santa Clara). In this case, the graphstructureencodes information, meaning the graphqueryneeds to understand that structure.", "However, the best knowledge graph for Graph RAG is a vector store containing unstructured content with structured metadata first, and support traversal of those structured relationships second. This means that any vector store with metadata filtering capabilities (all or nearly all) can be used for traversal.", "Important", "Traditional graph databases require materializing edges during ingestion, making them inflexible and costly to maintain as data evolves. Our approach operates on relationships present in the metadata without materializing them, eliminating the need to decide on the graph relationships during ingestion and enabling each query to operate on a different set of relationships. This makes it easy to add your structured metadata to the documents and traverse it for enhanced retrieval in RAG applications and adapts effortlessly to changing data.", "There are some things a vector store can support that make the kinds of metadata queries needed for traversal more efficient. See the support matrix in theAdapters guidefor more information."]}
{"url": "https://datastax.github.io/graph-rag/guide/traversal/", "title": "Traversal", "content": ["At a high level, traversal performs the following steps:", "Retrievestart_kmost similar to thequeryusing vector search.", "Find the nodes reachable from theinitial_root_ids.", "Discover thestart_knodes and the neighbors of the initial roots as \"depth 0\" candidates.", "Ask the strategy which nodes to visit next.", "If no more nodes to visit, exit and return the selected nodes.", "Record those nodes as selected and retrieve the topadjacent_knodes reachable from them.", "Discover the newly reachable nodes (updating depths as needed).", "Goto 4.", "start_k", "query", "initial_root_ids", "start_k", "adjacent_k"]}
{"url": "https://datastax.github.io/graph-rag/guide/traversal/", "title": "Traversal Methods", "content": ["Thegraph_retrieverpackage providestraverseandatraversefor performing traversals.", "graph_retriever", "traverse", "atraverse"]}
{"url": "https://datastax.github.io/graph-rag/guide/traversal/", "title": "LangChain Graph Retriever", "content": ["Thelangchain_graph_retrieverpackage providesGraphRetriever, an implementation of LangChain'sBaseRetrieverwhich performs traversals.", "langchain_graph_retriever", "GraphRetriever", "BaseRetriever"]}
{"url": "https://datastax.github.io/graph-rag/blog/", "title": "Introducing Graph Retrievers: Smarter, Simpler Document Graphs for Vector Stores", "content": ["We're excited to announce the release ofGraph Retrievers, a powerful new tool for leveraging graph traversal in your vector stores with ease!", "With Graph Retrievers, you can dynamically explore relationships between documents using metadata fields\u2014no need for complex preprocessing or building an entire knowledge graph upfront."]}
{"url": "https://datastax.github.io/graph-rag/examples/", "title": "Examples", "content": ["Lazy Graph RAGImplementsLazyGraphRAGusing LangChain andlangchain-graph-retriever.It loads Wikipedia articles and traverses based on links (\"mentions\") and named entities (extracted from the content). It retrieves a large number of articles, groups them by community, and extracts claims from each community. The best claims are used to answer the question.Lazy Graph RAG Example", "Code GenerationThis example notebook shows how to load documentation for python packages into avector store so that it can be used to provide context to an LLM for code generation.It uses LangChain andlangchain-graph-retrieverwith a custom traversal Strategyin order to improve LLM generated code output. It shows that using GraphRAG canprovide a significant increase in quality over using either an LLM alone or standardRAG.GraphRAG traverses cross references in the documentation like a software engineerwould, in order to determine how to solve a coding problem.Code Generation Example", "Lazy Graph RAG", "ImplementsLazyGraphRAGusing LangChain andlangchain-graph-retriever.", "langchain-graph-retriever", "It loads Wikipedia articles and traverses based on links (\"mentions\") and named entities (extracted from the content). It retrieves a large number of articles, groups them by community, and extracts claims from each community. The best claims are used to answer the question.", "Lazy Graph RAG Example", "Code Generation", "This example notebook shows how to load documentation for python packages into avector store so that it can be used to provide context to an LLM for code generation.", "It uses LangChain andlangchain-graph-retrieverwith a custom traversal Strategyin order to improve LLM generated code output. It shows that using GraphRAG canprovide a significant increase in quality over using either an LLM alone or standardRAG.", "langchain-graph-retriever", "GraphRAG traverses cross references in the documentation like a software engineerwould, in order to determine how to solve a coding problem.", "Code Generation Example"]}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "Adapters", "content": ["Adapters allowgraph-retrieverto connect to specific vector stores.", "graph-retriever", {"table": [["Vector Store", "Supported", "Collections", "Dict-In-List", "Nested Metadata", "Optimized Adjacency"], ["DataStax Astra", "", "", "", "", ""], ["OpenSearch", "", "", "", "", ""], ["Apache Cassandra", "", "", "", "", ""], ["Chroma", "", "", "", "", ""]]}, "Indicates whether a given store is completely supported () or has limited support ().", "Indicates whether the store supports lists in metadata values or not. Stores which do not support it directly () can be used by applying theShreddingTransformerdocument transformer to documents before writing, which spreads the items of the collection into multiple metadata keys.", "Indicates the store supports using a dict-value in a list for edges. Forexample, when using named-entity recognition, you may haveentities = [{\"type\":\"PERSON\", \"entity\": \"Bell\"}, ...]and wish to link nodes with the same entityusing an edge defined as(\"entities\", \"entities\").", "entities = [{\"type\":\"PERSON\", \"entity\": \"Bell\"}, ...]", "(\"entities\", \"entities\")", "Whether edges can be defined using values of nested metadata. For example,page_structure.sectionto access the section ID stored in metadata asmetadata[\"page_structure\"] = { \"section\": ... }.", "page_structure.section", "metadata[\"page_structure\"] = { \"section\": ... }", "Whether the store supports an optimized query for nodes adjacent to multiple edges. Without this optimization each edge must be queried separately. Stores that support the combined adjacent query perform much better, especially when retrieving large numbers of nodes and/or dealing with high connectivity.", "Warning", "Graph Retriever can be used with any of these supported Vector Stores. However, storesthat operate directly on nested collections (without denormalization) and support optimized adjacencymuch more performant and better suited for production use. Stores like Chroma are bestemployed for early experimentation, while it is generally recommended to use a store like DataStax AstraDB when scaling up."]}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "Supported Stores", "content": []}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "Astra", "content": ["DataStax AstraDBissupported by theAstraAdapter. The adaptersupports operating on metadata containing both primitive and list values.Additionally, it optimizes the request for nodes connected to multiple edges into a single query.", "AstraAdapter"]}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "OpenSearch", "content": ["OpenSearchis supported by theOpenSearchAdapter. The adapter supports operating on metadata containing both primitive and list values. It does not perform an optimized adjacent query.", "OpenSearchAdapter"]}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "Apache Cassandra", "content": ["Apache Cassandrais supported by theCassandraAdapter. The adapter requires shredding metadata containing lists in order to use them as edges. It does not perform an optimized adjacent query.", "CassandraAdapter"]}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "Chroma", "content": ["Chromais supported by theChromaAdapter. The adapter requires shredding metadata containing lists in order to use them as edges. It does not perform an optimized adjacent query.", "ChromaAdapter"]}
{"url": "https://datastax.github.io/graph-rag/guide/adapters/", "title": "Implementation", "content": ["TheAdapterinterface may be implemented directly. For LangChainVectorStores,LangchainAdapterandShreddedLangchainAdapterprovide much of the necessary functionality."]}
{"url": "https://datastax.github.io/graph-rag/guide/migration/", "title": "Migration", "content": ["This page discusses migration from LangChainGraphVectorStoreas well as between versions ofgraph-retrieverandlangchain-graph-retriever.", "GraphVectorStore", "graph-retriever", "langchain-graph-retriever"]}
{"url": "https://datastax.github.io/graph-rag/guide/migration/", "title": "From LangChain GraphVectorStore", "content": ["LangChainGraphVectorStorerelied on putting specially craftedLinkinstances intometadata[\"links\"]. Many cases used link extractors to compute these links, but it was also often useful (and necessary) to create them manually.", "GraphVectorStore", "Link", "metadata[\"links\"]", "When converting from aGraphVectorStoreto the newlangchain-graph-retrieverlibrary, you need to do the following:", "GraphVectorStore", "langchain-graph-retriever", "Replace uses of the link extractors with document transformers.", "Replace manualy link creation with metadata fields.", "ReplaceGraphVectorStoreusage with theGraphRetriever.", "GraphVectorStore", "GraphRetriever"]}
{"url": "https://datastax.github.io/graph-rag/guide/migration/", "title": "Replace Link Extractors with Document Transformers", "content": ["GLiNERLinkExtractor", "Replace withGLiNERTransformer, which will populate metadata fields for each label.", "HierarchyLinkExtractor", "If you already have a parent ID in the metadata, you can remove this. Otherwise, replace with theParentTransformerwhich populates aparentfield computed from a path. The parent field may be used with edges to achieve parent-to-child, child-to-parent, and sibling-to-sibling navigation.", "parent", "HtmlLinkExtractor", "Replace withHyperlinkTransformerwhich extracts hyperlinks from each chunk and writes them to a metadata field.", "KeybertLinkExtractor", "Replace withKeybertTransformer, which will populate a metadata field with the keywords."]}
{"url": "https://datastax.github.io/graph-rag/guide/migration/", "title": "Replace Manual Link Creation with Metadata Fields", "content": ["With the old library, you had to choose the direction of the links when they were created -- eitherin,outorbidir. With the new library, you simply create the corresponding fields and choose the direction of edges when you issue a query (seenext section).", "in", "out", "bidir", "# Document metadata for a page at `http://somesite` linking to some other URLs# and a few keyword links.doc=Document(...,metadata={\"links\":[Link.incoming(\"url\",\"http://somesite\"),Link.outgoing(\"url\",\"http://someothersite\"),Link.outgoing(\"url\",\"http://yetanothersite\"),Link.bidir(\"keyword\",\"sites\"),Link.bidir(\"keyword\",\"web\"),]})", "# Document metadata for a page at `http://somesite` linking to some other URLs# and a few keyword links.doc=Document(...,metadata={\"links\":[Link.incoming(\"url\",\"http://somesite\"),Link.outgoing(\"url\",\"http://someothersite\"),Link.outgoing(\"url\",\"http://yetanothersite\"),Link.bidir(\"keyword\",\"sites\"),Link.bidir(\"keyword\",\"web\"),]})", "doc=Document(...,metadata={\"url\":\"http://somesite\",\"hrefs\":[\"http://someothersite\",\"http://yetanothersite\"],\"keywords\":[\"sites\",\"web\"],})", "doc=Document(...,metadata={\"url\":\"http://somesite\",\"hrefs\":[\"http://someothersite\",\"http://yetanothersite\"],\"keywords\":[\"sites\",\"web\"],})", "These metadata fields can be used to accomplish a variety of graph traversals. For example:", "edges = [(\"hrefs\", \"url\"), ...]navigates from a site to the pages it links to (fromhrefstourl).", "edges = [(\"keywords\", \"keywords\"), ...]navigates from a site to other sites with the same keyword.", "edges = [(\"url\", \"hrefs\"), ...]navigates from a site to other sites that link to it.", "edges = [(\"hrefs\", \"url\"), ...]", "hrefs", "url", "edges = [(\"keywords\", \"keywords\"), ...]", "edges = [(\"url\", \"hrefs\"), ...]", "Per-Query Edges", "You can use different edges for each query, allowing you to navigate different directions depending on the needs. In the old library, you only ever navigated out from a site to the things it linked to, while with the new library the metadata captures the information (what URL is this document from, what URLs does it reference) and the edges determine which fileds are traversed at retrieval time."]}
{"url": "https://datastax.github.io/graph-rag/guide/migration/", "title": "Replace GraphVectorStore with the GraphRetriever", "content": ["Finally, rather than creating the links and writing them to aGraphVectorStoreyou write the documents (with metadata) to a standardVectorStoreand apply aGraphRetriever:", "GraphVectorStore", "VectorStore", "fromlangchain_graph_retrieverimportGraphRetrieverretriever=GraphRetriever(store=vector_store,edges=[(\"hrefs\",\"url\"),(\"keywords\",\"keywords\")],)", "fromlangchain_graph_retrieverimportGraphRetrieverretriever=GraphRetriever(store=vector_store,edges=[(\"hrefs\",\"url\"),(\"keywords\",\"keywords\")],)"]}
{"url": "https://datastax.github.io/graph-rag/guide/get-started/", "title": "Get Started", "content": ["This page demonstrates how to combine Graph Traversal and Vector Search usinglangchain-graph-retrieverwithlangchain.", "langchain-graph-retriever", "langchain"]}
{"url": "https://datastax.github.io/graph-rag/guide/get-started/", "title": "Pre-requisites", "content": ["We assume you already have a workinglangchaininstallation, including an LLM andembedding model as well as asupported vector store.", "langchain", "In that case, you only need to installlangchain-graph-retriever:", "langchain-graph-retriever", "pipinstalllangchainlangchain-graph-retriever", "pipinstalllangchainlangchain-graph-retriever"]}
{"url": "https://datastax.github.io/graph-rag/guide/get-started/", "title": "Preparing Data", "content": ["Loading data is exactly the same as for whichever vector store you use. The main thingto consider is what structured information you wish to include in the metadata tosupport traversal.", "For this guide, I have a JSON file with information about animals. Several exampleentries are shown below. The actual file has one entry per line, making it easy toload intoDocuments.", "Document", "{\"id\":\"alpaca\",\"text\":\"alpacas are domesticated mammals valued for their soft wool and friendly demeanor.\",\"metadata\":{\"type\":\"mammal\",\"number_of_legs\":4,\"keywords\":[\"wool\",\"domesticated\",\"friendly\"],\"origin\":\"south america\"}}{\"id\":\"caribou\",\"text\":\"caribou, also known as reindeer, are migratory mammals found in arctic regions.\",\"metadata\":{\"type\":\"mammal\",\"number_of_legs\":4,\"keywords\":[\"migratory\",\"arctic\",\"herbivore\",\"tundra\"],\"diet\":\"herbivorous\"}}{\"id\":\"cassowary\",\"text\":\"cassowaries are flightless birds known for their colorful necks and powerful legs.\",\"metadata\":{\"type\":\"bird\",\"number_of_legs\":2,\"keywords\":[\"flightless\",\"colorful\",\"powerful\"],\"habitat\":\"rainforest\"}}", "{\"id\":\"alpaca\",\"text\":\"alpacas are domesticated mammals valued for their soft wool and friendly demeanor.\",\"metadata\":{\"type\":\"mammal\",\"number_of_legs\":4,\"keywords\":[\"wool\",\"domesticated\",\"friendly\"],\"origin\":\"south america\"}}{\"id\":\"caribou\",\"text\":\"caribou, also known as reindeer, are migratory mammals found in arctic regions.\",\"metadata\":{\"type\":\"mammal\",\"number_of_legs\":4,\"keywords\":[\"migratory\",\"arctic\",\"herbivore\",\"tundra\"],\"diet\":\"herbivorous\"}}{\"id\":\"cassowary\",\"text\":\"cassowaries are flightless birds known for their colorful necks and powerful legs.\",\"metadata\":{\"type\":\"bird\",\"number_of_legs\":2,\"keywords\":[\"flightless\",\"colorful\",\"powerful\"],\"habitat\":\"rainforest\"}}", "fromgraph_rag_example_helpers.datasets.animalsimportfetch_documentsanimals=fetch_documents()", "fromgraph_rag_example_helpers.datasets.animalsimportfetch_documentsanimals=fetch_documents()"]}
{"url": "https://datastax.github.io/graph-rag/guide/get-started/", "title": "Populating the Vector Store", "content": ["The following shows how to populate a variety of vector stores with the animal data.", "fromdotenvimportload_dotenvfromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsload_dotenv()vector_store=AstraDBVectorStore.from_documents(collection_name=\"animals\",documents=animals,embedding=OpenAIEmbeddings(),)", "fromdotenvimportload_dotenvfromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsload_dotenv()vector_store=AstraDBVectorStore.from_documents(collection_name=\"animals\",documents=animals,embedding=OpenAIEmbeddings(),)", "fromlangchain_community.vectorstores.cassandraimportCassandrafromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()# (1)!vector_store=Cassandra.from_documents(documents=list(shredder.transform_documents(animals)),embedding=OpenAIEmbeddings(),table_name=\"animals\",)", "fromlangchain_community.vectorstores.cassandraimportCassandrafromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()# (1)!vector_store=Cassandra.from_documents(documents=list(shredder.transform_documents(animals)),embedding=OpenAIEmbeddings(),table_name=\"animals\",)", "Since Cassandra doesn't index items in lists for querying, it is necessary toshred metadata containing list to be queried. By default, theShreddingTransformershreds all keys. It may be configured to only shred thosemetadata keys used as edge targets.", "ShreddingTransformer", "fromlangchain_community.vectorstoresimportOpenSearchVectorSearchfromlangchain_openaiimportOpenAIEmbeddingsvector_store=OpenSearchVectorSearch.from_documents(opensearch_url=OPEN_SEARCH_URL,index_name=\"animals\",embedding=OpenAIEmbeddings(),engine=\"faiss\",documents=animals,bulk_size=500,# (1)!)", "fromlangchain_community.vectorstoresimportOpenSearchVectorSearchfromlangchain_openaiimportOpenAIEmbeddingsvector_store=OpenSearchVectorSearch.from_documents(opensearch_url=OPEN_SEARCH_URL,index_name=\"animals\",embedding=OpenAIEmbeddings(),engine=\"faiss\",documents=animals,bulk_size=500,# (1)!)", "There is currently a bug in the OpenSearchVectorStore implementation thatrequires this extra parameter.", "fromlangchain_chroma.vectorstoresimportChromafromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()# (1)!vector_store=Chroma.from_documents(documents=list(shredder.transform_documents(animals)),embedding=OpenAIEmbeddings(),collection_name=\"animals\",)", "fromlangchain_chroma.vectorstoresimportChromafromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()# (1)!vector_store=Chroma.from_documents(documents=list(shredder.transform_documents(animals)),embedding=OpenAIEmbeddings(),collection_name=\"animals\",)", "Since Chroma doesn't index items in lists for querying, it is necessary toshred metadata containing list to be queried. By default, theShreddingTransformershreds all keys. It may be configured to only shred thosemetadata keys used as edge targets.", "ShreddingTransformer"]}
{"url": "https://datastax.github.io/graph-rag/guide/get-started/", "title": "Simple Traversal", "content": ["For our first retrieval and graph traversal, we're going to start with a single animal bestmatching the query, and then traverse to other animals with the samehabitatand/ororigin.", "habitat", "origin", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieversimple=GraphRetriever(store=vector_store,edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieversimple=GraphRetriever(store=vector_store,edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieverfromlangchain_graph_retriever.adapters.cassandraimportCassandraAdaptersimple=GraphRetriever(store=CassandraAdapter(vector_store,shredder,{\"keywords\"}),,edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieverfromlangchain_graph_retriever.adapters.cassandraimportCassandraAdaptersimple=GraphRetriever(store=CassandraAdapter(vector_store,shredder,{\"keywords\"}),,edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieversimple=GraphRetriever(store=vector_store,edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieversimple=GraphRetriever(store=vector_store,edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieverfromlangchain_graph_retriever.adapters.chromaimportChromaAdaptersimple=GraphRetriever(store=ChromaAdapter(vector_store,shredder,{\"keywords\"}),edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "fromgraph_retriever.strategiesimportEagerfromlangchain_graph_retrieverimportGraphRetrieverfromlangchain_graph_retriever.adapters.chromaimportChromaAdaptersimple=GraphRetriever(store=ChromaAdapter(vector_store,shredder,{\"keywords\"}),edges=[(\"habitat\",\"habitat\"),(\"origin\",\"origin\"),(\"keywords\",\"keywords\")],strategy=Eager(k=10,start_k=1,max_depth=2),)", "Shredding", "The above code is exactly the same for all stores, however adapters for shreddedstores (Chroma and Apache Cassandra) require configuration to specify which metadatafields need to be rewritten when issuing queries.", "The above creates a graph traversing retriever that starts with the nearest animal(start_k=1), retrieves 10 documents (k=10) and limits the search to documents thatare at most 2 steps away from the first animal (max_depth=2).", "start_k=1", "k=10", "max_depth=2", "The edges define how metadata values can be used for traversal. In this case, everyanimal is connected to other animals with the same habitat and/or same origin.", "simple_results=simple.invoke(\"what mammals could be found near a capybara\")fordocinsimple_results:print(f\"{doc.id}:{doc.page_content}\")", "simple_results=simple.invoke(\"what mammals could be found near a capybara\")fordocinsimple_results:print(f\"{doc.id}:{doc.page_content}\")"]}
{"url": "https://datastax.github.io/graph-rag/guide/get-started/", "title": "Visualizing", "content": ["langchain-graph-retrieversincludes code for converting the document graph into anetworkxgraph, for rendering and other analysis. See @fig-document-graph", "langchain-graph-retrievers", "networkx", "importnetworkxasnximportmatplotlib.pyplotaspltfromlangchain_graph_retriever.document_graphimportcreate_graphdocument_graph=create_graph(documents=simple_results,edges=simple.edges,)nx.draw(document_graph,with_labels=True)plt.show()", "importnetworkxasnximportmatplotlib.pyplotaspltfromlangchain_graph_retriever.document_graphimportcreate_graphdocument_graph=create_graph(documents=simple_results,edges=simple.edges,)nx.draw(document_graph,with_labels=True)plt.show()"]}
{"url": "https://datastax.github.io/graph-rag/guide/edges/", "title": "Edges", "content": ["Edges specify how content should be linked.Often, content in existing vector stores has metadata based on structured information.For example, a vector store containing articles may have information about the authors, keywords, and citations of those articles.Such content can be traversed along relationships already present in that metadata!SeeSpecifying Edgesfor more on how edges are specified.", "Edges can be dynamically specified each time", "Since edges can be different each time the traversal is invoked, it is possible to tailor the relationships being used to the question."]}
{"url": "https://datastax.github.io/graph-rag/guide/edges/", "title": "Specifying Edges", "content": ["Edges are specified by passing theedgesparameter totraverseoratraverse. When used with LangChain, they may be provided when theGraphRetrieveris instantiated or wheninvokeorainvokeis called.", "edges", "traverse", "atraverse", "GraphRetriever", "invoke", "ainvoke", "The following example shows how edges can be defined using metadata from an example article.", "Specifying Edges", "Content(id=\"article1\",content=\"...\",metadata={\"keywords\":[\"GPT\",\"GenAI\"],\"authors\":[\"Ben\",\"Eric\"],\"primary_author\":\"Eric\",\"cites\":[\"article2\",\"article3\"],})", "Content(id=\"article1\",content=\"...\",metadata={\"keywords\":[\"GPT\",\"GenAI\"],\"authors\":[\"Ben\",\"Eric\"],\"primary_author\":\"Eric\",\"cites\":[\"article2\",\"article3\"],})", "(\"keywords\", \"keywords\")connects to other articles about GPT and GenAI.", "(\"authors\", \"authors\")connects to other articles by any of the same authors.", "(\"authors\", \"primary_author\")connects to other articles whose primary author was Ben or Eric.", "(\"cites\", \"$id\")connects to the articles cited (by ID).", "(\"$id\", \"cites\")connects to articles which cite this one.", "(\"cites\", \"cites\")connects to other articles with citations in common.", "(\"keywords\", \"keywords\")", "(\"authors\", \"authors\")", "(\"authors\", \"primary_author\")", "(\"cites\", \"$id\")", "(\"$id\", \"cites\")", "(\"cites\", \"cites\")"]}
{"url": "https://datastax.github.io/graph-rag/guide/edges/", "title": "Edge Functions", "content": ["While sometimes the information to traverse is missing and the vector storeneeds to be re-populated, in other cases the information exist but not quite bein a suitable format for traversal. For instance, the\"authors\"field maycontain a list of authors and their institution, making it impossible to link toother articles by the same author when they were at a different institution.", "\"authors\"", "In such cases, you can provide a customEdgeFunctionto extract the edges fortraversal.", "EdgeFunction"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "Transformers", "content": ["Transformers are optional, not mandatory", "Graph traversal operates on the structured metadata.Transformers provide tools for populating the metadata, but they are not necessary.In many cases you may have existing structured information that is usefulin addition or instead of what the transformers would populate.", "We provide two types of document transformers that can be useful in setting up yourdocuments for graph traversal.", "Information Extractors:These extract information out of document content    and add to the metadata.", "Metadata Utilities:These add to or modify document metadata to enable certain    features", "Information Extractors:These extract information out of document content    and add to the metadata.", "Metadata Utilities:These add to or modify document metadata to enable certain    features"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "Information Extractors", "content": ["Extras required", "Most of the Transformers in this section require extra packages to be installed.Either look at the specifics in the reference documentation for each transformer,or install all the extras via:", "pip install \"langchain-graph-retriever[all]\"", "pip install \"langchain-graph-retriever[all]\""]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "NLP-Model Based", "content": ["Several of our document transformers that extract information depend on pre-trainedNatural Language Processing (NLP) models.", "The following LangChain documents will be used for the code examples in this section:", "fromlangchain_core.documentsimportDocumentmodel_docs=[Document(id=\"red_fox\",page_content=\"\"\"The Red Fox is an omnivore, feeding on small mammals, birds, fruits, and insects. Itthrives in a wide range of habitats, including forests, grasslands, and even urban areaslike New York City, where it has adapted to human presence. This agile creature movesprimarily by walking and running, but it can also leap and climb when necessary. Itsbody is covered in thick fur, which helps it stay warm in colder climates. The NationalWildlife Federation has tracked their urban expansion, and their population washighlighted in the Wildlife Conservation Summit 2023.\"\"\",),Document(id=\"sea_turtle\",page_content=\"\"\"The Green Sea Turtle is a herbivore, grazing on seagrass and algae in coastal waters andshallow tropical seas, particularly around the Great Barrier Reef. It is a powerfulswimmer, using its large, flipper-like limbs to glide through the ocean. Unlike mammals,its body is covered in a tough, scaly shell, providing protection from predators.Conservation efforts by The World Wildlife Fund have played a significant role inprotecting this species, and it was a major focus of discussion at the Marine LifeProtection Conference 2024.\",),]", "fromlangchain_core.documentsimportDocumentmodel_docs=[Document(id=\"red_fox\",page_content=\"\"\"The Red Fox is an omnivore, feeding on small mammals, birds, fruits, and insects. Itthrives in a wide range of habitats, including forests, grasslands, and even urban areaslike New York City, where it has adapted to human presence. This agile creature movesprimarily by walking and running, but it can also leap and climb when necessary. Itsbody is covered in thick fur, which helps it stay warm in colder climates. The NationalWildlife Federation has tracked their urban expansion, and their population washighlighted in the Wildlife Conservation Summit 2023.\"\"\",),Document(id=\"sea_turtle\",page_content=\"\"\"The Green Sea Turtle is a herbivore, grazing on seagrass and algae in coastal waters andshallow tropical seas, particularly around the Great Barrier Reef. It is a powerfulswimmer, using its large, flipper-like limbs to glide through the ocean. Unlike mammals,its body is covered in a tough, scaly shell, providing protection from predators.Conservation efforts by The World Wildlife Fund have played a significant role inprotecting this species, and it was a major focus of discussion at the Marine LifeProtection Conference 2024.\",),]"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "GLiNERTransformer", "content": ["TheGLiNERTransformerextracts structured entity labels from text, identifying key attributes and categoriesto enrich document metadata with semantic information.", "GLiNERTransformer", "Example use:frompprintimportpprintfromlangchain_graph_retriever.transformers.glinerimportGLiNERTransformergliner=GLiNERTransformer(labels=[\"diet\",\"habitat\",\"locomotion\",\"body covering\"])gliner_docs=gliner.transform_documents(docs)fordocingliner_docs:pprint({\"id\":doc.id,\"metadata\":doc.metadata},width=100)", "frompprintimportpprintfromlangchain_graph_retriever.transformers.glinerimportGLiNERTransformergliner=GLiNERTransformer(labels=[\"diet\",\"habitat\",\"locomotion\",\"body covering\"])gliner_docs=gliner.transform_documents(docs)fordocingliner_docs:pprint({\"id\":doc.id,\"metadata\":doc.metadata},width=100)", "frompprintimportpprintfromlangchain_graph_retriever.transformers.glinerimportGLiNERTransformergliner=GLiNERTransformer(labels=[\"diet\",\"habitat\",\"locomotion\",\"body covering\"])gliner_docs=gliner.transform_documents(docs)fordocingliner_docs:pprint({\"id\":doc.id,\"metadata\":doc.metadata},width=100)", "Example output:{'id': 'red_fox','metadata': {'body covering': ['thick fur'],'diet': ['birds', 'omnivore', 'small mammals', 'insects', 'fruits'],'habitat': ['urban areas', 'new york city', 'forests', 'grasslands'],'locomotion': ['walking and running']}}{'id': 'sea_turtle','metadata': {'body covering': ['scaly shell'],'diet': ['seagrass and algae'],'habitat': ['coastal waters', 'shallow tropical seas', 'great barrier reef']}}", "{'id': 'red_fox','metadata': {'body covering': ['thick fur'],'diet': ['birds', 'omnivore', 'small mammals', 'insects', 'fruits'],'habitat': ['urban areas', 'new york city', 'forests', 'grasslands'],'locomotion': ['walking and running']}}{'id': 'sea_turtle','metadata': {'body covering': ['scaly shell'],'diet': ['seagrass and algae'],'habitat': ['coastal waters', 'shallow tropical seas', 'great barrier reef']}}", "{'id': 'red_fox','metadata': {'body covering': ['thick fur'],'diet': ['birds', 'omnivore', 'small mammals', 'insects', 'fruits'],'habitat': ['urban areas', 'new york city', 'forests', 'grasslands'],'locomotion': ['walking and running']}}{'id': 'sea_turtle','metadata': {'body covering': ['scaly shell'],'diet': ['seagrass and algae'],'habitat': ['coastal waters', 'shallow tropical seas', 'great barrier reef']}}"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "KeyBERTTransformer", "content": ["TheKeyBERTTransformerextracts key topics and concepts from text, generating metadata that highlights the mostrelevant terms to describe the content.", "KeyBERTTransformer", "Example use:fromlangchain_graph_retriever.transformers.keybertimportKeyBERTTransformerkeybert=KeyBERTTransformer()keybert_docs=keybert.transform_documents(model_docs)fordocinkeybert_docs:print(f\"{doc.id}:{doc.metadata}\")", "fromlangchain_graph_retriever.transformers.keybertimportKeyBERTTransformerkeybert=KeyBERTTransformer()keybert_docs=keybert.transform_documents(model_docs)fordocinkeybert_docs:print(f\"{doc.id}:{doc.metadata}\")", "fromlangchain_graph_retriever.transformers.keybertimportKeyBERTTransformerkeybert=KeyBERTTransformer()keybert_docs=keybert.transform_documents(model_docs)fordocinkeybert_docs:print(f\"{doc.id}:{doc.metadata}\")", "Example output:red_fox: {'keywords': ['wildlife', 'fox', 'mammals', 'habitats', 'omnivore']}sea_turtle: {'keywords': ['turtle', 'reef', 'marine', 'seagrass', 'wildlife']}", "red_fox: {'keywords': ['wildlife', 'fox', 'mammals', 'habitats', 'omnivore']}sea_turtle: {'keywords': ['turtle', 'reef', 'marine', 'seagrass', 'wildlife']}", "red_fox: {'keywords': ['wildlife', 'fox', 'mammals', 'habitats', 'omnivore']}sea_turtle: {'keywords': ['turtle', 'reef', 'marine', 'seagrass', 'wildlife']}"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "SpacyNERTransformer", "content": ["TheSpacyNERTransformeridentifies and labels named entities in text, extracting structured metadata such as organizations, locations, dates, and other key entity types.", "SpacyNERTransformer", "Example use:frompprintimportpprintfromlangchain_graph_retriever.transformers.spacyimportSpacyNERTransformerspacy=SpacyNERTransformer()spacy_docs=spacy.transform_documents(docs)fordocinspacy_docs:pprint({\"id\":doc.id,\"metadata\":doc.metadata},width=100)", "frompprintimportpprintfromlangchain_graph_retriever.transformers.spacyimportSpacyNERTransformerspacy=SpacyNERTransformer()spacy_docs=spacy.transform_documents(docs)fordocinspacy_docs:pprint({\"id\":doc.id,\"metadata\":doc.metadata},width=100)", "frompprintimportpprintfromlangchain_graph_retriever.transformers.spacyimportSpacyNERTransformerspacy=SpacyNERTransformer()spacy_docs=spacy.transform_documents(docs)fordocinspacy_docs:pprint({\"id\":doc.id,\"metadata\":doc.metadata},width=100)", "Example output:{'id': 'red_fox','metadata': {'entities': ['ORG: The National Wildlife Federation','GPE: New York City','ORG: the Wildlife Conservation Summit','DATE: 2023']}}{'id': 'sea_turtle','metadata': {'entities': ['ORG: The World Wildlife Fund','FAC: the Great Barrier Reef','ORG: the Marine Life Protection Conference','LOC: The Green Sea Turtle','DATE: 2024']}}", "{'id': 'red_fox','metadata': {'entities': ['ORG: The National Wildlife Federation','GPE: New York City','ORG: the Wildlife Conservation Summit','DATE: 2023']}}{'id': 'sea_turtle','metadata': {'entities': ['ORG: The World Wildlife Fund','FAC: the Great Barrier Reef','ORG: the Marine Life Protection Conference','LOC: The Green Sea Turtle','DATE: 2024']}}", "{'id': 'red_fox','metadata': {'entities': ['ORG: The National Wildlife Federation','GPE: New York City','ORG: the Wildlife Conservation Summit','DATE: 2023']}}{'id': 'sea_turtle','metadata': {'entities': ['ORG: The World Wildlife Fund','FAC: the Great Barrier Reef','ORG: the Marine Life Protection Conference','LOC: The Green Sea Turtle','DATE: 2024']}}"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "Parser Based", "content": ["The following document transformer uses a parser to extract metadata."]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "HyperlinkTransformer", "content": ["TheHyperlinkTransformerextracts hyperlinks from HTML content and stores them in document metadata.", "HyperlinkTransformer", "fromlangchain_core.documentsimportDocumentanimal_html=\"\"\"<!DOCTYPE html><html><head><title>Animals of the World</title></head><body><h2>Mammals</h2><p>The <a href=\"https://example.com/lion\">lion</a> is the king of the jungle.</p><p>The <a href=\"https://example.com/elephant\">elephant</a> is a large animal.</p><h2>Birds</h2><p>The <a href=\"https://example.com/eagle\">eagle</a> soars high in the sky.</p><p>The <a href=\"https://example.com/penguin\">penguin</a> thrives in icy areas.</p></body></html>\"\"\"html_doc=Document(page_content=animal_html,metadata={\"url\":\"https://example.com/animals\"})", "fromlangchain_core.documentsimportDocumentanimal_html=\"\"\"<!DOCTYPE html><html><head><title>Animals of the World</title></head><body><h2>Mammals</h2><p>The <a href=\"https://example.com/lion\">lion</a> is the king of the jungle.</p><p>The <a href=\"https://example.com/elephant\">elephant</a> is a large animal.</p><h2>Birds</h2><p>The <a href=\"https://example.com/eagle\">eagle</a> soars high in the sky.</p><p>The <a href=\"https://example.com/penguin\">penguin</a> thrives in icy areas.</p></body></html>\"\"\"html_doc=Document(page_content=animal_html,metadata={\"url\":\"https://example.com/animals\"})", "Note that each document needs to have an existingurlmetadata field.", "url", "Example use:frompprintimportpprintfromlangchain_graph_retriever.transformers.htmlimportHyperlinkTransformerhtml_transformer=HyperlinkTransformer()extracted_doc=html_transformer.transform_documents(html_docs)[0]pprint(extracted_doc.metadata)", "frompprintimportpprintfromlangchain_graph_retriever.transformers.htmlimportHyperlinkTransformerhtml_transformer=HyperlinkTransformer()extracted_doc=html_transformer.transform_documents(html_docs)[0]pprint(extracted_doc.metadata)", "frompprintimportpprintfromlangchain_graph_retriever.transformers.htmlimportHyperlinkTransformerhtml_transformer=HyperlinkTransformer()extracted_doc=html_transformer.transform_documents(html_docs)[0]pprint(extracted_doc.metadata)", "Example output:{'hyperlink': ['https://example.com/eagle','https://example.com/lion','https://example.com/elephant','https://example.com/penguin'],'url': 'https://example.com/animals'}", "{'hyperlink': ['https://example.com/eagle','https://example.com/lion','https://example.com/elephant','https://example.com/penguin'],'url': 'https://example.com/animals'}", "{'hyperlink': ['https://example.com/eagle','https://example.com/lion','https://example.com/elephant','https://example.com/penguin'],'url': 'https://example.com/animals'}"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "Metadata Utilities", "content": []}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "ParentTransformer", "content": ["TheParentTransformeradds the hierarchalParentpath to the document metadata.", "ParentTransformer", "Parent", "fromlangchain_core.documentsimportDocumentparent_docs=[Document(id=\"root\",page_content=\"test\",metadata={\"path\":\"root\"}),Document(id=\"h1\",page_content=\"test\",metadata={\"path\":\"root.h1\"}),Document(id=\"h1a\",page_content=\"test\",metadata={\"path\":\"root.h1.a\"}),]", "fromlangchain_core.documentsimportDocumentparent_docs=[Document(id=\"root\",page_content=\"test\",metadata={\"path\":\"root\"}),Document(id=\"h1\",page_content=\"test\",metadata={\"path\":\"root.h1\"}),Document(id=\"h1a\",page_content=\"test\",metadata={\"path\":\"root.h1.a\"}),]", "Note that each document needs to have an existingpathmetadata field.", "path", "Example use:fromlangchain_graph_retriever.transformersimportParentTransformertransformer=ParentTransformer(path_delimiter=\".\")transformed_docs=transformer.transform_documents(parent_docs)fordocintransformed_docs:print(f\"{doc.id}:{doc.metadata}\")", "fromlangchain_graph_retriever.transformersimportParentTransformertransformer=ParentTransformer(path_delimiter=\".\")transformed_docs=transformer.transform_documents(parent_docs)fordocintransformed_docs:print(f\"{doc.id}:{doc.metadata}\")", "fromlangchain_graph_retriever.transformersimportParentTransformertransformer=ParentTransformer(path_delimiter=\".\")transformed_docs=transformer.transform_documents(parent_docs)fordocintransformed_docs:print(f\"{doc.id}:{doc.metadata}\")", "Example output:root: {'path': 'root'}h1: {'path': 'root.h1', 'parent': 'root'}h1a: {'path': 'root.h1.a', 'parent': 'root.h1'}", "root: {'path': 'root'}h1: {'path': 'root.h1', 'parent': 'root'}h1a: {'path': 'root.h1.a', 'parent': 'root.h1'}", "root: {'path': 'root'}h1: {'path': 'root.h1', 'parent': 'root'}h1a: {'path': 'root.h1.a', 'parent': 'root.h1'}"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "ShreddingTransformer", "content": ["TheShreddingTransformeris primarily designed as a helper utility for vector stores that do not have nativesupport for collection-based metadata fields. It transforms these fields into multiplemetadata key-value pairs before database insertion. It also provides a method to restoremetadata back to its original format.", "ShreddingTransformer"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "Shredding", "content": ["fromlangchain_core.documentsimportDocumentcollection_doc=Document(id=\"red_fox\",page_content=\"test\",metadata={\"diet\":[\"birds\",\"omnivore\",\"small mammals\",\"insects\",\"fruits\"],\"size\":\"small\"})", "fromlangchain_core.documentsimportDocumentcollection_doc=Document(id=\"red_fox\",page_content=\"test\",metadata={\"diet\":[\"birds\",\"omnivore\",\"small mammals\",\"insects\",\"fruits\"],\"size\":\"small\"})", "Example use:frompprintimportpprintfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()shredded_docs=shredder.transform_documents([collection_doc])pprint(shredded_docs[0].metadata)", "frompprintimportpprintfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()shredded_docs=shredder.transform_documents([collection_doc])pprint(shredded_docs[0].metadata)", "frompprintimportpprintfromlangchain_graph_retriever.transformersimportShreddingTransformershredder=ShreddingTransformer()shredded_docs=shredder.transform_documents([collection_doc])pprint(shredded_docs[0].metadata)", "Example output:{'__shredded_keys': '[\"diet\"]','diet\u2192birds': '\u00a7','diet\u2192fruits': '\u00a7','diet\u2192insects': '\u00a7','diet\u2192omnivore': '\u00a7','diet\u2192small mammals': '\u00a7','size': 'small'}", "{'__shredded_keys': '[\"diet\"]','diet\u2192birds': '\u00a7','diet\u2192fruits': '\u00a7','diet\u2192insects': '\u00a7','diet\u2192omnivore': '\u00a7','diet\u2192small mammals': '\u00a7','size': 'small'}", "{'__shredded_keys': '[\"diet\"]','diet\u2192birds': '\u00a7','diet\u2192fruits': '\u00a7','diet\u2192insects': '\u00a7','diet\u2192omnivore': '\u00a7','diet\u2192small mammals': '\u00a7','size': 'small'}"]}
{"url": "https://datastax.github.io/graph-rag/guide/transformers/", "title": "Restoration", "content": ["This example uses the output from the Shredding Example above.", "Example use:restored_docs=shredder.restore_documents(shredded_docs)pprint(restored_docs[0].metadata)", "restored_docs=shredder.restore_documents(shredded_docs)pprint(restored_docs[0].metadata)", "restored_docs=shredder.restore_documents(shredded_docs)pprint(restored_docs[0].metadata)", "Example output:{'diet': ['birds', 'omnivore', 'small mammals', 'insects', 'fruits'],'size': 'small'}", "{'diet': ['birds', 'omnivore', 'small mammals', 'insects', 'fruits'],'size': 'small'}", "{'diet': ['birds', 'omnivore', 'small mammals', 'insects', 'fruits'],'size': 'small'}"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "graph_retriever.edges", "content": ["Specification and implementation of edges functions.", "These are responsible for extracting edges from nodes and expressing them in waythat the adapters can implement."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "EdgeFunctionmodule-attribute", "content": ["module-attribute", "EdgeFunction:TypeAlias=Callable[[Content],Edges]", "EdgeFunction:TypeAlias=Callable[[Content],Edges]", "A function for extracting edges from nodes.", "Implementations should be deterministic."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "EdgeSpecmodule-attribute", "content": ["module-attribute", "EdgeSpec:TypeAlias=tuple[str|Id,str|Id]", "EdgeSpec:TypeAlias=tuple[str|Id,str|Id]", "The definition of an edge for traversal, represented as a pair of fieldsrepresenting the source and target of the edge. Each may be:", "A string,key, indicatingdoc.metadata[key]as the value.", "The magic string\"$id\", indicatingdoc.idas the value.", "key", "doc.metadata[key]", "\"$id\"", "doc.id", "Examples:", "url_to_href_edge          = (\"url\", \"href\")keywords_to_keywords_edge = (\"keywords\", \"keywords\")mentions_to_id_edge       = (\"mentions\", \"$id\")id_to_mentions_edge       = (\"$id\", \"mentions)", "url_to_href_edge          = (\"url\", \"href\")keywords_to_keywords_edge = (\"keywords\", \"keywords\")mentions_to_id_edge       = (\"mentions\", \"$id\")id_to_mentions_edge       = (\"$id\", \"mentions)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "Edge", "content": ["Bases:ABC", "ABC", "An edge identifies properties necessary for finding matching nodes.", "Sub-classes should be hashable."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "Edgesdataclass", "content": ["dataclass", "Edges(incoming:set[Edge],outgoing:set[Edge])", "Edges(incoming:set[Edge],outgoing:set[Edge])", "Information about the incoming and outgoing edges.", {"table": [["PARAMETER", "DESCRIPTION"], ["incoming", "Incoming edges that link to this node.TYPE:set[Edge]"], ["outgoing", "Edges that this node link to. These edges should be defined in terms oftheincomingEdgethey match. For instance, a link from \"mentions\"to \"id\" would link toIdEdge(...).TYPE:set[Edge]"]]}, "incoming", "Incoming edges that link to this node.", "TYPE:set[Edge]", "set[Edge]", "outgoing", "Edges that this node link to. These edges should be defined in terms oftheincomingEdgethey match. For instance, a link from \"mentions\"to \"id\" would link toIdEdge(...).", "Edge", "IdEdge(...)", "TYPE:set[Edge]", "set[Edge]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "Id", "content": ["Place-holder type indicating that the ID should be used.", "Deprecated: Use \"$id\" instead."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "IdEdgedataclass", "content": ["dataclass", "IdEdge(id:str)", "IdEdge(id:str)", "Bases:Edge", "Edge", "AnIdEdgeconnects to nodes withnode.id == id.", "IdEdge", "node.id == id", {"table": [["PARAMETER", "DESCRIPTION"], ["id", "The ID of the node to link to.TYPE:str"]]}, "id", "The ID of the node to link to.", "TYPE:str", "str"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "MetadataEdgedataclass", "content": ["dataclass", "MetadataEdge(incoming_field:str,value:Any)", "MetadataEdge(incoming_field:str,value:Any)", "Bases:Edge", "Edge", "Link to nodes with specific metadata.", "AMetadataEdgeconnects to nodes with either:", "MetadataEdge", "node.metadata[field] == value", "node.metadata[field] CONTAINS value(if the metadata is a collection).", "node.metadata[field] == value", "node.metadata[field] CONTAINS value", {"table": [["PARAMETER", "DESCRIPTION"], ["incoming_field", "The name of the metadata field storing incoming edges.TYPE:str"], ["value", "The value associated with the key for this edgeTYPE:Any"]]}, "incoming_field", "The name of the metadata field storing incoming edges.", "TYPE:str", "str", "value", "The value associated with the key for this edge", "TYPE:Any", "Any", "packages/graph-retriever/src/graph_retriever/edges/_base.py", {"table": [["39404142434445464748", "def__init__(self,incoming_field:str,value:Any)->None:# `self.field = value` and `setattr(self, \"field\", value)` -- don't work# because of frozen. we need to call `__setattr__` directly (as the# default `__init__` would do) to initialize the fields of the frozen# dataclass.object.__setattr__(self,\"incoming_field\",incoming_field)ifisinstance(value,dict):value=immutabledict(value)object.__setattr__(self,\"value\",value)"]]}, "39404142434445464748", "def__init__(self,incoming_field:str,value:Any)->None:# `self.field = value` and `setattr(self, \"field\", value)` -- don't work# because of frozen. we need to call `__setattr__` directly (as the# default `__init__` would do) to initialize the fields of the frozen# dataclass.object.__setattr__(self,\"incoming_field\",incoming_field)ifisinstance(value,dict):value=immutabledict(value)object.__setattr__(self,\"value\",value)", "def__init__(self,incoming_field:str,value:Any)->None:# `self.field = value` and `setattr(self, \"field\", value)` -- don't work# because of frozen. we need to call `__setattr__` directly (as the# default `__init__` would do) to initialize the fields of the frozen# dataclass.object.__setattr__(self,\"incoming_field\",incoming_field)ifisinstance(value,dict):value=immutabledict(value)object.__setattr__(self,\"value\",value)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "MetadataEdgeFunction", "content": ["MetadataEdgeFunction(edges:list[EdgeSpec])", "MetadataEdgeFunction(edges:list[EdgeSpec])", "Helper for extracting and encoding edges in metadata.", "This class provides tools to extract incoming and outgoing edges fromdocument metadata. Both incoming and outgoing edges use the same targetname, enabling equality matching for keys.", {"table": [["PARAMETER", "DESCRIPTION"], ["edges", "Definitions of edges for traversal, represented as a pair of fieldsrepresenting the source and target of the edges.TYPE:list[EdgeSpec]"]]}, "edges", "Definitions of edges for traversal, represented as a pair of fieldsrepresenting the source and target of the edges.", "TYPE:list[EdgeSpec]", "list[EdgeSpec]", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If an invalid edge definition is provided."]]}, "ValueError", "If an invalid edge definition is provided.", "packages/graph-retriever/src/graph_retriever/edges/metadata.py", {"table": [["79808182838485868788", "def__init__(self,edges:list[EdgeSpec],)->None:self.edges=edgesforsource,targetinedges:ifnotisinstance(source,str|Id):raiseValueError(f\"Expected 'str | Id' but got:{source}\")ifnotisinstance(target,str|Id):raiseValueError(f\"Expected 'str | Id' but got:{target}\")"]]}, "79808182838485868788", "def__init__(self,edges:list[EdgeSpec],)->None:self.edges=edgesforsource,targetinedges:ifnotisinstance(source,str|Id):raiseValueError(f\"Expected 'str | Id' but got:{source}\")ifnotisinstance(target,str|Id):raiseValueError(f\"Expected 'str | Id' but got:{target}\")", "def__init__(self,edges:list[EdgeSpec],)->None:self.edges=edgesforsource,targetinedges:ifnotisinstance(source,str|Id):raiseValueError(f\"Expected 'str | Id' but got:{source}\")ifnotisinstance(target,str|Id):raiseValueError(f\"Expected 'str | Id' but got:{target}\")"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/edges/", "title": "__call__", "content": ["__call__(content:Content)->Edges", "__call__(content:Content)->Edges", "Extract incoming and outgoing edges for a piece of content.", "This method retrieves edges based on the declared edge definitions, takinginto account whether nested metadata is used.", {"table": [["PARAMETER", "DESCRIPTION"], ["content", "The content to extract edges from.TYPE:Content"]]}, "content", "The content to extract edges from.", "TYPE:Content", "Content", {"table": [["RETURNS", "DESCRIPTION"], ["Edges", "the incoming and outgoing edges of the node"]]}, "Edges", "the incoming and outgoing edges of the node", "packages/graph-retriever/src/graph_retriever/edges/metadata.py", {"table": [["149150151152153154155156157158159160161162163164165166167168169170171", "def__call__(self,content:Content)->Edges:\"\"\"Extract incoming and outgoing edges for a piece of content.This method retrieves edges based on the declared edge definitions, takinginto account whether nested metadata is used.Parameters----------content :The content to extract edges from.Returns-------:the incoming and outgoing edges of the node\"\"\"outgoing_edges=self._edges_from_dict(content.id,content.metadata)incoming_edges=self._edges_from_dict(content.id,content.metadata,incoming=True)returnEdges(incoming=incoming_edges,outgoing=outgoing_edges)"]]}, "149150151152153154155156157158159160161162163164165166167168169170171", "def__call__(self,content:Content)->Edges:\"\"\"Extract incoming and outgoing edges for a piece of content.This method retrieves edges based on the declared edge definitions, takinginto account whether nested metadata is used.Parameters----------content :The content to extract edges from.Returns-------:the incoming and outgoing edges of the node\"\"\"outgoing_edges=self._edges_from_dict(content.id,content.metadata)incoming_edges=self._edges_from_dict(content.id,content.metadata,incoming=True)returnEdges(incoming=incoming_edges,outgoing=outgoing_edges)", "def__call__(self,content:Content)->Edges:\"\"\"Extract incoming and outgoing edges for a piece of content.This method retrieves edges based on the declared edge definitions, takinginto account whether nested metadata is used.Parameters----------content :The content to extract edges from.Returns-------:the incoming and outgoing edges of the node\"\"\"outgoing_edges=self._edges_from_dict(content.id,content.metadata)incoming_edges=self._edges_from_dict(content.id,content.metadata,incoming=True)returnEdges(incoming=incoming_edges,outgoing=outgoing_edges)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "graph_retriever.strategies", "content": ["Strategies determine which nodes are selected during traversal."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "Eagerdataclass", "content": ["dataclass", "Eager(*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Eager(*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Bases:Strategy", "Strategy", "Eager traversal strategy (breadth-first).", "This strategy selects all discovered nodes at each traversal step. It ensuresbreadth-first traversal by processing nodes layer by layer, which is useful forscenarios where all nodes at the current depth should be explored before proceedingto the next depth.", {"table": [["PARAMETER", "DESCRIPTION"], ["select_k", "Maximum number of nodes to retrieve during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"], ["start_k", "Number of documents to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.TYPE:intDEFAULT:4"], ["adjacent_k", "Number of documents to fetch for each outgoing edge.TYPE:intDEFAULT:10"], ["max_depth", "Maximum traversal depth. IfNone, there is no limit.TYPE:int| NoneDEFAULT:None"], ["k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"]]}, "select_k", "Maximum number of nodes to retrieve during traversal.", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K", "start_k", "Number of documents to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.", "TYPE:intDEFAULT:4", "int", "4", "adjacent_k", "Number of documents to fetch for each outgoing edge.", "TYPE:intDEFAULT:10", "int", "10", "max_depth", "Maximum traversal depth. IfNone, there is no limit.", "None", "TYPE:int| NoneDEFAULT:None", "int| None", "None", "k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.", "select_k", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "__post_init__", "content": ["__post_init__()", "__post_init__()", "Allow passing the deprecated 'k' value instead of 'select_k'.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["138139140141142143", "def__post_init__(self):\"\"\"Allow passing the deprecated 'k' value instead of 'select_k'.\"\"\"ifself.select_k==DEFAULT_SELECT_Kandself.k!=DEFAULT_SELECT_K:self.select_k=self.kelse:self.k=self.select_k"]]}, "138139140141142143", "def__post_init__(self):\"\"\"Allow passing the deprecated 'k' value instead of 'select_k'.\"\"\"ifself.select_k==DEFAULT_SELECT_Kandself.k!=DEFAULT_SELECT_K:self.select_k=self.kelse:self.k=self.select_k", "def__post_init__(self):\"\"\"Allow passing the deprecated 'k' value instead of 'select_k'.\"\"\"ifself.select_k==DEFAULT_SELECT_Kandself.k!=DEFAULT_SELECT_K:self.select_k=self.kelse:self.k=self.select_k"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "buildstaticmethod", "content": ["staticmethod", "build(base_strategy:Strategy,**kwargs:Any)->Strategy", "build(base_strategy:Strategy,**kwargs:Any)->Strategy", "Build a strategy for a retrieval operation.", "Combines a base strategy with any provided keyword arguments tocreate a customized traversal strategy.", {"table": [["PARAMETER", "DESCRIPTION"], ["base_strategy", "The base strategy to start with.TYPE:Strategy"], ["kwargs", "Additional configuration options for the strategy.TYPE:AnyDEFAULT:{}"]]}, "base_strategy", "The base strategy to start with.", "TYPE:Strategy", "Strategy", "kwargs", "Additional configuration options for the strategy.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["Strategy", "A configured strategy instance."]]}, "Strategy", "A configured strategy instance.", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If 'strategy' is set incorrectly or extra arguments are invalid."]]}, "ValueError", "If 'strategy' is set incorrectly or extra arguments are invalid.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246", "@staticmethoddefbuild(base_strategy:Strategy,**kwargs:Any,)->Strategy:\"\"\"Build a strategy for a retrieval operation.Combines a base strategy with any provided keyword arguments tocreate a customized traversal strategy.Parameters----------base_strategy :The base strategy to start with.kwargs :Additional configuration options for the strategy.Returns-------:A configured strategy instance.Raises------ValueErrorIf 'strategy' is set incorrectly or extra arguments are invalid.\"\"\"# Check if there is a new strategy to use. Otherwise, use the base.strategy:Strategyif\"strategy\"inkwargs:ifnext(iter(kwargs.keys()))!=\"strategy\":raiseValueError(\"Error: 'strategy' must be set before other args.\")strategy=kwargs.pop(\"strategy\")ifnotisinstance(strategy,Strategy):raiseValueError(f\"Unsupported 'strategy' type{type(strategy).__name__}.\"\" Must be a sub-class of Strategy\")elifbase_strategyisnotNone:strategy=base_strategyelse:raiseValueError(\"'strategy' must be set in `__init__` or invocation\")# Apply the kwargs to update the strategy.assertstrategyisnotNoneif\"k\"inkwargs:kwargs[\"select_k\"]=kwargs.pop(\"k\")strategy=dataclasses.replace(strategy,**kwargs)returnstrategy"]]}, "196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246", "@staticmethoddefbuild(base_strategy:Strategy,**kwargs:Any,)->Strategy:\"\"\"Build a strategy for a retrieval operation.Combines a base strategy with any provided keyword arguments tocreate a customized traversal strategy.Parameters----------base_strategy :The base strategy to start with.kwargs :Additional configuration options for the strategy.Returns-------:A configured strategy instance.Raises------ValueErrorIf 'strategy' is set incorrectly or extra arguments are invalid.\"\"\"# Check if there is a new strategy to use. Otherwise, use the base.strategy:Strategyif\"strategy\"inkwargs:ifnext(iter(kwargs.keys()))!=\"strategy\":raiseValueError(\"Error: 'strategy' must be set before other args.\")strategy=kwargs.pop(\"strategy\")ifnotisinstance(strategy,Strategy):raiseValueError(f\"Unsupported 'strategy' type{type(strategy).__name__}.\"\" Must be a sub-class of Strategy\")elifbase_strategyisnotNone:strategy=base_strategyelse:raiseValueError(\"'strategy' must be set in `__init__` or invocation\")# Apply the kwargs to update the strategy.assertstrategyisnotNoneif\"k\"inkwargs:kwargs[\"select_k\"]=kwargs.pop(\"k\")strategy=dataclasses.replace(strategy,**kwargs)returnstrategy", "@staticmethoddefbuild(base_strategy:Strategy,**kwargs:Any,)->Strategy:\"\"\"Build a strategy for a retrieval operation.Combines a base strategy with any provided keyword arguments tocreate a customized traversal strategy.Parameters----------base_strategy :The base strategy to start with.kwargs :Additional configuration options for the strategy.Returns-------:A configured strategy instance.Raises------ValueErrorIf 'strategy' is set incorrectly or extra arguments are invalid.\"\"\"# Check if there is a new strategy to use. Otherwise, use the base.strategy:Strategyif\"strategy\"inkwargs:ifnext(iter(kwargs.keys()))!=\"strategy\":raiseValueError(\"Error: 'strategy' must be set before other args.\")strategy=kwargs.pop(\"strategy\")ifnotisinstance(strategy,Strategy):raiseValueError(f\"Unsupported 'strategy' type{type(strategy).__name__}.\"\" Must be a sub-class of Strategy\")elifbase_strategyisnotNone:strategy=base_strategyelse:raiseValueError(\"'strategy' must be set in `__init__` or invocation\")# Apply the kwargs to update the strategy.assertstrategyisnotNoneif\"k\"inkwargs:kwargs[\"select_k\"]=kwargs.pop(\"k\")strategy=dataclasses.replace(strategy,**kwargs)returnstrategy"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "finalize_nodes", "content": ["finalize_nodes(selected:Iterable[Node])->Iterable[Node]", "finalize_nodes(selected:Iterable[Node])->Iterable[Node]", "Finalize the selected nodes.", "This method is called before returning the final set of nodes. It allowsthe strategy to perform any final processing or re-ranking of the selectednodes.", {"table": [["PARAMETER", "DESCRIPTION"], ["selected", "The selected nodes to be finalizedTYPE:Iterable[Node]"]]}, "selected", "The selected nodes to be finalized", "TYPE:Iterable[Node]", "Iterable[Node]", {"table": [["RETURNS", "DESCRIPTION"], ["Iterable[Node]", "Finalized nodes."]]}, "Iterable[Node]", "Finalized nodes.", "The default implementation returns the firstself.select_kselected nodeswithout any additional processing.", "self.select_k", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["171172173174175176177178179180181182183184185186187188189190191192193194", "deffinalize_nodes(self,selected:Iterable[Node])->Iterable[Node]:\"\"\"Finalize the selected nodes.This method is called before returning the final set of nodes. It allowsthe strategy to perform any final processing or re-ranking of the selectednodes.Parameters----------selected :The selected nodes to be finalizedReturns-------:Finalized nodes.Notes------ The default implementation returns the first `self.select_k` selected nodeswithout any additional processing.\"\"\"returnlist(selected)[:self.select_k]"]]}, "171172173174175176177178179180181182183184185186187188189190191192193194", "deffinalize_nodes(self,selected:Iterable[Node])->Iterable[Node]:\"\"\"Finalize the selected nodes.This method is called before returning the final set of nodes. It allowsthe strategy to perform any final processing or re-ranking of the selectednodes.Parameters----------selected :The selected nodes to be finalizedReturns-------:Finalized nodes.Notes------ The default implementation returns the first `self.select_k` selected nodeswithout any additional processing.\"\"\"returnlist(selected)[:self.select_k]", "deffinalize_nodes(self,selected:Iterable[Node])->Iterable[Node]:\"\"\"Finalize the selected nodes.This method is called before returning the final set of nodes. It allowsthe strategy to perform any final processing or re-ranking of the selectednodes.Parameters----------selected :The selected nodes to be finalizedReturns-------:Finalized nodes.Notes------ The default implementation returns the first `self.select_k` selected nodeswithout any additional processing.\"\"\"returnlist(selected)[:self.select_k]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "iteration", "content": ["iteration(nodes:Iterable[Node],tracker:NodeTracker)->None", "iteration(nodes:Iterable[Node],tracker:NodeTracker)->None", "Process the newly discovered nodes on each iteration.", "This method should calltracker.traverse()and/ortracker.select()as appropriate to update the nodes that need to be traversed in this iterationor selected at the end of the retrieval, respectively.", "tracker.traverse()", "tracker.select()", {"table": [["PARAMETER", "DESCRIPTION"], ["nodes", "The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iterationTYPE:Iterable[Node]"], ["tracker", "The tracker object to manage the traversal and selection of nodes.TYPE:NodeTracker"]]}, "nodes", "The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iteration", "TYPE:Iterable[Node]", "Iterable[Node]", "tracker", "The tracker object to manage the traversal and selection of nodes.", "TYPE:NodeTracker", "NodeTracker", "This method is called once for each iteration of the traversal.", "In order to stop iterating either choose to not traverse any additional nodesor don't select any additional nodes for output.", "packages/graph-retriever/src/graph_retriever/strategies/scored.py", {"table": [["5556575859606162636465666768", "@overridedefiteration(self,nodes:Iterable[Node],tracker:NodeTracker)->None:fornodeinnodes:heapq.heappush(self._nodes,_ScoredNode(self.scorer(node),node))limit=tracker.num_remainingifself.per_iteration_limit:limit=min(limit,self.per_iteration_limit)whilelimit>0andself._nodes:highest=heapq.heappop(self._nodes)node=highest.nodenode.extra_metadata[\"_score\"]=highest.scorelimit-=tracker.select_and_traverse([node])"]]}, "5556575859606162636465666768", "@overridedefiteration(self,nodes:Iterable[Node],tracker:NodeTracker)->None:fornodeinnodes:heapq.heappush(self._nodes,_ScoredNode(self.scorer(node),node))limit=tracker.num_remainingifself.per_iteration_limit:limit=min(limit,self.per_iteration_limit)whilelimit>0andself._nodes:highest=heapq.heappop(self._nodes)node=highest.nodenode.extra_metadata[\"_score\"]=highest.scorelimit-=tracker.select_and_traverse([node])", "@overridedefiteration(self,nodes:Iterable[Node],tracker:NodeTracker)->None:fornodeinnodes:heapq.heappush(self._nodes,_ScoredNode(self.scorer(node),node))limit=tracker.num_remainingifself.per_iteration_limit:limit=min(limit,self.per_iteration_limit)whilelimit>0andself._nodes:highest=heapq.heappop(self._nodes)node=highest.nodenode.extra_metadata[\"_score\"]=highest.scorelimit-=tracker.select_and_traverse([node])"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "Mmrdataclass", "content": ["dataclass", "Mmr(lambda_mult:float=0.5,min_mmr_score:float=NEG_INF,_selected_ids:list[str]=list(),_candidate_id_to_index:dict[str,int]=dict(),_candidates:list[_MmrCandidate]=list(),_best_score:float=NEG_INF,_best_id:str|None=None,*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Mmr(lambda_mult:float=0.5,min_mmr_score:float=NEG_INF,_selected_ids:list[str]=list(),_candidate_id_to_index:dict[str,int]=dict(),_candidates:list[_MmrCandidate]=list(),_best_score:float=NEG_INF,_best_id:str|None=None,*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Bases:Strategy", "Strategy", "Maximal Marginal Relevance (MMR) traversal strategy.", "This strategy selects nodes by balancing relevance to the query and diversityamong the results. It uses alambda_multparameter to control the trade-offbetween relevance and redundancy. Nodes are scored based on their similarityto the query and their distance from already selected nodes.", "lambda_mult", {"table": [["PARAMETER", "DESCRIPTION"], ["select_k", "Maximum number of nodes to retrieve during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"], ["start_k", "Number of documents to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.TYPE:intDEFAULT:4"], ["adjacent_k", "Number of documents to fetch for each outgoing edge.TYPE:intDEFAULT:10"], ["max_depth", "Maximum traversal depth. IfNone, there is no limit.TYPE:int| NoneDEFAULT:None"], ["lambda_mult", "Controls the trade-off between relevance and diversity. A value closerto 1 prioritizes relevance, while a value closer to 0 prioritizesdiversity. Must be between 0 and 1 (inclusive).TYPE:floatDEFAULT:0.5"], ["min_mmr_score", "Only nodes with a score greater than or equal to this value will beselected.TYPE:floatDEFAULT:NEG_INF"], ["k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"]]}, "select_k", "Maximum number of nodes to retrieve during traversal.", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K", "start_k", "Number of documents to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.", "TYPE:intDEFAULT:4", "int", "4", "adjacent_k", "Number of documents to fetch for each outgoing edge.", "TYPE:intDEFAULT:10", "int", "10", "max_depth", "Maximum traversal depth. IfNone, there is no limit.", "None", "TYPE:int| NoneDEFAULT:None", "int| None", "None", "lambda_mult", "Controls the trade-off between relevance and diversity. A value closerto 1 prioritizes relevance, while a value closer to 0 prioritizesdiversity. Must be between 0 and 1 (inclusive).", "TYPE:floatDEFAULT:0.5", "float", "0.5", "min_mmr_score", "Only nodes with a score greater than or equal to this value will beselected.", "TYPE:floatDEFAULT:NEG_INF", "float", "NEG_INF", "k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.", "select_k", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "candidate_ids", "content": ["candidate_ids()->Iterable[str]", "candidate_ids()->Iterable[str]", "Return the IDs of the candidates.", {"table": [["RETURNS", "DESCRIPTION"], ["Iterable[str]", "The IDs of the candidates."]]}, "Iterable[str]", "The IDs of the candidates.", "packages/graph-retriever/src/graph_retriever/strategies/mmr.py", {"table": [["131132133134135136137138139140", "defcandidate_ids(self)->Iterable[str]:\"\"\"Return the IDs of the candidates.Returns-------Iterable[str]The IDs of the candidates.\"\"\"returnself._candidate_id_to_index.keys()"]]}, "131132133134135136137138139140", "defcandidate_ids(self)->Iterable[str]:\"\"\"Return the IDs of the candidates.Returns-------Iterable[str]The IDs of the candidates.\"\"\"returnself._candidate_id_to_index.keys()", "defcandidate_ids(self)->Iterable[str]:\"\"\"Return the IDs of the candidates.Returns-------Iterable[str]The IDs of the candidates.\"\"\"returnself._candidate_id_to_index.keys()"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "NodeTracker", "content": ["NodeTracker(select_k:int,max_depth:int|None)", "NodeTracker(select_k:int,max_depth:int|None)", "Helper class initiating node selection and traversal.", "Call .select(nodes) to add nodes to the result set.Call .traverse(nodes) to add nodes to the next traversal.Call .select_and_traverse(nodes) to add nodes to the result set and the next    traversal.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["25262728293031", "def__init__(self,select_k:int,max_depth:int|None)->None:self._select_k:int=select_kself._max_depth:int|None=max_depthself._visited_node_ids:set[str]=set()# use a dict to preserve orderself.to_traverse:dict[str,Node]=dict()self.selected:list[Node]=[]"]]}, "25262728293031", "def__init__(self,select_k:int,max_depth:int|None)->None:self._select_k:int=select_kself._max_depth:int|None=max_depthself._visited_node_ids:set[str]=set()# use a dict to preserve orderself.to_traverse:dict[str,Node]=dict()self.selected:list[Node]=[]", "def__init__(self,select_k:int,max_depth:int|None)->None:self._select_k:int=select_kself._max_depth:int|None=max_depthself._visited_node_ids:set[str]=set()# use a dict to preserve orderself.to_traverse:dict[str,Node]=dict()self.selected:list[Node]=[]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "num_remainingproperty", "content": ["property", "num_remaining", "num_remaining", "The remaining number of nodes to be selected."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "select", "content": ["select(nodes:Iterable[Node])->None", "select(nodes:Iterable[Node])->None", "Select nodes to be included in the result set.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["383940414243", "defselect(self,nodes:Iterable[Node])->None:\"\"\"Select nodes to be included in the result set.\"\"\"fornodeinnodes:node.extra_metadata[\"_depth\"]=node.depthnode.extra_metadata[\"_similarity_score\"]=node.similarity_scoreself.selected.extend(nodes)"]]}, "383940414243", "defselect(self,nodes:Iterable[Node])->None:\"\"\"Select nodes to be included in the result set.\"\"\"fornodeinnodes:node.extra_metadata[\"_depth\"]=node.depthnode.extra_metadata[\"_similarity_score\"]=node.similarity_scoreself.selected.extend(nodes)", "defselect(self,nodes:Iterable[Node])->None:\"\"\"Select nodes to be included in the result set.\"\"\"fornodeinnodes:node.extra_metadata[\"_depth\"]=node.depthnode.extra_metadata[\"_similarity_score\"]=node.similarity_scoreself.selected.extend(nodes)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "select_and_traverse", "content": ["select_and_traverse(nodes:Iterable[Node])->int", "select_and_traverse(nodes:Iterable[Node])->int", "Select nodes to be included in the result set and the next traversal.", {"table": [["RETURNS", "DESCRIPTION"], ["Number of nodes added for traversal.", ""]]}, "Number of nodes added for traversal.", "Nodes are only added for traversal if they have not been visited before.", "Nodes are only added for traversal if they do not exceed the maximum depth.", "If no new nodes are chosen for traversal, or selected for output, then    the traversal will stop.", "Traversal will also stop if the number of selected nodes reaches the select_k    limit.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["72737475767778798081828384858687888990", "defselect_and_traverse(self,nodes:Iterable[Node])->int:\"\"\"Select nodes to be included in the result set and the next traversal.Returns-------Number of nodes added for traversal.Notes------ Nodes are only added for traversal if they have not been visited before.- Nodes are only added for traversal if they do not exceed the maximum depth.- If no new nodes are chosen for traversal, or selected for output, thenthe traversal will stop.- Traversal will also stop if the number of selected nodes reaches the select_klimit.\"\"\"self.select(nodes)returnself.traverse(nodes)"]]}, "72737475767778798081828384858687888990", "defselect_and_traverse(self,nodes:Iterable[Node])->int:\"\"\"Select nodes to be included in the result set and the next traversal.Returns-------Number of nodes added for traversal.Notes------ Nodes are only added for traversal if they have not been visited before.- Nodes are only added for traversal if they do not exceed the maximum depth.- If no new nodes are chosen for traversal, or selected for output, thenthe traversal will stop.- Traversal will also stop if the number of selected nodes reaches the select_klimit.\"\"\"self.select(nodes)returnself.traverse(nodes)", "defselect_and_traverse(self,nodes:Iterable[Node])->int:\"\"\"Select nodes to be included in the result set and the next traversal.Returns-------Number of nodes added for traversal.Notes------ Nodes are only added for traversal if they have not been visited before.- Nodes are only added for traversal if they do not exceed the maximum depth.- If no new nodes are chosen for traversal, or selected for output, thenthe traversal will stop.- Traversal will also stop if the number of selected nodes reaches the select_klimit.\"\"\"self.select(nodes)returnself.traverse(nodes)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "traverse", "content": ["traverse(nodes:Iterable[Node])->int", "traverse(nodes:Iterable[Node])->int", "Select nodes to be included in the next traversal.", {"table": [["RETURNS", "DESCRIPTION"], ["Number of nodes added for traversal.", ""]]}, "Number of nodes added for traversal.", "Nodes are only added if they have not been visited before.", "Nodes are only added if they do not exceed the maximum depth.", "If no new nodes are chosen for traversal, or selected for output, then    the traversal will stop.", "Traversal will also stop if the number of selected nodes reaches the select_k    limit.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["4546474849505152535455565758596061626364656667686970", "deftraverse(self,nodes:Iterable[Node])->int:\"\"\"Select nodes to be included in the next traversal.Returns-------Number of nodes added for traversal.Notes------ Nodes are only added if they have not been visited before.- Nodes are only added if they do not exceed the maximum depth.- If no new nodes are chosen for traversal, or selected for output, thenthe traversal will stop.- Traversal will also stop if the number of selected nodes reaches the select_klimit.\"\"\"new_nodes={n.id:nforninnodesifself._not_visited(n.id)ifself._max_depthisNoneorn.depth<self._max_depth}self.to_traverse.update(new_nodes)self._visited_node_ids.update(new_nodes.keys())returnlen(new_nodes)"]]}, "4546474849505152535455565758596061626364656667686970", "deftraverse(self,nodes:Iterable[Node])->int:\"\"\"Select nodes to be included in the next traversal.Returns-------Number of nodes added for traversal.Notes------ Nodes are only added if they have not been visited before.- Nodes are only added if they do not exceed the maximum depth.- If no new nodes are chosen for traversal, or selected for output, thenthe traversal will stop.- Traversal will also stop if the number of selected nodes reaches the select_klimit.\"\"\"new_nodes={n.id:nforninnodesifself._not_visited(n.id)ifself._max_depthisNoneorn.depth<self._max_depth}self.to_traverse.update(new_nodes)self._visited_node_ids.update(new_nodes.keys())returnlen(new_nodes)", "deftraverse(self,nodes:Iterable[Node])->int:\"\"\"Select nodes to be included in the next traversal.Returns-------Number of nodes added for traversal.Notes------ Nodes are only added if they have not been visited before.- Nodes are only added if they do not exceed the maximum depth.- If no new nodes are chosen for traversal, or selected for output, thenthe traversal will stop.- Traversal will also stop if the number of selected nodes reaches the select_klimit.\"\"\"new_nodes={n.id:nforninnodesifself._not_visited(n.id)ifself._max_depthisNoneorn.depth<self._max_depth}self.to_traverse.update(new_nodes)self._visited_node_ids.update(new_nodes.keys())returnlen(new_nodes)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "Scoreddataclass", "content": ["dataclass", "Scored(scorer:Callable[[Node],float],_nodes:list[_ScoredNode]=list(),per_iteration_limit:int|None=None,*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Scored(scorer:Callable[[Node],float],_nodes:list[_ScoredNode]=list(),per_iteration_limit:int|None=None,*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Bases:Strategy", "Strategy", "Scored traversal strategy.", "This strategy uses a scoring function to select nodes using a local maximumapproach. In each iteration, it chooses the top scoring nodes available andthen traverses the connected nodes.", {"table": [["PARAMETER", "DESCRIPTION"], ["scorer", "A callable function that returns the score of a node.TYPE:Callable[[Node],float]"], ["select_k", "Maximum number of nodes to retrieve during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"], ["start_k", "Number of documents to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.TYPE:intDEFAULT:4"], ["adjacent_k", "Number of documents to fetch for each outgoing edge.TYPE:intDEFAULT:10"], ["max_depth", "Maximum traversal depth. IfNone, there is no limit.TYPE:int| NoneDEFAULT:None"], ["per_iteration_limit", "Maximum number of nodes to select and traverse during a singleiteration.TYPE:int| NoneDEFAULT:None"], ["k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"]]}, "scorer", "A callable function that returns the score of a node.", "TYPE:Callable[[Node],float]", "Callable[[Node],float]", "select_k", "Maximum number of nodes to retrieve during traversal.", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K", "start_k", "Number of documents to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.", "TYPE:intDEFAULT:4", "int", "4", "adjacent_k", "Number of documents to fetch for each outgoing edge.", "TYPE:intDEFAULT:10", "int", "10", "max_depth", "Maximum traversal depth. IfNone, there is no limit.", "None", "TYPE:int| NoneDEFAULT:None", "int| None", "None", "per_iteration_limit", "Maximum number of nodes to select and traverse during a singleiteration.", "TYPE:int| NoneDEFAULT:None", "int| None", "None", "k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.", "select_k", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "Strategydataclass", "content": ["dataclass", "Strategy(*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Strategy(*,select_k:int=DEFAULT_SELECT_K,start_k:int=4,adjacent_k:int=10,max_traverse:int|None=None,max_depth:int|None=None,k:int=DEFAULT_SELECT_K,_query_embedding:list[float]=list(),)", "Bases:ABC", "ABC", "Interface for configuring node selection and traversal strategies.", "This base class defines how nodes are selected, traversed, and finalized duringa graph traversal. Implementations can customize behaviors like limiting the depthof traversal, scoring nodes, or selecting the next set of nodes for exploration.", {"table": [["PARAMETER", "DESCRIPTION"], ["select_k", "Maximum number of nodes to select and return during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"], ["start_k", "Number of nodes to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.TYPE:intDEFAULT:4"], ["adjacent_k", "Number of nodes to fetch for each outgoing edge.TYPE:intDEFAULT:10"], ["max_traverse", "Maximum number of nodes to traverse outgoing edges from before returning.IfNone, there is no limit.TYPE:int| NoneDEFAULT:None"], ["max_depth", "Maximum traversal depth. IfNone, there is no limit.TYPE:int| NoneDEFAULT:None"], ["k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.TYPE:intDEFAULT:DEFAULT_SELECT_K"]]}, "select_k", "Maximum number of nodes to select and return during traversal.", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K", "start_k", "Number of nodes to fetch via similarity for starting the traversal.Added to any initial roots provided to the traversal.", "TYPE:intDEFAULT:4", "int", "4", "adjacent_k", "Number of nodes to fetch for each outgoing edge.", "TYPE:intDEFAULT:10", "int", "10", "max_traverse", "Maximum number of nodes to traverse outgoing edges from before returning.IfNone, there is no limit.", "None", "TYPE:int| NoneDEFAULT:None", "int| None", "None", "max_depth", "Maximum traversal depth. IfNone, there is no limit.", "None", "TYPE:int| NoneDEFAULT:None", "int| None", "None", "k", "Deprecated: Useselect_kinstead.Maximum number of nodes to select and return during traversal.", "select_k", "TYPE:intDEFAULT:DEFAULT_SELECT_K", "int", "DEFAULT_SELECT_K"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/", "title": "iterationabstractmethod", "content": ["abstractmethod", "iteration(*,nodes:Iterable[Node],tracker:NodeTracker)->None", "iteration(*,nodes:Iterable[Node],tracker:NodeTracker)->None", "Process the newly discovered nodes on each iteration.", "This method should calltracker.traverse()and/ortracker.select()as appropriate to update the nodes that need to be traversed in this iterationor selected at the end of the retrieval, respectively.", "tracker.traverse()", "tracker.select()", {"table": [["PARAMETER", "DESCRIPTION"], ["nodes", "The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iterationTYPE:Iterable[Node]"], ["tracker", "The tracker object to manage the traversal and selection of nodes.TYPE:NodeTracker"]]}, "nodes", "The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iteration", "TYPE:Iterable[Node]", "Iterable[Node]", "tracker", "The tracker object to manage the traversal and selection of nodes.", "TYPE:NodeTracker", "NodeTracker", "This method is called once for each iteration of the traversal.", "In order to stop iterating either choose to not traverse any additional nodesor don't select any additional nodes for output.", "packages/graph-retriever/src/graph_retriever/strategies/base.py", {"table": [["145146147148149150151152153154155156157158159160161162163164165166167168169", "@abc.abstractmethoddefiteration(self,*,nodes:Iterable[Node],tracker:NodeTracker)->None:\"\"\"Process the newly discovered nodes on each iteration.This method should call `tracker.traverse()` and/or `tracker.select()`as appropriate to update the nodes that need to be traversed in this iterationor selected at the end of the retrieval, respectively.Parameters----------nodes :The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iterationtracker :The tracker object to manage the traversal and selection of nodes.Notes------ This method is called once for each iteration of the traversal.- In order to stop iterating either choose to not traverse any additional nodesor don't select any additional nodes for output.\"\"\"..."]]}, "145146147148149150151152153154155156157158159160161162163164165166167168169", "@abc.abstractmethoddefiteration(self,*,nodes:Iterable[Node],tracker:NodeTracker)->None:\"\"\"Process the newly discovered nodes on each iteration.This method should call `tracker.traverse()` and/or `tracker.select()`as appropriate to update the nodes that need to be traversed in this iterationor selected at the end of the retrieval, respectively.Parameters----------nodes :The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iterationtracker :The tracker object to manage the traversal and selection of nodes.Notes------ This method is called once for each iteration of the traversal.- In order to stop iterating either choose to not traverse any additional nodesor don't select any additional nodes for output.\"\"\"...", "@abc.abstractmethoddefiteration(self,*,nodes:Iterable[Node],tracker:NodeTracker)->None:\"\"\"Process the newly discovered nodes on each iteration.This method should call `tracker.traverse()` and/or `tracker.select()`as appropriate to update the nodes that need to be traversed in this iterationor selected at the end of the retrieval, respectively.Parameters----------nodes :The newly discovered nodes found from either:- the initial vector store retrieval- incoming edges from nodes chosen for traversal in the previous iterationtracker :The tracker object to manage the traversal and selection of nodes.Notes------ This method is called once for each iteration of the traversal.- In order to stop iterating either choose to not traverse any additional nodesor don't select any additional nodes for output.\"\"\"..."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/", "title": "graph_retriever", "content": ["Provides retrieval functions combining vector and graph traversal.", "The main methods aretraverseandatraversewhich provide synchronous andasynchronous traversals.", "traverse", "atraverse"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/", "title": "Contentdataclass", "content": ["dataclass", "Content(id:str,content:str,embedding:list[float],metadata:dict[str,Any]=dict(),mime_type:str=\"text/plain\",)", "Content(id:str,content:str,embedding:list[float],metadata:dict[str,Any]=dict(),mime_type:str=\"text/plain\",)", "Model representing retrieved content.", {"table": [["PARAMETER", "DESCRIPTION"], ["id", "The ID of the content.TYPE:str"], ["content", "The content.TYPE:str"], ["embedding", "The embedding of the content.TYPE:list[float]"], ["metadata", "The metadata associated with the content.TYPE:dict[str,Any]DEFAULT:dict()"], ["mime_type", "The MIME type of the content.TYPE:strDEFAULT:'text/plain'"]]}, "id", "The ID of the content.", "TYPE:str", "str", "content", "The content.", "TYPE:str", "str", "embedding", "The embedding of the content.", "TYPE:list[float]", "list[float]", "metadata", "The metadata associated with the content.", "TYPE:dict[str,Any]DEFAULT:dict()", "dict[str,Any]", "dict()", "mime_type", "The MIME type of the content.", "TYPE:strDEFAULT:'text/plain'", "str", "'text/plain'"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/", "title": "newstaticmethod", "content": ["staticmethod", "new(id:str,content:str,embedding:list[float]|Callable[[str],list[float]],*,metadata:dict[str,Any]|None=None,mime_type:str=\"text/plain\",)->Content", "new(id:str,content:str,embedding:list[float]|Callable[[str],list[float]],*,metadata:dict[str,Any]|None=None,mime_type:str=\"text/plain\",)->Content", "Create a new content.", {"table": [["PARAMETER", "DESCRIPTION"], ["id", "The ID of the content.TYPE:str"], ["content", "The content.TYPE:str"], ["embedding", "The embedding, or a function to apply to the content to compute theembedding.TYPE:list[float] |Callable[[str],list[float]]"], ["metadata", "The metadata associated with the content.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["mime_type", "The MIME type of the content.TYPE:strDEFAULT:'text/plain'"]]}, "id", "The ID of the content.", "TYPE:str", "str", "content", "The content.", "TYPE:str", "str", "embedding", "The embedding, or a function to apply to the content to compute theembedding.", "TYPE:list[float] |Callable[[str],list[float]]", "list[float] |Callable[[str],list[float]]", "metadata", "The metadata associated with the content.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "mime_type", "The MIME type of the content.", "TYPE:strDEFAULT:'text/plain'", "str", "'text/plain'", {"table": [["RETURNS", "DESCRIPTION"], ["Content", "The created content."]]}, "Content", "The created content.", "packages/graph-retriever/src/graph_retriever/content.py", {"table": [["3334353637383940414243444546474849505152535455565758596061626364656667686970", "@staticmethoddefnew(id:str,content:str,embedding:list[float]|Callable[[str],list[float]],*,metadata:dict[str,Any]|None=None,mime_type:str=\"text/plain\",)->Content:\"\"\"Create a new content.Parameters----------id :The ID of the content.content :The content.embedding :The embedding, or a function to apply to the content to compute theembedding.metadata :The metadata associated with the content.mime_type :The MIME type of the content.Returns-------:The created content.\"\"\"returnContent(id=id,content=content,embedding=embedding(content)ifcallable(embedding)elseembedding,metadata=metadataor{},mime_type=mime_type,)"]]}, "3334353637383940414243444546474849505152535455565758596061626364656667686970", "@staticmethoddefnew(id:str,content:str,embedding:list[float]|Callable[[str],list[float]],*,metadata:dict[str,Any]|None=None,mime_type:str=\"text/plain\",)->Content:\"\"\"Create a new content.Parameters----------id :The ID of the content.content :The content.embedding :The embedding, or a function to apply to the content to compute theembedding.metadata :The metadata associated with the content.mime_type :The MIME type of the content.Returns-------:The created content.\"\"\"returnContent(id=id,content=content,embedding=embedding(content)ifcallable(embedding)elseembedding,metadata=metadataor{},mime_type=mime_type,)", "@staticmethoddefnew(id:str,content:str,embedding:list[float]|Callable[[str],list[float]],*,metadata:dict[str,Any]|None=None,mime_type:str=\"text/plain\",)->Content:\"\"\"Create a new content.Parameters----------id :The ID of the content.content :The content.embedding :The embedding, or a function to apply to the content to compute theembedding.metadata :The metadata associated with the content.mime_type :The MIME type of the content.Returns-------:The created content.\"\"\"returnContent(id=id,content=content,embedding=embedding(content)ifcallable(embedding)elseembedding,metadata=metadataor{},mime_type=mime_type,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/", "title": "Nodedataclass", "content": ["dataclass", "Node(id:str,content:str,depth:int,similarity_score:float,embedding:list[float],metadata:dict[str,Any]=dict(),incoming_edges:set[Edge]=set(),outgoing_edges:set[Edge]=set(),extra_metadata:dict[str,Any]=dict(),)", "Node(id:str,content:str,depth:int,similarity_score:float,embedding:list[float],metadata:dict[str,Any]=dict(),incoming_edges:set[Edge]=set(),outgoing_edges:set[Edge]=set(),extra_metadata:dict[str,Any]=dict(),)", "Represents a node in the traversal graph.", "TheNodeclass contains information about a documentduring graph traversal, including its depth, embedding, edges, and metadata.", {"table": [["PARAMETER", "DESCRIPTION"], ["id", "The unique identifier of the document represented by this node.TYPE:str"], ["content", "The content.TYPE:str"], ["depth", "The depth (number of edges) through which this node was discovered. Thisdepth may not reflect the true depth in the full graph if only a subsetof edges is retrieved.TYPE:int"], ["embedding", "The embedding vector of the document, used for similarity calculations.TYPE:list[float]"], ["metadata", "Metadata from the original document. This is a reference to the originaldocument metadata and should not be modified directly. Any updates tometadata should be made toextra_metadata.TYPE:dict[str,Any]DEFAULT:dict()"], ["extra_metadata", "Additional metadata to override or augment the original documentmetadata during traversal.TYPE:dict[str,Any]DEFAULT:dict()"]]}, "id", "The unique identifier of the document represented by this node.", "TYPE:str", "str", "content", "The content.", "TYPE:str", "str", "depth", "The depth (number of edges) through which this node was discovered. Thisdepth may not reflect the true depth in the full graph if only a subsetof edges is retrieved.", "TYPE:int", "int", "embedding", "The embedding vector of the document, used for similarity calculations.", "TYPE:list[float]", "list[float]", "metadata", "Metadata from the original document. This is a reference to the originaldocument metadata and should not be modified directly. Any updates tometadata should be made toextra_metadata.", "extra_metadata", "TYPE:dict[str,Any]DEFAULT:dict()", "dict[str,Any]", "dict()", "extra_metadata", "Additional metadata to override or augment the original documentmetadata during traversal.", "TYPE:dict[str,Any]DEFAULT:dict()", "dict[str,Any]", "dict()"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/", "title": "atraverseasync", "content": ["async", "atraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]", "atraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]", "Asynchronously perform a graph traversal to retrieve nodes for a specific query.", {"table": [["PARAMETER", "DESCRIPTION"], ["query", "The query string for the traversal.TYPE:str"], ["edges", "A list ofEdgeSpecfor use in creating aMetadataEdgeFunction,or anEdgeFunction.TYPE:list[EdgeSpec] |EdgeFunction"], ["strategy", "The traversal strategy that defines how nodes are discovered, selected,and finalized.TYPE:Strategy"], ["store", "The vector store adapter used for similarity searches and documentretrieval.TYPE:Adapter"], ["metadata_filter", "Optional filter for metadata during traversal.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["initial_root_ids", "IDs of the initial root nodes for the traversal.TYPE:Sequence[str]DEFAULT:()"], ["store_kwargs", "Additional arguments passed to the store adapter.TYPE:dict[str,Any]DEFAULT:{}"]]}, "query", "The query string for the traversal.", "TYPE:str", "str", "edges", "A list ofEdgeSpecfor use in creating aMetadataEdgeFunction,or anEdgeFunction.", "TYPE:list[EdgeSpec] |EdgeFunction", "list[EdgeSpec] |EdgeFunction", "strategy", "The traversal strategy that defines how nodes are discovered, selected,and finalized.", "TYPE:Strategy", "Strategy", "store", "The vector store adapter used for similarity searches and documentretrieval.", "TYPE:Adapter", "Adapter", "metadata_filter", "Optional filter for metadata during traversal.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "initial_root_ids", "IDs of the initial root nodes for the traversal.", "TYPE:Sequence[str]DEFAULT:()", "Sequence[str]", "()", "store_kwargs", "Additional arguments passed to the store adapter.", "TYPE:dict[str,Any]DEFAULT:{}", "dict[str,Any]", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Node]", "Nodes returned by the traversal."]]}, "list[Node]", "Nodes returned by the traversal.", "packages/graph-retriever/src/graph_retriever/traversal.py", {"table": [["66676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114", "asyncdefatraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]:\"\"\"Asynchronously perform a graph traversal to retrieve nodes for a specific query.Parameters----------query :The query string for the traversal.edges :A list of [EdgeSpec][graph_retriever.edges.EdgeSpec] for use in creating a[MetadataEdgeFunction][graph_retriever.edges.MetadataEdgeFunction],or an [EdgeFunction][graph_retriever.edges.EdgeFunction].strategy :The traversal strategy that defines how nodes are discovered, selected,and finalized.store :The vector store adapter used for similarity searches and documentretrieval.metadata_filter :Optional filter for metadata during traversal.initial_root_ids :IDs of the initial root nodes for the traversal.store_kwargs :Additional arguments passed to the store adapter.Returns-------:Nodes returned by the traversal.\"\"\"traversal=_Traversal(query=query,edges=edges,strategy=copy.deepcopy(strategy),store=store,metadata_filter=metadata_filter,initial_root_ids=initial_root_ids,store_kwargs=store_kwargs,)returnawaittraversal.atraverse()"]]}, "66676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114", "asyncdefatraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]:\"\"\"Asynchronously perform a graph traversal to retrieve nodes for a specific query.Parameters----------query :The query string for the traversal.edges :A list of [EdgeSpec][graph_retriever.edges.EdgeSpec] for use in creating a[MetadataEdgeFunction][graph_retriever.edges.MetadataEdgeFunction],or an [EdgeFunction][graph_retriever.edges.EdgeFunction].strategy :The traversal strategy that defines how nodes are discovered, selected,and finalized.store :The vector store adapter used for similarity searches and documentretrieval.metadata_filter :Optional filter for metadata during traversal.initial_root_ids :IDs of the initial root nodes for the traversal.store_kwargs :Additional arguments passed to the store adapter.Returns-------:Nodes returned by the traversal.\"\"\"traversal=_Traversal(query=query,edges=edges,strategy=copy.deepcopy(strategy),store=store,metadata_filter=metadata_filter,initial_root_ids=initial_root_ids,store_kwargs=store_kwargs,)returnawaittraversal.atraverse()", "asyncdefatraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]:\"\"\"Asynchronously perform a graph traversal to retrieve nodes for a specific query.Parameters----------query :The query string for the traversal.edges :A list of [EdgeSpec][graph_retriever.edges.EdgeSpec] for use in creating a[MetadataEdgeFunction][graph_retriever.edges.MetadataEdgeFunction],or an [EdgeFunction][graph_retriever.edges.EdgeFunction].strategy :The traversal strategy that defines how nodes are discovered, selected,and finalized.store :The vector store adapter used for similarity searches and documentretrieval.metadata_filter :Optional filter for metadata during traversal.initial_root_ids :IDs of the initial root nodes for the traversal.store_kwargs :Additional arguments passed to the store adapter.Returns-------:Nodes returned by the traversal.\"\"\"traversal=_Traversal(query=query,edges=edges,strategy=copy.deepcopy(strategy),store=store,metadata_filter=metadata_filter,initial_root_ids=initial_root_ids,store_kwargs=store_kwargs,)returnawaittraversal.atraverse()"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/", "title": "traverse", "content": ["traverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]", "traverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]", "Perform a graph traversal to retrieve nodes for a specific query.", {"table": [["PARAMETER", "DESCRIPTION"], ["query", "The query string for the traversal.TYPE:str"], ["edges", "A list ofEdgeSpecfor use in creating aMetadataEdgeFunction,or anEdgeFunction.TYPE:list[EdgeSpec] |EdgeFunction"], ["strategy", "The traversal strategy that defines how nodes are discovered, selected,and finalized.TYPE:Strategy"], ["store", "The vector store adapter used for similarity searches and documentretrieval.TYPE:Adapter"], ["metadata_filter", "Optional filter for metadata during traversal.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["initial_root_ids", "IDs of the initial root nodes for the traversal.TYPE:Sequence[str]DEFAULT:()"], ["store_kwargs", "Additional arguments passed to the store adapter.TYPE:dict[str,Any]DEFAULT:{}"]]}, "query", "The query string for the traversal.", "TYPE:str", "str", "edges", "A list ofEdgeSpecfor use in creating aMetadataEdgeFunction,or anEdgeFunction.", "TYPE:list[EdgeSpec] |EdgeFunction", "list[EdgeSpec] |EdgeFunction", "strategy", "The traversal strategy that defines how nodes are discovered, selected,and finalized.", "TYPE:Strategy", "Strategy", "store", "The vector store adapter used for similarity searches and documentretrieval.", "TYPE:Adapter", "Adapter", "metadata_filter", "Optional filter for metadata during traversal.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "initial_root_ids", "IDs of the initial root nodes for the traversal.", "TYPE:Sequence[str]DEFAULT:()", "Sequence[str]", "()", "store_kwargs", "Additional arguments passed to the store adapter.", "TYPE:dict[str,Any]DEFAULT:{}", "dict[str,Any]", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Node]", "Nodes returned by the traversal."]]}, "list[Node]", "Nodes returned by the traversal.", "packages/graph-retriever/src/graph_retriever/traversal.py", {"table": [["15161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263", "deftraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]:\"\"\"Perform a graph traversal to retrieve nodes for a specific query.Parameters----------query :The query string for the traversal.edges :A list of [EdgeSpec][graph_retriever.edges.EdgeSpec] for use in creating a[MetadataEdgeFunction][graph_retriever.edges.MetadataEdgeFunction],or an [EdgeFunction][graph_retriever.edges.EdgeFunction].strategy :The traversal strategy that defines how nodes are discovered, selected,and finalized.store :The vector store adapter used for similarity searches and documentretrieval.metadata_filter :Optional filter for metadata during traversal.initial_root_ids :IDs of the initial root nodes for the traversal.store_kwargs :Additional arguments passed to the store adapter.Returns-------:Nodes returned by the traversal.\"\"\"traversal=_Traversal(query=query,edges=edges,strategy=copy.deepcopy(strategy),store=store,metadata_filter=metadata_filter,initial_root_ids=initial_root_ids,store_kwargs=store_kwargs,)returntraversal.traverse()"]]}, "15161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263", "deftraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]:\"\"\"Perform a graph traversal to retrieve nodes for a specific query.Parameters----------query :The query string for the traversal.edges :A list of [EdgeSpec][graph_retriever.edges.EdgeSpec] for use in creating a[MetadataEdgeFunction][graph_retriever.edges.MetadataEdgeFunction],or an [EdgeFunction][graph_retriever.edges.EdgeFunction].strategy :The traversal strategy that defines how nodes are discovered, selected,and finalized.store :The vector store adapter used for similarity searches and documentretrieval.metadata_filter :Optional filter for metadata during traversal.initial_root_ids :IDs of the initial root nodes for the traversal.store_kwargs :Additional arguments passed to the store adapter.Returns-------:Nodes returned by the traversal.\"\"\"traversal=_Traversal(query=query,edges=edges,strategy=copy.deepcopy(strategy),store=store,metadata_filter=metadata_filter,initial_root_ids=initial_root_ids,store_kwargs=store_kwargs,)returntraversal.traverse()", "deftraverse(query:str,*,edges:list[EdgeSpec]|EdgeFunction,strategy:Strategy,store:Adapter,metadata_filter:dict[str,Any]|None=None,initial_root_ids:Sequence[str]=(),store_kwargs:dict[str,Any]={},)->list[Node]:\"\"\"Perform a graph traversal to retrieve nodes for a specific query.Parameters----------query :The query string for the traversal.edges :A list of [EdgeSpec][graph_retriever.edges.EdgeSpec] for use in creating a[MetadataEdgeFunction][graph_retriever.edges.MetadataEdgeFunction],or an [EdgeFunction][graph_retriever.edges.EdgeFunction].strategy :The traversal strategy that defines how nodes are discovered, selected,and finalized.store :The vector store adapter used for similarity searches and documentretrieval.metadata_filter :Optional filter for metadata during traversal.initial_root_ids :IDs of the initial root nodes for the traversal.store_kwargs :Additional arguments passed to the store adapter.Returns-------:Nodes returned by the traversal.\"\"\"traversal=_Traversal(query=query,edges=edges,strategy=copy.deepcopy(strategy),store=store,metadata_filter=metadata_filter,initial_root_ids=initial_root_ids,store_kwargs=store_kwargs,)returntraversal.traverse()"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "graph_retriever.testing", "content": ["Helpers for testing Graph Retriever implementations."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "adapter_tests", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "AdapterComplianceCasedataclass", "content": ["dataclass", "AdapterComplianceCase(*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "AdapterComplianceCase(*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "Bases:ABC", "ABC", "Base dataclass for test cases.", {"table": [["ATTRIBUTE", "DESCRIPTION"], ["id", "The ID of the test case.TYPE:str"], ["expected", "The expected results of the case.TYPE:list[str]"]]}, "id", "The ID of the test case.", "TYPE:str", "str", "expected", "The expected results of the case.", "TYPE:list[str]", "list[str]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "AdapterComplianceSuite", "content": ["Bases:ABC", "ABC", "Test suite for adapter compliance.", "To use this, create a sub-class containing a@pytest.fixturenamedadapterwhich returns anAdapterwith the documents fromanimals.jsonlloaded.", "@pytest.fixture", "adapter", "Adapter", "animals.jsonl"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "adjacent_case", "content": ["adjacent_case(request)->AdjacentCase", "adjacent_case(request)->AdjacentCase", "Fixture providing theget_adjacentandaget_adjacenttest cases.", "get_adjacent", "aget_adjacent", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["516517518519", "@pytest.fixture(params=ADJACENT_CASES,ids=lambdac:c.id)defadjacent_case(self,request)->AdjacentCase:\"\"\"Fixture providing the `get_adjacent` and `aget_adjacent` test cases.\"\"\"returnrequest.param"]]}, "516517518519", "@pytest.fixture(params=ADJACENT_CASES,ids=lambdac:c.id)defadjacent_case(self,request)->AdjacentCase:\"\"\"Fixture providing the `get_adjacent` and `aget_adjacent` test cases.\"\"\"returnrequest.param", "@pytest.fixture(params=ADJACENT_CASES,ids=lambdac:c.id)defadjacent_case(self,request)->AdjacentCase:\"\"\"Fixture providing the `get_adjacent` and `aget_adjacent` test cases.\"\"\"returnrequest.param"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "expected", "content": ["expected(method:str,case:AdapterComplianceCase)->list[str]", "expected(method:str,case:AdapterComplianceCase)->list[str]", "Override to change the expected behavior of a case.", "If the test is expected to fail, callpytest.xfail(reason), orpytest.skip(reason)if it can't be executed.", "pytest.xfail(reason)", "pytest.skip(reason)", "Generally, this shouldnotchange the expected results, unless the theadapter being tested uses wildly different distance metrics or adifferent embedding. TheAnimalsEmbeddingis deterministic and theresults across vector stores should generally be deterministic andconsistent.", "AnimalsEmbedding", {"table": [["PARAMETER", "DESCRIPTION"], ["method", "The method being tested. For instance,get,aget, orsimilarity_search_with_embedding, etc.TYPE:str"], ["case", "The case being tested.TYPE:AdapterComplianceCase"]]}, "method", "The method being tested. For instance,get,aget, orsimilarity_search_with_embedding, etc.", "get", "aget", "similarity_search_with_embedding", "TYPE:str", "str", "case", "The case being tested.", "TYPE:AdapterComplianceCase", "AdapterComplianceCase", {"table": [["RETURNS", "DESCRIPTION"], ["list[str]", "The expected animals."]]}, "list[str]", "The expected animals.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509", "defexpected(self,method:str,case:AdapterComplianceCase)->list[str]:\"\"\"Override to change the expected behavior of a case.If the test is expected to fail, call `pytest.xfail(reason)`, or`pytest.skip(reason)` if it can't be executed.Generally, this should *not* change the expected results, unless the theadapter being tested uses wildly different distance metrics or adifferent embedding. The `AnimalsEmbedding` is deterministic and theresults across vector stores should generally be deterministic andconsistent.Parameters----------method :The method being tested. For instance, `get`, `aget`, or`similarity_search_with_embedding`, etc.case :The case being tested.Returns-------:The expected animals.\"\"\"ifnotself.supports_nested_metadata()andcase.requires_nested:pytest.xfail(\"nested metadata not supported\")ifnotself.supports_dict_in_list()andcase.requires_dict_in_list:pytest.xfail(\"dict-in-list fields is not supported\")returncase.expected"]]}, "479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509", "defexpected(self,method:str,case:AdapterComplianceCase)->list[str]:\"\"\"Override to change the expected behavior of a case.If the test is expected to fail, call `pytest.xfail(reason)`, or`pytest.skip(reason)` if it can't be executed.Generally, this should *not* change the expected results, unless the theadapter being tested uses wildly different distance metrics or adifferent embedding. The `AnimalsEmbedding` is deterministic and theresults across vector stores should generally be deterministic andconsistent.Parameters----------method :The method being tested. For instance, `get`, `aget`, or`similarity_search_with_embedding`, etc.case :The case being tested.Returns-------:The expected animals.\"\"\"ifnotself.supports_nested_metadata()andcase.requires_nested:pytest.xfail(\"nested metadata not supported\")ifnotself.supports_dict_in_list()andcase.requires_dict_in_list:pytest.xfail(\"dict-in-list fields is not supported\")returncase.expected", "defexpected(self,method:str,case:AdapterComplianceCase)->list[str]:\"\"\"Override to change the expected behavior of a case.If the test is expected to fail, call `pytest.xfail(reason)`, or`pytest.skip(reason)` if it can't be executed.Generally, this should *not* change the expected results, unless the theadapter being tested uses wildly different distance metrics or adifferent embedding. The `AnimalsEmbedding` is deterministic and theresults across vector stores should generally be deterministic andconsistent.Parameters----------method :The method being tested. For instance, `get`, `aget`, or`similarity_search_with_embedding`, etc.case :The case being tested.Returns-------:The expected animals.\"\"\"ifnotself.supports_nested_metadata()andcase.requires_nested:pytest.xfail(\"nested metadata not supported\")ifnotself.supports_dict_in_list()andcase.requires_dict_in_list:pytest.xfail(\"dict-in-list fields is not supported\")returncase.expected"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "get_case", "content": ["get_case(request)->GetCase", "get_case(request)->GetCase", "Fixture providing thegetandagettest cases.", "get", "aget", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["511512513514", "@pytest.fixture(params=GET_CASES,ids=lambdac:c.id)defget_case(self,request)->GetCase:\"\"\"Fixture providing the `get` and `aget` test cases.\"\"\"returnrequest.param"]]}, "511512513514", "@pytest.fixture(params=GET_CASES,ids=lambdac:c.id)defget_case(self,request)->GetCase:\"\"\"Fixture providing the `get` and `aget` test cases.\"\"\"returnrequest.param", "@pytest.fixture(params=GET_CASES,ids=lambdac:c.id)defget_case(self,request)->GetCase:\"\"\"Fixture providing the `get` and `aget` test cases.\"\"\"returnrequest.param"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "search_case", "content": ["search_case(request)->SearchCase", "search_case(request)->SearchCase", "Fixture providing the(a)?similarity_search_*test cases.", "(a)?similarity_search_*", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["521522523524", "@pytest.fixture(params=SEARCH_CASES,ids=lambdac:c.id)defsearch_case(self,request)->SearchCase:\"\"\"Fixture providing the `(a)?similarity_search_*` test cases.\"\"\"returnrequest.param"]]}, "521522523524", "@pytest.fixture(params=SEARCH_CASES,ids=lambdac:c.id)defsearch_case(self,request)->SearchCase:\"\"\"Fixture providing the `(a)?similarity_search_*` test cases.\"\"\"returnrequest.param", "@pytest.fixture(params=SEARCH_CASES,ids=lambdac:c.id)defsearch_case(self,request)->SearchCase:\"\"\"Fixture providing the `(a)?similarity_search_*` test cases.\"\"\"returnrequest.param"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "supports_dict_in_list", "content": ["supports_dict_in_list()->bool", "supports_dict_in_list()->bool", "Return whether dicts can appear in list fields in metadata.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["475476477", "defsupports_dict_in_list(self)->bool:\"\"\"Return whether dicts can appear in list fields in metadata.\"\"\"returnTrue"]]}, "475476477", "defsupports_dict_in_list(self)->bool:\"\"\"Return whether dicts can appear in list fields in metadata.\"\"\"returnTrue", "defsupports_dict_in_list(self)->bool:\"\"\"Return whether dicts can appear in list fields in metadata.\"\"\"returnTrue"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "supports_nested_metadata", "content": ["supports_nested_metadata()->bool", "supports_nested_metadata()->bool", "Return whether nested metadata is expected to work.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["471472473", "defsupports_nested_metadata(self)->bool:\"\"\"Return whether nested metadata is expected to work.\"\"\"returnTrue"]]}, "471472473", "defsupports_nested_metadata(self)->bool:\"\"\"Return whether nested metadata is expected to work.\"\"\"returnTrue", "defsupports_nested_metadata(self)->bool:\"\"\"Return whether nested metadata is expected to work.\"\"\"returnTrue"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_aadjacentasync", "content": ["async", "test_aadjacent(adapter:Adapter,adjacent_case:AdjacentCase)->None", "test_aadjacent(adapter:Adapter,adjacent_case:AdjacentCase)->None", "Run tests for `aadjacent.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["586587588589590591592593594595596597598", "asyncdeftest_aadjacent(self,adapter:Adapter,adjacent_case:AdjacentCase)->None:\"\"\"Run tests for `aadjacent.\"\"\"expected=self.expected(\"aadjacent\",adjacent_case)embedding,_=awaitadapter.asearch_with_embedding(adjacent_case.query,k=0)results=awaitadapter.aadjacent(edges=adjacent_case.edges,query_embedding=embedding,k=adjacent_case.k,filter=adjacent_case.filter,)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]]}, "586587588589590591592593594595596597598", "asyncdeftest_aadjacent(self,adapter:Adapter,adjacent_case:AdjacentCase)->None:\"\"\"Run tests for `aadjacent.\"\"\"expected=self.expected(\"aadjacent\",adjacent_case)embedding,_=awaitadapter.asearch_with_embedding(adjacent_case.query,k=0)results=awaitadapter.aadjacent(edges=adjacent_case.edges,query_embedding=embedding,k=adjacent_case.k,filter=adjacent_case.filter,)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)", "asyncdeftest_aadjacent(self,adapter:Adapter,adjacent_case:AdjacentCase)->None:\"\"\"Run tests for `aadjacent.\"\"\"expected=self.expected(\"aadjacent\",adjacent_case)embedding,_=awaitadapter.asearch_with_embedding(adjacent_case.query,k=0)results=awaitadapter.aadjacent(edges=adjacent_case.edges,query_embedding=embedding,k=adjacent_case.k,filter=adjacent_case.filter,)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_adjacent", "content": ["test_adjacent(adapter:Adapter,adjacent_case:AdjacentCase)->None", "test_adjacent(adapter:Adapter,adjacent_case:AdjacentCase)->None", "Run tests for `adjacent.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["574575576577578579580581582583584", "deftest_adjacent(self,adapter:Adapter,adjacent_case:AdjacentCase)->None:\"\"\"Run tests for `adjacent.\"\"\"expected=self.expected(\"adjacent\",adjacent_case)embedding,_=adapter.search_with_embedding(adjacent_case.query,k=0)results=adapter.adjacent(edges=adjacent_case.edges,query_embedding=embedding,k=adjacent_case.k,filter=adjacent_case.filter,)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]]}, "574575576577578579580581582583584", "deftest_adjacent(self,adapter:Adapter,adjacent_case:AdjacentCase)->None:\"\"\"Run tests for `adjacent.\"\"\"expected=self.expected(\"adjacent\",adjacent_case)embedding,_=adapter.search_with_embedding(adjacent_case.query,k=0)results=adapter.adjacent(edges=adjacent_case.edges,query_embedding=embedding,k=adjacent_case.k,filter=adjacent_case.filter,)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)", "deftest_adjacent(self,adapter:Adapter,adjacent_case:AdjacentCase)->None:\"\"\"Run tests for `adjacent.\"\"\"expected=self.expected(\"adjacent\",adjacent_case)embedding,_=adapter.search_with_embedding(adjacent_case.query,k=0)results=adapter.adjacent(edges=adjacent_case.edges,query_embedding=embedding,k=adjacent_case.k,filter=adjacent_case.filter,)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_agetasync", "content": ["async", "test_aget(adapter:Adapter,get_case:GetCase)->None", "test_aget(adapter:Adapter,get_case:GetCase)->None", "Run tests foraget.", "aget", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["532533534535536", "asyncdeftest_aget(self,adapter:Adapter,get_case:GetCase)->None:\"\"\"Run tests for `aget`.\"\"\"expected=self.expected(\"aget\",get_case)results=awaitadapter.aget(get_case.request,filter=get_case.filter)assert_ids_any_order(results,expected)"]]}, "532533534535536", "asyncdeftest_aget(self,adapter:Adapter,get_case:GetCase)->None:\"\"\"Run tests for `aget`.\"\"\"expected=self.expected(\"aget\",get_case)results=awaitadapter.aget(get_case.request,filter=get_case.filter)assert_ids_any_order(results,expected)", "asyncdeftest_aget(self,adapter:Adapter,get_case:GetCase)->None:\"\"\"Run tests for `aget`.\"\"\"expected=self.expected(\"aget\",get_case)results=awaitadapter.aget(get_case.request,filter=get_case.filter)assert_ids_any_order(results,expected)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_asearchasync", "content": ["async", "test_asearch(adapter:Adapter,search_case:SearchCase)->None", "test_asearch(adapter:Adapter,search_case:SearchCase)->None", "Run tests forasearch.", "asearch", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["567568569570571572", "asyncdeftest_asearch(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `asearch`.\"\"\"expected=self.expected(\"asearch\",search_case)embedding,_=awaitadapter.asearch_with_embedding(search_case.query,k=0)results=awaitadapter.asearch(embedding,**search_case.kwargs)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]]}, "567568569570571572", "asyncdeftest_asearch(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `asearch`.\"\"\"expected=self.expected(\"asearch\",search_case)embedding,_=awaitadapter.asearch_with_embedding(search_case.query,k=0)results=awaitadapter.asearch(embedding,**search_case.kwargs)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)", "asyncdeftest_asearch(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `asearch`.\"\"\"expected=self.expected(\"asearch\",search_case)embedding,_=awaitadapter.asearch_with_embedding(search_case.query,k=0)results=awaitadapter.asearch(embedding,**search_case.kwargs)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_asearch_with_embeddingasync", "content": ["async", "test_asearch_with_embedding(adapter:Adapter,search_case:SearchCase)->None", "test_asearch_with_embedding(adapter:Adapter,search_case:SearchCase)->None", "Run tests forasearch_with_embedding.", "asearch_with_embedding", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["549550551552553554555556557558", "asyncdeftest_asearch_with_embedding(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `asearch_with_embedding`.\"\"\"expected=self.expected(\"asearch_with_embedding\",search_case)embedding,results=awaitadapter.asearch_with_embedding(search_case.query,**search_case.kwargs)assert_is_embedding(embedding)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]]}, "549550551552553554555556557558", "asyncdeftest_asearch_with_embedding(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `asearch_with_embedding`.\"\"\"expected=self.expected(\"asearch_with_embedding\",search_case)embedding,results=awaitadapter.asearch_with_embedding(search_case.query,**search_case.kwargs)assert_is_embedding(embedding)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)", "asyncdeftest_asearch_with_embedding(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `asearch_with_embedding`.\"\"\"expected=self.expected(\"asearch_with_embedding\",search_case)embedding,results=awaitadapter.asearch_with_embedding(search_case.query,**search_case.kwargs)assert_is_embedding(embedding)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_get", "content": ["test_get(adapter:Adapter,get_case:GetCase)->None", "test_get(adapter:Adapter,get_case:GetCase)->None", "Run tests forget.", "get", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["526527528529530", "deftest_get(self,adapter:Adapter,get_case:GetCase)->None:\"\"\"Run tests for `get`.\"\"\"expected=self.expected(\"get\",get_case)results=adapter.get(get_case.request,filter=get_case.filter)assert_ids_any_order(results,expected)"]]}, "526527528529530", "deftest_get(self,adapter:Adapter,get_case:GetCase)->None:\"\"\"Run tests for `get`.\"\"\"expected=self.expected(\"get\",get_case)results=adapter.get(get_case.request,filter=get_case.filter)assert_ids_any_order(results,expected)", "deftest_get(self,adapter:Adapter,get_case:GetCase)->None:\"\"\"Run tests for `get`.\"\"\"expected=self.expected(\"get\",get_case)results=adapter.get(get_case.request,filter=get_case.filter)assert_ids_any_order(results,expected)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_search", "content": ["test_search(adapter:Adapter,search_case:SearchCase)->None", "test_search(adapter:Adapter,search_case:SearchCase)->None", "Run tests forsearch.", "search", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["560561562563564565", "deftest_search(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `search`.\"\"\"expected=self.expected(\"search\",search_case)embedding,_=adapter.search_with_embedding(search_case.query,k=0)results=adapter.search(embedding,**search_case.kwargs)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]]}, "560561562563564565", "deftest_search(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `search`.\"\"\"expected=self.expected(\"search\",search_case)embedding,_=adapter.search_with_embedding(search_case.query,k=0)results=adapter.search(embedding,**search_case.kwargs)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)", "deftest_search(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `search`.\"\"\"expected=self.expected(\"search\",search_case)embedding,_=adapter.search_with_embedding(search_case.query,k=0)results=adapter.search(embedding,**search_case.kwargs)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "test_search_with_embedding", "content": ["test_search_with_embedding(adapter:Adapter,search_case:SearchCase)->None", "test_search_with_embedding(adapter:Adapter,search_case:SearchCase)->None", "Run tests forsearch_with_embedding.", "search_with_embedding", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["538539540541542543544545546547", "deftest_search_with_embedding(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `search_with_embedding`.\"\"\"expected=self.expected(\"search_with_embedding\",search_case)embedding,results=adapter.search_with_embedding(search_case.query,**search_case.kwargs)assert_is_embedding(embedding)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]]}, "538539540541542543544545546547", "deftest_search_with_embedding(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `search_with_embedding`.\"\"\"expected=self.expected(\"search_with_embedding\",search_case)embedding,results=adapter.search_with_embedding(search_case.query,**search_case.kwargs)assert_is_embedding(embedding)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)", "deftest_search_with_embedding(self,adapter:Adapter,search_case:SearchCase)->None:\"\"\"Run tests for `search_with_embedding`.\"\"\"expected=self.expected(\"search_with_embedding\",search_case)embedding,results=adapter.search_with_embedding(search_case.query,**search_case.kwargs)assert_is_embedding(embedding)assert_ids_in_cosine_similarity_order(results,expected,embedding,adapter)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "AdjacentCasedataclass", "content": ["dataclass", "AdjacentCase(query:str,edges:set[Edge],k:int=4,filter:dict[str,Any]|None=None,*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "AdjacentCase(query:str,edges:set[Edge],k:int=4,filter:dict[str,Any]|None=None,*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "Bases:AdapterComplianceCase", "AdapterComplianceCase", "A test case forget_adjacentandaget_adjacent.", "get_adjacent", "aget_adjacent"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "GetCasedataclass", "content": ["dataclass", "GetCase(request:list[str],filter:dict[str,Any]|None=None,*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "GetCase(request:list[str],filter:dict[str,Any]|None=None,*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "Bases:AdapterComplianceCase", "AdapterComplianceCase", "A test case forgetandaget.", "get", "aget"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "SearchCasedataclass", "content": ["dataclass", "SearchCase(query:str,k:int|None=None,filter:dict[str,str]|None=None,*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "SearchCase(query:str,k:int|None=None,filter:dict[str,str]|None=None,*,id:str,expected:list[str],requires_nested:bool=False,requires_dict_in_list:bool=False,)", "Bases:AdapterComplianceCase", "AdapterComplianceCase", "A test case forsimilarity_search_*andasimilarity_search_*methods.", "similarity_search_*", "asimilarity_search_*"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "kwargsproperty", "content": ["property", "kwargs", "kwargs", "Return keyword arguments for the test invocation."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "assert_ids_any_order", "content": ["assert_ids_any_order(results:Iterable[Content],expected:list[str])->None", "assert_ids_any_order(results:Iterable[Content],expected:list[str])->None", "Assert the results are valid and match the IDs.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["333435363738394041", "defassert_ids_any_order(results:Iterable[Content],expected:list[str],)->None:\"\"\"Assert the results are valid and match the IDs.\"\"\"assert_valid_results(results)result_ids=[r.idforrinresults]assertset(result_ids)==set(expected),\"should contain exactly expected IDs\""]]}, "333435363738394041", "defassert_ids_any_order(results:Iterable[Content],expected:list[str],)->None:\"\"\"Assert the results are valid and match the IDs.\"\"\"assert_valid_results(results)result_ids=[r.idforrinresults]assertset(result_ids)==set(expected),\"should contain exactly expected IDs\"", "defassert_ids_any_order(results:Iterable[Content],expected:list[str],)->None:\"\"\"Assert the results are valid and match the IDs.\"\"\"assert_valid_results(results)result_ids=[r.idforrinresults]assertset(result_ids)==set(expected),\"should contain exactly expected IDs\""]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "assert_ids_in_cosine_similarity_order", "content": ["assert_ids_in_cosine_similarity_order(results:Iterable[Content],expected:list[str],query_embedding:list[float],adapter:Adapter,)->None", "assert_ids_in_cosine_similarity_order(results:Iterable[Content],expected:list[str],query_embedding:list[float],adapter:Adapter,)->None", "Assert the results are valid and in cosine similarity order.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["68697071727374757677787980818283", "defassert_ids_in_cosine_similarity_order(results:Iterable[Content],expected:list[str],query_embedding:list[float],adapter:Adapter,)->None:\"\"\"Assert the results are valid and in cosine similarity order.\"\"\"assert_valid_results(results)result_ids=[r.idforrinresults]similarity_scores=cosine_similarity_scores(adapter,query_embedding,expected)expected=sorted(expected,key=lambdaid:similarity_scores[id],reverse=True)assertresult_ids==expected,(\"should contain expected IDs in cosine similarity order\")"]]}, "68697071727374757677787980818283", "defassert_ids_in_cosine_similarity_order(results:Iterable[Content],expected:list[str],query_embedding:list[float],adapter:Adapter,)->None:\"\"\"Assert the results are valid and in cosine similarity order.\"\"\"assert_valid_results(results)result_ids=[r.idforrinresults]similarity_scores=cosine_similarity_scores(adapter,query_embedding,expected)expected=sorted(expected,key=lambdaid:similarity_scores[id],reverse=True)assertresult_ids==expected,(\"should contain expected IDs in cosine similarity order\")", "defassert_ids_in_cosine_similarity_order(results:Iterable[Content],expected:list[str],query_embedding:list[float],adapter:Adapter,)->None:\"\"\"Assert the results are valid and in cosine similarity order.\"\"\"assert_valid_results(results)result_ids=[r.idforrinresults]similarity_scores=cosine_similarity_scores(adapter,query_embedding,expected)expected=sorted(expected,key=lambdaid:similarity_scores[id],reverse=True)assertresult_ids==expected,(\"should contain expected IDs in cosine similarity order\")"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "assert_is_embedding", "content": ["assert_is_embedding(value:Any)", "assert_is_embedding(value:Any)", "Assert the value is an embedding.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["2021222324", "defassert_is_embedding(value:Any):\"\"\"Assert the value is an embedding.\"\"\"assertisinstance(value,list)foriteminvalue:assertisinstance(item,float)"]]}, "2021222324", "defassert_is_embedding(value:Any):\"\"\"Assert the value is an embedding.\"\"\"assertisinstance(value,list)foriteminvalue:assertisinstance(item,float)", "defassert_is_embedding(value:Any):\"\"\"Assert the value is an embedding.\"\"\"assertisinstance(value,list)foriteminvalue:assertisinstance(item,float)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "assert_valid_result", "content": ["assert_valid_result(content:Content)", "assert_valid_result(content:Content)", "Assert the content is valid.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["14151617", "defassert_valid_result(content:Content):\"\"\"Assert the content is valid.\"\"\"assertisinstance(content.id,str)assert_is_embedding(content.embedding)"]]}, "14151617", "defassert_valid_result(content:Content):\"\"\"Assert the content is valid.\"\"\"assertisinstance(content.id,str)assert_is_embedding(content.embedding)", "defassert_valid_result(content:Content):\"\"\"Assert the content is valid.\"\"\"assertisinstance(content.id,str)assert_is_embedding(content.embedding)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "assert_valid_results", "content": ["assert_valid_results(docs:Iterable[Content])", "assert_valid_results(docs:Iterable[Content])", "Assert all of the contents are valid results.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["27282930", "defassert_valid_results(docs:Iterable[Content]):\"\"\"Assert all of the contents are valid results.\"\"\"fordocindocs:assert_valid_result(doc)"]]}, "27282930", "defassert_valid_results(docs:Iterable[Content]):\"\"\"Assert all of the contents are valid results.\"\"\"fordocindocs:assert_valid_result(doc)", "defassert_valid_results(docs:Iterable[Content]):\"\"\"Assert all of the contents are valid results.\"\"\"fordocindocs:assert_valid_result(doc)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "cosine_similarity_scores", "content": ["cosine_similarity_scores(adapter:Adapter,query_or_embedding:str|list[float],ids:list[str],)->dict[str,float]", "cosine_similarity_scores(adapter:Adapter,query_or_embedding:str|list[float],ids:list[str],)->dict[str,float]", "Return the cosine similarity scores for the given IDs and query embedding.", "packages/graph-retriever/src/graph_retriever/testing/adapter_tests.py", {"table": [["44454647484950515253545556575859606162636465", "defcosine_similarity_scores(adapter:Adapter,query_or_embedding:str|list[float],ids:list[str])->dict[str,float]:\"\"\"Return the cosine similarity scores for the given IDs and query embedding.\"\"\"iflen(ids)==0:return{}docs=adapter.get(ids)found_ids=(d.idfordindocs)assertset(ids)==set(found_ids),\"can't find all IDs\"ifisinstance(query_or_embedding,str):query_embedding=adapter.search_with_embedding(query_or_embedding,k=0)[0]else:query_embedding=query_or_embeddingscores:list[float]=cosine_similarity([query_embedding],[d.embeddingfordindocs],)[0]return{doc.id:scorefordoc,scoreinzip(docs,scores)}"]]}, "44454647484950515253545556575859606162636465", "defcosine_similarity_scores(adapter:Adapter,query_or_embedding:str|list[float],ids:list[str])->dict[str,float]:\"\"\"Return the cosine similarity scores for the given IDs and query embedding.\"\"\"iflen(ids)==0:return{}docs=adapter.get(ids)found_ids=(d.idfordindocs)assertset(ids)==set(found_ids),\"can't find all IDs\"ifisinstance(query_or_embedding,str):query_embedding=adapter.search_with_embedding(query_or_embedding,k=0)[0]else:query_embedding=query_or_embeddingscores:list[float]=cosine_similarity([query_embedding],[d.embeddingfordindocs],)[0]return{doc.id:scorefordoc,scoreinzip(docs,scores)}", "defcosine_similarity_scores(adapter:Adapter,query_or_embedding:str|list[float],ids:list[str])->dict[str,float]:\"\"\"Return the cosine similarity scores for the given IDs and query embedding.\"\"\"iflen(ids)==0:return{}docs=adapter.get(ids)found_ids=(d.idfordindocs)assertset(ids)==set(found_ids),\"can't find all IDs\"ifisinstance(query_or_embedding,str):query_embedding=adapter.search_with_embedding(query_or_embedding,k=0)[0]else:query_embedding=query_or_embeddingscores:list[float]=cosine_similarity([query_embedding],[d.embeddingfordindocs],)[0]return{doc.id:scorefordoc,scoreinzip(docs,scores)}"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "embeddings", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "AnimalEmbeddings", "content": ["AnimalEmbeddings()", "AnimalEmbeddings()", "Bases:WordEmbeddings", "WordEmbeddings", "Embeddings for animal test-case.", "packages/graph-retriever/src/graph_retriever/testing/embeddings.py", {"table": [["888990919293949596979899100101102103104105106107108109110111112113114115116", "def__init__(self):super().__init__(words=\"\"\"alli alpa amer amph ante ante antl appe aqua arct arma aust babobadg barr bask bear beav beet beha bird biso bite blac blue boarbobc brig buff bugl burr bush butt came cani capy cari carn casscate cham chee chic chim chin chir clim coas coat cobr cock colocolo comm comp cour coyo crab cran croa croc crow crus wing woolcult cunn curi curl damb danc deer defe defe deme dese digg dingdise dist dive dolp dome dome donk dove drag drag duck ecos effoeigh elab eleg elev elon euca extr eyes falc famo famo fast fastfeat feet ferr fier figh finc fish flam flig flig food fore founfres frie frog gaze geck gees gent gill gira goat gori grac grasgras graz grou grou grou guin hams hard hawk hedg herb herd herohigh hipp honk horn hors hove howl huma humm hump hunt hyen idenigua inde inse inte jack jagu jell jump jung kang koal komo larklarv lemu leop life lion liza lobs long loud loya mada magp mammmana mari mars mass mati meat medi melo meta migr milk mimi moosmosq moth narw nati natu neck newt noct nort ocea octo ostr packpain patt peac pest pinc pink play plum poll post pouc powe precpred prey prid prim prob prot prow quac quil rais rapi reac regarege regi rego rein rept resi rive roam rode sava scav seab seafseas semi shar shed shel skil smal snak soci soft soli song songsoun sout spec spee spik spor spot stag stic stin stin stor strestre stre stro surv surv sust symb tail tall talo team teet tentterm terr thou tiny tong toug tree agil tuft tund tusk umbr unicuniq vast vege veno vibr vita vora wadi wasp wate webb wetl wildant bat bee cat cow dog eel elk emu fox pet pig\"\"\".split())"]]}, "888990919293949596979899100101102103104105106107108109110111112113114115116", "def__init__(self):super().__init__(words=\"\"\"alli alpa amer amph ante ante antl appe aqua arct arma aust babobadg barr bask bear beav beet beha bird biso bite blac blue boarbobc brig buff bugl burr bush butt came cani capy cari carn casscate cham chee chic chim chin chir clim coas coat cobr cock colocolo comm comp cour coyo crab cran croa croc crow crus wing woolcult cunn curi curl damb danc deer defe defe deme dese digg dingdise dist dive dolp dome dome donk dove drag drag duck ecos effoeigh elab eleg elev elon euca extr eyes falc famo famo fast fastfeat feet ferr fier figh finc fish flam flig flig food fore founfres frie frog gaze geck gees gent gill gira goat gori grac grasgras graz grou grou grou guin hams hard hawk hedg herb herd herohigh hipp honk horn hors hove howl huma humm hump hunt hyen idenigua inde inse inte jack jagu jell jump jung kang koal komo larklarv lemu leop life lion liza lobs long loud loya mada magp mammmana mari mars mass mati meat medi melo meta migr milk mimi moosmosq moth narw nati natu neck newt noct nort ocea octo ostr packpain patt peac pest pinc pink play plum poll post pouc powe precpred prey prid prim prob prot prow quac quil rais rapi reac regarege regi rego rein rept resi rive roam rode sava scav seab seafseas semi shar shed shel skil smal snak soci soft soli song songsoun sout spec spee spik spor spot stag stic stin stin stor strestre stre stro surv surv sust symb tail tall talo team teet tentterm terr thou tiny tong toug tree agil tuft tund tusk umbr unicuniq vast vege veno vibr vita vora wadi wasp wate webb wetl wildant bat bee cat cow dog eel elk emu fox pet pig\"\"\".split())", "def__init__(self):super().__init__(words=\"\"\"alli alpa amer amph ante ante antl appe aqua arct arma aust babobadg barr bask bear beav beet beha bird biso bite blac blue boarbobc brig buff bugl burr bush butt came cani capy cari carn casscate cham chee chic chim chin chir clim coas coat cobr cock colocolo comm comp cour coyo crab cran croa croc crow crus wing woolcult cunn curi curl damb danc deer defe defe deme dese digg dingdise dist dive dolp dome dome donk dove drag drag duck ecos effoeigh elab eleg elev elon euca extr eyes falc famo famo fast fastfeat feet ferr fier figh finc fish flam flig flig food fore founfres frie frog gaze geck gees gent gill gira goat gori grac grasgras graz grou grou grou guin hams hard hawk hedg herb herd herohigh hipp honk horn hors hove howl huma humm hump hunt hyen idenigua inde inse inte jack jagu jell jump jung kang koal komo larklarv lemu leop life lion liza lobs long loud loya mada magp mammmana mari mars mass mati meat medi melo meta migr milk mimi moosmosq moth narw nati natu neck newt noct nort ocea octo ostr packpain patt peac pest pinc pink play plum poll post pouc powe precpred prey prid prim prob prot prow quac quil rais rapi reac regarege regi rego rein rept resi rive roam rode sava scav seab seafseas semi shar shed shel skil smal snak soci soft soli song songsoun sout spec spee spik spor spot stag stic stin stin stor strestre stre stro surv surv sust symb tail tall talo team teet tentterm terr thou tiny tong toug tree agil tuft tund tusk umbr unicuniq vast vege veno vibr vita vora wadi wasp wate webb wetl wildant bat bee cat cow dog eel elk emu fox pet pig\"\"\".split())"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "__call__", "content": ["__call__(text:str)->list[float]", "__call__(text:str)->list[float]", "Return the embedding.", "packages/graph-retriever/src/graph_retriever/testing/embeddings.py", {"table": [["777879808182", "def__call__(self,text:str)->list[float]:\"\"\"Return the embedding.\"\"\"return[1.0+(100/self._offsets[i])ifwordintextelse0.2/(i+1)fori,wordinenumerate(self._words)]"]]}, "777879808182", "def__call__(self,text:str)->list[float]:\"\"\"Return the embedding.\"\"\"return[1.0+(100/self._offsets[i])ifwordintextelse0.2/(i+1)fori,wordinenumerate(self._words)]", "def__call__(self,text:str)->list[float]:\"\"\"Return the embedding.\"\"\"return[1.0+(100/self._offsets[i])ifwordintextelse0.2/(i+1)fori,wordinenumerate(self._words)]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "ParserEmbeddings", "content": ["ParserEmbeddings(dimension:int=10)", "ParserEmbeddings(dimension:int=10)", "Parse the tuext as a list of floats, otherwise return zeros.", "packages/graph-retriever/src/graph_retriever/testing/embeddings.py", {"table": [["5152", "def__init__(self,dimension:int=10)->None:self.dimension=dimension"]]}, "5152", "def__init__(self,dimension:int=10)->None:self.dimension=dimension", "def__init__(self,dimension:int=10)->None:self.dimension=dimension"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "WordEmbeddings", "content": ["WordEmbeddings(words:list[str])", "WordEmbeddings(words:list[str])", "Embeddings based on a word list.", "packages/graph-retriever/src/graph_retriever/testing/embeddings.py", {"table": [["7172737475", "def__init__(self,words:list[str]):self._words=wordsself._offsets=[_string_to_number(w)*((-1)**i)fori,winenumerate(words)]"]]}, "7172737475", "def__init__(self,words:list[str]):self._words=wordsself._offsets=[_string_to_number(w)*((-1)**i)fori,winenumerate(words)]", "def__init__(self,words:list[str]):self._words=wordsself._offsets=[_string_to_number(w)*((-1)**i)fori,winenumerate(words)]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "angular_2d_embedding", "content": ["angular_2d_embedding(text:str)->list[float]", "angular_2d_embedding(text:str)->list[float]", "Convert input text to a 'vector' (list of floats).", {"table": [["PARAMETER", "DESCRIPTION"], ["text", "The text to embed.TYPE:str"]]}, "text", "The text to embed.", "TYPE:str", "str", {"table": [["RETURNS", "DESCRIPTION"], ["list[float]", "If the text is a number, use it as the angle for the unit vector inunits of pi.Any other input text becomes the singular result[0, 0]."]]}, "list[float]", "If the text is a number, use it as the angle for the unit vector inunits of pi.", "Any other input text becomes the singular result[0, 0].", "[0, 0]", "packages/graph-retriever/src/graph_retriever/testing/embeddings.py", {"table": [["678910111213141516171819202122232425262728", "defangular_2d_embedding(text:str)->list[float]:\"\"\"Convert input text to a 'vector' (list of floats).Parameters----------text: strThe text to embed.Returns-------:If the text is a number, use it as the angle for the unit vector inunits of pi.Any other input text becomes the singular result `[0, 0]`.\"\"\"try:angle=float(text)return[math.cos(angle*math.pi),math.sin(angle*math.pi)]exceptValueError:# Assume: just test string, no attention is paid to values.return[0.0,0.0]"]]}, "678910111213141516171819202122232425262728", "defangular_2d_embedding(text:str)->list[float]:\"\"\"Convert input text to a 'vector' (list of floats).Parameters----------text: strThe text to embed.Returns-------:If the text is a number, use it as the angle for the unit vector inunits of pi.Any other input text becomes the singular result `[0, 0]`.\"\"\"try:angle=float(text)return[math.cos(angle*math.pi),math.sin(angle*math.pi)]exceptValueError:# Assume: just test string, no attention is paid to values.return[0.0,0.0]", "defangular_2d_embedding(text:str)->list[float]:\"\"\"Convert input text to a 'vector' (list of floats).Parameters----------text: strThe text to embed.Returns-------:If the text is a number, use it as the angle for the unit vector inunits of pi.Any other input text becomes the singular result `[0, 0]`.\"\"\"try:angle=float(text)return[math.cos(angle*math.pi),math.sin(angle*math.pi)]exceptValueError:# Assume: just test string, no attention is paid to values.return[0.0,0.0]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/testing/", "title": "earth_embeddings", "content": ["earth_embeddings(text:str)->list[float]", "earth_embeddings(text:str)->list[float]", "Split words and return a vector based on that.", "packages/graph-retriever/src/graph_retriever/testing/embeddings.py", {"table": [["313233343536373839404142434445", "defearth_embeddings(text:str)->list[float]:\"\"\"Split words and return a vector based on that.\"\"\"defvector_near(value:float)->list[float]:base_point=[value,(1-value**2)**0.5]fluctuation=random.random()/100.0return[base_point[0]+fluctuation,base_point[1]-fluctuation]words=set(text.lower().split())if\"earth\"inwords:returnvector_near(0.9)elif{\"planet\",\"world\",\"globe\",\"sphere\"}.intersection(words):returnvector_near(0.8)else:returnvector_near(0.1)"]]}, "313233343536373839404142434445", "defearth_embeddings(text:str)->list[float]:\"\"\"Split words and return a vector based on that.\"\"\"defvector_near(value:float)->list[float]:base_point=[value,(1-value**2)**0.5]fluctuation=random.random()/100.0return[base_point[0]+fluctuation,base_point[1]-fluctuation]words=set(text.lower().split())if\"earth\"inwords:returnvector_near(0.9)elif{\"planet\",\"world\",\"globe\",\"sphere\"}.intersection(words):returnvector_near(0.8)else:returnvector_near(0.1)", "defearth_embeddings(text:str)->list[float]:\"\"\"Split words and return a vector based on that.\"\"\"defvector_near(value:float)->list[float]:base_point=[value,(1-value**2)**0.5]fluctuation=random.random()/100.0return[base_point[0]+fluctuation,base_point[1]-fluctuation]words=set(text.lower().split())if\"earth\"inwords:returnvector_near(0.9)elif{\"planet\",\"world\",\"globe\",\"sphere\"}.intersection(words):returnvector_near(0.8)else:returnvector_near(0.1)"]}
{"url": "https://datastax.github.io/graph-rag/guide/strategies/", "title": "Strategies", "content": ["Strategies determine which nodes are selected duringtraversal.", "All strategies allow you to control how many nodes are retrieved (k) as wellas how many nodes are found during the initial vector search (start_k) andeach step of the traversal (adjacent_k) as well as bounding the nodesretrieved based on depth (max_depth).", "k", "start_k", "adjacent_k", "max_depth"]}
{"url": "https://datastax.github.io/graph-rag/guide/strategies/", "title": "Eager", "content": ["TheEagerstrategy selects all of the discovered nodes at each step of the traversal.", "Eager", "It doesn't support configuration beyond the standard options."]}
{"url": "https://datastax.github.io/graph-rag/guide/strategies/", "title": "MMR", "content": ["TheMMRstrategy selects nodes with thehighest maximum marginal relevance score at each iteration.", "MMR", "It can be configured with alambda_multwhich controls the trade-off between relevance and diversity.", "lambda_mult"]}
{"url": "https://datastax.github.io/graph-rag/guide/strategies/", "title": "Scored", "content": ["TheScoredstrategy applies a user-defined function to each node to assign a score, and selects a number of nodes with the highest scores.", "Scored"]}
{"url": "https://datastax.github.io/graph-rag/guide/strategies/", "title": "User-Defined Strategies", "content": ["You can also implement your ownStrategy. This allows you to control how discovered nodes are tracked and selected for traversal.", "Strategy"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/", "title": "langchain_graph_retriever", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/", "title": "GraphRetriever", "content": ["Bases:BaseRetriever", "BaseRetriever", "Retriever combining vector search and graph traversal.", "TheGraphRetrieverclassretrieves documents by first performing a vector search to identify relevantdocuments, followed by graph traversal to explore their connections. Itsupports multiple traversal strategies and integrates seamlessly withLangChain's retriever framework.", {"table": [["ATTRIBUTE", "DESCRIPTION"], ["store", "The adapter or vector store used for document retrieval.TYPE:Adapter|VectorStore"], ["edges", "A list ofEdgeSpecfor use in creating aMetadataEdgeFunction,or anEdgeFunction.TYPE:list[EdgeSpec] |EdgeFunction"], ["strategy", "The traversal strategy to use.TYPE:Strategy"]]}, "store", "The adapter or vector store used for document retrieval.", "TYPE:Adapter|VectorStore", "Adapter|VectorStore", "edges", "A list ofEdgeSpecfor use in creating aMetadataEdgeFunction,or anEdgeFunction.", "TYPE:list[EdgeSpec] |EdgeFunction", "list[EdgeSpec] |EdgeFunction", "strategy", "The traversal strategy to use.", "TYPE:Strategy", "Strategy"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/", "title": "adaptercachedproperty", "content": ["cached", "property", "adapter:Adapter", "adapter:Adapter", "The adapter to use during traversals."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/", "title": "apply_extra", "content": ["apply_extra()->Self", "apply_extra()->Self", "Apply extra configuration to the traversal strategy.", "This method captures additional fields provided in themodel_extraargumentand applies them to the current traversal strategy. Any extra fields arecleared after they are applied.", "model_extra", {"table": [["RETURNS", "DESCRIPTION"], ["Self", "The updated GraphRetriever instance."]]}, "Self", "The updated GraphRetriever instance.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/graph_retriever.py", {"table": [["5556575859606162636465666768697071727374", "@model_validator(mode=\"after\")defapply_extra(self)->Self:\"\"\"Apply extra configuration to the traversal strategy.This method captures additional fields provided in the `model_extra` argumentand applies them to the current traversal strategy. Any extra fields arecleared after they are applied.Returns-------:The updated GraphRetriever instance.\"\"\"ifself.model_extra:if\"k\"inself.model_extra:self.model_extra[\"select_k\"]=self.model_extra.pop(\"k\")self.strategy=dataclasses.replace(self.strategy,**self.model_extra)self.model_extra.clear()returnself"]]}, "5556575859606162636465666768697071727374", "@model_validator(mode=\"after\")defapply_extra(self)->Self:\"\"\"Apply extra configuration to the traversal strategy.This method captures additional fields provided in the `model_extra` argumentand applies them to the current traversal strategy. Any extra fields arecleared after they are applied.Returns-------:The updated GraphRetriever instance.\"\"\"ifself.model_extra:if\"k\"inself.model_extra:self.model_extra[\"select_k\"]=self.model_extra.pop(\"k\")self.strategy=dataclasses.replace(self.strategy,**self.model_extra)self.model_extra.clear()returnself", "@model_validator(mode=\"after\")defapply_extra(self)->Self:\"\"\"Apply extra configuration to the traversal strategy.This method captures additional fields provided in the `model_extra` argumentand applies them to the current traversal strategy. Any extra fields arecleared after they are applied.Returns-------:The updated GraphRetriever instance.\"\"\"ifself.model_extra:if\"k\"inself.model_extra:self.model_extra[\"select_k\"]=self.model_extra.pop(\"k\")self.strategy=dataclasses.replace(self.strategy,**self.model_extra)self.model_extra.clear()returnself"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "langchain_graph_retriever.adapters", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "astra", "content": ["Provides an adapter for AstraDB vector store integration."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "AstraAdapter", "content": ["AstraAdapter(vector_store:AstraDBVectorStore)", "AstraAdapter(vector_store:AstraDBVectorStore)", "Bases:Adapter", "Adapter", "Adapter for theAstraDBvector store.", "This class integrates the LangChain AstraDB vector store with the graphretriever system, providing functionality for similarity search and documentretrieval.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The AstraDB vector store instance.TYPE:AstraDBVectorStore"]]}, "vector_store", "The AstraDB vector store instance.", "TYPE:AstraDBVectorStore", "AstraDBVectorStore", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/astra.py", {"table": [["131132133134", "def__init__(self,vector_store:AstraDBVectorStore)->None:self.vector_store=vector_store.copy(component_name=\"langchain_graph_retriever\")"]]}, "131132133134", "def__init__(self,vector_store:AstraDBVectorStore)->None:self.vector_store=vector_store.copy(component_name=\"langchain_graph_retriever\")", "def__init__(self,vector_store:AstraDBVectorStore)->None:self.vector_store=vector_store.copy(component_name=\"langchain_graph_retriever\")"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "aadjacentasync", "content": ["async", "aadjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "aadjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "Asynchronously return the content items with at least one matching edge.", {"table": [["PARAMETER", "DESCRIPTION"], ["edges", "The edges to look for.TYPE:set[Edge]"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "The number of relevant content items to select for the edges.TYPE:int"], ["filter", "Optional metadata to filter the results.TYPE:dict[str,Any] | None"], ["kwargs", "Keyword arguments to pass to the similarity search.TYPE:AnyDEFAULT:{}"]]}, "edges", "The edges to look for.", "TYPE:set[Edge]", "set[Edge]", "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "The number of relevant content items to select for the edges.", "TYPE:int", "int", "filter", "Optional metadata to filter the results.", "TYPE:dict[str,Any] | None", "dict[str,Any] | None", "kwargs", "Keyword arguments to pass to the similarity search.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["Iterable[Content]", "Iterable of adjacent content items."]]}, "Iterable[Content]", "Iterable of adjacent content items.", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If unsupported edge types are encountered."]]}, "ValueError", "If unsupported edge types are encountered.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356", "asyncdefaadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Asynchronously return the content items with at least one matching edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select for the edges.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"tasks=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):tasks.append(self.asearch(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,))elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:tasks.append(self.aget(ids,filter))results:list[Content]=[cforcompleted_taskinasyncio.as_completed(tasks)forcinawaitcompleted_task]returntop_k(results,embedding=query_embedding,k=k,)"]]}, "292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356", "asyncdefaadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Asynchronously return the content items with at least one matching edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select for the edges.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"tasks=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):tasks.append(self.asearch(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,))elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:tasks.append(self.aget(ids,filter))results:list[Content]=[cforcompleted_taskinasyncio.as_completed(tasks)forcinawaitcompleted_task]returntop_k(results,embedding=query_embedding,k=k,)", "asyncdefaadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Asynchronously return the content items with at least one matching edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select for the edges.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"tasks=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):tasks.append(self.asearch(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,))elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:tasks.append(self.aget(ids,filter))results:list[Content]=[cforcompleted_taskinasyncio.as_completed(tasks)forcinawaitcompleted_task]returntop_k(results,embedding=query_embedding,k=k,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "adjacent", "content": ["adjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "adjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "Return the content items with at least one matching incoming edge.", {"table": [["PARAMETER", "DESCRIPTION"], ["edges", "The edges to look for.TYPE:set[Edge]"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "The number of relevant content items to select.TYPE:int"], ["filter", "Optional metadata to filter the results.TYPE:dict[str,Any] | None"], ["kwargs", "Keyword arguments to pass to the similarity search.TYPE:AnyDEFAULT:{}"]]}, "edges", "The edges to look for.", "TYPE:set[Edge]", "set[Edge]", "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "The number of relevant content items to select.", "TYPE:int", "int", "filter", "Optional metadata to filter the results.", "TYPE:dict[str,Any] | None", "dict[str,Any] | None", "kwargs", "Keyword arguments to pass to the similarity search.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["Iterable[Content]", "Iterable of adjacent content items."]]}, "Iterable[Content]", "Iterable of adjacent content items.", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If unsupported edge types are encountered."]]}, "ValueError", "If unsupported edge types are encountered.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290", "defadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Return the content items with at least one matching incoming edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"results:list[Content]=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):docs=self.search(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,)results.extend(docs)elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:results.extend(self.get(ids,filter=filter))returntop_k(results,embedding=query_embedding,k=k,)"]]}, "232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290", "defadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Return the content items with at least one matching incoming edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"results:list[Content]=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):docs=self.search(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,)results.extend(docs)elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:results.extend(self.get(ids,filter=filter))returntop_k(results,embedding=query_embedding,k=k,)", "defadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Return the content items with at least one matching incoming edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"results:list[Content]=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):docs=self.search(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,)results.extend(docs)elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:results.extend(self.get(ids,filter=filter))returntop_k(results,embedding=query_embedding,k=k,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "agetasync", "content": ["async", "aget(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "aget(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "Asynchronously get content items by ID.", "Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method shouldNOTraise exceptions if no content items are found for some IDs.", "Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.", {"table": [["PARAMETER", "DESCRIPTION"], ["ids", "List of IDs to get.TYPE:Sequence[str]"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments. These are up to the implementation.TYPE:AnyDEFAULT:{}"]]}, "ids", "List of IDs to get.", "TYPE:Sequence[str]", "Sequence[str]", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments. These are up to the implementation.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items that were found."]]}, "list[Content]", "List of content items that were found.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["311312313314315316317318319", "@overrideasyncdefaget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:docs=awaitself._aget(self._remove_duplicates(ids),filter,**kwargs)returnself.format_documents_hook(docs)"]]}, "311312313314315316317318319", "@overrideasyncdefaget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:docs=awaitself._aget(self._remove_duplicates(ids),filter,**kwargs)returnself.format_documents_hook(docs)", "@overrideasyncdefaget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:docs=awaitself._aget(self._remove_duplicates(ids),filter,**kwargs)returnself.format_documents_hook(docs)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "asearchasync", "content": ["async", "asearch(embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]", "asearch(embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]", "Asynchronously return content items most similar to the query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items most similar to the query vector."]]}, "list[Content]", "List of content items most similar to the query vector.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["233234235236237238239240241242243244245246247248249250", "@overrideasyncdefasearch(self,embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]:ifk==0:return[]docs=awaitself._asearch(embedding=embedding,k=k,filter=self.update_filter_hook(filter),**kwargs,)returnself.format_documents_hook(docs)"]]}, "233234235236237238239240241242243244245246247248249250", "@overrideasyncdefasearch(self,embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]:ifk==0:return[]docs=awaitself._asearch(embedding=embedding,k=k,filter=self.update_filter_hook(filter),**kwargs,)returnself.format_documents_hook(docs)", "@overrideasyncdefasearch(self,embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]:ifk==0:return[]docs=awaitself._asearch(embedding=embedding,k=k,filter=self.update_filter_hook(filter),**kwargs,)returnself.format_documents_hook(docs)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "asearch_with_embeddingasync", "content": ["async", "asearch_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "asearch_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "Asynchronously return content items most similar to the query.", "Also returns the embedded query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["query", "Input text.TYPE:str"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "query", "Input text.", "TYPE:str", "str", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["contents", "List of up tokcontent items most similar to the queryvector.TYPE:list[Content]"]]}, "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "contents", "List of up tokcontent items most similar to the queryvector.", "k", "TYPE:list[Content]", "list[Content]", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["109110111112113114115116117118119120121122123124", "@overrideasyncdefasearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:query_embedding=awaitself.aembed_query(query)docs=awaitself.asearch(embedding=query_embedding,k=k,filter=filter,**kwargs,)returnquery_embedding,docs"]]}, "109110111112113114115116117118119120121122123124", "@overrideasyncdefasearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:query_embedding=awaitself.aembed_query(query)docs=awaitself.asearch(embedding=query_embedding,k=k,filter=filter,**kwargs,)returnquery_embedding,docs", "@overrideasyncdefasearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:query_embedding=awaitself.aembed_query(query)docs=awaitself.asearch(embedding=query_embedding,k=k,filter=filter,**kwargs,)returnquery_embedding,docs"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "get", "content": ["get(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "get(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "Get content items by ID.", "Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method shouldNOTraise exceptions if no content items are found for some IDs.", "Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.", {"table": [["PARAMETER", "DESCRIPTION"], ["ids", "List of IDs to get.TYPE:Sequence[str]"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments. These are up to the implementation.TYPE:AnyDEFAULT:{}"]]}, "ids", "List of IDs to get.", "TYPE:Sequence[str]", "Sequence[str]", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments. These are up to the implementation.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items that were found."]]}, "list[Content]", "List of content items that were found.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["268269270271272273274275276", "@overridedefget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:docs=self._get(self._remove_duplicates(ids),filter,**kwargs)returnself.format_documents_hook(docs)"]]}, "268269270271272273274275276", "@overridedefget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:docs=self._get(self._remove_duplicates(ids),filter,**kwargs)returnself.format_documents_hook(docs)", "@overridedefget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:docs=self._get(self._remove_duplicates(ids),filter,**kwargs)returnself.format_documents_hook(docs)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "search", "content": ["search(embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]", "search(embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]", "Return contents most similar to the query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["embedding", "Embedding to look up documents similar to.TYPE:list[float]"], ["k", "Number of Documents to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,str] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "embedding", "Embedding to look up documents similar to.", "TYPE:list[float]", "list[float]", "k", "Number of Documents to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,str] | NoneDEFAULT:None", "dict[str,str] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of Contents most similar to the query vector."]]}, "list[Content]", "List of Contents most similar to the query vector.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193", "@overridedefsearch(self,embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Return contents most similar to the query vector.Parameters----------embedding :Embedding to look up documents similar to.k :Number of Documents to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of Contents most similar to the query vector.\"\"\"ifk==0:return[]docs=self._search(embedding=embedding,k=k,filter=self.update_filter_hook(filter),**kwargs,)returnself.format_documents_hook(docs)"]]}, "157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193", "@overridedefsearch(self,embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Return contents most similar to the query vector.Parameters----------embedding :Embedding to look up documents similar to.k :Number of Documents to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of Contents most similar to the query vector.\"\"\"ifk==0:return[]docs=self._search(embedding=embedding,k=k,filter=self.update_filter_hook(filter),**kwargs,)returnself.format_documents_hook(docs)", "@overridedefsearch(self,embedding:list[float],k:int=4,filter:dict[str,str]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Return contents most similar to the query vector.Parameters----------embedding :Embedding to look up documents similar to.k :Number of Documents to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of Contents most similar to the query vector.\"\"\"ifk==0:return[]docs=self._search(embedding=embedding,k=k,filter=self.update_filter_hook(filter),**kwargs,)returnself.format_documents_hook(docs)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "search_with_embedding", "content": ["search_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "search_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "Return content items most similar to the query.", "Also returns the embedded query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["query", "Input text.TYPE:str"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "query", "Input text.", "TYPE:str", "str", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["contents", "List of up tokcontent items most similar to the query vector.TYPE:list[Content]"]]}, "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "contents", "List of up tokcontent items most similar to the query vector.", "k", "TYPE:list[Content]", "list[Content]", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["9293949596979899100101102103104105106107", "@overridedefsearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:query_embedding=self.embed_query(query)docs=self.search(embedding=query_embedding,k=k,filter=filter,**kwargs,)returnquery_embedding,docs"]]}, "9293949596979899100101102103104105106107", "@overridedefsearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:query_embedding=self.embed_query(query)docs=self.search(embedding=query_embedding,k=k,filter=filter,**kwargs,)returnquery_embedding,docs", "@overridedefsearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:query_embedding=self.embed_query(query)docs=self.search(embedding=query_embedding,k=k,filter=filter,**kwargs,)returnquery_embedding,docs"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "empty_async_iterableasync", "content": ["async", "empty_async_iterable()->AsyncIterable[AstraDBQueryResult]", "empty_async_iterable()->AsyncIterable[AstraDBQueryResult]", "Create an empty async iterable.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/astra.py", {"table": [["111112113114", "asyncdefempty_async_iterable()->AsyncIterable[AstraDBQueryResult]:\"\"\"Create an empty async iterable.\"\"\"ifFalse:yield"]]}, "111112113114", "asyncdefempty_async_iterable()->AsyncIterable[AstraDBQueryResult]:\"\"\"Create an empty async iterable.\"\"\"ifFalse:yield", "asyncdefempty_async_iterable()->AsyncIterable[AstraDBQueryResult]:\"\"\"Create an empty async iterable.\"\"\"ifFalse:yield"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "cassandra", "content": ["Provides an adapter for Cassandra vector store integration."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "CassandraAdapter", "content": ["CassandraAdapter(vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),)", "CassandraAdapter(vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),)", "Bases:ShreddedLangchainAdapter[Cassandra]", "ShreddedLangchainAdapter[Cassandra]", "Adapter for theApache Cassandravector store.", "This class integrates the LangChain Cassandra vector store with the graphretriever system, providing functionality for similarity search and documentretrieval.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The Cassandra vector store instance.TYPE:StoreT"], ["shredder", "An instance of the ShreddingTransformer used for doc insertion.If not passed then a default instance of ShreddingTransformer is used.TYPE:ShreddingTransformer| NoneDEFAULT:None"]]}, "vector_store", "The Cassandra vector store instance.", "TYPE:StoreT", "StoreT", "shredder", "An instance of the ShreddingTransformer used for doc insertion.If not passed then a default instance of ShreddingTransformer is used.", "TYPE:ShreddingTransformer| NoneDEFAULT:None", "ShreddingTransformer| None", "None", "Initialize the base adapter.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["380381382383384385386387388389", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields"]]}, "380381382383384385386387388389", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "aembed_queryasync", "content": ["async", "aembed_query(query:str)", "aembed_query(query:str)", "Return the embedding of the query.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["545556", "asyncdefaembed_query(self,query:str):\"\"\"Return the embedding of the query.\"\"\"returnawaitself._safe_embedding.aembed_query(query)"]]}, "545556", "asyncdefaembed_query(self,query:str):\"\"\"Return the embedding of the query.\"\"\"returnawaitself._safe_embedding.aembed_query(query)", "asyncdefaembed_query(self,query:str):\"\"\"Return the embedding of the query.\"\"\"returnawaitself._safe_embedding.aembed_query(query)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "embed_query", "content": ["embed_query(query:str)", "embed_query(query:str)", "Return the embedding of the query.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["505152", "defembed_query(self,query:str):\"\"\"Return the embedding of the query.\"\"\"returnself._safe_embedding.embed_query(query)"]]}, "505152", "defembed_query(self,query:str):\"\"\"Return the embedding of the query.\"\"\"returnself._safe_embedding.embed_query(query)", "defembed_query(self,query:str):\"\"\"Return the embedding of the query.\"\"\"returnself._safe_embedding.embed_query(query)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "format_documents_hook", "content": ["format_documents_hook(docs:list[Document],)->list[Content]", "format_documents_hook(docs:list[Document],)->list[Content]", "Format the documents as content after executing the query.", {"table": [["PARAMETER", "DESCRIPTION"], ["docs", "The documents returned from the vector storeTYPE:list[Document]"]]}, "docs", "The documents returned from the vector store", "TYPE:list[Document]", "list[Document]", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "The formatted content."]]}, "list[Content]", "The formatted content.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["767778798081828384858687888990", "defformat_documents_hook(self,docs:list[Document])->list[Content]:\"\"\"Format the documents as content after executing the query.Parameters----------docs :The documents returned from the vector storeReturns-------:The formatted content.\"\"\"return[doc_to_content(doc)fordocindocs]"]]}, "767778798081828384858687888990", "defformat_documents_hook(self,docs:list[Document])->list[Content]:\"\"\"Format the documents as content after executing the query.Parameters----------docs :The documents returned from the vector storeReturns-------:The formatted content.\"\"\"return[doc_to_content(doc)fordocindocs]", "defformat_documents_hook(self,docs:list[Document])->list[Content]:\"\"\"Format the documents as content after executing the query.Parameters----------docs :The documents returned from the vector storeReturns-------:The formatted content.\"\"\"return[doc_to_content(doc)fordocindocs]"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "update_filter_hook", "content": ["update_filter_hook(filter:dict[str,Any]|None,)->dict[str,Any]|None", "update_filter_hook(filter:dict[str,Any]|None,)->dict[str,Any]|None", "Update the metadata filter before executing the query.", {"table": [["PARAMETER", "DESCRIPTION"], ["filter", "Filter on the metadata to update.TYPE:dict[str,Any] | None"]]}, "filter", "Filter on the metadata to update.", "TYPE:dict[str,Any] | None", "dict[str,Any] | None", {"table": [["RETURNS", "DESCRIPTION"], ["dict[str,Any] | None", "The updated filter on the metadata to apply."]]}, "dict[str,Any] | None", "The updated filter on the metadata to apply.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["5859606162636465666768697071727374", "defupdate_filter_hook(self,filter:dict[str,Any]|None)->dict[str,Any]|None:\"\"\"Update the metadata filter before executing the query.Parameters----------filter :Filter on the metadata to update.Returns-------:The updated filter on the metadata to apply.\"\"\"returnfilter"]]}, "5859606162636465666768697071727374", "defupdate_filter_hook(self,filter:dict[str,Any]|None)->dict[str,Any]|None:\"\"\"Update the metadata filter before executing the query.Parameters----------filter :Filter on the metadata to update.Returns-------:The updated filter on the metadata to apply.\"\"\"returnfilter", "defupdate_filter_hook(self,filter:dict[str,Any]|None)->dict[str,Any]|None:\"\"\"Update the metadata filter before executing the query.Parameters----------filter :Filter on the metadata to update.Returns-------:The updated filter on the metadata to apply.\"\"\"returnfilter"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "chroma", "content": ["Provides an adapter for Chroma vector store integration."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "ChromaAdapter", "content": ["ChromaAdapter(vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),)", "ChromaAdapter(vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),)", "Bases:ShreddedLangchainAdapter[Chroma]", "ShreddedLangchainAdapter[Chroma]", "Adapter forChromavector store.", "This adapter integrates the LangChain Chroma vector store with thegraph retriever system, allowing for similarity search and document retrieval.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The Chroma vector store instance.TYPE:StoreT"], ["shredder", "An instance of the ShreddingTransformer used for doc insertion.If not passed then a default instance of ShreddingTransformer is used.TYPE:ShreddingTransformer| NoneDEFAULT:None"]]}, "vector_store", "The Chroma vector store instance.", "TYPE:StoreT", "StoreT", "shredder", "An instance of the ShreddingTransformer used for doc insertion.If not passed then a default instance of ShreddingTransformer is used.", "TYPE:ShreddingTransformer| NoneDEFAULT:None", "ShreddingTransformer| None", "None", "Initialize the base adapter.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["380381382383384385386387388389", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields"]]}, "380381382383384385386387388389", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "in_memory", "content": ["Provides an adapter for the InMemoryVectorStore integration."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "InMemoryAdapter", "content": ["InMemoryAdapter(vector_store:StoreT)", "InMemoryAdapter(vector_store:StoreT)", "Bases:LangchainAdapter[InMemoryVectorStore]", "LangchainAdapter[InMemoryVectorStore]", "Adapter for InMemoryVectorStore vector store.", "This adapter integrates the LangChain In-Memory vector store with the graphretriever system, enabling similarity search and document retrieval.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The in-memory vector store instance.TYPE:StoreT"]]}, "vector_store", "The in-memory vector store instance.", "TYPE:StoreT", "StoreT", "Initialize the base adapter.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["363738394041", "def__init__(self,vector_store:StoreT,):\"\"\"Initialize the base adapter.\"\"\"self.vector_store=vector_store"]]}, "363738394041", "def__init__(self,vector_store:StoreT,):\"\"\"Initialize the base adapter.\"\"\"self.vector_store=vector_store", "def__init__(self,vector_store:StoreT,):\"\"\"Initialize the base adapter.\"\"\"self.vector_store=vector_store"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "inference", "content": ["Infers the appropriate adapter for a given vector store."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "infer_adapter", "content": ["infer_adapter(store:Adapter|VectorStore)->Adapter", "infer_adapter(store:Adapter|VectorStore)->Adapter", "Dynamically infer the adapter for a given vector store.", "This function identifies the correct adapter based on the vector store typeand instantiates it with the provided arguments.", {"table": [["PARAMETER", "DESCRIPTION"], ["store", "The vector store instance.TYPE:Adapter|VectorStore"]]}, "store", "The vector store instance.", "TYPE:Adapter|VectorStore", "Adapter|VectorStore", {"table": [["RETURNS", "DESCRIPTION"], ["Adapter", "The initialized adapter for the given vector store."]]}, "Adapter", "The initialized adapter for the given vector store.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/inference.py", {"table": [["697071727374757677787980818283848586878889909192", "definfer_adapter(store:Adapter|VectorStore)->Adapter:\"\"\"Dynamically infer the adapter for a given vector store.This function identifies the correct adapter based on the vector store typeand instantiates it with the provided arguments.Parameters----------store :The vector store instance.Returns-------:The initialized adapter for the given vector store.\"\"\"ifisinstance(store,Adapter):returnstoremodule_name,class_name=_infer_adapter_name(store.__class__)adapter_module=importlib.import_module(module_name)adapter_class=getattr(adapter_module,class_name)returnadapter_class(store)"]]}, "697071727374757677787980818283848586878889909192", "definfer_adapter(store:Adapter|VectorStore)->Adapter:\"\"\"Dynamically infer the adapter for a given vector store.This function identifies the correct adapter based on the vector store typeand instantiates it with the provided arguments.Parameters----------store :The vector store instance.Returns-------:The initialized adapter for the given vector store.\"\"\"ifisinstance(store,Adapter):returnstoremodule_name,class_name=_infer_adapter_name(store.__class__)adapter_module=importlib.import_module(module_name)adapter_class=getattr(adapter_module,class_name)returnadapter_class(store)", "definfer_adapter(store:Adapter|VectorStore)->Adapter:\"\"\"Dynamically infer the adapter for a given vector store.This function identifies the correct adapter based on the vector store typeand instantiates it with the provided arguments.Parameters----------store :The vector store instance.Returns-------:The initialized adapter for the given vector store.\"\"\"ifisinstance(store,Adapter):returnstoremodule_name,class_name=_infer_adapter_name(store.__class__)adapter_module=importlib.import_module(module_name)adapter_class=getattr(adapter_module,class_name)returnadapter_class(store)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "langchain", "content": ["Defines the base class for vector store adapters."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "LangchainAdapter", "content": ["LangchainAdapter(vector_store:StoreT)", "LangchainAdapter(vector_store:StoreT)", "Bases:Generic[StoreT],Adapter", "Generic[StoreT]", "Adapter", "Base adapter for integrating vector stores with the graph retriever system.", "This class provides a foundation for custom adapters, enabling consistentinteraction with various vector store implementations.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The vector store instance.TYPE:StoreT"]]}, "vector_store", "The vector store instance.", "TYPE:StoreT", "StoreT", "Initialize the base adapter.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["363738394041", "def__init__(self,vector_store:StoreT,):\"\"\"Initialize the base adapter.\"\"\"self.vector_store=vector_store"]]}, "363738394041", "def__init__(self,vector_store:StoreT,):\"\"\"Initialize the base adapter.\"\"\"self.vector_store=vector_store", "def__init__(self,vector_store:StoreT,):\"\"\"Initialize the base adapter.\"\"\"self.vector_store=vector_store"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "ShreddedLangchainAdapter", "content": ["ShreddedLangchainAdapter(vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),)", "ShreddedLangchainAdapter(vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),)", "Bases:LangchainAdapter[StoreT]", "LangchainAdapter[StoreT]", "Base adapter for integrating vector stores with the graph retriever system.", "This class provides a foundation for custom adapters, enabling consistentinteraction with various vector store implementations that do not supportsearching on list-based metadata values.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The vector store instance.TYPE:StoreT"], ["shredder", "An instance of the ShreddingTransformer used for doc insertion.If not passed then a default instance of ShreddingTransformer is used.TYPE:ShreddingTransformer| NoneDEFAULT:None"], ["nested_metadata_fields", "The set of metadata fields that contain nested values.TYPE:set[str]DEFAULT:set()"]]}, "vector_store", "The vector store instance.", "TYPE:StoreT", "StoreT", "shredder", "An instance of the ShreddingTransformer used for doc insertion.If not passed then a default instance of ShreddingTransformer is used.", "TYPE:ShreddingTransformer| NoneDEFAULT:None", "ShreddingTransformer| None", "None", "nested_metadata_fields", "The set of metadata fields that contain nested values.", "TYPE:set[str]DEFAULT:set()", "set[str]", "set()", "Initialize the base adapter.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/langchain.py", {"table": [["380381382383384385386387388389", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields"]]}, "380381382383384385386387388389", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields", "def__init__(self,vector_store:StoreT,shredder:ShreddingTransformer|None=None,nested_metadata_fields:set[str]=set(),):\"\"\"Initialize the base adapter.\"\"\"super().__init__(vector_store=vector_store)self.shredder=ShreddingTransformer()ifshredderisNoneelseshredderself.nested_metadata_fields=nested_metadata_fields"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "open_search", "content": ["Provides an adapter for OpenSearch vector store integration."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/adapters/", "title": "OpenSearchAdapter", "content": ["OpenSearchAdapter(vector_store:OpenSearchVectorSearch)", "OpenSearchAdapter(vector_store:OpenSearchVectorSearch)", "Bases:LangchainAdapter[OpenSearchVectorSearch]", "LangchainAdapter[OpenSearchVectorSearch]", "Adapter to traverse OpenSearch vector stores.", "This adapter enables similarity search and document retrieval using anOpenSearch vector store.", {"table": [["PARAMETER", "DESCRIPTION"], ["vector_store", "The OpenSearch vector store instance.TYPE:OpenSearchVectorSearch"]]}, "vector_store", "The OpenSearch vector store instance.", "TYPE:OpenSearchVectorSearch", "OpenSearchVectorSearch", "Graph Traversal is only supported when using either the\"lucene\"or\"faiss\"engine.", "\"lucene\"", "\"faiss\"", "For more info, see theOpenSearch Documentation", "Initialize the base adapter.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/adapters/open_search.py", {"table": [["4142434445464748495051525354", "def__init__(self,vector_store:OpenSearchVectorSearch):ifvector_store.enginenotin[\"lucene\",\"faiss\"]:msg=(f\"Invalid engine for Traversal: '{self.vector_store.engine}'\"\" please instantiate the Open Search Vector Store with\"\" either the 'lucene' or 'faiss' engine\")raiseValueError(msg)super().__init__(vector_store)ifvector_store.is_aoss:self._id_field=\"id\"else:self._id_field=\"_id\""]]}, "4142434445464748495051525354", "def__init__(self,vector_store:OpenSearchVectorSearch):ifvector_store.enginenotin[\"lucene\",\"faiss\"]:msg=(f\"Invalid engine for Traversal: '{self.vector_store.engine}'\"\" please instantiate the Open Search Vector Store with\"\" either the 'lucene' or 'faiss' engine\")raiseValueError(msg)super().__init__(vector_store)ifvector_store.is_aoss:self._id_field=\"id\"else:self._id_field=\"_id\"", "def__init__(self,vector_store:OpenSearchVectorSearch):ifvector_store.enginenotin[\"lucene\",\"faiss\"]:msg=(f\"Invalid engine for Traversal: '{self.vector_store.engine}'\"\" please instantiate the Open Search Vector Store with\"\" either the 'lucene' or 'faiss' engine\")raiseValueError(msg)super().__init__(vector_store)ifvector_store.is_aoss:self._id_field=\"id\"else:self._id_field=\"_id\""]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Code Generation with GraphRAG", "content": []}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Introduction", "content": ["In this notebook, we demonstrate thatGraphRAG significantly outperforms standard vector-based retrievalfor generating working code from documentation. While traditional vector search retrieves relevant snippets, it often lacks the structured understanding needed to produce executable results. In contrast,GraphRAG enables the LLM to follow logical relationships within documentation, leading to functional code generation.", "We achieve this by leveraging a custom traversal strategy, selecting nodes that contain bothcode examples and descriptive text, allowing the LLM to assemble more complete responses."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Getting Started", "content": ["Below we will experiment with the AstraPy documentation to evaluate how well GraphRAG can generate working code.", "Using AstraDB as the vector store, we compare GraphRAG\u2019s structured retrieval with standard vector search to solve a specific coding task.The query we will be sending to the LLM is the following:", "query=\"\"\"Generate a function for connecting to an AstraDB cluster using the AstraPy library,and retrieve some rows from a collection. The number of rows to return should be aparameter on the method. Use Token Authentication. Assume the cluster is hosted onAstraDB. Include the necessary imports and any other necessary setup. The followingenvironment variables are available for your use:- `ASTRA_DB_API_ENDPOINT`: The Astra DB API endpoint.- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.- `ASTRA_DB_KEYSPACE`: The Astra DB keyspace.- `ASTRA_DB_COLLECTION`: The Astra DB collection.\"\\\"\"\"", "The following block will configure the environment from the Colab Secrets.To run it, you should have the following Colab Secrets defined and accessible to this notebook:", "OPENAI_API_KEY: The OpenAI key.", "ASTRA_DB_API_ENDPOINT: The Astra DB API endpoint.", "ASTRA_DB_APPLICATION_TOKEN: The Astra DB Application token.", "LANGCHAIN_API_KEY: Optional. If defined, will enable LangSmith tracing.", "ASTRA_DB_KEYSPACE: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default.", "OPENAI_API_KEY", "ASTRA_DB_API_ENDPOINT", "ASTRA_DB_APPLICATION_TOKEN", "LANGCHAIN_API_KEY", "ASTRA_DB_KEYSPACE", "If you don't yet have access to an AstraDB database, or need to check your credentials, see the helphere.", "# Install modules.%pipinstall\\langchain-core\\langchain-astradb\\langchain-openai\\langchain-graph-retriever\\graph-rag-example-helpers", "The last package --graph-rag-example-helpers-- includes the helpers and example documents that we will use in this notebook.", "graph-rag-example-helpers", "# Configure import paths.importosimportsysfromlangchain_core.documentsimportDocumentsys.path.append(\"../../\")# Initialize environment variables.fromgraph_rag_example_helpers.envimportEnvironment,initialize_environmentinitialize_environment(Environment.ASTRAPY)os.environ[\"LANGCHAIN_PROJECT\"]=\"code-generation\"os.environ[\"ASTRA_DB_COLLECTION\"]=\"code_generation\"defprint_doc_ids(docs:list[Document]):[print(f\"`{doc.id}` has example:{'example'indoc.metadata}\")fordocindocs]"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Part 1: Loading Data", "content": ["First, we'll demonstrate how to load the example AstraPy documentation intoAstraDBVectorStore. We will be creating a LangChain Document for every module, class, attribute, and function in the package.", "AstraDBVectorStore", "We will use the pydoc description field for thepage_contentfield in the document. Note that not every item in the package has a description. Because of this, there will be many documents that have no page content.", "page_content", "Besides the description, we will also include a bunch of extra information related to the item in themetadatafield. This info can include the item's name, kind, parameters, return type, base class, etc.", "metadata", "The item'sidwill be the items path in the package.", "id", "Below are two example documents... One with page content and one without."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Example doc with page content", "content": ["id:astrapy.client.DataAPIClientpage_content:|A client for using the Data API. This is the main entry point and sitsat the top of the conceptual \"client -> database -> collection\" hierarchy.A client is created first, optionally passing it a suitable Access Token.Starting from the client, then:- databases (Database and AsyncDatabase) are created for working with data- AstraDBAdmin objects can be created for admin-level workmetadata:name:DataAPIClientkind:classpath:astrapy.client.DataAPIClientparameters:token:|str | TokenProvider | None = Nonean Access Token to the database. Example: `\"AstraCS:xyz...\"`.This can be either a literal token string or a subclass of`astrapy.authentication.TokenProvider`.environment:|str | None = Nonea string representing the target Data API environment.It can be left unspecified for the default value of `Environment.PROD`;other values include `Environment.OTHER`, `Environment.DSE`.callers:|Sequence[CallerType] = []a list of caller identities, i.e. applications, or frameworks,on behalf of which Data API and DevOps API calls are performed.These end up in the request user-agent.Each caller identity is a (\"caller_name\", \"caller_version\") pair.example:|>>> from astrapy import DataAPIClient>>> my_client = DataAPIClient(\"AstraCS:...\")>>> my_db0 = my_client.get_database(...     \"https://01234567-....apps.astra.datastax.com\"... )>>> my_coll = my_db0.create_collection(\"movies\", dimension=2)>>> my_coll.insert_one({\"title\": \"The Title\", \"$vector\": [0.1, 0.3]})>>> my_db1 = my_client.get_database(\"01234567-...\")>>> my_db2 = my_client.get_database(\"01234567-...\", region=\"us-east1\")>>> my_adm0 = my_client.get_admin()>>> my_adm1 = my_client.get_admin(token=more_powerful_token_override)>>> database_list = my_adm0.list_databases()references:astrapy.client.DataAPIClientgathered_types:astrapy.constants.CallerTypeastrapy.authentication.TokenProvider", "This is the documentation forastrapy.client.DataAPIClientclass. Thepage_contentfield contains the description of the class, and themetadatafield contains the rest of the details, including example code of how to use the class.", "astrapy.client.DataAPIClient", "page_content", "metadata", "Thereferencesmetadata field contains the list of related items used in the example code block. Thegathered_typesfield contains the list of types from the parameters section. In GraphRAG, we can use these fields to link to other documents.", "references", "gathered_types"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Example doc without page content", "content": ["id:astrapy.admin.AstraDBAdmin.callerspage_content:\"\"metadata:name:callerspath:astrapy.admin.AstraDBAdmin.callerskind:attribute", "This is the documentation forastrapy.admin.AstraDBAdmin.callers. Thepage_contentfield is empty, and themetadatafield contains the details.", "astrapy.admin.AstraDBAdmin.callers", "page_content", "metadata", "Despite having no page content, this document can still be useful for Graph RAG.  We'll add aparentfield to the metadata at vector store insertion time to link it to the parent document:astrapy.admin.AstraDBAdmin, and we can use this for traversal.", "parent", "astrapy.admin.AstraDBAdmin"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Create the AstraDBVectorStore", "content": ["Next, we'll create the Vector Store we're going to load these documents into.In our case, we'll use DataStax Astra DB with Open AI embeddings.", "fromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsstore=AstraDBVectorStore(embedding=OpenAIEmbeddings(),collection_name=os.getenv(\"ASTRA_DB_COLLECTION\"),)"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Loading Data", "content": ["Now its time to load the data into our Vector Store. We'll use a helper method to download already prepared documents from thegraph-rag-example-helperspackage. If you want to see how these documents were created from the AstraPy package, see details in the Appendix.", "graph-rag-example-helpers", "We will use theParentTransformerto add a parent field to the metadata document field. This will allow us to traverse the graph from a child to its parent.", "ParentTransformer", "fromgraph_rag_example_helpers.datasets.astrapyimportfetch_documentsfromlangchain_graph_retriever.transformersimportParentTransformertransformer=ParentTransformer(path_delimiter=\".\")doc_ids=store.add_documents(transformer.transform_documents(fetch_documents()))", "We can retrieve a sample document to check if the parent field was added correctly:", "fromgraph_rag_example_helpers.examples.code_generationimportformat_documentprint(format_document(store.get_by_document_id(\"astrapy.admin.AstraDBAdmin.callers\"),debug=True))", "callers (attribute)path: astrapy.admin.AstraDBAdmin.callerscallers = callers_paramparent: astrapy.admin.AstraDBAdmin", "At this point, we've created a Vector Store with all the documents from the AstraPy documentation. Each document contains metadata about the module, class, attribute, or function, and the page content contains the description of the item.", "In the next section we'll see how to build relationships from the metadata in order to traverse through the documentation in a similar way to how a human would."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Part 2: Graph Traversal", "content": ["The GraphRAG library allows us to traverse through the documents in the Vector Store.  By changing theStrategy, we can control how the traversal is performed.", "Strategy"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Basic Traversal", "content": ["We'll start with the defaultEagerstrategy, which will traverse the graph in a breadth-first manner. In order to do this we need to set up the relationships between the documents. This is done by defining the \"edges\" between the documents.", "Eager", "In our case we will connect the \"references\", \"gathered_types\", \"parent\", \"implemented_by\", and \"bases\" fields in the metadata to the \"id\" field of the document they reference.", "edges=[(\"gathered_types\",\"$id\"),(\"references\",\"$id\"),(\"parent\",\"$id\"),(\"implemented_by\",\"$id\"),(\"bases\",\"$id\"),]", "Note that edges are directional, and indicate metadata fields by default.  The magic string$idis used to indicate the document's id.", "$id", "In the aboveedgeslist, any document id found ingathered_typeswill be connected to documents with the corresponding id. The other edges will work in a similar way.", "edges", "gathered_types", "Lets use these edges to create a LangChain retriever and documents for our query.", "fromlangchain_graph_retrieverimportGraphRetrieverdefault_retriever=GraphRetriever(store=store,edges=edges)print_doc_ids(default_retriever.invoke(query,select_k=6,start_k=3,max_depth=2))", "`astrapy.core.db.AsyncAstraDB.collection` has example: False`astrapy.core.db.AstraDB.collection` has example: False`astrapy.admin.DataAPIDatabaseAdmin.list_keyspaces` has example: True`astrapy.admin.DataAPIDatabaseAdmin` has example: True`astrapy.core.db.AsyncAstraDB` has example: False`astrapy.core.db.AstraDBCollection` has example: False", "Notes on the extra keyword args:", "select_kin GraphRAG is equivalent tokin LangChain. It specifies the number of nodes to select during retrieval.", "start_kindicates the number of nodes to select using standard vector retrieval before moving onto graph traversal.", "max_depthis the maximum depth to traverse in the graph.", "select_k", "k", "start_k", "max_depth", "With this configuration, we were only able to find 2 documents with example code."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Custom Strategy", "content": ["Now we will create a custom strategy that will traverse a larger portion of the graph and return the documents that contain code examples or descriptive text.", "To do this, we need to implement a class that inherits from the baseStrategyclass and overridesiterationmethod:", "Strategy", "iteration", "importdataclassesfromcollections.abcimportIterablefromgraph_retriever.strategiesimportNodeTracker,Strategyfromgraph_retriever.typesimportNode@dataclasses.dataclassclassCodeExamples(Strategy):# internal dictionary to store all nodes found during the traversal_nodes:dict[str,Node]=dataclasses.field(default_factory=dict)defiteration(self,*,nodes:Iterable[Node],tracker:NodeTracker)->None:# save all newly found nodes to the internal node dictionary for later useself._nodes.update({n.id:nforninnodes})# traverse the newly found nodesnew_count=tracker.traverse(nodes=nodes)# if no new nodes were found, we have reached the end of the traversalifnew_count==0:example_nodes=[]description_nodes=[]# iterate over all nodes and separate nodes with examples from nodes with# descriptionsfornodeinself._nodes.values():if\"example\"innode.metadata:example_nodes.append(node)elifnode.content!=\"\":description_nodes.append(node)# select the nodes with examples first and descriptions second# note: the base `finalize_nodes` method will truncate the list to the#   `select_k` number of nodestracker.select(example_nodes)tracker.select(description_nodes)", "As described in the comments above, this custom strategy will first try to select documents that contain code examples, and then will use documents that contain descriptive text.", "We can now use this custom strategy to build a custom retriever, and ask the query again:", "custom_retriever=GraphRetriever(store=store,edges=edges,strategy=CodeExamples())print_doc_ids(custom_retriever.invoke(query,select_k=6,start_k=3,max_depth=2))", "`astrapy.admin.DataAPIDatabaseAdmin.list_keyspaces` has example: True`astrapy.admin.DataAPIDatabaseAdmin` has example: True`astrapy.client.DataAPIClient` has example: True`astrapy.database.AsyncDatabase` has example: True`astrapy.database.Database` has example: True`astrapy.authentication.UsernamePasswordTokenProvider` has example: True", "Now we have found 6 documents with code examples! That is a significant improvement over the default strategy."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Step 3: Using GraphRAG to Generate Code", "content": ["We now use theCodeExamplesstrategy inside a Langchain pipeline to generate code snippets.", "CodeExamples", "We will also use a custom document formatter, which will format the document in a way that makes it look like standard documentation. In particular, it will format all the extra details stored in the metadata in a way that is easy to read.  This will help the LLM use the information in the documents to generate code.", "fromgraph_rag_example_helpers.examples.code_generationimportformat_docsfromlangchain.chat_modelsimportinit_chat_modelfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnablePassthroughllm=init_chat_model(\"gpt-4o-mini\",model_provider=\"openai\")prompt=ChatPromptTemplate.from_template(\"\"\"Generate a block of runnable python code using the following documentation asguidance. Return only the code. Don't include any example usage.Each documentation page is separated by three dashes (---) on its own line.If certain pages of the provided documentation aren't useful for answering thequestion, feel free to ignore them.Question: {question}Related Documentation:{context}\"\"\")graph_chain=({\"context\":custom_retriever|format_docs,\"question\":RunnablePassthrough()}|prompt|llm|StrOutputParser())print(graph_chain.invoke(query))", "```pythonimport osfrom astrapy.client import DataAPIClientfrom astrapy.collection import Collectiondef connect_and_retrieve_rows(num_rows):    api_endpoint = os.getenv('ASTRA_DB_API_ENDPOINT')    application_token = os.getenv('ASTRA_DB_APPLICATION_TOKEN')    keyspace = os.getenv('ASTRA_DB_KEYSPACE')    collection_name = os.getenv('ASTRA_DB_COLLECTION')    client = DataAPIClient(token=application_token)    database = client.get_database(api_endpoint)    collection = Collection(database=database, name=collection_name, keyspace=keyspace)    rows = collection.find(limit=num_rows)    return list(rows)```", "We can try running this generated code to see if it works:", "importosfromastrapy.clientimportDataAPIClientfromastrapy.collectionimportCollectiondefconnect_and_retrieve_rows(num_rows):api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\")application_token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")keyspace=os.getenv(\"ASTRA_DB_KEYSPACE\")collection_name=os.getenv(\"ASTRA_DB_COLLECTION\")client=DataAPIClient(token=application_token)database=client.get_database(api_endpoint)collection=Collection(database=database,name=collection_name,keyspace=keyspace)rows=collection.find(limit=num_rows)returnlist(rows)", "forrowinconnect_and_retrieve_rows(5):print(row)", "{'_id': 'astrapy.info.EmbeddingProviderAuthentication', 'content': 'A representation of an authentication mode for using an embedding model,\\nmodeling the corresponding part of the response returned by the\\n\\'findEmbeddingProviders\\' Data API endpoint (namely \"supportedAuthentication\").', 'metadata': {'kind': 'class', 'name': 'EmbeddingProviderAuthentication', 'path': 'astrapy.info.EmbeddingProviderAuthentication', 'parameters': [{'name': 'enabled', 'type': 'bool'}, {'name': 'tokens', 'type': 'list[EmbeddingProviderToken]'}], 'attributes': [{'name': 'enabled', 'type': 'bool', 'description': 'whether this authentication mode is available for a given model.'}, {'name': 'tokens', 'type': 'list[EmbeddingProviderToken]', 'description': 'a list of `EmbeddingProviderToken` objects,\\ndetailing the secrets required for the authentication mode.'}], 'gathered_types': ['EmbeddingProviderToken'], 'parent': 'astrapy.info'}}{'_id': 'astrapy.defaults.DEV_OPS_RESPONSE_HTTP_CREATED', 'content': '', 'metadata': {'kind': 'attribute', 'name': 'DEV_OPS_RESPONSE_HTTP_CREATED', 'path': 'astrapy.defaults.DEV_OPS_RESPONSE_HTTP_CREATED', 'value': 'DEV_OPS_RESPONSE_HTTP_CREATED = 201', 'parent': 'astrapy.defaults'}}{'_id': 'astrapy.info.CollectionInfo.full_name', 'content': '', 'metadata': {'kind': 'attribute', 'name': 'full_name', 'path': 'astrapy.info.CollectionInfo.full_name', 'value': 'full_name: str', 'parent': 'astrapy.info.CollectionInfo'}}{'_id': 'astrapy.collection.Collection.full_name', 'content': 'The fully-qualified collection name within the database,\\nin the form \"keyspace.collection_name\".', 'metadata': {'kind': 'attribute', 'name': 'full_name', 'path': 'astrapy.collection.Collection.full_name', 'value': 'full_name: str', 'example': \">>> my_coll.full_name\\n'default_keyspace.my_v_collection'\", 'parent': 'astrapy.collection.Collection'}}{'_id': 'astrapy.exceptions.DataAPIErrorDescriptor', 'content': 'An object representing a single error returned from the Data API,\\ntypically with an error code and a text message.\\nAn API request would return with an HTTP 200 success error code,\\nbut contain a nonzero amount of these.\\n\\nA single response from the Data API may return zero, one or more of these.\\nMoreover, some operations, such as an insert_many, may partally succeed\\nyet return these errors about the rest of the operation (such as,\\nsome of the input documents could not be inserted).', 'metadata': {'kind': 'class', 'name': 'DataAPIErrorDescriptor', 'path': 'astrapy.exceptions.DataAPIErrorDescriptor', 'parameters': [{'name': 'error_dict', 'type': 'dict[str, str]'}], 'attributes': [{'name': 'error_code', 'type': 'str | None', 'description': 'a string code as found in the API \"error\" item.'}, {'name': 'message', 'type': 'str | None', 'description': 'the text found in the API \"error\" item.'}, {'name': 'attributes', 'type': 'dict[str, Any]', 'description': 'a dict with any further key-value pairs returned by the API.'}], 'parent': 'astrapy.exceptions'}}"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Conclusion", "content": ["The results clearly demonstrate thatGraphRAG leads to functional code generation, while standard vector-based retrieval fails.", "In contrast, attempts usingonly an LLMorstandard vector-based RAGresulted inincomplete or non-functional outputs. The appendix includes examples illustrating these limitations.", "By structuring document relationships effectively,GraphRAG improves retrieval quality, enabling more reliable LLM-assisted code generation."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Appendix", "content": []}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "LLM Alone", "content": ["Here we show how to use the LLM alone to generate code for the query. We will use the same query as before, but modify the prompt to not include any context.", "llm_only_prompt=ChatPromptTemplate.from_template(\"\"\"Generate a block of runnable python code. Return only the code.Don't include any example usage.Question: {question}\"\"\")llm_only_chain=({\"question\":RunnablePassthrough()}|llm_only_prompt|llm|StrOutputParser())print(llm_only_chain.invoke(query))", "```pythonimport osfrom astra import AstraClientdef fetch_rows_from_astra_db(num_rows):    # Retrieve environment variables    api_endpoint = os.getenv(\"ASTRA_DB_API_ENDPOINT\")    application_token = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")    keyspace = os.getenv(\"ASTRA_DB_KEYSPACE\")    collection = os.getenv(\"ASTRA_DB_COLLECTION\")        # Initialize the Astra DB client    client = AstraClient(api_endpoint, application_token)        # Retrieve rows from the specified collection    query = f'SELECT * FROM {keyspace}.{collection} LIMIT {num_rows}'    response = client.execute_statement(query)        # Return the rows retrieved    return response['rows']```", "This code is not functional. The packageastraand the classAstraClientdo not exist.", "astra", "AstraClient"]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Standard RAG", "content": ["Here we show how to use the LLM with standard RAG to generate code for the query. We will use the same query and prompt as we did with GraphRAG.", "rag_chain=({\"context\":store.as_retriever(k=6)|format_docs,\"question\":RunnablePassthrough(),}|prompt|llm|StrOutputParser())print(rag_chain.invoke(query))", "```pythonimport osfrom astra import AstraClientdef fetch_rows_from_astradb(num_rows):    endpoint = os.getenv('ASTRA_DB_API_ENDPOINT')    token = os.getenv('ASTRA_DB_APPLICATION_TOKEN')    keyspace = os.getenv('ASTRA_DB_KEYSPACE')    collection = os.getenv('ASTRA_DB_COLLECTION')    client = AstraClient(        endpoint=endpoint,        token=token    )    query = f'SELECT * FROM {keyspace}.{collection} LIMIT {num_rows}'    response = client.execute(query)    return response['data']```", "This code is also not functional."]}
{"url": "https://datastax.github.io/graph-rag/examples/code-generation/", "title": "Converting AstraPy Documentation", "content": ["The AstraPy documentation was converted into a JSONL format via some custom code that is not included in this notebook. However, the code is available in thegraph-rag-example-helperspackagehere.", "graph-rag-example-helpers"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "graph_retriever.adapters", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "Adapter", "content": ["Adapter()", "Adapter()", "Bases:ABC", "ABC", "Base adapter for integrating vector stores with the graph retriever system.", "This class provides a foundation for custom adapters, enabling consistentinteraction with various vector store implementations.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["2425", "def__init__(self)->None:pass"]]}, "2425", "def__init__(self)->None:pass", "def__init__(self)->None:pass"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "aadjacentasync", "content": ["async", "aadjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "aadjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "Asynchronously return the content items with at least one matching edge.", {"table": [["PARAMETER", "DESCRIPTION"], ["edges", "The edges to look for.TYPE:set[Edge]"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "The number of relevant content items to select for the edges.TYPE:int"], ["filter", "Optional metadata to filter the results.TYPE:dict[str,Any] | None"], ["kwargs", "Keyword arguments to pass to the similarity search.TYPE:AnyDEFAULT:{}"]]}, "edges", "The edges to look for.", "TYPE:set[Edge]", "set[Edge]", "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "The number of relevant content items to select for the edges.", "TYPE:int", "int", "filter", "Optional metadata to filter the results.", "TYPE:dict[str,Any] | None", "dict[str,Any] | None", "kwargs", "Keyword arguments to pass to the similarity search.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["Iterable[Content]", "Iterable of adjacent content items."]]}, "Iterable[Content]", "Iterable of adjacent content items.", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If unsupported edge types are encountered."]]}, "ValueError", "If unsupported edge types are encountered.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356", "asyncdefaadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Asynchronously return the content items with at least one matching edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select for the edges.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"tasks=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):tasks.append(self.asearch(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,))elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:tasks.append(self.aget(ids,filter))results:list[Content]=[cforcompleted_taskinasyncio.as_completed(tasks)forcinawaitcompleted_task]returntop_k(results,embedding=query_embedding,k=k,)"]]}, "292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356", "asyncdefaadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Asynchronously return the content items with at least one matching edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select for the edges.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"tasks=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):tasks.append(self.asearch(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,))elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:tasks.append(self.aget(ids,filter))results:list[Content]=[cforcompleted_taskinasyncio.as_completed(tasks)forcinawaitcompleted_task]returntop_k(results,embedding=query_embedding,k=k,)", "asyncdefaadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Asynchronously return the content items with at least one matching edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select for the edges.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"tasks=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):tasks.append(self.asearch(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,))elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:tasks.append(self.aget(ids,filter))results:list[Content]=[cforcompleted_taskinasyncio.as_completed(tasks)forcinawaitcompleted_task]returntop_k(results,embedding=query_embedding,k=k,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "adjacent", "content": ["adjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "adjacent(edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]", "Return the content items with at least one matching incoming edge.", {"table": [["PARAMETER", "DESCRIPTION"], ["edges", "The edges to look for.TYPE:set[Edge]"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "The number of relevant content items to select.TYPE:int"], ["filter", "Optional metadata to filter the results.TYPE:dict[str,Any] | None"], ["kwargs", "Keyword arguments to pass to the similarity search.TYPE:AnyDEFAULT:{}"]]}, "edges", "The edges to look for.", "TYPE:set[Edge]", "set[Edge]", "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "The number of relevant content items to select.", "TYPE:int", "int", "filter", "Optional metadata to filter the results.", "TYPE:dict[str,Any] | None", "dict[str,Any] | None", "kwargs", "Keyword arguments to pass to the similarity search.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["Iterable[Content]", "Iterable of adjacent content items."]]}, "Iterable[Content]", "Iterable of adjacent content items.", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If unsupported edge types are encountered."]]}, "ValueError", "If unsupported edge types are encountered.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290", "defadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Return the content items with at least one matching incoming edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"results:list[Content]=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):docs=self.search(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,)results.extend(docs)elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:results.extend(self.get(ids,filter=filter))returntop_k(results,embedding=query_embedding,k=k,)"]]}, "232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290", "defadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Return the content items with at least one matching incoming edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"results:list[Content]=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):docs=self.search(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,)results.extend(docs)elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:results.extend(self.get(ids,filter=filter))returntop_k(results,embedding=query_embedding,k=k,)", "defadjacent(self,edges:set[Edge],query_embedding:list[float],k:int,filter:dict[str,Any]|None,**kwargs:Any,)->Iterable[Content]:\"\"\"Return the content items with at least one matching incoming edge.Parameters----------edges :The edges to look for.query_embedding :The query embedding used for selecting the most relevant content.k :The number of relevant content items to select.filter :Optional metadata to filter the results.kwargs :Keyword arguments to pass to the similarity search.Returns-------:Iterable of adjacent content items.Raises------ValueErrorIf unsupported edge types are encountered.\"\"\"results:list[Content]=[]ids=[]foredgeinedges:ifisinstance(edge,MetadataEdge):docs=self.search(embedding=query_embedding,k=k,filter=self._metadata_filter(base_filter=filter,edge=edge),**kwargs,)results.extend(docs)elifisinstance(edge,IdEdge):ids.append(edge.id)else:raiseValueError(f\"Unsupported edge:{edge}\")ifids:results.extend(self.get(ids,filter=filter))returntop_k(results,embedding=query_embedding,k=k,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "agetasync", "content": ["async", "aget(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "aget(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "Asynchronously get content items by ID.", "Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method shouldNOTraise exceptions if no content items are found for some IDs.", "Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.", {"table": [["PARAMETER", "DESCRIPTION"], ["ids", "List of IDs to get.TYPE:Sequence[str]"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments. These are up to the implementation.TYPE:AnyDEFAULT:{}"]]}, "ids", "List of IDs to get.", "TYPE:Sequence[str]", "Sequence[str]", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments. These are up to the implementation.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items that were found."]]}, "list[Content]", "List of content items that were found.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230", "asyncdefaget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Asynchronously get content items by ID.Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method should **NOT**raise exceptions if no content items are found for some IDs.Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.Parameters----------ids :List of IDs to get.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments. These are up to the implementation.Returns-------:List of content items that were found.\"\"\"returnawaitrun_in_executor(None,self.get,ids,filter,**kwargs,)"]]}, "193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230", "asyncdefaget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Asynchronously get content items by ID.Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method should **NOT**raise exceptions if no content items are found for some IDs.Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.Parameters----------ids :List of IDs to get.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments. These are up to the implementation.Returns-------:List of content items that were found.\"\"\"returnawaitrun_in_executor(None,self.get,ids,filter,**kwargs,)", "asyncdefaget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Asynchronously get content items by ID.Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method should **NOT**raise exceptions if no content items are found for some IDs.Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.Parameters----------ids :List of IDs to get.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments. These are up to the implementation.Returns-------:List of content items that were found.\"\"\"returnawaitrun_in_executor(None,self.get,ids,filter,**kwargs,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "asearchasync", "content": ["async", "asearch(embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "asearch(embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "Asynchronously return content items most similar to the query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items most similar to the query vector."]]}, "list[Content]", "List of content items most similar to the query vector.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157", "asyncdefasearch(self,embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Asynchronously return content items most similar to the query vector.Parameters----------embedding :The query embedding used for selecting the most relevant content.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of content items most similar to the query vector.\"\"\"returnawaitrun_in_executor(None,self.search,embedding,k,filter,**kwargs,)"]]}, "124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157", "asyncdefasearch(self,embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Asynchronously return content items most similar to the query vector.Parameters----------embedding :The query embedding used for selecting the most relevant content.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of content items most similar to the query vector.\"\"\"returnawaitrun_in_executor(None,self.search,embedding,k,filter,**kwargs,)", "asyncdefasearch(self,embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Asynchronously return content items most similar to the query vector.Parameters----------embedding :The query embedding used for selecting the most relevant content.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of content items most similar to the query vector.\"\"\"returnawaitrun_in_executor(None,self.search,embedding,k,filter,**kwargs,)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "asearch_with_embeddingasync", "content": ["async", "asearch_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "asearch_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "Asynchronously return content items most similar to the query.", "Also returns the embedded query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["query", "Input text.TYPE:str"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "query", "Input text.", "TYPE:str", "str", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["contents", "List of up tokcontent items most similar to the queryvector.TYPE:list[Content]"]]}, "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "contents", "List of up tokcontent items most similar to the queryvector.", "k", "TYPE:list[Content]", "list[Content]", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["60616263646566676869707172737475767778798081828384858687888990919293", "asyncdefasearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:\"\"\"Asynchronously return content items most similar to the query.Also returns the embedded query vector.Parameters----------query :Input text.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------query_embedding :The query embedding used for selecting the most relevant content.contents :List of up to `k` content items most similar to the queryvector.\"\"\"returnawaitrun_in_executor(None,self.search_with_embedding,query,k,filter,**kwargs)"]]}, "60616263646566676869707172737475767778798081828384858687888990919293", "asyncdefasearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:\"\"\"Asynchronously return content items most similar to the query.Also returns the embedded query vector.Parameters----------query :Input text.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------query_embedding :The query embedding used for selecting the most relevant content.contents :List of up to `k` content items most similar to the queryvector.\"\"\"returnawaitrun_in_executor(None,self.search_with_embedding,query,k,filter,**kwargs)", "asyncdefasearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:\"\"\"Asynchronously return content items most similar to the query.Also returns the embedded query vector.Parameters----------query :Input text.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------query_embedding :The query embedding used for selecting the most relevant content.contents :List of up to `k` content items most similar to the queryvector.\"\"\"returnawaitrun_in_executor(None,self.search_with_embedding,query,k,filter,**kwargs)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "getabstractmethod", "content": ["abstractmethod", "get(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "get(ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "Get content items by ID.", "Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method shouldNOTraise exceptions if no content items are found for some IDs.", "Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.", {"table": [["PARAMETER", "DESCRIPTION"], ["ids", "List of IDs to get.TYPE:Sequence[str]"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments. These are up to the implementation.TYPE:AnyDEFAULT:{}"]]}, "ids", "List of IDs to get.", "TYPE:Sequence[str]", "Sequence[str]", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments. These are up to the implementation.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items that were found."]]}, "list[Content]", "List of content items that were found.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191", "@abc.abstractmethoddefget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Get content items by ID.Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method should **NOT**raise exceptions if no content items are found for some IDs.Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.Parameters----------ids :List of IDs to get.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments. These are up to the implementation.Returns-------:List of content items that were found.\"\"\"..."]]}, "159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191", "@abc.abstractmethoddefget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Get content items by ID.Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method should **NOT**raise exceptions if no content items are found for some IDs.Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.Parameters----------ids :List of IDs to get.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments. These are up to the implementation.Returns-------:List of content items that were found.\"\"\"...", "@abc.abstractmethoddefget(self,ids:Sequence[str],filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Get content items by ID.Fewer content items may be returned than requested if some IDs arenot found or if there are duplicated IDs. This method should **NOT**raise exceptions if no content items are found for some IDs.Users should not assume that the order of the returned content itemsmatches  the order of the input IDs. Instead, users should rely onthe ID field of the returned content items.Parameters----------ids :List of IDs to get.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments. These are up to the implementation.Returns-------:List of content items that were found.\"\"\"..."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "searchabstractmethod", "content": ["abstractmethod", "search(embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "search(embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]", "Return content items most similar to the query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "List of content items most similar to the query vector."]]}, "list[Content]", "List of content items most similar to the query vector.", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["9596979899100101102103104105106107108109110111112113114115116117118119120121122", "@abc.abstractmethoddefsearch(self,embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Return content items most similar to the query vector.Parameters----------embedding :The query embedding used for selecting the most relevant content.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of content items most similar to the query vector.\"\"\"..."]]}, "9596979899100101102103104105106107108109110111112113114115116117118119120121122", "@abc.abstractmethoddefsearch(self,embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Return content items most similar to the query vector.Parameters----------embedding :The query embedding used for selecting the most relevant content.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of content items most similar to the query vector.\"\"\"...", "@abc.abstractmethoddefsearch(self,embedding:list[float],k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->list[Content]:\"\"\"Return content items most similar to the query vector.Parameters----------embedding :The query embedding used for selecting the most relevant content.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------:List of content items most similar to the query vector.\"\"\"..."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/adapters/", "title": "search_with_embeddingabstractmethod", "content": ["abstractmethod", "search_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "search_with_embedding(query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]", "Return content items most similar to the query.", "Also returns the embedded query vector.", {"table": [["PARAMETER", "DESCRIPTION"], ["query", "Input text.TYPE:str"], ["k", "Number of content items to return.TYPE:intDEFAULT:4"], ["filter", "Filter on the metadata to apply.TYPE:dict[str,Any] | NoneDEFAULT:None"], ["kwargs", "Additional keyword arguments.TYPE:AnyDEFAULT:{}"]]}, "query", "Input text.", "TYPE:str", "str", "k", "Number of content items to return.", "TYPE:intDEFAULT:4", "int", "4", "filter", "Filter on the metadata to apply.", "TYPE:dict[str,Any] | NoneDEFAULT:None", "dict[str,Any] | None", "None", "kwargs", "Additional keyword arguments.", "TYPE:AnyDEFAULT:{}", "Any", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["query_embedding", "The query embedding used for selecting the most relevant content.TYPE:list[float]"], ["contents", "List of up tokcontent items most similar to the query vector.TYPE:list[Content]"]]}, "query_embedding", "The query embedding used for selecting the most relevant content.", "TYPE:list[float]", "list[float]", "contents", "List of up tokcontent items most similar to the query vector.", "k", "TYPE:list[Content]", "list[Content]", "packages/graph-retriever/src/graph_retriever/adapters/base.py", {"table": [["2728293031323334353637383940414243444546474849505152535455565758", "@abc.abstractmethoddefsearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:\"\"\"Return content items most similar to the query.Also returns the embedded query vector.Parameters----------query :Input text.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------query_embedding :The query embedding used for selecting the most relevant content.contents :List of up to `k` content items most similar to the query vector.\"\"\"..."]]}, "2728293031323334353637383940414243444546474849505152535455565758", "@abc.abstractmethoddefsearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:\"\"\"Return content items most similar to the query.Also returns the embedded query vector.Parameters----------query :Input text.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------query_embedding :The query embedding used for selecting the most relevant content.contents :List of up to `k` content items most similar to the query vector.\"\"\"...", "@abc.abstractmethoddefsearch_with_embedding(self,query:str,k:int=4,filter:dict[str,Any]|None=None,**kwargs:Any,)->tuple[list[float],list[Content]]:\"\"\"Return content items most similar to the query.Also returns the embedded query vector.Parameters----------query :Input text.k :Number of content items to return.filter :Filter on the metadata to apply.kwargs :Additional keyword arguments.Returns-------query_embedding :The query embedding used for selecting the most relevant content.contents :List of up to `k` content items most similar to the query vector.\"\"\"..."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "graph_rag_example_helpers", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "datasets", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "animals", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "fetch_documents", "content": ["fetch_documents()->list[Document]", "fetch_documents()->list[Document]", "Download and parse a list of Documents for use with Graph Retriever.", "This dataset contains the documentation for the AstraPy project as of version 1.5.2.", "This method downloads the dataset each time -- generally it is preferableto invoke this only once and store the documents in memory or a vectorstore.", {"table": [["RETURNS", "DESCRIPTION"], ["list[Document]", "The fetched astra-py documentation Documents."]]}, "list[Document]", "The fetched astra-py documentation Documents.", "The dataset is setup in a way where the path of the item is theid, the pydocdescription is thepage_content, and the items other attributes are stored in themetadata.", "There are many documents that contain an id and metadata, but no page_content.", "id", "page_content", "metadata", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/datasets/astrapy/fetch.py", {"table": [["91011121314151617181920212223242526272829303132333435363738", "deffetch_documents()->list[Document]:\"\"\"Download and parse a list of Documents for use with Graph Retriever.This dataset contains the documentation for the AstraPy project as of version 1.5.2.This method downloads the dataset each time -- generally it is preferableto invoke this only once and store the documents in memory or a vectorstore.Returns-------:The fetched astra-py documentation Documents.Notes------ The dataset is setup in a way where the path of the item is the `id`, the pydocdescription is the `page_content`, and the items other attributes are stored in the`metadata`.- There are many documents that contain an id and metadata, but no page_content.\"\"\"response=requests.get(ASTRAPY_JSONL_URL)response.raise_for_status()# Ensure we got a valid responsereturn[Document(id=data[\"id\"],page_content=data[\"text\"],metadata=data[\"metadata\"])forlineinresponse.text.splitlines()if(data:=json.loads(line))]"]]}, "91011121314151617181920212223242526272829303132333435363738", "deffetch_documents()->list[Document]:\"\"\"Download and parse a list of Documents for use with Graph Retriever.This dataset contains the documentation for the AstraPy project as of version 1.5.2.This method downloads the dataset each time -- generally it is preferableto invoke this only once and store the documents in memory or a vectorstore.Returns-------:The fetched astra-py documentation Documents.Notes------ The dataset is setup in a way where the path of the item is the `id`, the pydocdescription is the `page_content`, and the items other attributes are stored in the`metadata`.- There are many documents that contain an id and metadata, but no page_content.\"\"\"response=requests.get(ASTRAPY_JSONL_URL)response.raise_for_status()# Ensure we got a valid responsereturn[Document(id=data[\"id\"],page_content=data[\"text\"],metadata=data[\"metadata\"])forlineinresponse.text.splitlines()if(data:=json.loads(line))]", "deffetch_documents()->list[Document]:\"\"\"Download and parse a list of Documents for use with Graph Retriever.This dataset contains the documentation for the AstraPy project as of version 1.5.2.This method downloads the dataset each time -- generally it is preferableto invoke this only once and store the documents in memory or a vectorstore.Returns-------:The fetched astra-py documentation Documents.Notes------ The dataset is setup in a way where the path of the item is the `id`, the pydocdescription is the `page_content`, and the items other attributes are stored in the`metadata`.- There are many documents that contain an id and metadata, but no page_content.\"\"\"response=requests.get(ASTRAPY_JSONL_URL)response.raise_for_status()# Ensure we got a valid responsereturn[Document(id=data[\"id\"],page_content=data[\"text\"],metadata=data[\"metadata\"])forlineinresponse.text.splitlines()if(data:=json.loads(line))]"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "fetch", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "astrapy", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "wikimultihop", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "BatchPreparermodule-attribute", "content": ["module-attribute", "BatchPreparer=Callable[[Iterator[bytes]],Iterator[Document]]", "BatchPreparer=Callable[[Iterator[bytes]],Iterator[Document]]", "Function to apply to batches of lines to produce the document."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "aload_2wikimultihopasync", "content": ["async", "aload_2wikimultihop(limit:int|None,*,full_para_with_hyperlink_zip_path:str,store:VectorStore,batch_prepare:BatchPreparer,)->None", "aload_2wikimultihop(limit:int|None,*,full_para_with_hyperlink_zip_path:str,store:VectorStore,batch_prepare:BatchPreparer,)->None", "Load 2wikimultihop data into the givenVectorStore.", "VectorStore", {"table": [["PARAMETER", "DESCRIPTION"], ["limit", "Maximum number of lines to load.If a number less than one thousand, limits loading to the given number of lines.IfNone, loads all content.TYPE:int| None"], ["full_para_with_hyperlink_zip_path", "Path topara_with_hyperlink.zipdownloaded following the instructionsin2wikimultihop.TYPE:str"], ["store", "The VectorStore to populate.TYPE:VectorStore"], ["batch_prepare", "Function to apply to batches of lines to produce the document.TYPE:BatchPreparer"]]}, "limit", "Maximum number of lines to load.If a number less than one thousand, limits loading to the given number of lines.IfNone, loads all content.", "None", "TYPE:int| None", "int| None", "full_para_with_hyperlink_zip_path", "Path topara_with_hyperlink.zipdownloaded following the instructionsin2wikimultihop.", "para_with_hyperlink.zip", "TYPE:str", "str", "store", "The VectorStore to populate.", "TYPE:VectorStore", "VectorStore", "batch_prepare", "Function to apply to batches of lines to produce the document.", "TYPE:BatchPreparer", "BatchPreparer", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/datasets/wikimultihop/load.py", {"table": [["60616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178", "asyncdefaload_2wikimultihop(limit:int|None,*,full_para_with_hyperlink_zip_path:str,store:VectorStore,batch_prepare:BatchPreparer,)->None:\"\"\"Load 2wikimultihop data into the given `VectorStore`.Parameters----------limit :Maximum number of lines to load.If a number less than one thousand, limits loading to the given number of lines.If `None`, loads all content.full_para_with_hyperlink_zip_path :Path to `para_with_hyperlink.zip` downloaded following the instructionsin[2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).store :The VectorStore to populate.batch_prepare :Function to apply to batches of lines to produce the document.\"\"\"iflimitisNoneorlimit>LINES_IN_FILE:limit=LINES_IN_FILEiflimit<=1000:local_path=\"../../data/para_with_hyperlink_short.jsonl\"ifos.path.isfile(local_path):forbatchinbatched(itertools.islice(open(local_path,\"rb\").readlines(),limit),BATCH_SIZE):docs=batch_prepare(iter(batch))store.add_documents(list(docs))print(f\"Loaded from{local_path}\")# noqa: T201else:print(f\"{local_path}not found, fetching short dataset\")# noqa: T201response=requests.get(SHORT_URL)response.raise_for_status()# Ensure we get a valid responseforbatchinbatched(itertools.islice(response.content.splitlines(),limit),BATCH_SIZE):docs=batch_prepare(iter(batch))store.add_documents(list(docs))print(f\"Loaded from{SHORT_URL}\")# noqa: T201returnassertos.path.isfile(full_para_with_hyperlink_zip_path)persistence=PersistentIteration(journal_name=\"load_2wikimultihop.jrnl\",iterator=batched(itertools.islice(wikipedia_lines(full_para_with_hyperlink_zip_path),limit),BATCH_SIZE,),)total_batches=ceil(limit/BATCH_SIZE)-persistence.completed_count()ifpersistence.completed_count()>0:print(# noqa: T201f\"Resuming loading with{persistence.completed_count()}\"f\" completed,{total_batches}remaining\")@backoff.on_exception(backoff.expo,EXCEPTIONS_TO_RETRY,max_tries=MAX_RETRIES,)asyncdefadd_docs(batch_docs,offset)->None:fromastrapy.exceptionsimportCollectionInsertManyExceptiontry:awaitstore.aadd_documents(batch_docs)persistence.ack(offset)exceptCollectionInsertManyExceptionaserr:forexpinerr.exceptions:exp_desc=str(exp)if\"DOCUMENT_ALREADY_EXISTS\"notinexp_desc:print(exp_desc)# noqa: T201raise# We can't use asyncio.TaskGroup in 3.10. This would be simpler with that.tasks:list[asyncio.Task]=[]foroffset,batch_linesintqdm(persistence,total=total_batches):batch_docs=batch_prepare(batch_lines)ifbatch_docs:task=asyncio.create_task(add_docs(batch_docs,offset))# It is OK if tasks are lost upon failure since that means we're# aborting the loading.tasks.append(task)whilelen(tasks)>=MAX_IN_FLIGHT:completed,pending=awaitasyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED)forcompleteincompleted:if(e:=complete.exception())isnotNone:print(f\"Exception in task:{e}\")# noqa: T201tasks=list(pending)else:persistence.ack(offset)# Make sure all the tasks are done.# This wouldn't be necessary if we used a taskgroup, but that is Python 3.11+.whilelen(tasks)>0:completed,pending=awaitasyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED)forcompleteincompleted:if(e:=complete.exception())isnotNone:print(f\"Exception in task:{e}\")# noqa: T201tasks=list(pending)assertlen(tasks)==0assertpersistence.pending_count()==0"]]}, "60616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178", "asyncdefaload_2wikimultihop(limit:int|None,*,full_para_with_hyperlink_zip_path:str,store:VectorStore,batch_prepare:BatchPreparer,)->None:\"\"\"Load 2wikimultihop data into the given `VectorStore`.Parameters----------limit :Maximum number of lines to load.If a number less than one thousand, limits loading to the given number of lines.If `None`, loads all content.full_para_with_hyperlink_zip_path :Path to `para_with_hyperlink.zip` downloaded following the instructionsin[2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).store :The VectorStore to populate.batch_prepare :Function to apply to batches of lines to produce the document.\"\"\"iflimitisNoneorlimit>LINES_IN_FILE:limit=LINES_IN_FILEiflimit<=1000:local_path=\"../../data/para_with_hyperlink_short.jsonl\"ifos.path.isfile(local_path):forbatchinbatched(itertools.islice(open(local_path,\"rb\").readlines(),limit),BATCH_SIZE):docs=batch_prepare(iter(batch))store.add_documents(list(docs))print(f\"Loaded from{local_path}\")# noqa: T201else:print(f\"{local_path}not found, fetching short dataset\")# noqa: T201response=requests.get(SHORT_URL)response.raise_for_status()# Ensure we get a valid responseforbatchinbatched(itertools.islice(response.content.splitlines(),limit),BATCH_SIZE):docs=batch_prepare(iter(batch))store.add_documents(list(docs))print(f\"Loaded from{SHORT_URL}\")# noqa: T201returnassertos.path.isfile(full_para_with_hyperlink_zip_path)persistence=PersistentIteration(journal_name=\"load_2wikimultihop.jrnl\",iterator=batched(itertools.islice(wikipedia_lines(full_para_with_hyperlink_zip_path),limit),BATCH_SIZE,),)total_batches=ceil(limit/BATCH_SIZE)-persistence.completed_count()ifpersistence.completed_count()>0:print(# noqa: T201f\"Resuming loading with{persistence.completed_count()}\"f\" completed,{total_batches}remaining\")@backoff.on_exception(backoff.expo,EXCEPTIONS_TO_RETRY,max_tries=MAX_RETRIES,)asyncdefadd_docs(batch_docs,offset)->None:fromastrapy.exceptionsimportCollectionInsertManyExceptiontry:awaitstore.aadd_documents(batch_docs)persistence.ack(offset)exceptCollectionInsertManyExceptionaserr:forexpinerr.exceptions:exp_desc=str(exp)if\"DOCUMENT_ALREADY_EXISTS\"notinexp_desc:print(exp_desc)# noqa: T201raise# We can't use asyncio.TaskGroup in 3.10. This would be simpler with that.tasks:list[asyncio.Task]=[]foroffset,batch_linesintqdm(persistence,total=total_batches):batch_docs=batch_prepare(batch_lines)ifbatch_docs:task=asyncio.create_task(add_docs(batch_docs,offset))# It is OK if tasks are lost upon failure since that means we're# aborting the loading.tasks.append(task)whilelen(tasks)>=MAX_IN_FLIGHT:completed,pending=awaitasyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED)forcompleteincompleted:if(e:=complete.exception())isnotNone:print(f\"Exception in task:{e}\")# noqa: T201tasks=list(pending)else:persistence.ack(offset)# Make sure all the tasks are done.# This wouldn't be necessary if we used a taskgroup, but that is Python 3.11+.whilelen(tasks)>0:completed,pending=awaitasyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED)forcompleteincompleted:if(e:=complete.exception())isnotNone:print(f\"Exception in task:{e}\")# noqa: T201tasks=list(pending)assertlen(tasks)==0assertpersistence.pending_count()==0", "asyncdefaload_2wikimultihop(limit:int|None,*,full_para_with_hyperlink_zip_path:str,store:VectorStore,batch_prepare:BatchPreparer,)->None:\"\"\"Load 2wikimultihop data into the given `VectorStore`.Parameters----------limit :Maximum number of lines to load.If a number less than one thousand, limits loading to the given number of lines.If `None`, loads all content.full_para_with_hyperlink_zip_path :Path to `para_with_hyperlink.zip` downloaded following the instructionsin[2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).store :The VectorStore to populate.batch_prepare :Function to apply to batches of lines to produce the document.\"\"\"iflimitisNoneorlimit>LINES_IN_FILE:limit=LINES_IN_FILEiflimit<=1000:local_path=\"../../data/para_with_hyperlink_short.jsonl\"ifos.path.isfile(local_path):forbatchinbatched(itertools.islice(open(local_path,\"rb\").readlines(),limit),BATCH_SIZE):docs=batch_prepare(iter(batch))store.add_documents(list(docs))print(f\"Loaded from{local_path}\")# noqa: T201else:print(f\"{local_path}not found, fetching short dataset\")# noqa: T201response=requests.get(SHORT_URL)response.raise_for_status()# Ensure we get a valid responseforbatchinbatched(itertools.islice(response.content.splitlines(),limit),BATCH_SIZE):docs=batch_prepare(iter(batch))store.add_documents(list(docs))print(f\"Loaded from{SHORT_URL}\")# noqa: T201returnassertos.path.isfile(full_para_with_hyperlink_zip_path)persistence=PersistentIteration(journal_name=\"load_2wikimultihop.jrnl\",iterator=batched(itertools.islice(wikipedia_lines(full_para_with_hyperlink_zip_path),limit),BATCH_SIZE,),)total_batches=ceil(limit/BATCH_SIZE)-persistence.completed_count()ifpersistence.completed_count()>0:print(# noqa: T201f\"Resuming loading with{persistence.completed_count()}\"f\" completed,{total_batches}remaining\")@backoff.on_exception(backoff.expo,EXCEPTIONS_TO_RETRY,max_tries=MAX_RETRIES,)asyncdefadd_docs(batch_docs,offset)->None:fromastrapy.exceptionsimportCollectionInsertManyExceptiontry:awaitstore.aadd_documents(batch_docs)persistence.ack(offset)exceptCollectionInsertManyExceptionaserr:forexpinerr.exceptions:exp_desc=str(exp)if\"DOCUMENT_ALREADY_EXISTS\"notinexp_desc:print(exp_desc)# noqa: T201raise# We can't use asyncio.TaskGroup in 3.10. This would be simpler with that.tasks:list[asyncio.Task]=[]foroffset,batch_linesintqdm(persistence,total=total_batches):batch_docs=batch_prepare(batch_lines)ifbatch_docs:task=asyncio.create_task(add_docs(batch_docs,offset))# It is OK if tasks are lost upon failure since that means we're# aborting the loading.tasks.append(task)whilelen(tasks)>=MAX_IN_FLIGHT:completed,pending=awaitasyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED)forcompleteincompleted:if(e:=complete.exception())isnotNone:print(f\"Exception in task:{e}\")# noqa: T201tasks=list(pending)else:persistence.ack(offset)# Make sure all the tasks are done.# This wouldn't be necessary if we used a taskgroup, but that is Python 3.11+.whilelen(tasks)>0:completed,pending=awaitasyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED)forcompleteincompleted:if(e:=complete.exception())isnotNone:print(f\"Exception in task:{e}\")# noqa: T201tasks=list(pending)assertlen(tasks)==0assertpersistence.pending_count()==0"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "load", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "wikipedia_lines", "content": ["wikipedia_lines(para_with_hyperlink_zip_path:str,)->Iterable[bytes]", "wikipedia_lines(para_with_hyperlink_zip_path:str,)->Iterable[bytes]", "Return iterable of lines from the wikipedia file.", {"table": [["PARAMETER", "DESCRIPTION"], ["para_with_hyperlink_zip_path", "Path topara_with_hyperlink.zipdownloaded following the instructionsin2wikimultihop.TYPE:str"]]}, "para_with_hyperlink_zip_path", "Path topara_with_hyperlink.zipdownloaded following the instructionsin2wikimultihop.", "para_with_hyperlink.zip", "TYPE:str", "str", {"table": [["YIELDS", "DESCRIPTION"], ["str", "Lines from the Wikipedia file."]]}, "str", "Lines from the Wikipedia file.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/datasets/wikimultihop/load.py", {"table": [["23242526272829303132333435363738394041", "defwikipedia_lines(para_with_hyperlink_zip_path:str)->Iterable[bytes]:\"\"\"Return iterable of lines from the wikipedia file.Parameters----------para_with_hyperlink_zip_path :Path to `para_with_hyperlink.zip` downloaded following the instructionsin[2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).Yields------strLines from the Wikipedia file.\"\"\"withzipfile.ZipFile(para_with_hyperlink_zip_path,\"r\")asarchive:witharchive.open(\"para_with_hyperlink.jsonl\",\"r\")aspara_with_hyperlink:yield frompara_with_hyperlink"]]}, "23242526272829303132333435363738394041", "defwikipedia_lines(para_with_hyperlink_zip_path:str)->Iterable[bytes]:\"\"\"Return iterable of lines from the wikipedia file.Parameters----------para_with_hyperlink_zip_path :Path to `para_with_hyperlink.zip` downloaded following the instructionsin[2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).Yields------strLines from the Wikipedia file.\"\"\"withzipfile.ZipFile(para_with_hyperlink_zip_path,\"r\")asarchive:witharchive.open(\"para_with_hyperlink.jsonl\",\"r\")aspara_with_hyperlink:yield frompara_with_hyperlink", "defwikipedia_lines(para_with_hyperlink_zip_path:str)->Iterable[bytes]:\"\"\"Return iterable of lines from the wikipedia file.Parameters----------para_with_hyperlink_zip_path :Path to `para_with_hyperlink.zip` downloaded following the instructionsin[2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).Yields------strLines from the Wikipedia file.\"\"\"withzipfile.ZipFile(para_with_hyperlink_zip_path,\"r\")asarchive:witharchive.open(\"para_with_hyperlink.jsonl\",\"r\")aspara_with_hyperlink:yield frompara_with_hyperlink"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "env", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "NON_SECRETSmodule-attribute", "content": ["module-attribute", "NON_SECRETS={\"ASTRA_DB_API_ENDPOINT\",\"ASTRA_DB_DATABASE_ID\",}", "NON_SECRETS={\"ASTRA_DB_API_ENDPOINT\",\"ASTRA_DB_DATABASE_ID\",}", "Environment variables that can useinputinstead ofgetpass.", "input", "getpass"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "Environment", "content": ["Bases:Enum", "Enum", "Enumeration of supported environments for examples."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "ASTRAPYclass-attributeinstance-attribute", "content": ["class-attribute", "instance-attribute", "ASTRAPY=auto()", "ASTRAPY=auto()", "Environment variables for connecting to AstraDB via AstraPy"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "CASSIOclass-attributeinstance-attribute", "content": ["class-attribute", "instance-attribute", "CASSIO=auto()", "CASSIO=auto()", "Environment variables for connecting to AstraDB via CassIO"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "required_envvars", "content": ["required_envvars()->list[str]", "required_envvars()->list[str]", "Return the required environment variables for this environment.", {"table": [["RETURNS", "DESCRIPTION"], ["list[str]", "The environment variables required in this environment."]]}, "list[str]", "The environment variables required in this environment.", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If the environment isn't recognized."]]}, "ValueError", "If the environment isn't recognized.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/env.py", {"table": [["16171819202122232425262728293031323334353637", "defrequired_envvars(self)->list[str]:\"\"\"Return the required environment variables for this environment.Returns-------:The environment variables required in this environment.Raises------ValueErrorIf the environment isn't recognized.\"\"\"required=[\"OPENAI_API_KEY\",\"ASTRA_DB_APPLICATION_TOKEN\"]ifself==Environment.CASSIO:required.append(\"ASTRA_DB_DATABASE_ID\")elifself==Environment.ASTRAPY:required.append(\"ASTRA_DB_API_ENDPOINT\")else:raiseValueError(f\"Unrecognized environment '{self}\")returnrequired"]]}, "16171819202122232425262728293031323334353637", "defrequired_envvars(self)->list[str]:\"\"\"Return the required environment variables for this environment.Returns-------:The environment variables required in this environment.Raises------ValueErrorIf the environment isn't recognized.\"\"\"required=[\"OPENAI_API_KEY\",\"ASTRA_DB_APPLICATION_TOKEN\"]ifself==Environment.CASSIO:required.append(\"ASTRA_DB_DATABASE_ID\")elifself==Environment.ASTRAPY:required.append(\"ASTRA_DB_API_ENDPOINT\")else:raiseValueError(f\"Unrecognized environment '{self}\")returnrequired", "defrequired_envvars(self)->list[str]:\"\"\"Return the required environment variables for this environment.Returns-------:The environment variables required in this environment.Raises------ValueErrorIf the environment isn't recognized.\"\"\"required=[\"OPENAI_API_KEY\",\"ASTRA_DB_APPLICATION_TOKEN\"]ifself==Environment.CASSIO:required.append(\"ASTRA_DB_DATABASE_ID\")elifself==Environment.ASTRAPY:required.append(\"ASTRA_DB_API_ENDPOINT\")else:raiseValueError(f\"Unrecognized environment '{self}\")returnrequired"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "initialize_environment", "content": ["initialize_environment(env:Environment=CASSIO)", "initialize_environment(env:Environment=CASSIO)", "Initialize the environment variables.", {"table": [["PARAMETER", "DESCRIPTION"], ["env", "The environment to initializeTYPE:EnvironmentDEFAULT:CASSIO"]]}, "env", "The environment to initialize", "TYPE:EnvironmentDEFAULT:CASSIO", "Environment", "CASSIO", "This uses the following:1. If a `.env` file is found, load environment variables from that.2. If not, and running in colab, set necessary environment variables from    secrets.3. If necessary variables aren't set by the above, then prompts the user.", "This uses the following:1. If a `.env` file is found, load environment variables from that.2. If not, and running in colab, set necessary environment variables from    secrets.3. If necessary variables aren't set by the above, then prompts the user.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/env.py", {"table": [["858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119", "definitialize_environment(env:Environment=Environment.CASSIO):\"\"\"Initialize the environment variables.Parameters----------env :The environment to initializeNotes-----This uses the following:1. If a `.env` file is found, load environment variables from that.2. If not, and running in colab, set necessary environment variables fromsecrets.3. If necessary variables aren't set by the above, then prompts the user.\"\"\"# 1. If a `.env` file is found, load environment variables from that.ifdotenv_path:=find_dotenv():load_dotenv(dotenv_path)verify_environment(env)return# 2. If not, and running in colab, set necesary environment variables from secrets.try:initialize_from_colab_userdata(env)verify_environment(env)returnexcept(ImportError,ModuleNotFoundError):pass# 3. Initialize from prompts.initialize_from_prompts(env)verify_environment(env)"]]}, "858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119", "definitialize_environment(env:Environment=Environment.CASSIO):\"\"\"Initialize the environment variables.Parameters----------env :The environment to initializeNotes-----This uses the following:1. If a `.env` file is found, load environment variables from that.2. If not, and running in colab, set necessary environment variables fromsecrets.3. If necessary variables aren't set by the above, then prompts the user.\"\"\"# 1. If a `.env` file is found, load environment variables from that.ifdotenv_path:=find_dotenv():load_dotenv(dotenv_path)verify_environment(env)return# 2. If not, and running in colab, set necesary environment variables from secrets.try:initialize_from_colab_userdata(env)verify_environment(env)returnexcept(ImportError,ModuleNotFoundError):pass# 3. Initialize from prompts.initialize_from_prompts(env)verify_environment(env)", "definitialize_environment(env:Environment=Environment.CASSIO):\"\"\"Initialize the environment variables.Parameters----------env :The environment to initializeNotes-----This uses the following:1. If a `.env` file is found, load environment variables from that.2. If not, and running in colab, set necessary environment variables fromsecrets.3. If necessary variables aren't set by the above, then prompts the user.\"\"\"# 1. If a `.env` file is found, load environment variables from that.ifdotenv_path:=find_dotenv():load_dotenv(dotenv_path)verify_environment(env)return# 2. If not, and running in colab, set necesary environment variables from secrets.try:initialize_from_colab_userdata(env)verify_environment(env)returnexcept(ImportError,ModuleNotFoundError):pass# 3. Initialize from prompts.initialize_from_prompts(env)verify_environment(env)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "initialize_from_colab_userdata", "content": ["initialize_from_colab_userdata(env:Environment=CASSIO)", "initialize_from_colab_userdata(env:Environment=CASSIO)", "Try to initialize environment from colabuserdata.", "userdata", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/env.py", {"table": [["5051525354555657585960616263646566676869", "definitialize_from_colab_userdata(env:Environment=Environment.CASSIO):\"\"\"Try to initialize environment from colab `userdata`.\"\"\"fromgoogle.colabimportuserdata# type: ignore[import-untyped]forrequiredinenv.required_envvars():os.environ[required]=userdata.get(required)try:os.environ[\"ASTRA_DB_KEYSPACE\"]=userdata.get(\"ASTRA_DB_KEYSPACE\")exceptuserdata.SecretNotFoundErroras_:# User doesn't have a keyspace set, so use the default.os.environ.pop(\"ASTRA_DB_KEYSPACE\",None)try:os.environ[\"LANGCHAIN_API_KEY\"]=userdata.get(\"LANGCHAIN_API_KEY\")os.environ[\"LANGCHAIN_TRACING_V2\"]=\"True\"except(userdata.SecretNotFoundError,userdata.NotebookAccessError):print(\"Colab Secret not set / accessible. Not configuring tracing\")# noqa: T201os.environ.pop(\"LANGCHAIN_API_KEY\")os.environ.pop(\"LANGCHAIN_TRACING_V2\")"]]}, "5051525354555657585960616263646566676869", "definitialize_from_colab_userdata(env:Environment=Environment.CASSIO):\"\"\"Try to initialize environment from colab `userdata`.\"\"\"fromgoogle.colabimportuserdata# type: ignore[import-untyped]forrequiredinenv.required_envvars():os.environ[required]=userdata.get(required)try:os.environ[\"ASTRA_DB_KEYSPACE\"]=userdata.get(\"ASTRA_DB_KEYSPACE\")exceptuserdata.SecretNotFoundErroras_:# User doesn't have a keyspace set, so use the default.os.environ.pop(\"ASTRA_DB_KEYSPACE\",None)try:os.environ[\"LANGCHAIN_API_KEY\"]=userdata.get(\"LANGCHAIN_API_KEY\")os.environ[\"LANGCHAIN_TRACING_V2\"]=\"True\"except(userdata.SecretNotFoundError,userdata.NotebookAccessError):print(\"Colab Secret not set / accessible. Not configuring tracing\")# noqa: T201os.environ.pop(\"LANGCHAIN_API_KEY\")os.environ.pop(\"LANGCHAIN_TRACING_V2\")", "definitialize_from_colab_userdata(env:Environment=Environment.CASSIO):\"\"\"Try to initialize environment from colab `userdata`.\"\"\"fromgoogle.colabimportuserdata# type: ignore[import-untyped]forrequiredinenv.required_envvars():os.environ[required]=userdata.get(required)try:os.environ[\"ASTRA_DB_KEYSPACE\"]=userdata.get(\"ASTRA_DB_KEYSPACE\")exceptuserdata.SecretNotFoundErroras_:# User doesn't have a keyspace set, so use the default.os.environ.pop(\"ASTRA_DB_KEYSPACE\",None)try:os.environ[\"LANGCHAIN_API_KEY\"]=userdata.get(\"LANGCHAIN_API_KEY\")os.environ[\"LANGCHAIN_TRACING_V2\"]=\"True\"except(userdata.SecretNotFoundError,userdata.NotebookAccessError):print(\"Colab Secret not set / accessible. Not configuring tracing\")# noqa: T201os.environ.pop(\"LANGCHAIN_API_KEY\")os.environ.pop(\"LANGCHAIN_TRACING_V2\")"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "initialize_from_prompts", "content": ["initialize_from_prompts(env:Environment=CASSIO)", "initialize_from_prompts(env:Environment=CASSIO)", "Initialize the environment by prompting the user.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/env.py", {"table": [["7273747576777879808182", "definitialize_from_prompts(env:Environment=Environment.CASSIO):\"\"\"Initialize the environment by prompting the user.\"\"\"importgetpassforrequiredinenv.required_envvars():ifrequiredinos.environ:continueelifrequiredinNON_SECRETS:os.environ[required]=input(required)else:os.environ[required]=getpass.getpass(required)"]]}, "7273747576777879808182", "definitialize_from_prompts(env:Environment=Environment.CASSIO):\"\"\"Initialize the environment by prompting the user.\"\"\"importgetpassforrequiredinenv.required_envvars():ifrequiredinos.environ:continueelifrequiredinNON_SECRETS:os.environ[required]=input(required)else:os.environ[required]=getpass.getpass(required)", "definitialize_from_prompts(env:Environment=Environment.CASSIO):\"\"\"Initialize the environment by prompting the user.\"\"\"importgetpassforrequiredinenv.required_envvars():ifrequiredinos.environ:continueelifrequiredinNON_SECRETS:os.environ[required]=input(required)else:os.environ[required]=getpass.getpass(required)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "verify_environment", "content": ["verify_environment(env:Environment=CASSIO)", "verify_environment(env:Environment=CASSIO)", "Verify the necessary environment variables are set.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/env.py", {"table": [["44454647", "defverify_environment(env:Environment=Environment.CASSIO):\"\"\"Verify the necessary environment variables are set.\"\"\"forrequiredinenv.required_envvars():assertrequiredinos.environ,f'\"{required}\" not defined in environment'"]]}, "44454647", "defverify_environment(env:Environment=Environment.CASSIO):\"\"\"Verify the necessary environment variables are set.\"\"\"forrequiredinenv.required_envvars():assertrequiredinos.environ,f'\"{required}\" not defined in environment'", "defverify_environment(env:Environment=Environment.CASSIO):\"\"\"Verify the necessary environment variables are set.\"\"\"forrequiredinenv.required_envvars():assertrequiredinos.environ,f'\"{required}\" not defined in environment'"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "examples", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "code_generation", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "format_docs", "content": ["format_docs(docs:list[Document])->str", "format_docs(docs:list[Document])->str", "Format documents as documentation for including as context in a LLM query.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/examples/code_generation/format.py", {"table": [["929394", "defformat_docs(docs:list[Document])->str:\"\"\"Format documents as documentation for including as context in a LLM query.\"\"\"return\"\\n---\\n\".join(format_document(doc)fordocindocs)"]]}, "929394", "defformat_docs(docs:list[Document])->str:\"\"\"Format documents as documentation for including as context in a LLM query.\"\"\"return\"\\n---\\n\".join(format_document(doc)fordocindocs)", "defformat_docs(docs:list[Document])->str:\"\"\"Format documents as documentation for including as context in a LLM query.\"\"\"return\"\\n---\\n\".join(format_document(doc)fordocindocs)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "format_document", "content": ["format_document(doc:Document,debug:bool=False)->str", "format_document(doc:Document,debug:bool=False)->str", "Format a document as documentation for including as context in a LLM query.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/examples/code_generation/format.py", {"table": [["3435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889", "defformat_document(doc:Document,debug:bool=False)->str:\"\"\"Format a document as documentation for including as context in a LLM query.\"\"\"metadata=doc.metadatatext=f\"{metadata['name']}({metadata['kind']})\\n\\n\"text+=f\"path:\\n\\t{metadata['path']}\\n\\n\"forkeyin[\"bases\",\"exports\",\"implemented_by\"]:ifkeyinmetadata:values=\"\\n\".join(metadata[key])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"if\"properties\"inmetadata:props=[f\"{k}:{v}\"fork,vinmetadata[\"properties\"].items()]values=\"\\n\".join(props)text+=f\"properties:\\n\\t{_add_tabs(values)}\\n\\n\"ifdoc.page_content!=\"\":text+=f\"description:\\n\\t{_add_tabs(doc.page_content)}\\n\\n\"elif\"value\"inmetadata:text+=f\"{metadata['value']}\\n\\n\"forkeyin[\"attributes\",\"parameters\"]:ifkeyinmetadata:values=\"\\n\\n\".join([_format_parameter(v)forvinmetadata[key]])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"returns\",\"yields\"]:ifkeyinmetadata:values=\"\\n\\n\".join([_format_return(v)forvinmetadata[key]])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"note\",\"example\"]:ifkeyinmetadata:text+=f\"{key}:\\n\\t{_add_tabs(metadata[key])}\\n\\n\"ifdebug:if\"imports\"inmetadata:imports=[]foras_name,real_nameinmetadata[\"imports\"].items():ifreal_name==as_name:imports.append(real_name)else:imports.append(f\"{real_name}as{as_name}\")values=\"\\n\".join(imports)text+=f\"imports:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"references\",\"gathered_types\"]:ifkeyinmetadata:values=\"\\n\".join(metadata[key])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"if\"parent\"inmetadata:text+=f\"parent:{metadata['parent']}\\n\\n\"returntext"]]}, "3435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889", "defformat_document(doc:Document,debug:bool=False)->str:\"\"\"Format a document as documentation for including as context in a LLM query.\"\"\"metadata=doc.metadatatext=f\"{metadata['name']}({metadata['kind']})\\n\\n\"text+=f\"path:\\n\\t{metadata['path']}\\n\\n\"forkeyin[\"bases\",\"exports\",\"implemented_by\"]:ifkeyinmetadata:values=\"\\n\".join(metadata[key])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"if\"properties\"inmetadata:props=[f\"{k}:{v}\"fork,vinmetadata[\"properties\"].items()]values=\"\\n\".join(props)text+=f\"properties:\\n\\t{_add_tabs(values)}\\n\\n\"ifdoc.page_content!=\"\":text+=f\"description:\\n\\t{_add_tabs(doc.page_content)}\\n\\n\"elif\"value\"inmetadata:text+=f\"{metadata['value']}\\n\\n\"forkeyin[\"attributes\",\"parameters\"]:ifkeyinmetadata:values=\"\\n\\n\".join([_format_parameter(v)forvinmetadata[key]])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"returns\",\"yields\"]:ifkeyinmetadata:values=\"\\n\\n\".join([_format_return(v)forvinmetadata[key]])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"note\",\"example\"]:ifkeyinmetadata:text+=f\"{key}:\\n\\t{_add_tabs(metadata[key])}\\n\\n\"ifdebug:if\"imports\"inmetadata:imports=[]foras_name,real_nameinmetadata[\"imports\"].items():ifreal_name==as_name:imports.append(real_name)else:imports.append(f\"{real_name}as{as_name}\")values=\"\\n\".join(imports)text+=f\"imports:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"references\",\"gathered_types\"]:ifkeyinmetadata:values=\"\\n\".join(metadata[key])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"if\"parent\"inmetadata:text+=f\"parent:{metadata['parent']}\\n\\n\"returntext", "defformat_document(doc:Document,debug:bool=False)->str:\"\"\"Format a document as documentation for including as context in a LLM query.\"\"\"metadata=doc.metadatatext=f\"{metadata['name']}({metadata['kind']})\\n\\n\"text+=f\"path:\\n\\t{metadata['path']}\\n\\n\"forkeyin[\"bases\",\"exports\",\"implemented_by\"]:ifkeyinmetadata:values=\"\\n\".join(metadata[key])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"if\"properties\"inmetadata:props=[f\"{k}:{v}\"fork,vinmetadata[\"properties\"].items()]values=\"\\n\".join(props)text+=f\"properties:\\n\\t{_add_tabs(values)}\\n\\n\"ifdoc.page_content!=\"\":text+=f\"description:\\n\\t{_add_tabs(doc.page_content)}\\n\\n\"elif\"value\"inmetadata:text+=f\"{metadata['value']}\\n\\n\"forkeyin[\"attributes\",\"parameters\"]:ifkeyinmetadata:values=\"\\n\\n\".join([_format_parameter(v)forvinmetadata[key]])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"returns\",\"yields\"]:ifkeyinmetadata:values=\"\\n\\n\".join([_format_return(v)forvinmetadata[key]])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"note\",\"example\"]:ifkeyinmetadata:text+=f\"{key}:\\n\\t{_add_tabs(metadata[key])}\\n\\n\"ifdebug:if\"imports\"inmetadata:imports=[]foras_name,real_nameinmetadata[\"imports\"].items():ifreal_name==as_name:imports.append(real_name)else:imports.append(f\"{real_name}as{as_name}\")values=\"\\n\".join(imports)text+=f\"imports:\\n\\t{_add_tabs(values)}\\n\\n\"forkeyin[\"references\",\"gathered_types\"]:ifkeyinmetadata:values=\"\\n\".join(metadata[key])text+=f\"{key}:\\n\\t{_add_tabs(values)}\\n\\n\"if\"parent\"inmetadata:text+=f\"parent:{metadata['parent']}\\n\\n\"returntext"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "converter", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "convert", "content": ["convert(package_name:str,search_paths:list[str],docstring_parser:DocstringStyle,output_path:str,)->None", "convert(package_name:str,search_paths:list[str],docstring_parser:DocstringStyle,output_path:str,)->None", "Load and convert a package's objects and documentation into a JSONL file.", "This method converts the internal documentation of modules, classes, functions, andattributes of a package into a format that is better suited for RAG (and GraphRAGin particular).", "The code uses thegriffelibrary, which is a Python code analysis tool thatextracts information from Python code and docstrings.", "griffe", "The JSONL file contains one JSON object per line, with the following structure:    id: the path to the object in the package    text: the description of the object (if any, can be empty)    metadata: Always includesname,path,kindkeys.              The remaining keys below are included when available.        name: the name of the object        path: the path to the object in the package        kind: eithermodule,class,function, orattributeparameters: the parameters for a class or function. Includes type            information, default values, and descriptions        attributes: the attributes on a class or module. Includes type            information and descriptions        gathered_types: list of non-standard types in the parameters and attributes        imports: list of non-standard types imported by the class or module        exports: list of non-standard types exported by the module        properties: list of boolean properties about the module        example: any code examples for the class, function, or module        references: list of any non-standard types used in the example code        returns: the return type and description        yields: the yield type and description        bases: list of base types inherited by the class        implemented_by: list of types that implement the a base class", "name", "path", "kind", "module", "class", "function", "attribute", {"table": [["PARAMETER", "DESCRIPTION"], ["package_name", "The name of the package to convert.TYPE:str"], ["search_paths", "The paths to search for the package.TYPE:list[str]"], ["docstring_parser", "The docstring parser to use.TYPE:DocstringStyle"], ["output_path", "The path to save the JSONL file.TYPE:str"]]}, "package_name", "The name of the package to convert.", "TYPE:str", "str", "search_paths", "The paths to search for the package.", "TYPE:list[str]", "list[str]", "docstring_parser", "The docstring parser to use.", "TYPE:DocstringStyle", "DocstringStyle", "output_path", "The path to save the JSONL file.", "TYPE:str", "str", "Examples:", "from graph_rag_example_helpers.examples.code_generation.converter import convertconvert(\"astrapy\", [\".venv/lib/python3.12/site-packages\"], \"google\", \"data\")", "This code was written thecode-generationexample andastrapy==1.5.2. It will  probably need tweaking for use with other python packages. Use at your own risk.", "code-generation", "astrapy==1.5.2", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/examples/code_generation/converter.py", {"table": [["1112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990", "defconvert(package_name:str,search_paths:list[str],docstring_parser:griffe.DocstringStyle,output_path:str,)->None:\"\"\"Load and convert a package's objects and documentation into a JSONL file.This method converts the internal documentation of modules, classes, functions, andattributes of a package into a format that is better suited for RAG (and GraphRAGin particular).The code uses the `griffe` library, which is a Python code analysis tool thatextracts information from Python code and docstrings.The JSONL file contains one JSON object per line, with the following structure:id: the path to the object in the packagetext: the description of the object (if any, can be empty)metadata: Always includes `name`, `path`, `kind` keys.The remaining keys below are included when available.name: the name of the objectpath: the path to the object in the packagekind: either `module`, `class`, `function`, or `attribute`parameters: the parameters for a class or function. Includes typeinformation, default values, and descriptionsattributes: the attributes on a class or module. Includes typeinformation and descriptionsgathered_types: list of non-standard types in the parameters and attributesimports: list of non-standard types imported by the class or moduleexports: list of non-standard types exported by the moduleproperties: list of boolean properties about the moduleexample: any code examples for the class, function, or modulereferences: list of any non-standard types used in the example codereturns: the return type and descriptionyields: the yield type and descriptionbases: list of base types inherited by the classimplemented_by: list of types that implement the a base classParameters----------package_name :The name of the package to convert.search_paths :The paths to search for the package.docstring_parser :The docstring parser to use.output_path :The path to save the JSONL file.Examples--------from graph_rag_example_helpers.examples.code_generation.converter import convertconvert(\"astrapy\", [\".venv/lib/python3.12/site-packages\"], \"google\", \"data\")Notes------ This code was written the `code-generation` example and `astrapy==1.5.2`. It willprobably need tweaking for use with other python packages. Use at your own risk.\"\"\"my_package=griffe.load(package_name,search_paths=search_paths,docstring_parser=docstring_parser)converter=_Converter()items=converter._convert(package_name,my_package)withopen(os.path.join(output_path,f\"{package_name}.jsonl\"),\"w\")asf:foriteminitems:text=item.pop(\"text\",\"\")id=item.get(\"path\")metadata=itemforkey,valueinmetadata.items():ifisinstance(value,set):metadata[key]=list(value)f.write(json.dumps({\"id\":id,\"text\":text,\"metadata\":metadata}))f.write(\"\\n\")"]]}, "1112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990", "defconvert(package_name:str,search_paths:list[str],docstring_parser:griffe.DocstringStyle,output_path:str,)->None:\"\"\"Load and convert a package's objects and documentation into a JSONL file.This method converts the internal documentation of modules, classes, functions, andattributes of a package into a format that is better suited for RAG (and GraphRAGin particular).The code uses the `griffe` library, which is a Python code analysis tool thatextracts information from Python code and docstrings.The JSONL file contains one JSON object per line, with the following structure:id: the path to the object in the packagetext: the description of the object (if any, can be empty)metadata: Always includes `name`, `path`, `kind` keys.The remaining keys below are included when available.name: the name of the objectpath: the path to the object in the packagekind: either `module`, `class`, `function`, or `attribute`parameters: the parameters for a class or function. Includes typeinformation, default values, and descriptionsattributes: the attributes on a class or module. Includes typeinformation and descriptionsgathered_types: list of non-standard types in the parameters and attributesimports: list of non-standard types imported by the class or moduleexports: list of non-standard types exported by the moduleproperties: list of boolean properties about the moduleexample: any code examples for the class, function, or modulereferences: list of any non-standard types used in the example codereturns: the return type and descriptionyields: the yield type and descriptionbases: list of base types inherited by the classimplemented_by: list of types that implement the a base classParameters----------package_name :The name of the package to convert.search_paths :The paths to search for the package.docstring_parser :The docstring parser to use.output_path :The path to save the JSONL file.Examples--------from graph_rag_example_helpers.examples.code_generation.converter import convertconvert(\"astrapy\", [\".venv/lib/python3.12/site-packages\"], \"google\", \"data\")Notes------ This code was written the `code-generation` example and `astrapy==1.5.2`. It willprobably need tweaking for use with other python packages. Use at your own risk.\"\"\"my_package=griffe.load(package_name,search_paths=search_paths,docstring_parser=docstring_parser)converter=_Converter()items=converter._convert(package_name,my_package)withopen(os.path.join(output_path,f\"{package_name}.jsonl\"),\"w\")asf:foriteminitems:text=item.pop(\"text\",\"\")id=item.get(\"path\")metadata=itemforkey,valueinmetadata.items():ifisinstance(value,set):metadata[key]=list(value)f.write(json.dumps({\"id\":id,\"text\":text,\"metadata\":metadata}))f.write(\"\\n\")", "defconvert(package_name:str,search_paths:list[str],docstring_parser:griffe.DocstringStyle,output_path:str,)->None:\"\"\"Load and convert a package's objects and documentation into a JSONL file.This method converts the internal documentation of modules, classes, functions, andattributes of a package into a format that is better suited for RAG (and GraphRAGin particular).The code uses the `griffe` library, which is a Python code analysis tool thatextracts information from Python code and docstrings.The JSONL file contains one JSON object per line, with the following structure:id: the path to the object in the packagetext: the description of the object (if any, can be empty)metadata: Always includes `name`, `path`, `kind` keys.The remaining keys below are included when available.name: the name of the objectpath: the path to the object in the packagekind: either `module`, `class`, `function`, or `attribute`parameters: the parameters for a class or function. Includes typeinformation, default values, and descriptionsattributes: the attributes on a class or module. Includes typeinformation and descriptionsgathered_types: list of non-standard types in the parameters and attributesimports: list of non-standard types imported by the class or moduleexports: list of non-standard types exported by the moduleproperties: list of boolean properties about the moduleexample: any code examples for the class, function, or modulereferences: list of any non-standard types used in the example codereturns: the return type and descriptionyields: the yield type and descriptionbases: list of base types inherited by the classimplemented_by: list of types that implement the a base classParameters----------package_name :The name of the package to convert.search_paths :The paths to search for the package.docstring_parser :The docstring parser to use.output_path :The path to save the JSONL file.Examples--------from graph_rag_example_helpers.examples.code_generation.converter import convertconvert(\"astrapy\", [\".venv/lib/python3.12/site-packages\"], \"google\", \"data\")Notes------ This code was written the `code-generation` example and `astrapy==1.5.2`. It willprobably need tweaking for use with other python packages. Use at your own risk.\"\"\"my_package=griffe.load(package_name,search_paths=search_paths,docstring_parser=docstring_parser)converter=_Converter()items=converter._convert(package_name,my_package)withopen(os.path.join(output_path,f\"{package_name}.jsonl\"),\"w\")asf:foriteminitems:text=item.pop(\"text\",\"\")id=item.get(\"path\")metadata=itemforkey,valueinmetadata.items():ifisinstance(value,set):metadata[key]=list(value)f.write(json.dumps({\"id\":id,\"text\":text,\"metadata\":metadata}))f.write(\"\\n\")"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "format", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "persistent_iteration", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "Offsetdataclass", "content": ["dataclass", "Offset(index:int)", "Offset(index:int)", "Class for tracking a position in the iteraiton."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "PersistentIteration", "content": ["PersistentIteration(journal_name:str,iterator:Iterator[T])", "PersistentIteration(journal_name:str,iterator:Iterator[T])", "Bases:Generic[T]", "Generic[T]", "Create a persistent iteration.", "This creates a journal file with the namejournal_namecontaining the indicesof completed items. When resuming iteration, the already processed indices willbe skipped.", "journal_name", {"table": [["PARAMETER", "DESCRIPTION"], ["journal_name", "Name of the journal file to use. If it doesn't exist it will becreated. The indices of completed items will be written to thejournal.TYPE:str"], ["iterator", "The iterator to process persistently. It must be deterministic --elements should always be returned in the same order on restarts.TYPE:Iterator[T]"]]}, "journal_name", "Name of the journal file to use. If it doesn't exist it will becreated. The indices of completed items will be written to thejournal.", "TYPE:str", "str", "iterator", "The iterator to process persistently. It must be deterministic --elements should always be returned in the same order on restarts.", "TYPE:Iterator[T]", "Iterator[T]", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/persistent_iteration.py", {"table": [["34353637383940414243444546", "def__init__(self,journal_name:str,iterator:Iterator[T])->None:self.iterator=enumerate(iterator)self.pending:dict[Offset,T]={}self._completed=set()try:read_journal=open(journal_name)forlineinread_journal:self._completed.add(Offset(index=int(line)))exceptFileNotFoundError:passself._write_journal=open(journal_name,\"a\")"]]}, "34353637383940414243444546", "def__init__(self,journal_name:str,iterator:Iterator[T])->None:self.iterator=enumerate(iterator)self.pending:dict[Offset,T]={}self._completed=set()try:read_journal=open(journal_name)forlineinread_journal:self._completed.add(Offset(index=int(line)))exceptFileNotFoundError:passself._write_journal=open(journal_name,\"a\")", "def__init__(self,journal_name:str,iterator:Iterator[T])->None:self.iterator=enumerate(iterator)self.pending:dict[Offset,T]={}self._completed=set()try:read_journal=open(journal_name)forlineinread_journal:self._completed.add(Offset(index=int(line)))exceptFileNotFoundError:passself._write_journal=open(journal_name,\"a\")"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "__iter__", "content": ["__iter__()->Iterator[tuple[Offset,T]]", "__iter__()->Iterator[tuple[Offset,T]]", "Iterate over pairs of offsets and elements.", {"table": [["RETURNS", "DESCRIPTION"], ["Iterator[tuple[Offset,T]]", ""]]}, "Iterator[tuple[Offset,T]]", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/persistent_iteration.py", {"table": [["707172737475767778", "def__iter__(self)->Iterator[tuple[Offset,T]]:\"\"\"Iterate over pairs of offsets and elements.Returns-------:\"\"\"returnself"]]}, "707172737475767778", "def__iter__(self)->Iterator[tuple[Offset,T]]:\"\"\"Iterate over pairs of offsets and elements.Returns-------:\"\"\"returnself", "def__iter__(self)->Iterator[tuple[Offset,T]]:\"\"\"Iterate over pairs of offsets and elements.Returns-------:\"\"\"returnself"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "__next__", "content": ["__next__()->tuple[Offset,T]", "__next__()->tuple[Offset,T]", "Return the next offset and item.", {"table": [["RETURNS", "DESCRIPTION"], ["offset", "The offset of the next item. Should be acknowledge after the itemis finished processing.TYPE:Offset"], ["item", "The next item.TYPE:T"]]}, "offset", "The offset of the next item. Should be acknowledge after the itemis finished processing.", "TYPE:Offset", "Offset", "item", "The next item.", "TYPE:T", "T", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/persistent_iteration.py", {"table": [["484950515253545556575859606162636465666768", "def__next__(self)->tuple[Offset,T]:\"\"\"Return the next offset and item.Returns-------offset :The offset of the next item. Should be acknowledge after the itemis finished processing.item :The next item.\"\"\"index,item=next(self.iterator)offset=Offset(index)whileoffsetinself._completed:index,item=next(self.iterator)offset=Offset(index)self.pending[offset]=itemreturn(offset,item)"]]}, "484950515253545556575859606162636465666768", "def__next__(self)->tuple[Offset,T]:\"\"\"Return the next offset and item.Returns-------offset :The offset of the next item. Should be acknowledge after the itemis finished processing.item :The next item.\"\"\"index,item=next(self.iterator)offset=Offset(index)whileoffsetinself._completed:index,item=next(self.iterator)offset=Offset(index)self.pending[offset]=itemreturn(offset,item)", "def__next__(self)->tuple[Offset,T]:\"\"\"Return the next offset and item.Returns-------offset :The offset of the next item. Should be acknowledge after the itemis finished processing.item :The next item.\"\"\"index,item=next(self.iterator)offset=Offset(index)whileoffsetinself._completed:index,item=next(self.iterator)offset=Offset(index)self.pending[offset]=itemreturn(offset,item)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "ack", "content": ["ack(offset:Offset)->int", "ack(offset:Offset)->int", "Acknowledge the given offset.", "This should only be called after the elements in that offset have beenpersisted.", {"table": [["PARAMETER", "DESCRIPTION"], ["offset", "The offset to acknowledge.TYPE:Offset"]]}, "offset", "The offset to acknowledge.", "TYPE:Offset", "Offset", {"table": [["RETURNS", "DESCRIPTION"], ["int", "The numebr of pending elements."]]}, "int", "The numebr of pending elements.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/persistent_iteration.py", {"table": [["8081828384858687888990919293949596979899100101102", "defack(self,offset:Offset)->int:\"\"\"Acknowledge the given offset.This should only be called after the elements in that offset have beenpersisted.Parameters----------offset :The offset to acknowledge.Returns-------:The numebr of pending elements.\"\"\"self._write_journal.write(f\"{offset.index}\\n\")self._write_journal.flush()self._completed.add(offset)self.pending.pop(offset)returnlen(self.pending)"]]}, "8081828384858687888990919293949596979899100101102", "defack(self,offset:Offset)->int:\"\"\"Acknowledge the given offset.This should only be called after the elements in that offset have beenpersisted.Parameters----------offset :The offset to acknowledge.Returns-------:The numebr of pending elements.\"\"\"self._write_journal.write(f\"{offset.index}\\n\")self._write_journal.flush()self._completed.add(offset)self.pending.pop(offset)returnlen(self.pending)", "defack(self,offset:Offset)->int:\"\"\"Acknowledge the given offset.This should only be called after the elements in that offset have beenpersisted.Parameters----------offset :The offset to acknowledge.Returns-------:The numebr of pending elements.\"\"\"self._write_journal.write(f\"{offset.index}\\n\")self._write_journal.flush()self._completed.add(offset)self.pending.pop(offset)returnlen(self.pending)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "completed_count", "content": ["completed_count()->int", "completed_count()->int", "Return the numebr of completed elements.", {"table": [["RETURNS", "DESCRIPTION"], ["int", "The number of completed elements."]]}, "int", "The number of completed elements.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/persistent_iteration.py", {"table": [["115116117118119120121122123124", "defcompleted_count(self)->int:\"\"\"Return the numebr of completed elements.Returns-------:The number of completed elements.\"\"\"returnlen(self._completed)"]]}, "115116117118119120121122123124", "defcompleted_count(self)->int:\"\"\"Return the numebr of completed elements.Returns-------:The number of completed elements.\"\"\"returnlen(self._completed)", "defcompleted_count(self)->int:\"\"\"Return the numebr of completed elements.Returns-------:The number of completed elements.\"\"\"returnlen(self._completed)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_rag_example_helpers/", "title": "pending_count", "content": ["pending_count()->int", "pending_count()->int", "Return the number of pending (not processed) elements.", {"table": [["RETURNS", "DESCRIPTION"], ["int", "The number of pending elements."]]}, "int", "The number of pending elements.", "packages/graph-rag-example-helpers/src/graph_rag_example_helpers/persistent_iteration.py", {"table": [["104105106107108109110111112113", "defpending_count(self)->int:\"\"\"Return the number of pending (not processed) elements.Returns-------:The number of pending elements.\"\"\"returnlen(self.pending)"]]}, "104105106107108109110111112113", "defpending_count(self)->int:\"\"\"Return the number of pending (not processed) elements.Returns-------:The number of pending elements.\"\"\"returnlen(self.pending)", "defpending_count(self)->int:\"\"\"Return the number of pending (not processed) elements.Returns-------:The number of pending elements.\"\"\"returnlen(self.pending)"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "langchain_graph_retriever.transformers", "content": ["Package containing useful Document Transformers.", "Many of these add metadata that could be useful for linking content, such asextracting named entities or keywords from the page content.", "Also includes a transformer for shredding metadata, for use with storesthat do not support querying on elements of lists."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "ParentTransformer", "content": ["ParentTransformer(*,path_metadata_key:str=\"path\",parent_metadata_key:str=\"parent\",path_delimiter:str=\"\\\\\",)", "ParentTransformer(*,path_metadata_key:str=\"path\",parent_metadata_key:str=\"parent\",path_delimiter:str=\"\\\\\",)", "Bases:BaseDocumentTransformer", "BaseDocumentTransformer", "Adds the hierarchal Parent path to the document metadata.", {"table": [["PARAMETER", "DESCRIPTION"], ["path_metadata_key", "Metadata key containing the path.This may correspond to paths in a file system, hierarchy in a document, etc.TYPE:strDEFAULT:'path'"], ["parent_metadata_key", "Metadata key for the added parent pathTYPE:strDEFAULT:'parent'"], ["path_delimiter", "Delimiter of items in the path.TYPE:strDEFAULT:'\\\\'"]]}, "path_metadata_key", "Metadata key containing the path.This may correspond to paths in a file system, hierarchy in a document, etc.", "TYPE:strDEFAULT:'path'", "str", "'path'", "parent_metadata_key", "Metadata key for the added parent path", "TYPE:strDEFAULT:'parent'", "str", "'parent'", "path_delimiter", "Delimiter of items in the path.", "TYPE:strDEFAULT:'\\\\'", "str", "'\\\\'", "An example of how to use this transformer existsHEREin the guide.", "Expects each document to contain itspathin its metadata.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/parent.py", {"table": [["32333435363738394041", "def__init__(self,*,path_metadata_key:str=\"path\",parent_metadata_key:str=\"parent\",path_delimiter:str=\"\\\\\",):self._path_metadata_key=path_metadata_keyself._parent_metadata_key=parent_metadata_keyself._path_delimiter=path_delimiter"]]}, "32333435363738394041", "def__init__(self,*,path_metadata_key:str=\"path\",parent_metadata_key:str=\"parent\",path_delimiter:str=\"\\\\\",):self._path_metadata_key=path_metadata_keyself._parent_metadata_key=parent_metadata_keyself._path_delimiter=path_delimiter", "def__init__(self,*,path_metadata_key:str=\"path\",parent_metadata_key:str=\"parent\",path_delimiter:str=\"\\\\\",):self._path_metadata_key=path_metadata_keyself._parent_metadata_key=parent_metadata_keyself._path_delimiter=path_delimiter"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "ShreddingTransformer", "content": ["ShreddingTransformer(*,keys:set[str]=set(),path_delimiter:str=DEFAULT_PATH_DELIMITER,static_value:Any=DEFAULT_STATIC_VALUE,)", "ShreddingTransformer(*,keys:set[str]=set(),path_delimiter:str=DEFAULT_PATH_DELIMITER,static_value:Any=DEFAULT_STATIC_VALUE,)", "Bases:BaseDocumentTransformer", "BaseDocumentTransformer", "Shreds sequence-based metadata fields.", "Certain vector stores do not support storing or searching on metadata fieldswith sequence-based values. This transformer converts sequence-based fieldsinto simple metadata values.", "An example of how to use this transformer existsHEREin the guide.", {"table": [["PARAMETER", "DESCRIPTION"], ["keys", "A set of metadata keys to shred.If empty, all sequence-based fields will be shredded.TYPE:set[str]DEFAULT:set()"], ["path_delimiter", "The path delimiter to use when building shredded keys.TYPE:strDEFAULT:DEFAULT_PATH_DELIMITER"], ["static_value", "The value to set on each shredded key.TYPE:AnyDEFAULT:DEFAULT_STATIC_VALUE"]]}, "keys", "A set of metadata keys to shred.If empty, all sequence-based fields will be shredded.", "TYPE:set[str]DEFAULT:set()", "set[str]", "set()", "path_delimiter", "The path delimiter to use when building shredded keys.", "TYPE:strDEFAULT:DEFAULT_PATH_DELIMITER", "str", "DEFAULT_PATH_DELIMITER", "static_value", "The value to set on each shredded key.", "TYPE:AnyDEFAULT:DEFAULT_STATIC_VALUE", "Any", "DEFAULT_STATIC_VALUE", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/shredding.py", {"table": [["39404142434445464748", "def__init__(self,*,keys:set[str]=set(),path_delimiter:str=DEFAULT_PATH_DELIMITER,static_value:Any=DEFAULT_STATIC_VALUE,):self.keys=keysself.path_delimiter=path_delimiterself.static_value=static_value"]]}, "39404142434445464748", "def__init__(self,*,keys:set[str]=set(),path_delimiter:str=DEFAULT_PATH_DELIMITER,static_value:Any=DEFAULT_STATIC_VALUE,):self.keys=keysself.path_delimiter=path_delimiterself.static_value=static_value", "def__init__(self,*,keys:set[str]=set(),path_delimiter:str=DEFAULT_PATH_DELIMITER,static_value:Any=DEFAULT_STATIC_VALUE,):self.keys=keysself.path_delimiter=path_delimiterself.static_value=static_value"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "restore_documents", "content": ["restore_documents(documents:Sequence[Document],**kwargs:Any)->Sequence[Document]", "restore_documents(documents:Sequence[Document],**kwargs:Any)->Sequence[Document]", "Restore documents transformed by the ShreddingTransformer.", "Restore documents transformed by the ShreddingTransformer back totheir original state before shredding.", "Note that any non-string values inside lists will be converted to stringsafter restoring.", "Args:    documents: A sequence of Documents to be transformed.", {"table": [["RETURNS", "DESCRIPTION"], ["Sequence[Document]", "A sequence of transformed Documents."]]}, "Sequence[Document]", "A sequence of transformed Documents.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/shredding.py", {"table": [["7778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123", "defrestore_documents(self,documents:Sequence[Document],**kwargs:Any)->Sequence[Document]:\"\"\"Restore documents transformed by the ShreddingTransformer.Restore documents transformed by the ShreddingTransformer back totheir original state before shredding.Note that any non-string values inside lists will be converted to stringsafter restoring.Args:documents: A sequence of Documents to be transformed.Returns-------Sequence[Document]A sequence of transformed Documents.\"\"\"restored_docs=[]fordocumentindocuments:new_doc=Document(id=document.id,page_content=document.page_content)shredded_keys=set(json.loads(document.metadata.pop(SHREDDED_KEYS_KEY,\"[]\")))forkey,valueindocument.metadata.items():# Check if the key belongs to a shredded groupsplit_key=key.split(self.path_delimiter,1)if(len(split_key)==2andsplit_key[0]inshredded_keysandvalue==self.static_value):original_key,original_value=split_keyvalue=json.loads(original_value)iforiginal_keynotinnew_doc.metadata:new_doc.metadata[original_key]=[]new_doc.metadata[original_key].append(value)else:# Retain non-shredded metadata as isnew_doc.metadata[key]=valuerestored_docs.append(new_doc)returnrestored_docs"]]}, "7778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123", "defrestore_documents(self,documents:Sequence[Document],**kwargs:Any)->Sequence[Document]:\"\"\"Restore documents transformed by the ShreddingTransformer.Restore documents transformed by the ShreddingTransformer back totheir original state before shredding.Note that any non-string values inside lists will be converted to stringsafter restoring.Args:documents: A sequence of Documents to be transformed.Returns-------Sequence[Document]A sequence of transformed Documents.\"\"\"restored_docs=[]fordocumentindocuments:new_doc=Document(id=document.id,page_content=document.page_content)shredded_keys=set(json.loads(document.metadata.pop(SHREDDED_KEYS_KEY,\"[]\")))forkey,valueindocument.metadata.items():# Check if the key belongs to a shredded groupsplit_key=key.split(self.path_delimiter,1)if(len(split_key)==2andsplit_key[0]inshredded_keysandvalue==self.static_value):original_key,original_value=split_keyvalue=json.loads(original_value)iforiginal_keynotinnew_doc.metadata:new_doc.metadata[original_key]=[]new_doc.metadata[original_key].append(value)else:# Retain non-shredded metadata as isnew_doc.metadata[key]=valuerestored_docs.append(new_doc)returnrestored_docs", "defrestore_documents(self,documents:Sequence[Document],**kwargs:Any)->Sequence[Document]:\"\"\"Restore documents transformed by the ShreddingTransformer.Restore documents transformed by the ShreddingTransformer back totheir original state before shredding.Note that any non-string values inside lists will be converted to stringsafter restoring.Args:documents: A sequence of Documents to be transformed.Returns-------Sequence[Document]A sequence of transformed Documents.\"\"\"restored_docs=[]fordocumentindocuments:new_doc=Document(id=document.id,page_content=document.page_content)shredded_keys=set(json.loads(document.metadata.pop(SHREDDED_KEYS_KEY,\"[]\")))forkey,valueindocument.metadata.items():# Check if the key belongs to a shredded groupsplit_key=key.split(self.path_delimiter,1)if(len(split_key)==2andsplit_key[0]inshredded_keysandvalue==self.static_value):original_key,original_value=split_keyvalue=json.loads(original_value)iforiginal_keynotinnew_doc.metadata:new_doc.metadata[original_key]=[]new_doc.metadata[original_key].append(value)else:# Retain non-shredded metadata as isnew_doc.metadata[key]=valuerestored_docs.append(new_doc)returnrestored_docs"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "shredded_key", "content": ["shredded_key(key:str,value:Any)->str", "shredded_key(key:str,value:Any)->str", "Get the shredded key for a key/value pair.", {"table": [["PARAMETER", "DESCRIPTION"], ["key", "The metadata key to shredTYPE:str"], ["value", "The metadata value to shredTYPE:Any"]]}, "key", "The metadata key to shred", "TYPE:str", "str", "value", "The metadata value to shred", "TYPE:Any", "Any", {"table": [["RETURNS", "DESCRIPTION"], ["str", "the shredded key"]]}, "str", "the shredded key", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/shredding.py", {"table": [["125126127128129130131132133134135136137138139140141", "defshredded_key(self,key:str,value:Any)->str:\"\"\"Get the shredded key for a key/value pair.Parameters----------key :The metadata key to shredvalue :The metadata value to shredReturns-------strthe shredded key\"\"\"returnf\"{key}{self.path_delimiter}{json.dumps(value)}\""]]}, "125126127128129130131132133134135136137138139140141", "defshredded_key(self,key:str,value:Any)->str:\"\"\"Get the shredded key for a key/value pair.Parameters----------key :The metadata key to shredvalue :The metadata value to shredReturns-------strthe shredded key\"\"\"returnf\"{key}{self.path_delimiter}{json.dumps(value)}\"", "defshredded_key(self,key:str,value:Any)->str:\"\"\"Get the shredded key for a key/value pair.Parameters----------key :The metadata key to shredvalue :The metadata value to shredReturns-------strthe shredded key\"\"\"returnf\"{key}{self.path_delimiter}{json.dumps(value)}\""]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "shredded_value", "content": ["shredded_value()->str", "shredded_value()->str", "Get the shredded value for a key/value pair.", {"table": [["RETURNS", "DESCRIPTION"], ["str", "the shredded value"]]}, "str", "the shredded value", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/shredding.py", {"table": [["143144145146147148149150151152", "defshredded_value(self)->str:\"\"\"Get the shredded value for a key/value pair.Returns-------strthe shredded value\"\"\"returnself.static_value"]]}, "143144145146147148149150151152", "defshredded_value(self)->str:\"\"\"Get the shredded value for a key/value pair.Returns-------strthe shredded value\"\"\"returnself.static_value", "defshredded_value(self)->str:\"\"\"Get the shredded value for a key/value pair.Returns-------strthe shredded value\"\"\"returnself.static_value"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "gliner", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "GLiNERTransformer", "content": ["GLiNERTransformer(labels:list[str],*,batch_size:int=8,metadata_key_prefix:str=\"\",model:str|GLiNER=\"urchade/gliner_mediumv2.1\",)", "GLiNERTransformer(labels:list[str],*,batch_size:int=8,metadata_key_prefix:str=\"\",model:str|GLiNER=\"urchade/gliner_mediumv2.1\",)", "Bases:BaseDocumentTransformer", "BaseDocumentTransformer", "Add metadata to documents about named entities usingGLiNER.", "Extracts structured entity labels from text, identifying key attributes andcategories to enrich document metadata with semantic information.", "GLiNERis a Named EntityRecognition (NER) model capable of identifying any entity type using abidirectional transformer encoder (BERT-like).", "This transformer requires theglinerextra to be installed.", "gliner", "pip install -qU langchain_graph_retriever[gliner]", "pip install -qU langchain_graph_retriever[gliner]", "An example of how to use this transformer existsHEREin the guide.", {"table": [["PARAMETER", "DESCRIPTION"], ["labels", "List of entity kinds to extract.TYPE:list[str]"], ["batch_size", "The number of documents to process in each batch.TYPE:intDEFAULT:8"], ["metadata_key_prefix", "A prefix to add to metadata keys outputted by the extractor.This will be prepended to the label, with the value (or values) holding thegenerated keywords for that entity kind.TYPE:strDEFAULT:''"], ["model", "The GLiNER model to use. Pass the name of a model to load orpass an instantiated GLiNER model instance.TYPE:str|GLiNERDEFAULT:'urchade/gliner_mediumv2.1'"]]}, "labels", "List of entity kinds to extract.", "TYPE:list[str]", "list[str]", "batch_size", "The number of documents to process in each batch.", "TYPE:intDEFAULT:8", "int", "8", "metadata_key_prefix", "A prefix to add to metadata keys outputted by the extractor.This will be prepended to the label, with the value (or values) holding thegenerated keywords for that entity kind.", "TYPE:strDEFAULT:''", "str", "''", "model", "The GLiNER model to use. Pass the name of a model to load orpass an instantiated GLiNER model instance.", "TYPE:str|GLiNERDEFAULT:'urchade/gliner_mediumv2.1'", "str|GLiNER", "'urchade/gliner_mediumv2.1'", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/gliner.py", {"table": [["515253545556575859606162636465666768", "def__init__(self,labels:list[str],*,batch_size:int=8,metadata_key_prefix:str=\"\",model:str|GLiNER=\"urchade/gliner_mediumv2.1\",):ifisinstance(model,GLiNER):self._model=modelelifisinstance(model,str):self._model=GLiNER.from_pretrained(model)else:raiseValueError(f\"Invalid model:{model}\")self._batch_size=batch_sizeself._labels=labelsself.metadata_key_prefix=metadata_key_prefix"]]}, "515253545556575859606162636465666768", "def__init__(self,labels:list[str],*,batch_size:int=8,metadata_key_prefix:str=\"\",model:str|GLiNER=\"urchade/gliner_mediumv2.1\",):ifisinstance(model,GLiNER):self._model=modelelifisinstance(model,str):self._model=GLiNER.from_pretrained(model)else:raiseValueError(f\"Invalid model:{model}\")self._batch_size=batch_sizeself._labels=labelsself.metadata_key_prefix=metadata_key_prefix", "def__init__(self,labels:list[str],*,batch_size:int=8,metadata_key_prefix:str=\"\",model:str|GLiNER=\"urchade/gliner_mediumv2.1\",):ifisinstance(model,GLiNER):self._model=modelelifisinstance(model,str):self._model=GLiNER.from_pretrained(model)else:raiseValueError(f\"Invalid model:{model}\")self._batch_size=batch_sizeself._labels=labelsself.metadata_key_prefix=metadata_key_prefix"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "html", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "HyperlinkTransformer", "content": ["HyperlinkTransformer(*,url_metadata_key:str=\"url\",metadata_key:str=\"hyperlink\",drop_fragments:bool=True,)", "HyperlinkTransformer(*,url_metadata_key:str=\"url\",metadata_key:str=\"hyperlink\",drop_fragments:bool=True,)", "Bases:BaseDocumentTransformer", "BaseDocumentTransformer", "Extracts hyperlinks from HTML content and stores them in document metadata.", "This transformer requires thehtmlextra to be installed.", "html", "pip install -qU langchain_graph_retriever[html]", "pip install -qU langchain_graph_retriever[html]", "An example of how to use this transformer existsHEREin the guide.", {"table": [["PARAMETER", "DESCRIPTION"], ["url_metadata_key", "The metadata field containing the URL of the document. Must be setbefore transforming. Needed to resolve relative paths.TYPE:strDEFAULT:'url'"], ["metadata_key", "The metadata field to populate with documents linked from this content.TYPE:strDEFAULT:'hyperlink'"], ["drop_fragments", "Whether fragments in URLs and links should be dropped.TYPE:boolDEFAULT:True"]]}, "url_metadata_key", "The metadata field containing the URL of the document. Must be setbefore transforming. Needed to resolve relative paths.", "TYPE:strDEFAULT:'url'", "str", "'url'", "metadata_key", "The metadata field to populate with documents linked from this content.", "TYPE:strDEFAULT:'hyperlink'", "str", "'hyperlink'", "drop_fragments", "Whether fragments in URLs and links should be dropped.", "TYPE:boolDEFAULT:True", "bool", "True", "Expects each document to contain itsURLin its metadata.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/html.py", {"table": [["47484950515253545556", "def__init__(self,*,url_metadata_key:str=\"url\",metadata_key:str=\"hyperlink\",drop_fragments:bool=True,):self._url_metadata_key=url_metadata_keyself._metadata_key=metadata_keyself._drop_fragments=drop_fragments"]]}, "47484950515253545556", "def__init__(self,*,url_metadata_key:str=\"url\",metadata_key:str=\"hyperlink\",drop_fragments:bool=True,):self._url_metadata_key=url_metadata_keyself._metadata_key=metadata_keyself._drop_fragments=drop_fragments", "def__init__(self,*,url_metadata_key:str=\"url\",metadata_key:str=\"hyperlink\",drop_fragments:bool=True,):self._url_metadata_key=url_metadata_keyself._metadata_key=metadata_keyself._drop_fragments=drop_fragments"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "keybert", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "KeyBERTTransformer", "content": ["KeyBERTTransformer(*,batch_size:int=8,metadata_key:str=\"keywords\",model:str|KeyBERT=\"all-MiniLM-L6-v2\",)", "KeyBERTTransformer(*,batch_size:int=8,metadata_key:str=\"keywords\",model:str|KeyBERT=\"all-MiniLM-L6-v2\",)", "Bases:BaseDocumentTransformer", "BaseDocumentTransformer", "Add metadata to documents about keywords usingKeyBERT.", "Extracts key topics and concepts from text, generating metadata that highlightsthe most relevant terms to describe the content.", "KeyBERTis a minimal and easy-to-usekeyword extraction technique that leverages BERT embeddings to create keywords andkeyphrases that are most similar to a document.", "This transformer requires thekeybertextra to be installed.", "keybert", "pip install -qU langchain_graph_retriever[keybert]", "pip install -qU langchain_graph_retriever[keybert]", "An example of how to use this transformer existsHEREin the guide.", {"table": [["PARAMETER", "DESCRIPTION"], ["batch_size", "The number of documents to process in each batch.TYPE:intDEFAULT:8"], ["metadata_key", "The name of the key used in the metadata output.TYPE:strDEFAULT:'keywords'"], ["model", "The KeyBERT model to use. Pass the name of a model to loador pass an instantiated KeyBERT model instance.TYPE:str|KeyBERTDEFAULT:'all-MiniLM-L6-v2'"]]}, "batch_size", "The number of documents to process in each batch.", "TYPE:intDEFAULT:8", "int", "8", "metadata_key", "The name of the key used in the metadata output.", "TYPE:strDEFAULT:'keywords'", "str", "'keywords'", "model", "The KeyBERT model to use. Pass the name of a model to loador pass an instantiated KeyBERT model instance.", "TYPE:str|KeyBERTDEFAULT:'all-MiniLM-L6-v2'", "str|KeyBERT", "'all-MiniLM-L6-v2'", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/keybert.py", {"table": [["454647484950515253545556575859", "def__init__(self,*,batch_size:int=8,metadata_key:str=\"keywords\",model:str|KeyBERT=\"all-MiniLM-L6-v2\",):ifisinstance(model,KeyBERT):self._kw_model=modelelifisinstance(model,str):self._kw_model=KeyBERT(model=model)else:raiseValueError(f\"Invalid model:{model}\")self._batch_size=batch_sizeself._metadata_key=metadata_key"]]}, "454647484950515253545556575859", "def__init__(self,*,batch_size:int=8,metadata_key:str=\"keywords\",model:str|KeyBERT=\"all-MiniLM-L6-v2\",):ifisinstance(model,KeyBERT):self._kw_model=modelelifisinstance(model,str):self._kw_model=KeyBERT(model=model)else:raiseValueError(f\"Invalid model:{model}\")self._batch_size=batch_sizeself._metadata_key=metadata_key", "def__init__(self,*,batch_size:int=8,metadata_key:str=\"keywords\",model:str|KeyBERT=\"all-MiniLM-L6-v2\",):ifisinstance(model,KeyBERT):self._kw_model=modelelifisinstance(model,str):self._kw_model=KeyBERT(model=model)else:raiseValueError(f\"Invalid model:{model}\")self._batch_size=batch_sizeself._metadata_key=metadata_key"]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "parent", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "shredding", "content": ["Shredding Transformer for sequence-based metadata fields."]}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "spacy", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/", "title": "SpacyNERTransformer", "content": ["SpacyNERTransformer(*,include_labels:set[str]=set(),exclude_labels:set[str]=set(),limit:int|None=None,metadata_key:str=\"entities\",model:str|Language=\"en_core_web_sm\",)", "SpacyNERTransformer(*,include_labels:set[str]=set(),exclude_labels:set[str]=set(),limit:int|None=None,metadata_key:str=\"entities\",model:str|Language=\"en_core_web_sm\",)", "Bases:BaseDocumentTransformer", "BaseDocumentTransformer", "Add metadata to documents about named entities usingspaCy.", "Identifies and labels named entities in text, extracting structuredmetadata such as organizations, locations, dates, and other key entity types.", "spaCyis a library for Natural Language Processingin Python. Here it is used for Named Entity Recognition (NER) to extract valuesfrom document content.", "This transformer requires thespacyextra to be installed.", "spacy", "pip install -qU langchain_graph_retriever[spacy]", "pip install -qU langchain_graph_retriever[spacy]", "An example of how to use this transformer existsHEREin the guide.", {"table": [["PARAMETER", "DESCRIPTION"], ["include_labels", "Set of entity labels to include. Will include all labels if empty.TYPE:set[str]DEFAULT:set()"], ["exclude_labels", "Set of entity labels to exclude. Will not exclude anything if empty.TYPE:set[str]DEFAULT:set()"], ["metadata_key", "The metadata key to store the extracted entities in.TYPE:strDEFAULT:'entities'"], ["model", "The spaCy model to use. Pass the name of a model to loador pass an instantiated spaCy model instance.TYPE:str|LanguageDEFAULT:'en_core_web_sm'"]]}, "include_labels", "Set of entity labels to include. Will include all labels if empty.", "TYPE:set[str]DEFAULT:set()", "set[str]", "set()", "exclude_labels", "Set of entity labels to exclude. Will not exclude anything if empty.", "TYPE:set[str]DEFAULT:set()", "set[str]", "set()", "metadata_key", "The metadata key to store the extracted entities in.", "TYPE:strDEFAULT:'entities'", "str", "'entities'", "model", "The spaCy model to use. Pass the name of a model to loador pass an instantiated spaCy model instance.", "TYPE:str|LanguageDEFAULT:'en_core_web_sm'", "str|Language", "'en_core_web_sm'", "See spaCy docs for the selected model to determine what NER labels will be used. The default modelen_core_web_smproduces:CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL,ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART.", "packages/langchain-graph-retriever/src/langchain_graph_retriever/transformers/spacy.py", {"table": [["57585960616263646566676869707172737475767778", "def__init__(self,*,include_labels:set[str]=set(),exclude_labels:set[str]=set(),limit:int|None=None,metadata_key:str=\"entities\",model:str|Language=\"en_core_web_sm\",):self.include_labels=include_labelsself.exclude_labels=exclude_labelsself.limit=limitself.metadata_key=metadata_keyifisinstance(model,str):ifnotspacy.util.is_package(model):spacy.cli.download(model)# type: ignoreself.model=spacy.load(model)elifisinstance(model,Language):self.model=modelelse:raiseValueError(f\"Invalid model:{model}\")"]]}, "57585960616263646566676869707172737475767778", "def__init__(self,*,include_labels:set[str]=set(),exclude_labels:set[str]=set(),limit:int|None=None,metadata_key:str=\"entities\",model:str|Language=\"en_core_web_sm\",):self.include_labels=include_labelsself.exclude_labels=exclude_labelsself.limit=limitself.metadata_key=metadata_keyifisinstance(model,str):ifnotspacy.util.is_package(model):spacy.cli.download(model)# type: ignoreself.model=spacy.load(model)elifisinstance(model,Language):self.model=modelelse:raiseValueError(f\"Invalid model:{model}\")", "def__init__(self,*,include_labels:set[str]=set(),exclude_labels:set[str]=set(),limit:int|None=None,metadata_key:str=\"entities\",model:str|Language=\"en_core_web_sm\",):self.include_labels=include_labelsself.exclude_labels=exclude_labelsself.limit=limitself.metadata_key=metadata_keyifisinstance(model,str):ifnotspacy.util.is_package(model):spacy.cli.download(model)# type: ignoreself.model=spacy.load(model)elifisinstance(model,Language):self.model=modelelse:raiseValueError(f\"Invalid model:{model}\")"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "LazyGraphRAG in LangChain", "content": ["Finally, we produce a chain that puts everything together.Given aGraphRetrieverit retrieves documents, creates communities using edges amongst the retrieved documents, extracts claims from those communities, ranks and selects the best claims, and then answers the question using those claims.", "GraphRetriever", "fromtypingimportAnyfromgraph_retriever.edgesimportEdgeSpec,MetadataEdgeFunctionfromlangchain_core.language_modelsimportBaseLanguageModelfromlangchain_core.runnablesimportchainfromlangchain_graph_retrieverimportGraphRetrieverfromlangchain_graph_retriever.document_graphimportcreate_graph,group_by_community@chainasyncdeflazy_graph_rag(question:str,*,retriever:GraphRetriever,model:BaseLanguageModel,edges:Iterable[EdgeSpec]|MetadataEdgeFunction|None=None,max_tokens:int=1000,**kwargs:Any,)->str:\"\"\"Retrieve claims relating to the question using LazyGraphRAG.Returns the top claims up to the given `max_tokens` as a markdown list.\"\"\"edges=edgesorretriever.edgesifedgesisNone:raiseValueError(\"Must specify 'edges' in invocation or retriever\")# 1. Retrieve documents using the (traversing) retriever.documents=awaitretriever.ainvoke(question,edges=edges,**kwargs)# 2. Create a graph and extract communities.document_graph=create_graph(documents,edges=edges)communities=group_by_community(document_graph)# 3. Extract claims from the communities.claims=awaitclaims_chain.ainvoke({\"question\":question,\"communities\":communities})# 4. Rank the claims and select claims up to the given token limit.result_claims=[]tokens=0forclaiminawaitrank_chain.ainvoke({\"question\":question,\"claims\":claims}):claim_str=f\"-{claim.claim}(Source:{claim.source_id})\"tokens+=model.get_num_tokens(claim_str)iftokens>max_tokens:breakresult_claims.append(claim_str)return\"\\n\".join(result_claims)"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Introduction", "content": ["InLazyGraphRAG, Microsoft demonstrates significant cost and performance benefits to delaying the construction of a knowledge graph.This is largely because not all documents need to be analyzed.However, it is also benefical that documents by the time documents are analyzed the question is already known, allowing irrelevant information to be ignored.", "We've noticed similar cost benefits to building a document graph linking content based on simple properties such as extracted keywords compared to building a complete knowledge graph.For the Wikipedia dataset used in this notebook, we estimated it would have taken $70k to build a knowledege graph using theexample from LangChain, while the document graph was basically free.", "In this notebook we demonstrate how to populate a document graph with Wikipedia articles linked based on mentions in the articles and extracted keywords.Keyword extraction uses a localKeyBERTmodel, making it fast and cost-effective to construct these graphs.We'll then show how to build out a chain which does the steps of Lazy GraphRAG -- retrieving articles, extracting claims from each community, ranking and selecting the top claims, and generating an answer based on those claims."]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Environment Setup", "content": ["The following block will configure the environment from the Colab Secrets.To run it, you should have the following Colab Secrets defined and accessible to this notebook:", "OPENAI_API_KEY: The OpenAI key.", "ASTRA_DB_API_ENDPOINT: The Astra DB API endpoint.", "ASTRA_DB_APPLICATION_TOKEN: The Astra DB Application token.", "LANGCHAIN_API_KEY: Optional. If defined, will enable LangSmith tracing.", "ASTRA_DB_KEYSPACE: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default.", "OPENAI_API_KEY", "ASTRA_DB_API_ENDPOINT", "ASTRA_DB_APPLICATION_TOKEN", "LANGCHAIN_API_KEY", "ASTRA_DB_KEYSPACE", "# Install modules.## On Apple hardware, \"spacy[apple]\" will improve performance.%pipinstall\\langchain-core\\langchain-astradb\\langchain-openai\\langchain-graph-retriever\\spacy\\graph-rag-example-helpers", "The last package --graph-rag-example-helpers-- includes some helpers for setting up environment helpers and allowing the loading of wikipedia data to be restarted if it fails.", "graph-rag-example-helpers", "# Downloads the model used by Spacy for extracting entities.!python-mspacydownloaden_core_web_sm", "# Configure import paths.importosimportsyssys.path.append(\"../../\")# Initialize environment variables.fromgraph_rag_example_helpers.envimportEnvironment,initialize_environmentinitialize_environment(Environment.ASTRAPY)os.environ[\"LANGCHAIN_PROJECT\"]=\"lazy-graph-rag\"# The full dataset is ~6m documents, and takes hours to load.# The short dataset is 1000 documents and loads quickly.# Change this to `True` to use the larger dataset.USE_SHORT_DATASET=True"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Part 1: Loading Data", "content": ["First, we'll demonstrate how to load Wikipedia data into anAstraDBVectorStore, using the mentioned articles and keywords as metadata fields.In this section, we're not actually doing anything special for the graph -- we're just populating the metadata with fields that useful describe our content.", "AstraDBVectorStore"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Create Documents from Wikipedia Articles", "content": ["The first thing we need to do is create theLangChainDocuments we'll import.", "LangChain", "Document", "To do this, we write some code to convert lines from a JSON file downloaded from2wikimultihopand create aDocument.We populate theidandmetadata[\"mentions\"]from information in this file.", "Document", "id", "metadata[\"mentions\"]", "Then, we run those documents through theSpacyNERTransformerto populatemetadata[\"entities\"]with entities named in the article.", "SpacyNERTransformer", "metadata[\"entities\"]", "importjsonfromcollections.abcimportIteratorfromlangchain_core.documentsimportDocumentfromlangchain_graph_retriever.transformers.spacyimport(SpacyNERTransformer,)defparse_document(line:bytes)->Document:\"\"\"Reads one JSON line from the wikimultihop dump.\"\"\"para=json.loads(line)id=para[\"id\"]title=para[\"title\"]# Use structured information (mentioned Wikipedia IDs) as metadata.mentioned_ids=[idforminpara[\"mentions\"]forminm[\"ref_ids\"]or[]]returnDocument(id=id,page_content=\" \".join(para[\"sentences\"]),metadata={\"mentions\":mentioned_ids,\"title\":title,},)NER_TRANSFORMER=SpacyNERTransformer(limit=1000,exclude_labels={\"CARDINAL\",\"MONEY\",\"QUANTITY\",\"TIME\",\"PERCENT\",\"ORDINAL\"},)# Load data in batches, using GLiNER to extract entities.defprepare_batch(lines:Iterator[str])->Iterator[Document]:# Parse documents from the batch of lines.docs=[parse_document(line)forlineinlines]docs=NER_TRANSFORMER.transform_documents(docs)returndocs"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Create the AstraDBVectorStore", "content": ["Next, we create the Vector Store we're going to load these documents into.In our case, we use DataStax Astra DB with Open AI embeddings.", "fromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsCOLLECTION=\"lazy_graph_rag_short\"ifUSE_SHORT_DATASETelse\"lazy_graph_rag\"store=AstraDBVectorStore(embedding=OpenAIEmbeddings(),collection_name=COLLECTION,pre_delete_collection=USE_SHORT_DATASET,)"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Loading Data into the Store", "content": ["Next, we perform the actual loading.This takes a while, so we use a helper utility to persist which batches have been written so we can resume if there are any failures.", "On OS X, it is useful to runcaffeinate -disin a shell to prevent the machine from going to sleep and seems to reduce errors.", "caffeinate -dis", "importosimportos.pathfromgraph_rag_example_helpers.datasets.wikimultihopimportaload_2wikimultihop# Path to the file `para_with_hyperlink.zip`.# See instructions here to download from# [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).PARA_WITH_HYPERLINK_ZIP=os.path.join(os.getcwd(),\"para_with_hyperlink.zip\")awaitaload_2wikimultihop(limit=100ifUSE_SHORT_DATASETelseNone,full_para_with_hyperlink_zip_path=PARA_WITH_HYPERLINK_ZIP,store=store,batch_prepare=prepare_batch,)", "At this point, we've created aVectorStorewith the Wikipedia articles.Each article is associated with metadata identifying other articles it mentions and entities from the article.", "VectorStore", "As is, this is useful for performing a vector search filtered to articles mentioning a specific term or performing an entity seach on the documents.The librarylangchain-graph-retrievermakes this even more useful by allowing articles to be traversed based on relationships such as articles mentioned in the current article (or mentioning the current article) or articles providing more information on the entities mentioned in the current article.", "langchain-graph-retriever", "In the next section we'll see not just how we can use the relationships in the metadata to retrieve more articles, but we'll go a step further and perform Lazy GraphRAG to extract relevant claims from both the similar and related articles and use the most relevant claims to answer the question."]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Part 2: Lazy Graph RAG via Hierarchical Summarization", "content": ["As we've noted before, eagerly building a knowledge graph is prohibitively expensive.Microsoft seems to agree, and recently introduced LazyGraphRAG, which enables GraphRAG to be performed late -- after a query is retrieved.", "We implement the LazyGraphRAG technique using the traversing retrievers as follows:", "Retrieve a good number of nodes using a traversing retrieval.", "Identify communities in the retrieved sub-graph.", "Extract claims from each community relevant to the query using an LLM.", "Rank each of the claims based on the relevance to the question and select the top claims.", "Generate an answer to the question based on the extracted claims."]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "LangChain for Extracting Claims", "content": ["The first thing we do is create a chain that produces the claims. Given an input containing the question and the retrieved communities, it applies an LLM in parallel extracting claims from each community.", "A claim is just a string representing the statement and thesource_idof the document. We request structured output so we get a list of claims.", "source_id", "fromcollections.abcimportIterablefromoperatorimportitemgetterfromtypingimportTypedDictfromlangchain_core.documentsimportDocumentfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnableLambda,RunnableParallel,chainfromlangchain_openaiimportChatOpenAIfrompydanticimportBaseModel,FieldclassClaim(BaseModel):\"\"\"Representation of an individual claim from a source document(s).\"\"\"claim:str=Field(description=\"The claim from the original document(s).\")source_id:str=Field(description=\"Document ID containing the claim.\")classClaims(BaseModel):\"\"\"Claims extracted from a set of source document(s).\"\"\"claims:list[Claim]=Field(description=\"The extracted claims.\")MODEL=ChatOpenAI(model=\"gpt-4o\",temperature=0)CLAIMS_MODEL=MODEL.with_structured_output(Claims)CLAIMS_PROMPT=ChatPromptTemplate.from_template(\"\"\"Extract claims from the following related documents.Only return claims appearing within the specified documents.If no documents are provided, do not make up claims or documents.Claims (and scores) should be relevant to the question.Don't include claims from the documents if they are not directly or indirectlyrelevant to the question.If none of the documents make any claims relevant to the question, return anempty list of claims.If multiple documents make similar claims, include the original text of each asseparate claims. Score the most useful and authoritative claim higher thansimilar, lower-quality claims.Question:{question}{formatted_documents}\"\"\")# TODO: Few-shot examples? Possibly with a selector?defformat_documents_with_ids(documents:Iterable[Document])->str:formatted_docs=\"\\n\\n\".join(f\"Document ID:{doc.id}\\nContent:{doc.page_content}\"fordocindocuments)returnformatted_docsCLAIM_CHAIN=(RunnableParallel({\"question\":itemgetter(\"question\"),\"formatted_documents\":itemgetter(\"documents\")|RunnableLambda(format_documents_with_ids),})|CLAIMS_PROMPT|CLAIMS_MODEL)classClaimsChainInput(TypedDict):question:strcommunities:Iterable[Iterable[Document]]@chainasyncdefclaims_chain(input:ClaimsChainInput)->Iterable[Claim]:question=input[\"question\"]communities=input[\"communities\"]# TODO: Use openai directly so this can use the batch API for performance/cost?community_claims=awaitCLAIM_CHAIN.abatch([{\"question\":question,\"documents\":community}forcommunityincommunities])return[claimforcommunityincommunity_claimsforclaimincommunity.claims]"]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "LangChain for Ranking Claims", "content": ["The next chain is used for ranking the claims so we can select the most relevant to the question.", "This is based on ideas fromRankRAG.Specifically, the prompt is constructed so that the next token should beTrueif the content is relevant andFalseif not.The probability of the token is used to determine the relevance --Truewith a higher probability is more relevant thanTruewith a lesser probability.", "True", "False", "True", "True", "importmathfromlangchain_core.runnablesimportchainRANK_PROMPT=ChatPromptTemplate.from_template(\"\"\"Rank the relevance of the following claim to the question.Output \"True\" if the claim is relevant and \"False\" if it is not.Only output True or False.Question: Where is Seattle?Claim: Seattle is in Washington State.Relevant: TrueQuestion: Where is LA?Claim: New York City is in New York State.Relevant: FalseQuestion:{question}Claim:{claim}Relevant:\"\"\")defcompute_rank(msg):logprob=msg.response_metadata[\"logprobs\"][\"content\"][0]prob=math.exp(logprob[\"logprob\"])token=logprob[\"token\"]iftoken==\"True\":returnprobeliftoken==\"False\":return1.0-probelse:raiseValueError(f\"Unexpected logprob:{logprob}\")RANK_CHAIN=RANK_PROMPT|MODEL.bind(logprobs=True)|RunnableLambda(compute_rank)classRankChainInput(TypedDict):question:strclaims:Iterable[Claim]@chainasyncdefrank_chain(input:RankChainInput)->Iterable[Claim]:# TODO: Use openai directly so this can use the batch API for performance/cost?claims=input[\"claims\"]ranks=awaitRANK_CHAIN.abatch([{\"question\":input[\"question\"],\"claim\":claim}forclaiminclaims])rank_claims=sorted(zip(ranks,claims,strict=True),key=lambdarank_claim:rank_claim[0])return[claimfor_,claiminrank_claims]", "We could extend this by using an MMR-like strategy for selecting claims.Specifically, we could combine the relevance of the claim to the question and the diversity compared to already selected claims to select the best variety of claims."]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Using Lazy GraphRAG in LangChain", "content": ["Finally, we sue the Lazy GraphRAG chain we created on the store we populated earlier.", "fromlangchain_core.promptsimportPromptTemplatefromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_graph_retrieverimportGraphRetrieverRETRIEVER=GraphRetriever(store=store,edges=[(\"mentions\",\"$id\"),(\"entities\",\"entities\")],k=100,start_k=30,adjacent_k=20,max_depth=3,)ANSWER_PROMPT=PromptTemplate.from_template(\"\"\"Answer the question based on the supporting claims.Only use information from the claims. Do not guess or make up any information.Where possible, reference and quote the supporting claims.Question:{question}Claims:{claims}\"\"\")LAZY_GRAPH_RAG_CHAIN=({\"question\":RunnablePassthrough(),\"claims\":RunnablePassthrough()|lazy_graph_rag.bind(retriever=RETRIEVER,model=MODEL,max_tokens=1000,),}|ANSWER_PROMPT|MODEL)", "QUESTION=\"Why are Bermudan sloop ships widely prized compared to other ships?\"result=awaitLAZY_GRAPH_RAG_CHAIN.ainvoke(QUESTION)result.content", "'Bermudan sloop ships are widely prized for several reasons. Firstly, they feature the Bermuda rig, which is popular because it is easier to sail with a smaller crew or even single-handed, is cheaper due to having less hardware, and performs well when sailing into the wind (Source: 48520). Additionally, Bermuda sloops were constructed using Bermuda cedar, a material valued for its durability and resistance to rot, contributing to the ships' longevity and performance (Source: 17186373). These factors combined make Bermudan sloops highly valued compared to other ships.'", "For comparison, below are the results to the same question using a basic RAG pattern with just vector similarity.", "fromlangchain_core.promptsimportPromptTemplatefromlangchain_core.runnablesimportRunnablePassthroughVECTOR_ANSWER_PROMPT=PromptTemplate.from_template(\"\"\"Answer the question based on the provided documents.Only use information from the documents. Do not guess or make up any information.Question:{question}Documents:{documents}\"\"\")defformat_docs(docs):return\"\\n\\n\".join(doc.page_contentfordocindocs)VECTOR_CHAIN=({\"question\":RunnablePassthrough(),\"documents\":(store.as_retriever()|format_docs),}|VECTOR_ANSWER_PROMPT|MODEL)result=VECTOR_CHAIN.invoke(QUESTION)result.content", "'The documents do not provide specific reasons why Bermudan sloop ships are widely prized compared to other ships. They describe the development and characteristics of the Bermuda sloop, such as its fore-and-aft rigged single-masted design and the use of the Bermuda rig with triangular sails, but do not explicitly state why these ships are particularly valued over others.'", "The LazyGraphRAG chain is great when a question needs to consider a large amount of relevant information in order to produce a thorough answer."]}
{"url": "https://datastax.github.io/graph-rag/examples/lazy-graph-rag/", "title": "Conclusion", "content": ["This post demonstrated how easy it is to implement Lazy GraphRAG on top of a document graph.", "It usedlangchain-graph-retrieverfrom thegraph-rag projectto implement the document graph and graph-based retrieval on top of an existing LangChainVectorStore.This means you can focus on populating and using yourVectorStorewith useful metadata and add graph-based retrieval and even Lazy GraphRAG when you need it.", "langchain-graph-retriever", "VectorStore", "VectorStore", "Any LangChainVectorStorecan be used with Lazy GraphRAG without needing to change or re-ingest the stored documents.Knowledge Graphs and GraphRAG shouldn't be hard or scary.Start simple and easily overlay edges when you need them.", "VectorStore", "Graph retrievers and LazyGraph RAG work well with agents.You can allow the agent to retrieve differently depending on the question -- doing a vector only search for simple questions, traversing to mentioned articles for a deeper question or traversing to articles that cite this to see if there is newer information available.We'll show how to combine these techniques with agents in a future post.Until then, givelangchain-graph-retrievera try and let us know how it goes!", "langchain-graph-retriever"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "graph_retriever.utils", "content": ["Utilities used in graph_retriever and related packages."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "math", "content": ["Math utility functions for vector operations."]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "cosine_similarity", "content": ["cosine_similarity(X:Matrix,Y:Matrix)->ndarray", "cosine_similarity(X:Matrix,Y:Matrix)->ndarray", "Compute row-wise cosine similarity between two equal-width matrices.", {"table": [["PARAMETER", "DESCRIPTION"], ["X", "A matrix of shape (m, n), wheremis the number of rows andnis thenumber of columns (features).TYPE:Matrix"], ["Y", "A matrix of shape (p, n), wherepis the number of rows andnis thenumber of columns (features).TYPE:Matrix"]]}, "X", "A matrix of shape (m, n), wheremis the number of rows andnis thenumber of columns (features).", "m", "n", "TYPE:Matrix", "Matrix", "Y", "A matrix of shape (p, n), wherepis the number of rows andnis thenumber of columns (features).", "p", "n", "TYPE:Matrix", "Matrix", {"table": [["RETURNS", "DESCRIPTION"], ["ndarray", "A matrix of shape (m, p) containing the cosine similarity scoresbetween each row ofXand each row ofY."]]}, "ndarray", "A matrix of shape (m, p) containing the cosine similarity scoresbetween each row ofXand each row ofY.", "X", "Y", {"table": [["RAISES", "DESCRIPTION"], ["ValueError", "If the number of columns inXandYare not equal."]]}, "ValueError", "If the number of columns inXandYare not equal.", "X", "Y", "If thesimsimdlibrary is available, it will be used for performance  optimization. Otherwise, the function falls back to a NumPy implementation.", "Divide-by-zero and invalid values in similarity calculations are replaced  with 0.0 in the output.", "simsimd", "packages/graph-retriever/src/graph_retriever/utils/math.py", {"table": [["121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071", "defcosine_similarity(X:Matrix,Y:Matrix)->np.ndarray:\"\"\"Compute row-wise cosine similarity between two equal-width matrices.Parameters----------X :A matrix of shape (m, n), where `m` is the number of rows and `n` is thenumber of columns (features).Y :A matrix of shape (p, n), where `p` is the number of rows and `n` is thenumber of columns (features).Returns-------:A matrix of shape (m, p) containing the cosine similarity scoresbetween each row of `X` and each row of `Y`.Raises------ValueErrorIf the number of columns in `X` and `Y` are not equal.Notes------ If the `simsimd` library is available, it will be used for performanceoptimization. Otherwise, the function falls back to a NumPy implementation.- Divide-by-zero and invalid values in similarity calculations are replacedwith 0.0 in the output.\"\"\"iflen(X)==0orlen(Y)==0:returnnp.array([])X=np.array(X)Y=np.array(Y)ifX.shape[1]!=Y.shape[1]:raiseValueError(f\"Number of columns in X and Y must be the same. X has shape{X.shape}\"f\"and Y has shape{Y.shape}.\")try:importsimsimdassimdX=np.array(X,dtype=np.float32)Y=np.array(Y,dtype=np.float32)Z=1-np.array(simd.cdist(X,Y,metric=\"cosine\"))returnZexceptImportError:logger.debug(\"Unable to import simsimd, defaulting to NumPy implementation. If you want \"\"to use simsimd please install with `pip install simsimd`.\")X_norm=np.linalg.norm(X,axis=1)Y_norm=np.linalg.norm(Y,axis=1)# Ignore divide by zero errors run time warnings as those are handled below.withnp.errstate(divide=\"ignore\",invalid=\"ignore\"):similarity=np.dot(X,Y.T)/np.outer(X_norm,Y_norm)similarity[np.isnan(similarity)|np.isinf(similarity)]=0.0returnsimilarity"]]}, "121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071", "defcosine_similarity(X:Matrix,Y:Matrix)->np.ndarray:\"\"\"Compute row-wise cosine similarity between two equal-width matrices.Parameters----------X :A matrix of shape (m, n), where `m` is the number of rows and `n` is thenumber of columns (features).Y :A matrix of shape (p, n), where `p` is the number of rows and `n` is thenumber of columns (features).Returns-------:A matrix of shape (m, p) containing the cosine similarity scoresbetween each row of `X` and each row of `Y`.Raises------ValueErrorIf the number of columns in `X` and `Y` are not equal.Notes------ If the `simsimd` library is available, it will be used for performanceoptimization. Otherwise, the function falls back to a NumPy implementation.- Divide-by-zero and invalid values in similarity calculations are replacedwith 0.0 in the output.\"\"\"iflen(X)==0orlen(Y)==0:returnnp.array([])X=np.array(X)Y=np.array(Y)ifX.shape[1]!=Y.shape[1]:raiseValueError(f\"Number of columns in X and Y must be the same. X has shape{X.shape}\"f\"and Y has shape{Y.shape}.\")try:importsimsimdassimdX=np.array(X,dtype=np.float32)Y=np.array(Y,dtype=np.float32)Z=1-np.array(simd.cdist(X,Y,metric=\"cosine\"))returnZexceptImportError:logger.debug(\"Unable to import simsimd, defaulting to NumPy implementation. If you want \"\"to use simsimd please install with `pip install simsimd`.\")X_norm=np.linalg.norm(X,axis=1)Y_norm=np.linalg.norm(Y,axis=1)# Ignore divide by zero errors run time warnings as those are handled below.withnp.errstate(divide=\"ignore\",invalid=\"ignore\"):similarity=np.dot(X,Y.T)/np.outer(X_norm,Y_norm)similarity[np.isnan(similarity)|np.isinf(similarity)]=0.0returnsimilarity", "defcosine_similarity(X:Matrix,Y:Matrix)->np.ndarray:\"\"\"Compute row-wise cosine similarity between two equal-width matrices.Parameters----------X :A matrix of shape (m, n), where `m` is the number of rows and `n` is thenumber of columns (features).Y :A matrix of shape (p, n), where `p` is the number of rows and `n` is thenumber of columns (features).Returns-------:A matrix of shape (m, p) containing the cosine similarity scoresbetween each row of `X` and each row of `Y`.Raises------ValueErrorIf the number of columns in `X` and `Y` are not equal.Notes------ If the `simsimd` library is available, it will be used for performanceoptimization. Otherwise, the function falls back to a NumPy implementation.- Divide-by-zero and invalid values in similarity calculations are replacedwith 0.0 in the output.\"\"\"iflen(X)==0orlen(Y)==0:returnnp.array([])X=np.array(X)Y=np.array(Y)ifX.shape[1]!=Y.shape[1]:raiseValueError(f\"Number of columns in X and Y must be the same. X has shape{X.shape}\"f\"and Y has shape{Y.shape}.\")try:importsimsimdassimdX=np.array(X,dtype=np.float32)Y=np.array(Y,dtype=np.float32)Z=1-np.array(simd.cdist(X,Y,metric=\"cosine\"))returnZexceptImportError:logger.debug(\"Unable to import simsimd, defaulting to NumPy implementation. If you want \"\"to use simsimd please install with `pip install simsimd`.\")X_norm=np.linalg.norm(X,axis=1)Y_norm=np.linalg.norm(Y,axis=1)# Ignore divide by zero errors run time warnings as those are handled below.withnp.errstate(divide=\"ignore\",invalid=\"ignore\"):similarity=np.dot(X,Y.T)/np.outer(X_norm,Y_norm)similarity[np.isnan(similarity)|np.isinf(similarity)]=0.0returnsimilarity"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "cosine_similarity_top_k", "content": ["cosine_similarity_top_k(X:Matrix,Y:Matrix,top_k:int|None,score_threshold:float|None=None,)->tuple[list[tuple[int,int]],list[float]]", "cosine_similarity_top_k(X:Matrix,Y:Matrix,top_k:int|None,score_threshold:float|None=None,)->tuple[list[tuple[int,int]],list[float]]", "Row-wise cosine similarity with optional top-k and score threshold filtering.", {"table": [["PARAMETER", "DESCRIPTION"], ["X", "A matrix of shape (m, n), wheremis the number of rows andnis thenumber of columns (features).TYPE:Matrix"], ["Y", "A matrix of shape (p, n), wherepis the number of rows andnis thenumber of columns (features).TYPE:Matrix"], ["top_k", "Max number of results to return.TYPE:int| None"], ["score_threshold", "Minimum score to return.TYPE:float| NoneDEFAULT:None"]]}, "X", "A matrix of shape (m, n), wheremis the number of rows andnis thenumber of columns (features).", "m", "n", "TYPE:Matrix", "Matrix", "Y", "A matrix of shape (p, n), wherepis the number of rows andnis thenumber of columns (features).", "p", "n", "TYPE:Matrix", "Matrix", "top_k", "Max number of results to return.", "TYPE:int| None", "int| None", "score_threshold", "Minimum score to return.", "TYPE:float| NoneDEFAULT:None", "float| None", "None", {"table": [["RETURNS", "DESCRIPTION"], ["list[tuple[int,int]]", "Two-tuples of indices(X_idx, Y_idx)indicating the respective rows inXandY."], ["list[float]", "The corresponding cosine similarities."]]}, "list[tuple[int,int]]", "Two-tuples of indices(X_idx, Y_idx)indicating the respective rows inXandY.", "(X_idx, Y_idx)", "X", "Y", "list[float]", "The corresponding cosine similarities.", "packages/graph-retriever/src/graph_retriever/utils/math.py", {"table": [["7475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114", "defcosine_similarity_top_k(X:Matrix,Y:Matrix,top_k:int|None,score_threshold:float|None=None,)->tuple[list[tuple[int,int]],list[float]]:\"\"\"Row-wise cosine similarity with optional top-k and score threshold filtering.Parameters----------X :A matrix of shape (m, n), where `m` is the number of rows and `n` is thenumber of columns (features).Y :A matrix of shape (p, n), where `p` is the number of rows and `n` is thenumber of columns (features).top_k :Max number of results to return.score_threshold:Minimum score to return.Returns-------list[tuple[int, int]]Two-tuples of indices `(X_idx, Y_idx)` indicating the respective rows in`X` and `Y`.list[float]The corresponding cosine similarities.\"\"\"iflen(X)==0orlen(Y)==0:return[],[]score_array=cosine_similarity(X,Y)score_threshold=score_thresholdor-1.0score_array[score_array<score_threshold]=0top_k=min(top_korlen(score_array),np.count_nonzero(score_array))top_k_idxs=np.argpartition(score_array,-top_k,axis=None)[-top_k:]top_k_idxs=top_k_idxs[np.argsort(score_array.ravel()[top_k_idxs])][::-1]ret_idxs=np.unravel_index(top_k_idxs,score_array.shape)scores=score_array.ravel()[top_k_idxs].tolist()returnlist(zip(*ret_idxs)),scores# type: ignore"]]}, "7475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114", "defcosine_similarity_top_k(X:Matrix,Y:Matrix,top_k:int|None,score_threshold:float|None=None,)->tuple[list[tuple[int,int]],list[float]]:\"\"\"Row-wise cosine similarity with optional top-k and score threshold filtering.Parameters----------X :A matrix of shape (m, n), where `m` is the number of rows and `n` is thenumber of columns (features).Y :A matrix of shape (p, n), where `p` is the number of rows and `n` is thenumber of columns (features).top_k :Max number of results to return.score_threshold:Minimum score to return.Returns-------list[tuple[int, int]]Two-tuples of indices `(X_idx, Y_idx)` indicating the respective rows in`X` and `Y`.list[float]The corresponding cosine similarities.\"\"\"iflen(X)==0orlen(Y)==0:return[],[]score_array=cosine_similarity(X,Y)score_threshold=score_thresholdor-1.0score_array[score_array<score_threshold]=0top_k=min(top_korlen(score_array),np.count_nonzero(score_array))top_k_idxs=np.argpartition(score_array,-top_k,axis=None)[-top_k:]top_k_idxs=top_k_idxs[np.argsort(score_array.ravel()[top_k_idxs])][::-1]ret_idxs=np.unravel_index(top_k_idxs,score_array.shape)scores=score_array.ravel()[top_k_idxs].tolist()returnlist(zip(*ret_idxs)),scores# type: ignore", "defcosine_similarity_top_k(X:Matrix,Y:Matrix,top_k:int|None,score_threshold:float|None=None,)->tuple[list[tuple[int,int]],list[float]]:\"\"\"Row-wise cosine similarity with optional top-k and score threshold filtering.Parameters----------X :A matrix of shape (m, n), where `m` is the number of rows and `n` is thenumber of columns (features).Y :A matrix of shape (p, n), where `p` is the number of rows and `n` is thenumber of columns (features).top_k :Max number of results to return.score_threshold:Minimum score to return.Returns-------list[tuple[int, int]]Two-tuples of indices `(X_idx, Y_idx)` indicating the respective rows in`X` and `Y`.list[float]The corresponding cosine similarities.\"\"\"iflen(X)==0orlen(Y)==0:return[],[]score_array=cosine_similarity(X,Y)score_threshold=score_thresholdor-1.0score_array[score_array<score_threshold]=0top_k=min(top_korlen(score_array),np.count_nonzero(score_array))top_k_idxs=np.argpartition(score_array,-top_k,axis=None)[-top_k:]top_k_idxs=top_k_idxs[np.argsort(score_array.ravel()[top_k_idxs])][::-1]ret_idxs=np.unravel_index(top_k_idxs,score_array.shape)scores=score_array.ravel()[top_k_idxs].tolist()returnlist(zip(*ret_idxs)),scores# type: ignore"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "merge", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "amergeasync", "content": ["async", "amerge(*async_iterables:AsyncIterator[T],queue_size:int=10,)->AsyncIterator[T]", "amerge(*async_iterables:AsyncIterator[T],queue_size:int=10,)->AsyncIterator[T]", "Merge async iterables into a single async iterator.", "Elements are yielded in the order they become available.", {"table": [["PARAMETER", "DESCRIPTION"], ["async_iterables", "The async iterators to merge.TYPE:AsyncIterator[T]DEFAULT:()"], ["queue_size", "Number of elements to buffer in the queue.TYPE:intDEFAULT:10"]]}, "async_iterables", "The async iterators to merge.", "TYPE:AsyncIterator[T]DEFAULT:()", "AsyncIterator[T]", "()", "queue_size", "Number of elements to buffer in the queue.", "TYPE:intDEFAULT:10", "int", "10", {"table": [["YIELDS", "DESCRIPTION"], ["AsyncIterator[T]", "The elements of the iterators as they become available."]]}, "AsyncIterator[T]", "The elements of the iterators as they become available.", "packages/graph-retriever/src/graph_retriever/utils/merge.py", {"table": [["14151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465", "asyncdefamerge(*async_iterables:AsyncIterator[T],queue_size:int=10,)->AsyncIterator[T]:\"\"\"Merge async iterables into a single async iterator.Elements are yielded in the order they become available.Parameters----------async_iterables :The async iterators to merge.queue_size :Number of elements to buffer in the queue.Yields------:The elements of the iterators as they become available.\"\"\"queue:asyncio.Queue[T|_Done]=asyncio.Queue(queue_size)asyncdefpump(aiter:AsyncIterator[T])->None:try:asyncforiteminaiter:awaitqueue.put(item)awaitqueue.put(_Done(exception=False))except:awaitqueue.put(_Done(exception=True))raisetasks=[asyncio.create_task(pump(aiter))foraiterinasync_iterables]try:pending_count=len(async_iterables)whilepending_count>0:item=awaitqueue.get()ifisinstance(item,_Done):ifitem.exception:# If there has been an exception, end early.breakelse:pending_count-=1else:yielditemqueue.task_done()finally:fortaskintasks:ifnottask.done():task.cancel()awaitasyncio.gather(*tasks)"]]}, "14151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465", "asyncdefamerge(*async_iterables:AsyncIterator[T],queue_size:int=10,)->AsyncIterator[T]:\"\"\"Merge async iterables into a single async iterator.Elements are yielded in the order they become available.Parameters----------async_iterables :The async iterators to merge.queue_size :Number of elements to buffer in the queue.Yields------:The elements of the iterators as they become available.\"\"\"queue:asyncio.Queue[T|_Done]=asyncio.Queue(queue_size)asyncdefpump(aiter:AsyncIterator[T])->None:try:asyncforiteminaiter:awaitqueue.put(item)awaitqueue.put(_Done(exception=False))except:awaitqueue.put(_Done(exception=True))raisetasks=[asyncio.create_task(pump(aiter))foraiterinasync_iterables]try:pending_count=len(async_iterables)whilepending_count>0:item=awaitqueue.get()ifisinstance(item,_Done):ifitem.exception:# If there has been an exception, end early.breakelse:pending_count-=1else:yielditemqueue.task_done()finally:fortaskintasks:ifnottask.done():task.cancel()awaitasyncio.gather(*tasks)", "asyncdefamerge(*async_iterables:AsyncIterator[T],queue_size:int=10,)->AsyncIterator[T]:\"\"\"Merge async iterables into a single async iterator.Elements are yielded in the order they become available.Parameters----------async_iterables :The async iterators to merge.queue_size :Number of elements to buffer in the queue.Yields------:The elements of the iterators as they become available.\"\"\"queue:asyncio.Queue[T|_Done]=asyncio.Queue(queue_size)asyncdefpump(aiter:AsyncIterator[T])->None:try:asyncforiteminaiter:awaitqueue.put(item)awaitqueue.put(_Done(exception=False))except:awaitqueue.put(_Done(exception=True))raisetasks=[asyncio.create_task(pump(aiter))foraiterinasync_iterables]try:pending_count=len(async_iterables)whilepending_count>0:item=awaitqueue.get()ifisinstance(item,_Done):ifitem.exception:# If there has been an exception, end early.breakelse:pending_count-=1else:yielditemqueue.task_done()finally:fortaskintasks:ifnottask.done():task.cancel()awaitasyncio.gather(*tasks)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "run_in_executor", "content": []}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "run_in_executorasync", "content": ["async", "run_in_executor(executor:Executor|None,func:Callable[P,T],*args:args,**kwargs:kwargs,)->T", "run_in_executor(executor:Executor|None,func:Callable[P,T],*args:args,**kwargs:kwargs,)->T", "Run a function in an executor.", {"table": [["PARAMETER", "DESCRIPTION"], ["executor", "The executor to run in.TYPE:Executor| None"], ["func", "The function.TYPE:Callable[P,T]"], ["*args", "The positional arguments to the function.TYPE:argsDEFAULT:()"], ["kwargs", "The keyword arguments to the function.TYPE:kwargsDEFAULT:{}"]]}, "executor", "The executor to run in.", "TYPE:Executor| None", "Executor| None", "func", "The function.", "TYPE:Callable[P,T]", "Callable[P,T]", "*args", "The positional arguments to the function.", "TYPE:argsDEFAULT:()", "args", "()", "kwargs", "The keyword arguments to the function.", "TYPE:kwargsDEFAULT:{}", "kwargs", "{}", {"table": [["RETURNS", "DESCRIPTION"], ["T", "The output of the function."]]}, "T", "The output of the function.", {"table": [["RAISES", "DESCRIPTION"], ["RuntimeError", "If the function raises a StopIteration."]]}, "RuntimeError", "If the function raises a StopIteration.", "packages/graph-retriever/src/graph_retriever/utils/run_in_executor.py", {"table": [["121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859", "asyncdefrun_in_executor(executor:Executor|None,func:Callable[P,T],*args:P.args,**kwargs:P.kwargs,)->T:# noqa: DOC502\"\"\"Run a function in an executor.Parameters----------executor :The executor to run in.func :The function.*args :The positional arguments to the function.kwargs :The keyword arguments to the function.Returns-------:The output of the function.Raises------RuntimeErrorIf the function raises a StopIteration.\"\"\"# noqa: DOC502defwrapper()->T:try:returnfunc(*args,**kwargs)exceptStopIterationasexc:# StopIteration can't be set on an asyncio.Future# it raises a TypeError and leaves the Future pending forever# so we need to convert it to a RuntimeErrorraiseRuntimeErrorfromexcifexecutorisNoneorisinstance(executor,dict):# Use default executor with context copied from current contextreturnawaitasyncio.get_running_loop().run_in_executor(None,cast(Callable[...,T],partial(copy_context().run,wrapper)),)returnawaitasyncio.get_running_loop().run_in_executor(executor,wrapper)"]]}, "121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859", "asyncdefrun_in_executor(executor:Executor|None,func:Callable[P,T],*args:P.args,**kwargs:P.kwargs,)->T:# noqa: DOC502\"\"\"Run a function in an executor.Parameters----------executor :The executor to run in.func :The function.*args :The positional arguments to the function.kwargs :The keyword arguments to the function.Returns-------:The output of the function.Raises------RuntimeErrorIf the function raises a StopIteration.\"\"\"# noqa: DOC502defwrapper()->T:try:returnfunc(*args,**kwargs)exceptStopIterationasexc:# StopIteration can't be set on an asyncio.Future# it raises a TypeError and leaves the Future pending forever# so we need to convert it to a RuntimeErrorraiseRuntimeErrorfromexcifexecutorisNoneorisinstance(executor,dict):# Use default executor with context copied from current contextreturnawaitasyncio.get_running_loop().run_in_executor(None,cast(Callable[...,T],partial(copy_context().run,wrapper)),)returnawaitasyncio.get_running_loop().run_in_executor(executor,wrapper)", "asyncdefrun_in_executor(executor:Executor|None,func:Callable[P,T],*args:P.args,**kwargs:P.kwargs,)->T:# noqa: DOC502\"\"\"Run a function in an executor.Parameters----------executor :The executor to run in.func :The function.*args :The positional arguments to the function.kwargs :The keyword arguments to the function.Returns-------:The output of the function.Raises------RuntimeErrorIf the function raises a StopIteration.\"\"\"# noqa: DOC502defwrapper()->T:try:returnfunc(*args,**kwargs)exceptStopIterationasexc:# StopIteration can't be set on an asyncio.Future# it raises a TypeError and leaves the Future pending forever# so we need to convert it to a RuntimeErrorraiseRuntimeErrorfromexcifexecutorisNoneorisinstance(executor,dict):# Use default executor with context copied from current contextreturnawaitasyncio.get_running_loop().run_in_executor(None,cast(Callable[...,T],partial(copy_context().run,wrapper)),)returnawaitasyncio.get_running_loop().run_in_executor(executor,wrapper)"]}
{"url": "https://datastax.github.io/graph-rag/reference/graph_retriever/utils/", "title": "top_k", "content": ["top_k(contents:Iterable[Content],*,embedding:list[float],k:int,)->list[Content]", "top_k(contents:Iterable[Content],*,embedding:list[float],k:int,)->list[Content]", "Select the top-k contents from the given content.", {"table": [["PARAMETER", "DESCRIPTION"], ["contents", "The content from which to select the top-K.TYPE:Iterable[Content]"], ["embedding", "The embedding we're looking for.TYPE:list[float]"], ["k", "The number of items to select.TYPE:int"]]}, "contents", "The content from which to select the top-K.", "TYPE:Iterable[Content]", "Iterable[Content]", "embedding", "The embedding we're looking for.", "TYPE:list[float]", "list[float]", "k", "The number of items to select.", "TYPE:int", "int", {"table": [["RETURNS", "DESCRIPTION"], ["list[Content]", "Top-K by similarity."]]}, "list[Content]", "Top-K by similarity.", "packages/graph-retriever/src/graph_retriever/utils/top_k.py", {"table": [["7891011121314151617181920212223242526272829303132333435363738394041424344", "deftop_k(contents:Iterable[Content],*,embedding:list[float],k:int,)->list[Content]:\"\"\"Select the top-k contents from the given content.Parameters----------contents :The content from which to select the top-K.embedding: list[float]The embedding we're looking for.k :The number of items to select.Returns-------list[Content]Top-K by similarity.\"\"\"# TODO: Consider handling specially cases of already-sorted batches (merge).# TODO: Consider passing threshold here to limit results.# Use dicts to de-duplicate by ID. This ensures we choose the top K distinct# content (rather than K copies of the same content).unscored={c.id:cforcincontents}top_scored=_similarity_sort_top_k(list(unscored.values()),embedding=embedding,k=k)sorted=list(top_scored.values())sorted.sort(key=_score,reverse=True)return[c[0]forcinsorted]"]]}, "7891011121314151617181920212223242526272829303132333435363738394041424344", "deftop_k(contents:Iterable[Content],*,embedding:list[float],k:int,)->list[Content]:\"\"\"Select the top-k contents from the given content.Parameters----------contents :The content from which to select the top-K.embedding: list[float]The embedding we're looking for.k :The number of items to select.Returns-------list[Content]Top-K by similarity.\"\"\"# TODO: Consider handling specially cases of already-sorted batches (merge).# TODO: Consider passing threshold here to limit results.# Use dicts to de-duplicate by ID. This ensures we choose the top K distinct# content (rather than K copies of the same content).unscored={c.id:cforcincontents}top_scored=_similarity_sort_top_k(list(unscored.values()),embedding=embedding,k=k)sorted=list(top_scored.values())sorted.sort(key=_score,reverse=True)return[c[0]forcinsorted]", "deftop_k(contents:Iterable[Content],*,embedding:list[float],k:int,)->list[Content]:\"\"\"Select the top-k contents from the given content.Parameters----------contents :The content from which to select the top-K.embedding: list[float]The embedding we're looking for.k :The number of items to select.Returns-------list[Content]Top-K by similarity.\"\"\"# TODO: Consider handling specially cases of already-sorted batches (merge).# TODO: Consider passing threshold here to limit results.# Use dicts to de-duplicate by ID. This ensures we choose the top K distinct# content (rather than K copies of the same content).unscored={c.id:cforcincontents}top_scored=_similarity_sort_top_k(list(unscored.values()),embedding=embedding,k=k)sorted=list(top_scored.values())sorted.sort(key=_score,reverse=True)return[c[0]forcinsorted]"]}
{"url": "https://datastax.github.io/graph-rag/blog/archive/2025/", "title": "2025", "content": ["January 31, 2025", "inlangchain,news", "2 min read"]}
{"url": "https://datastax.github.io/graph-rag/blog/archive/2025/", "title": "Introducing Graph Retrievers: Smarter, Simpler Document Graphs for Vector Stores", "content": ["We're excited to announce the release ofGraph Retrievers, a powerful new tool for leveraging graph traversal in your vector stores with ease!", "With Graph Retrievers, you can dynamically explore relationships between documents using metadata fields\u2014no need for complex preprocessing or building an entire knowledge graph upfront."]}
{"url": "https://datastax.github.io/graph-rag/blog/category/news/", "title": "news", "content": ["January 31, 2025", "inlangchain,news", "2 min read"]}
{"url": "https://datastax.github.io/graph-rag/blog/category/news/", "title": "Introducing Graph Retrievers: Smarter, Simpler Document Graphs for Vector Stores", "content": ["We're excited to announce the release ofGraph Retrievers, a powerful new tool for leveraging graph traversal in your vector stores with ease!", "With Graph Retrievers, you can dynamically explore relationships between documents using metadata fields\u2014no need for complex preprocessing or building an entire knowledge graph upfront."]}
{"url": "https://datastax.github.io/graph-rag/blog/category/langchain/", "title": "langchain", "content": ["January 31, 2025", "inlangchain,news", "2 min read"]}
{"url": "https://datastax.github.io/graph-rag/blog/category/langchain/", "title": "Introducing Graph Retrievers: Smarter, Simpler Document Graphs for Vector Stores", "content": ["We're excited to announce the release ofGraph Retrievers, a powerful new tool for leveraging graph traversal in your vector stores with ease!", "With Graph Retrievers, you can dynamically explore relationships between documents using metadata fields\u2014no need for complex preprocessing or building an entire knowledge graph upfront."]}
{"url": "https://datastax.github.io/graph-rag/blog/2025/01/31/introducing-graph-rag/", "title": "Introducing Graph Retrievers: Smarter, Simpler Document Graphs for Vector Stores", "content": ["We're excited to announce the release ofGraph Retrievers, a powerful new tool for leveraging graph traversal in your vector stores with ease!", "With Graph Retrievers, you can dynamically explore relationships between documents using metadata fields\u2014no need for complex preprocessing or building an entire knowledge graph upfront."]}
{"url": "https://datastax.github.io/graph-rag/blog/2025/01/31/introducing-graph-rag/", "title": "A Brief History: Where We Started", "content": ["We originally developedGraphVectorStoreto efficiently handle structured relationships between documents. This approach proved especially useful forreducing costs in knowledge graph creation. By lazily traversing metadata instead of building a full graph, we made real-time retrieval more efficient and cost-effective.", "GraphVectorStore", "Recently,Microsoft introduced LazyGraphRAGwhich found similar cost and performance benefits by linking documents based on named entities rather than building a full knowledge graph.", "Since GraphVectorStore was introduced into LangChain, the concept of traversing document graphs has evolved significantly, and today, Graph Retrievers offers an easier and more flexible way to bring graph-like capabilities to your vector stores."]}
{"url": "https://datastax.github.io/graph-rag/blog/2025/01/31/introducing-graph-rag/", "title": "What\u2019s New in Graph Retrievers?", "content": ["Effortless Metadata LinkingDocuments can now be linked via metadata fields without additional processing. You define relationships on-the-fly.   Use different configurations to tailor traversal to your needs, such as exploring citations or co-authorships.", "Pluggable Traversal StrategiesIn addition to built-in strategies like eager traversal and MMR, you can now define your own logic for graph exploration.", "Broad CompatibilityAdapters are available for DataStax Astra DB, Apache Cassandra, Chroma DB, and OpenSearch, with support for additional stores easily added.", "Effortless Metadata LinkingDocuments can now be linked via metadata fields without additional processing. You define relationships on-the-fly.   Use different configurations to tailor traversal to your needs, such as exploring citations or co-authorships.", "Pluggable Traversal StrategiesIn addition to built-in strategies like eager traversal and MMR, you can now define your own logic for graph exploration.", "Broad CompatibilityAdapters are available for DataStax Astra DB, Apache Cassandra, Chroma DB, and OpenSearch, with support for additional stores easily added."]}
{"url": "https://datastax.github.io/graph-rag/blog/2025/01/31/introducing-graph-rag/", "title": "Example: Getting Started with Graph Retrievers", "content": ["Here\u2019s how you can use Graph Retrievers with an existingAstraDBVectorStorethat includes metadata fields for article mentions and named entities.", "AstraDBVectorStore", "Assuming you already have a LangChain project using a Vector Store, all you need to do is:", "The following example assumes you already have a LangChain Vector Store.   We're using an existingAstraDBVectorStoresimilar to:fromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsvector_store=AstraDBVectorStore(collection_name=\"animals\",embedding=OpenAIEmbeddings(),)", "Installlangchain-graph-retriever:pipinstalllangchain-graph-retriever", "Add the following code using your existing Vector Store.fromlangchain_graph_retrieverimportGraphRetriever# Define your graph traversaltraversal=GraphRetriever(store=vector_store,edges=[(\"mentions\",\"id\"),(\"entities\",\"entites\")],)# Query the graphtraversal.invoke(\"Where is Lithuania?\")", "The following example assumes you already have a LangChain Vector Store.   We're using an existingAstraDBVectorStoresimilar to:", "AstraDBVectorStore", "fromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsvector_store=AstraDBVectorStore(collection_name=\"animals\",embedding=OpenAIEmbeddings(),)", "fromlangchain_astradbimportAstraDBVectorStorefromlangchain_openaiimportOpenAIEmbeddingsvector_store=AstraDBVectorStore(collection_name=\"animals\",embedding=OpenAIEmbeddings(),)", "Installlangchain-graph-retriever:", "langchain-graph-retriever", "pipinstalllangchain-graph-retriever", "pipinstalllangchain-graph-retriever", "Add the following code using your existing Vector Store.", "fromlangchain_graph_retrieverimportGraphRetriever# Define your graph traversaltraversal=GraphRetriever(store=vector_store,edges=[(\"mentions\",\"id\"),(\"entities\",\"entites\")],)# Query the graphtraversal.invoke(\"Where is Lithuania?\")", "fromlangchain_graph_retrieverimportGraphRetriever# Define your graph traversaltraversal=GraphRetriever(store=vector_store,edges=[(\"mentions\",\"id\"),(\"entities\",\"entites\")],)# Query the graphtraversal.invoke(\"Where is Lithuania?\")", "With just a few lines of code, you can navigate relationships between articles, dynamically retrieving the most relevant information for your query."]}
{"url": "https://datastax.github.io/graph-rag/blog/2025/01/31/introducing-graph-rag/", "title": "Try It Out Today!", "content": ["Reflecting these improvements, we've moved the implementation to a newpackageandrepository, making it even easier to integrate and explore.", "Documentation: Learn how to get started in theofficial documentation.", "Join the Community: Share feedback or contribute by opening an issue or pull request in theGitHub repo.", "Give Graph Retrievers a try today and take your retrieval-augmented generation (RAG) workflows to the next level. We can\u2019t wait to hear what you build!"]}
