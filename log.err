Error querying OpenAI:
Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-MUQ2mi84ezqp39BqS0tFhNwr on tokens per min (TPM): Limit 30000, Requested 45064. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
Traceback:
Traceback (most recent call last):
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/src/graphrag_agent/flow_chat_v1.py", line 219, in query_openai
    response = await client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-MUQ2mi84ezqp39BqS0tFhNwr on tokens per min (TPM): Limit 30000, Requested 45064. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

MESSAGES:
[{'role': 'system', 'content': "You are an AI assistant specialized in generating Python code for creating flows in langflow-ai. Use the provided context and documentation to generate accurate and efficient solutions. Flows are composed of components and connections. Each component has a set of inputs and outputs, along with mandatory attributes. Your job is to instantiate the components, populate their attributes, and connect them to other components. The output should strictly adhere to the provided context and consist only of valid Python code and necessary comments. When selecting the class name for components look at Component Information section and extract the class name. Remember that the flow must end with a  Graph(start=<first component>, end=<last component>) statement. If you don't know how to answer, say 'I don't know'."}, {'role': 'user', 'content': 'Based on the following documentation, components, and sample code, generate a python function. Reply with code only. Do not provide any commentary:\n\n### Documentation:\n### Documentation\nThis flow creates a summarizing "for each" loop with the [Loop](/components-logic#loop) component.\nThe component iterates over a list of [Data](/concepts-objects#data-object) objects until it\'s completed, and then the **Done** loop aggregates the results.\nThe **File** component loads text files from your local machine, and then the **Parse Data** component parses them into a list of structured `Data` objects.\nThe **Loop** component passes each `Data` object to a **Prompt** to be summarized.\nWhen the **Loop** component runs out of `Data`, the **Done** loop activates, which counts the number of pages and summarizes their tone with another **Prompt**.\nThis is represented in Langflow by connecting the Parse Data component\'s **Data List** output to the Loop component\'s `Data` loop input.\n![Sample Flow looping summarizer](/img/loop-text-summarizer.png)\nThe output will look similar to this:\n\nThis component generates text using MistralAI LLMs.\nFor more information, see [Mistral AI documentation](https://docs.mistral.ai/).\n### Inputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens (advanced). |\n| model_name | String | The name of the Mistral AI model to use. Options include `open-mixtral-8x7b`, `open-mixtral-8x22b`, `mistral-small-latest`, `mistral-medium-latest`, `mistral-large-latest`, and `codestral-latest`. Default: `codestral-latest`. |\n| mistral_api_base | String | The base URL of the Mistral API. Defaults to `https://api.mistral.ai/v1` (advanced). |\n| api_key | SecretString | The Mistral API Key to use for authentication. |\n| temperature | Float | Controls randomness in the output. Default: 0.5. |\n| max_retries | Integer | Maximum number of retries for API calls. Default: 5 (advanced). |\n| timeout | Integer | Timeout for API calls in seconds. Default: 60 (advanced). |\n| max_concurrent_requests | Integer | Maximum number of concurrent API requests. Default: 3 (advanced). |\n| top_p | Float | Nucleus sampling parameter. Default: 1 (advanced). |\n| random_seed | Integer | Seed for random number generation. Default: 1 (advanced). |\n| safe_mode | Boolean | Enables safe mode for content generation (advanced). |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatMistralAI configured with the specified parameters. |\n\nThe **Split Text** processing component in this flow splits the incoming [Data](/concepts-objects) into chunks to be embedded into the vector store component.\nThe component offers control over chunk size, overlap, and separator, which affect context and granularity in vector store retrieval results.\n![A vector store ingesting documents](/img/vector-store-document-ingestion.png)\n\nThis flow creates a question-and-answer chatbot for documents that are loaded into the flow.\nThe [Unstructured.io](https://unstructured.io/) loader component loads files from your local machine, and then parses them into a list of structured [Data](/concepts-objects) objects.\nThis loaded data informs the **Open AI** component\'s responses to your questions.\n![Sample Flow retrieving data with unstructured](/img/starter-flow-unstructured-qa.png)\n\nThis component generates embeddings using [MistralAI](https://docs.mistral.ai/) models.\n### Inputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | String | The MistralAI model to use (default: "mistral-embed") |\n| mistral_api_key | SecretString | API key for authenticating with MistralAI |\n| max_concurrent_requests | Integer | Maximum number of concurrent API requests (default: 64) |\n| max_retries | Integer | Maximum number of retry attempts for failed requests (default: 5) |\n| timeout | Integer | Request timeout in seconds (default: 120) |\n| endpoint | String | Custom API endpoint URL (default: `https://api.mistral.ai/v1/`) |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | MistralAIEmbeddings instance for generating embeddings |\n\nThis component loads and parses files of various supported formats and converts the content into a [Data](/concepts-objects) object. It supports multiple file types and provides options for parallel processing and error handling.\nTo load a document, follow these steps:\nClick the **Select files** button.\nSelect a local file or a file loaded with [File management](/concepts-file-management), and then click **Select file**.\nThe loaded file name appears in the component.\nThe default maximum supported file size is 100 MB.\nTo modify this value, see [--max-file-size-upload](/environment-variables#LANGFLOW_MAX_FILE_SIZE_UPLOAD).\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| path | Files | Path to file(s) to load. Supports individual files or bundled archives. |\n| file_path | Server File Path | Data object with a `file_path` property pointing to the server file or a Message object with a path to the file. Supersedes \'Path\' but supports the same file types. |\n| separator | Separator | Specify the separator to use between multiple outputs in Message format. |\n| silent_errors | Silent Errors | If true, errors do not raise an exception. |\n| delete_server_file_after_processing | Delete Server File After Processing | If true, the Server File Path is deleted after processing. |\n| ignore_unsupported_extensions | Ignore Unsupported Extensions | If true, files with unsupported extensions are not processed. |\n| ignore_unspecified_files | Ignore Unspecified Files | If true, `Data` with no `file_path` property is ignored. |\n| use_multithreading | [Deprecated] Use Multithreading | Set \'Processing Concurrency\' greater than `1` to enable multithreading. This option is deprecated. |\n| concurrency_multithreading | Processing Concurrency | When multiple files are being processed, the number of files to process concurrently. Default is 1. Values greater than 1 enable parallel processing for 2 or more files. |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | Parsed content of the file as a [Data](/concepts-objects) object. |\n| dataframe | DataFrame | File content as a [DataFrame](/concepts-objects#dataframe-object) object. |\n| message | Message | File content as a [Message](/concepts-objects#message-object) object. |\n### Supported File Types\nText files:\n`.txt` - Text files\n`.md`, `.mdx` - Markdown files\n`.csv` - CSV files\n`.json` - JSON files\n`.yaml`, `.yml` - YAML files\n`.xml` - XML files\n`.html`, `.htm` - HTML files\n`.pdf` - PDF files\n`.docx` - Word documents\n`.py` - Python files\n`.sh` - Shell scripts\n`.sql` - SQL files\n`.js` - JavaScript files\n`.ts`, `.tsx` - TypeScript files\nArchive formats (for bundling multiple files):\n`.zip` - ZIP archives\n`.tar` - TAR archives\n`.tgz` - Gzipped TAR archives\n`.bz2` - Bzip2 compressed files\n`.gz` - Gzip compressed files\n\nThis example uses the **Astra DB vector store** component. Your vector store component\'s parameters and authentication may be different, but the document ingestion workflow is the same. A document is loaded from a local machine and chunked. The Astra DB vector store generates embeddings with the connected [model](/components-models) component, and stores them in the connected Astra DB database.\nThis vector data can then be retrieved for workloads like Retrieval Augmented Generation.\n![](/img/vector-store-retrieval.png)\nThe user\'s chat input is embedded and compared to the vectors embedded during document ingestion for a similarity search.\nThe results are output from the vector database component as a [Data](/concepts-objects) object and parsed into text.\nThis text fills the `{context}` variable in the **Prompt** component, which informs the **Open AI model** component\'s responses.\nAlternatively, connect the vector database component\'s **Retriever** port to a [retriever tool](components-tools#retriever-tool), and then to an [agent](/components-agents) component. This enables the agent to use your vector database as a tool and make decisions based on the available data.\n![](/img/vector-store-agent-retrieval-tool.png)\n\nThis component uses the [Unstructured.io](https://unstructured.io/) Serverless API to load and parse files into a list of structured [Data](/concepts-objects) objects.\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| file | File | The path to the file to be parsed (supported types are listed [here](https://docs.unstructured.io/api-reference/api-services/overview#supported-file-types)) |\n| api_key | API Key | Unstructured.io Serverless API Key |\n| api_url | Unstructured.io API URL | Optional URL for the Unstructured API |\n| chunking_strategy | Chunking Strategy | Strategy for chunking the document (options: "", "basic", "by_title", "by_page", "by_similarity") |\n| unstructured_args | Additional Arguments | Optional dictionary of additional arguments for the Unstructured.io API |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | List of Data objects containing the parsed content from the input file |\n\nThe Confluence component integrates with the Confluence wiki collaboration platform to load and process documents. It utilizes the ConfluenceLoader from LangChain to fetch content from a specified Confluence space.\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| url | Site URL | The base URL of the Confluence Space (e.g., `https://company.atlassian.net/wiki`) |\n| username | Username | Atlassian User E-mail (e.g., `email@example.com`) |\n| api_key | API Key | Atlassian API Key (Create an API key at: [Atlassian](https://id.atlassian.com/manage-profile/security/api-tokens)) |\n| space_key | Space Key | The key of the Confluence space to access |\n| cloud | Use Cloud? | Whether to use Confluence Cloud (default: true) |\n| content_format | Content Format | Specify content format (default: STORAGE) |\n| max_pages | Max Pages | Maximum number of pages to retrieve (default: 1000) |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | List of Data objects containing the loaded Confluence documents |\n\nModel components receive inputs and prompts for generating text, and the generated text is sent to an output component.\nThe model output can also be sent to the **Language Model** port and on to a **Parse Data** component, where the output can be parsed into structured [Data](/concepts-objects) objects.\nThis example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/starter-projects-basic-prompting).\n![](/img/starter-flow-basic-prompting.png)\n\n### Component Information\n### Component Information\nComponent: MistralAIModelComponent\nName: MistralModel\nInputs: [*LCModelComponent._base_inputs, IntInput(name=\'max_tokens\', display_name=\'Max Tokens\', advanced=True, info=\'The maximum number of tokens to generate. Set to 0 for unlimited tokens.\'), DropdownInput(name=\'model_name\', display_name=\'Model Name\', advanced=False, options=[\'open-mixtral-8x7b\', \'open-mixtral-8x22b\', \'mistral-small-latest\', \'mistral-medium-latest\', \'mistral-large-latest\', \'codestral-latest\'], value=\'codestral-latest\'), StrInput(name=\'mistral_api_base\', display_name=\'Mistral API Base\', advanced=True, info=\'The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\'), SecretStrInput(name=\'api_key\', display_name=\'Mistral API Key\', info=\'The Mistral API Key to use for the Mistral model.\', advanced=False, required=True, value=\'MISTRAL_API_KEY\'), FloatInput(name=\'temperature\', display_name=\'Temperature\', value=0.1, advanced=True), IntInput(name=\'max_retries\', display_name=\'Max Retries\', advanced=True, value=5), IntInput(name=\'timeout\', display_name=\'Timeout\', advanced=True, value=60), IntInput(name=\'max_concurrent_requests\', display_name=\'Max Concurrent Requests\', advanced=True, value=3), FloatInput(name=\'top_p\', display_name=\'Top P\', advanced=True, value=1), IntInput(name=\'random_seed\', display_name=\'Random Seed\', value=1, advanced=True), BoolInput(name=\'safe_mode\', display_name=\'Safe Mode\', advanced=True, value=False)]\n\nComponent: DocumentsToDataComponent\nName: DocumentsToData\nInputs: {}\n\nComponent: MistralAIEmbeddingsComponent\nName: MistalAIEmbeddings\nInputs: [DropdownInput(name=\'model\', display_name=\'Model\', advanced=False, options=[\'mistral-embed\'], value=\'mistral-embed\'), SecretStrInput(name=\'mistral_api_key\', display_name=\'Mistral API Key\', required=True), IntInput(name=\'max_concurrent_requests\', display_name=\'Max Concurrent Requests\', advanced=True, value=64), IntInput(name=\'max_retries\', display_name=\'Max Retries\', advanced=True, value=5), IntInput(name=\'timeout\', display_name=\'Request Timeout\', advanced=True, value=120), MessageTextInput(name=\'endpoint\', display_name=\'API Endpoint\', advanced=True, value=\'https://api.mistral.ai/v1/\')]\n\nComponent: ListFlowsComponent\nName: ListFlows\nInputs: {}\n\nComponent: RunFlowComponent\nName: RunFlow\nInputs: RunFlowBaseComponent._base_inputs\n\nComponent: FlowToolComponent\nName: FlowTool\nInputs: [DropdownInput(name=\'flow_name\', display_name=\'Flow Name\', info=\'The name of the flow to run.\', refresh_button=True), StrInput(name=\'tool_name\', display_name=\'Name\', info=\'The name of the tool.\'), StrInput(name=\'tool_description\', display_name=\'Description\', info="The description of the tool; defaults to the Flow\'s description."), BoolInput(name=\'return_direct\', display_name=\'Return Direct\', info=\'Return the result directly from the Tool.\', advanced=True)]\n\nComponent: ScrapeGraphMarkdownifyApi\nName: ScrapeGraphMarkdownifyApi\nInputs: [SecretStrInput(name=\'api_key\', display_name=\'ScrapeGraph API Key\', required=True, password=True, info=\'The API key to use ScrapeGraph API.\'), MessageTextInput(name=\'url\', display_name=\'URL\', tool_mode=True, info=\'The URL to markdownify.\')]\n\nComponent: AssemblyAILeMUR\nName: None\nInputs: [SecretStrInput(name=\'api_key\', display_name=\'Assembly API Key\', info=\'Your AssemblyAI API key. You can get one from https://www.assemblyai.com/\', advanced=False, required=True), DataInput(name=\'transcription_result\', display_name=\'Transcription Result\', info=\'The transcription result from AssemblyAI\', required=True), MultilineInput(name=\'prompt\', display_name=\'Input Prompt\', info=\'The text to prompt the model\', required=True), DropdownInput(name=\'final_model\', display_name=\'Final Model\', options=[\'claude3_5_sonnet\', \'claude3_opus\', \'claude3_haiku\', \'claude3_sonnet\'], value=\'claude3_5_sonnet\', info=\'The model that is used for the final prompt after compression is performed\', advanced=True), FloatInput(name=\'temperature\', display_name=\'Temperature\', advanced=True, value=0.0, info=\'The temperature to use for the model\'), IntInput(name=\'max_output_size\', display_name=\' Max Output Size\', advanced=True, value=2000, info=\'Max output size in tokens, up to 4000\'), DropdownInput(name=\'endpoint\', display_name=\'Endpoint\', options=[\'task\', \'summary\', \'question-answer\'], value=\'task\', info="The LeMUR endpoint to use. For \'summary\' and \'question-answer\', no prompt input is needed. See https://www.assemblyai.com/docs/api-reference/lemur/ for more info.", advanced=True), MultilineInput(name=\'questions\', display_name=\'Questions\', info="Comma-separated list of your questions. Only used if Endpoint is \'question-answer\'", advanced=True), MultilineInput(name=\'transcript_ids\', display_name=\'Transcript IDs\', info=\'Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided, the Transcription Result is ignored.\', advanced=True)]\n\nComponent: AstraDBToolComponent\nName: None\nInputs: [StrInput(name=\'tool_name\', display_name=\'Tool Name\', info=\'The name of the tool to be passed to the LLM.\', required=True), StrInput(name=\'tool_description\', display_name=\'Tool Description\', info=\'Describe the tool to LLM. Add any information that can help the LLM to use the tool.\', required=True), StrInput(name=\'keyspace\', display_name=\'Keyspace Name\', info=\'The name of the keyspace within Astra where the collection is stored.\', value=\'default_keyspace\', advanced=True), StrInput(name=\'collection_name\', display_name=\'Collection Name\', info=\'The name of the collection within Astra DB where the vectors will be stored.\', required=True), SecretStrInput(name=\'token\', display_name=\'Astra DB Application Token\', info=\'Authentication token for accessing Astra DB.\', value=\'ASTRA_DB_APPLICATION_TOKEN\', required=True), SecretStrInput(name=\'api_endpoint\', display_name=\'Database\' if os.getenv(\'ASTRA_ENHANCED\', \'false\').lower() == \'true\' else \'API Endpoint\', info=\'API endpoint URL for the Astra DB service.\', value=\'ASTRA_DB_API_ENDPOINT\', required=True), StrInput(name=\'projection_attributes\', display_name=\'Projection Attributes\', info=\'Attributes to be returned by the tool separated by comma.\', required=True, value=\'*\', advanced=True), TableInput(name=\'tools_params_v2\', display_name=\'Tools Parameters\', info=\'Define the structure for the tool parameters. Describe the parameters in a way the LLM can understand how to use them.\', required=False, table_schema=[{\'name\': \'name\', \'display_name\': \'Name\', \'type\': \'str\', \'description\': \'Specify the name of the output field/parameter for the model.\', \'default\': \'field\', \'edit_mode\': EditMode.INLINE}, {\'name\': \'attribute_name\', \'display_name\': \'Attribute Name\', \'type\': \'str\', \'description\': \'Specify the attribute name to be filtered on the collection. Leave empty if the attribute name is the same as the name of the field.\', \'default\': \'\', \'edit_mode\': EditMode.INLINE}, {\'name\': \'description\', \'display_name\': \'Description\', \'type\': \'str\', \'description\': \'Describe the purpose of the output field.\', \'default\': \'description of field\', \'edit_mode\': EditMode.POPOVER}, {\'name\': \'metadata\', \'display_name\': \'Is Metadata\', \'type\': \'boolean\', \'edit_mode\': EditMode.INLINE, \'description\': \'Indicate if the field is included in the metadata field.\', \'options\': [\'True\', \'False\'], \'default\': \'False\'}, {\'name\': \'mandatory\', \'display_name\': \'Is Mandatory\', \'type\': \'boolean\', \'edit_mode\': EditMode.INLINE, \'description\': \'Indicate if the field is mandatory.\', \'options\': [\'True\', \'False\'], \'default\': \'False\'}, {\'name\': \'is_timestamp\', \'display_name\': \'Is Timestamp\', \'type\': \'boolean\', \'edit_mode\': EditMode.INLINE, \'description\': \'Indicate if the field is a timestamp.\', \'options\': [\'True\', \'False\'], \'default\': \'False\'}, {\'name\': \'operator\', \'display_name\': \'Operator\', \'type\': \'str\', \'description\': \'Set the operator for the field. https://docs.datastax.com/en/astra-db-serverless/api-reference/documents.html#operators\', \'default\': \'$eq\', \'options\': [\'$gt\', \'$gte\', \'$lt\', \'$lte\', \'$eq\', \'$ne\', \'$in\', \'$nin\', \'$exists\', \'$all\', \'$size\'], \'edit_mode\': EditMode.INLINE}], value=[]), DictInput(name=\'tool_params\', info=\'DEPRECATED: Attributes to filter and description to the model. Add ! for mandatory (e.g: !customerId)\', display_name=\'Tool params\', is_list=True, advanced=True), DictInput(name=\'static_filters\', info=\'Attributes to filter and correspoding value\', display_name=\'Static filters\', advanced=True, is_list=True), IntInput(name=\'number_of_results\', display_name=\'Number of Results\', info=\'Number of results to return.\', advanced=True, value=5), BoolInput(name=\'use_search_query\', display_name=\'Semantic Search\', info=\'When this parameter is activated, the search query parameter will be used to search the collection.\', advanced=False, value=False), BoolInput(name=\'use_vectorize\', display_name=\'Use Astra DB Vectorize\', info=\'When this parameter is activated, Astra DB Vectorize method will be used to generate the embeddings.\', advanced=False, value=False), HandleInput(name=\'embedding\', display_name=\'Embedding Model\', input_types=[\'Embeddings\']), StrInput(name=\'semantic_search_instruction\', display_name=\'Semantic Search Instruction\', info=\'The instruction to use for the semantic search.\', required=True, value=\'Search query to find relevant documents.\', advanced=True)]\n\nComponent: FirecrawlMapApi\nName: FirecrawlMapApi\nInputs: [SecretStrInput(name=\'api_key\', display_name=\'API Key\', required=True, password=True, info=\'The API key to use Firecrawl API.\'), MultilineInput(name=\'urls\', display_name=\'URLs\', required=True, info=\'List of URLs to create maps from (separated by commas or new lines).\', tool_mode=True), BoolInput(name=\'ignore_sitemap\', display_name=\'Ignore Sitemap\', info=\'When true, the sitemap.xml file will be ignored during crawling.\'), BoolInput(name=\'sitemap_only\', display_name=\'Sitemap Only\', info=\'When true, only links found in the sitemap will be returned.\'), BoolInput(name=\'include_subdomains\', display_name=\'Include Subdomains\', info=\'When true, subdomains of the provided URL will also be scanned.\')]\n\n### Code Examples\n### Code Examples\n```python\ndef document_qa_graph(template: str | None = None):\n    """\n    Creates a document question-answering graph.\n    \n    This function builds a graph that:\n    1. Loads document files\n    2. Parses the document content\n    3. Takes user questions as input\n    4. Combines document content and question in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the model\'s answer\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that presents the document and user question.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - FileComponent\n        - ParseDataComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph\n        \n        # Create document QA graph with default template\n        graph = document_qa_graph()\n        \n        # Create graph with custom template\n        custom_template = "Document content:\\n{Document}\\n\\nAnswer this: {Question}"\n        custom_graph = document_qa_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = """Answer user\'s questions based on the document below:\n\n---\n\n{Document}\n\n---\n\nQuestion:\n{Question}\n\nAnswer:\n"""\n    file_component = FileComponent()\n    parse_data_component = ParseDataComponent()\n    parse_data_component.set(data=file_component.load_files)\n\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        context=parse_data_component.parse_data,\n        question=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\n    Creates a document question-answering graph.\n    \n    This function builds a graph that:\n    1. Loads document files\n    2. Parses the document content\n    3. Takes user questions as input\n    4. Combines document content and question in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the model\'s answer\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that presents the document and user question.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - FileComponent\n        - ParseDataComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph\n        \n        # Create document QA graph with default template\n        graph = document_qa_graph()\n        \n        # Create graph with custom template\n        custom_template = "Document content:\n{Document}\n\nAnswer this: {Question}"\n        custom_graph = document_qa_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef ingestion_graph():\n    """\n    Creates a document ingestion graph for RAG applications.\n    \n    This function builds a graph that:\n    1. Loads files using a FileComponent\n    2. Splits the text into chunks with SplitTextComponent\n    3. Creates embeddings using OpenAI\'s embedding model\n    4. Stores the embeddings in an AstraDB vector store\n    \n    Returns:\n        Graph: A Langflow Graph object with file_component as start and vector_store as end\n    \n    Dependencies:\n        - FileComponent\n        - SplitTextComponent\n        - OpenAIEmbeddingsComponent\n        - AstraDBVectorStoreComponent\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph\n        \n        # Create the ingestion graph\n        graph = ingestion_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    # Ingestion Graph\n    file_component = FileComponent()\n    text_splitter = SplitTextComponent()\n    text_splitter.set(data_inputs=file_component.load_files)\n    openai_embeddings = OpenAIEmbeddingsComponent()\n    vector_store = AstraDBVectorStoreComponent()\n    vector_store.set(\n        embedding_model=openai_embeddings.build_embeddings,\n        ingest_data=text_splitter.split_text,\n    )\n\n    return Graph(file_component, vector_store)\n```\n\nDocstring:\nCreates a document ingestion graph for RAG applications.\n\nThis function builds a graph that:\n1. Loads files using a FileComponent\n2. Splits the text into chunks with SplitTextComponent\n3. Creates embeddings using OpenAI\'s embedding model\n4. Stores the embeddings in an AstraDB vector store\n\nReturns:\n    Graph: A Langflow Graph object with file_component as start and vector_store as end\n\nDependencies:\n    - FileComponent\n    - SplitTextComponent\n    - OpenAIEmbeddingsComponent\n    - AstraDBVectorStoreComponent\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph\n    \n    # Create the ingestion graph\n    graph = ingestion_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef sequential_tasks_agent_graph():\n    """\n    Creates a sequential task execution agent graph.\n    \n    This function builds a graph that:\n    1. Takes a topic as input via TextInputComponent\n    2. Sets up three specialized agents: Researcher, Editor, and Comedian\n    3. Configures sequential tasks where each agent builds upon previous agent\'s work\n    4. Uses SearchAPI tool for research capabilities\n    5. Produces a final creative blog post about the given topic\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - TextInputComponent\n        - OpenAIModelComponent\n        - PromptComponent\n        - SequentialTaskAgentComponent\n        - SequentialCrewComponent\n        - SearchAPIComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph\n        \n        # Create sequential tasks agent graph\n        graph = sequential_tasks_agent_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    llm = OpenAIModelComponent()\n    search_api_tool = SearchAPIComponent()\n\n    text_input = TextInputComponent(_display_name="Topic")\n    text_input.set(input_value="Agile")\n\n    # Document Prompt for Researcher\n    document_prompt_component = PromptComponent()\n    document_prompt_component.set(\n        template="""Topic: {topic}\n\nBuild a document about this topic.""",\n        topic=text_input.text_response,\n    )\n\n    # Researcher Task Agent\n    researcher_task_agent = SequentialTaskAgentComponent()\n    researcher_task_agent.set(\n        role="Researcher",\n        goal="Search Google to find information to complete the task.",\n        backstory="Research has always been your thing. You can quickly find things on the web because of your skills.",\n        tools=[search_api_tool.build_tool],\n        llm=llm.build_model,\n        task_description=document_prompt_component.build_prompt,\n        expected_output="Bullet points and small phrases about the research topic.",\n    )\n\n    # Revision Prompt for Editor\n    revision_prompt_component = PromptComponent()\n    revision_prompt_component.set(\n        template="""Topic: {topic}\n\nRevise this document.""",\n        topic=text_input.text_response,\n    )\n\n    # Editor Task Agent\n    editor_task_agent = SequentialTaskAgentComponent()\n    editor_task_agent.set(\n        role="Editor",\n        goal="You should edit the information provided by the Researcher to make it more palatable and to not contain "\n        "misleading information.",\n        backstory="You are the editor of the most reputable journal in the world.",\n        llm=llm.build_model,\n        task_description=revision_prompt_component.build_prompt,\n        expected_output="Small paragraphs and bullet points with the corrected content.",\n        previous_task=researcher_task_agent.build_agent_and_task,\n    )\n\n    # Blog Prompt for Comedian\n    blog_prompt_component = PromptComponent()\n    blog_prompt_component.set(\n        template="""Topic: {topic}\n\nBuild a fun blog post about this topic.""",\n        topic=text_input.text_response,\n    )\n\n    # Comedian Task Agent\n    comedian_task_agent = SequentialTaskAgentComponent()\n    comedian_task_agent.set(\n        role="Comedian",\n        goal="You write comedic content based on the information provided by the editor.",\n        backstory="Your formal occupation is Comedian-in-Chief. "\n        "You write jokes, do standup comedy, and write funny articles.",\n        llm=llm.build_model,\n        task_description=blog_prompt_component.build_prompt,\n        expected_output="A small blog about the topic.",\n        previous_task=editor_task_agent.build_agent_and_task,\n    )\n\n    crew_component = SequentialCrewComponent()\n    crew_component.set(\n        tasks=comedian_task_agent.build_agent_and_task,\n    )\n\n    # Set up the output component\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    # Create the graph\n    return Graph(\n        start=text_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nCreates a sequential task execution agent graph.\n\nThis function builds a graph that:\n1. Takes a topic as input via TextInputComponent\n2. Sets up three specialized agents: Researcher, Editor, and Comedian\n3. Configures sequential tasks where each agent builds upon previous agent\'s work\n4. Uses SearchAPI tool for research capabilities\n5. Produces a final creative blog post about the given topic\n\nReturns:\n    Graph: A Langflow Graph object with text_input as start and chat_output as end\n\nDependencies:\n    - TextInputComponent\n    - OpenAIModelComponent\n    - PromptComponent\n    - SequentialTaskAgentComponent\n    - SequentialCrewComponent\n    - SearchAPIComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph\n    \n    # Create sequential tasks agent graph\n    graph = sequential_tasks_agent_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef blog_writer_graph(template: str | None = None):\n    """\n    Creates a blog writing graph that generates content based on reference URLs.\n    \n    This function builds a graph that:\n    1. Fetches content from specified URLs\n    2. Parses the fetched content\n    3. Takes user instructions as input\n    4. Combines references and instructions in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the generated blog content\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that combines references and instructions.\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - URLComponent\n        - ParseDataComponent\n        - TextInputComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph\n        \n        # Create blog writer graph with default template\n        graph = blog_writer_graph()\n        \n        # Create graph with custom template\n        custom_template = "Data: {references}\\n\\nWrite a blog post about: {instructions}"\n        custom_graph = blog_writer_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = dedent("""Reference 1:\n\n{references}\n\n---\n\n{instructions}\n\nBlog:\n""")\n    url_component = URLComponent()\n    url_component.set(urls=["https://langflow.org/", "https://docs.langflow.org/"])\n    parse_data_component = ParseDataComponent()\n    parse_data_component.set(data=url_component.fetch_content)\n\n    text_input = TextInputComponent(_display_name="Instructions")\n    text_input.set(\n        input_value="Use the references above for style to write a new blog/tutorial about Langflow and AI. "\n        "Suggest non-covered topics."\n    )\n\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        instructions=text_input.text_response,\n        references=parse_data_component.parse_data,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=text_input, end=chat_output)\n```\n\nDocstring:\n    Creates a blog writing graph that generates content based on reference URLs.\n    \n    This function builds a graph that:\n    1. Fetches content from specified URLs\n    2. Parses the fetched content\n    3. Takes user instructions as input\n    4. Combines references and instructions in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the generated blog content\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that combines references and instructions.\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - URLComponent\n        - ParseDataComponent\n        - TextInputComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph\n        \n        # Create blog writer graph with default template\n        graph = blog_writer_graph()\n        \n        # Create graph with custom template\n        custom_template = "Data: {references}\n\nWrite a blog post about: {instructions}"\n        custom_graph = blog_writer_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef memory_chatbot_graph(template: str | None = None):\n    """\n    Creates a chatbot graph with conversation memory.\n    \n    This function builds a graph that:\n    1. Maintains conversation history using MemoryComponent\n    2. Takes user input via ChatInput\n    3. Combines conversation context and current message in a prompt\n    4. Sends the prompt to an OpenAI model\n    5. Returns the model\'s response while updating conversation memory\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that includes conversation context and user message.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - MemoryComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph\n        \n        # Create memory chatbot with default template\n        graph = memory_chatbot_graph()\n        \n        # Create graph with custom template\n        custom_template = "Previous messages:\\n{context}\\n\\nUser: {user_message}\\nAI:"\n        custom_graph = memory_chatbot_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = """{context}\n\n    User: {user_message}\n    AI: """\n    memory_component = MemoryComponent()\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template, user_message=chat_input.message_response, context=memory_component.retrieve_messages_as_text\n    )\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(chat_input, chat_output)\n```\n\nDocstring:\n    Creates a chatbot graph with conversation memory.\n    \n    This function builds a graph that:\n    1. Maintains conversation history using MemoryComponent\n    2. Takes user input via ChatInput\n    3. Combines conversation context and current message in a prompt\n    4. Sends the prompt to an OpenAI model\n    5. Returns the model\'s response while updating conversation memory\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that includes conversation context and user message.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - MemoryComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph\n        \n        # Create memory chatbot with default template\n        graph = memory_chatbot_graph()\n        \n        # Create graph with custom template\n        custom_template = "Previous messages:\n{context}\n\nUser: {user_message}\nAI:"\n        custom_graph = memory_chatbot_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef vector_store_rag_graph():\n    """\n    Combines ingestion and RAG graphs into a complete vector store RAG flow.\n    \n    This function:\n    1. Creates an ingestion graph for processing documents into a vector store\n    2. Creates a RAG graph for querying the vector store and generating responses\n    3. Combines them into a single operational graph\n    \n    Returns:\n        Graph: A combined graph representing the complete RAG workflow\n    \n    Dependencies:\n        - All dependencies from ingestion_graph() and rag_graph()\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph\n        \n        # Create the complete RAG workflow\n        graph = vector_store_rag_graph()\n        \n        # Build and run the graph\n        graph.build()\n        # Run ingestion first with documents\n        # Then run queries against the stored vectors\n        ```\n    """\n    return ingestion_graph() + rag_graph()\n```\n\nDocstring:\nCombines ingestion and RAG graphs into a complete vector store RAG flow.\n\nThis function:\n1. Creates an ingestion graph for processing documents into a vector store\n2. Creates a RAG graph for querying the vector store and generating responses\n3. Combines them into a single operational graph\n\nReturns:\n    Graph: A combined graph representing the complete RAG workflow\n\nDependencies:\n    - All dependencies from ingestion_graph() and rag_graph()\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph\n    \n    # Create the complete RAG workflow\n    graph = vector_store_rag_graph()\n    \n    # Build and run the graph\n    graph.build()\n    # Run ingestion first with documents\n    # Then run queries against the stored vectors\n    ```\n\n```python\ndef basic_prompting_graph(template: str | None = None):\n    """\n    Creates a basic conversation graph with prompt templating.\n    \n    This function builds a simple graph that:\n    1. Takes user input via ChatInput\n    2. Processes the input through a template prompt\n    3. Sends the formatted prompt to an OpenAI model\n    4. Returns the model\'s response as output\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use. \n            Defaults to a pirate-themed response template.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph\n        \n        # Create graph with default pirate template\n        default_graph = basic_prompting_graph()\n        \n        # Create graph with custom template\n        custom_template = "Answer as a helpful assistant: {user_input}"\n        custom_graph = basic_prompting_graph(template=custom_template)\n        \n        # Build and run the graph\n        default_graph.build()\n        ```\n    """\n    if template is None:\n        template = """Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer:\n"""\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        user_input=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\nCreates a basic conversation graph with prompt templating.\n\nThis function builds a simple graph that:\n1. Takes user input via ChatInput\n2. Processes the input through a template prompt\n3. Sends the formatted prompt to an OpenAI model\n4. Returns the model\'s response as output\n\nArgs:\n    template (str | None, optional): Custom prompt template to use. \n        Defaults to a pirate-themed response template.\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - PromptComponent\n    - OpenAIModelComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph\n    \n    # Create graph with default pirate template\n    default_graph = basic_prompting_graph()\n    \n    # Create graph with custom template\n    custom_template = "Answer as a helpful assistant: {user_input}"\n    custom_graph = basic_prompting_graph(template=custom_template)\n    \n    # Build and run the graph\n    default_graph.build()\n    ```\n\n```python\ndef rag_graph():\n    """\n    Creates a Retrieval Augmented Generation (RAG) query graph.\n    \n    This function builds a graph that:\n    1. Takes user input via ChatInput\n    2. Uses the input to query a vector store (AstraDB)\n    3. Processes and parses retrieved documents\n    4. Formats a prompt with retrieved context and user query\n    5. Sends the prompt to OpenAI model\n    6. Returns the model\'s response as output\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - OpenAIEmbeddingsComponent\n        - AstraDBVectorStoreComponent\n        - ParseDataComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph\n        \n        # Create the RAG query graph\n        graph = rag_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    # RAG Graph\n    openai_embeddings = OpenAIEmbeddingsComponent()\n    chat_input = ChatInput()\n    rag_vector_store = AstraDBVectorStoreComponent()\n    rag_vector_store.set(\n        search_query=chat_input.message_response,\n        embedding_model=openai_embeddings.build_embeddings,\n    )\n\n    parse_data = ParseDataComponent()\n    parse_data.set(data=rag_vector_store.search_documents)\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=dedent("""Given the following context, answer the question.\n                         Context:{context}\n\n                         Question: {question}\n                         Answer:"""),\n        context=parse_data.parse_data,\n        question=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\nCreates a Retrieval Augmented Generation (RAG) query graph.\n\nThis function builds a graph that:\n1. Takes user input via ChatInput\n2. Uses the input to query a vector store (AstraDB)\n3. Processes and parses retrieved documents\n4. Formats a prompt with retrieved context and user query\n5. Sends the prompt to OpenAI model\n6. Returns the model\'s response as output\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - OpenAIEmbeddingsComponent\n    - AstraDBVectorStoreComponent\n    - ParseDataComponent\n    - PromptComponent\n    - OpenAIModelComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph\n    \n    # Create the RAG query graph\n    graph = rag_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef hierarchical_tasks_agent_graph():\n    llm = OpenAIModelComponent(model_name="gpt-4o-mini")\n    manager_llm = OpenAIModelComponent(model_name="gpt-4o")\n    search_api_tool = SearchAPIComponent()\n    researcher_agent = CrewAIAgentComponent()\n    chat_input = ChatInput()\n    researcher_agent.set(\n        tools=[search_api_tool.build_tool],\n        llm=llm.build_model,\n        role="Researcher",\n        goal="Search for information about the User\'s query and answer as best as you can",\n        backstory="You are a reliable researcher and journalist ",\n    )\n\n    editor_agent = CrewAIAgentComponent()\n\n    editor_agent.set(\n        llm=llm.build_model,\n        role="Editor",\n        goal="Evaluate the information for misleading or biased data.",\n        backstory="You are a reliable researcher and journalist ",\n    )\n\n    response_prompt = PromptComponent()\n    response_prompt.set(\n        template="""User\'s query:\n{query}\n\nRespond to the user with as much as information as you can about the topic. Delete if needed.\nIf it is just a general query (e.g a greeting) you can respond them directly.""",\n        query=chat_input.message_response,\n    )\n    manager_agent = CrewAIAgentComponent()\n    manager_agent.set(\n        llm=manager_llm.build_model,\n        role="Manager",\n        goal="You can answer general questions from the User and may call others for help if needed.",\n        backstory="You are polite and helpful. You\'ve always been a beacon of politeness.",\n    )\n    task = HierarchicalTaskComponent()\n    task.set(\n        task_description=response_prompt.build_prompt,\n        expected_output="Succinct response that answers the User\'s query.",\n    )\n    crew_component = HierarchicalCrewComponent()\n    crew_component.set(\n        tasks=task.build_task,\n        agents=[researcher_agent.build_output, editor_agent.build_output],\n        manager_agent=manager_agent.build_output,\n    )\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    return Graph(\n        start=chat_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nNone\n\n```python\ndef complex_agent_graph():\n    """\n    Creates a complex agent graph with dynamic role assignment and hierarchical task execution.\n    \n    This function builds a graph that:\n    1. Takes user input via ChatInput\n    2. Dynamically determines appropriate role, goal, and backstory based on user query\n    3. Creates a specialized agent with search and financial tools\n    4. Sets up a manager agent to coordinate responses\n    5. Structures tasks in a hierarchical manner\n    6. Returns comprehensive responses to user queries\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - OpenAIModelComponent\n        - PromptComponent\n        - CrewAIAgentComponent\n        - HierarchicalTaskComponent\n        - HierarchicalCrewComponent\n        - SearchAPIComponent\n        - YfinanceToolComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph\n        \n        # Create complex agent graph\n        graph = complex_agent_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    llm = OpenAIModelComponent(model_name="gpt-4o-mini")\n    manager_llm = OpenAIModelComponent(model_name="gpt-4o")\n    search_api_tool = SearchAPIComponent()\n    yahoo_search_tool = YfinanceToolComponent()\n    dynamic_agent = CrewAIAgentComponent()\n    chat_input = ChatInput()\n    role_prompt = PromptComponent(_display_name="Role Prompt")\n    role_prompt.set(\n        template="""Define a Role that could execute or answer well the user\'s query.\n\nUser\'s query: {query}\n\nRole should be two words max. Something like "Researcher" or "Software Developer".\n"""\n    )\n\n    goal_prompt = PromptComponent(_display_name="Goal Prompt")\n    goal_prompt.set(\n        template="""Define the Goal of this Role, given the User\'s Query.\nUser\'s query: {query}\n\nRole: {role}\n\nThe goal should be concise and specific.\nGoal:\n""",\n        query=chat_input.message_response,\n        role=role_prompt.build_prompt,\n    )\n    backstory_prompt = PromptComponent(_display_name="Backstory Prompt")\n    backstory_prompt.set(\n        template="""Define a Backstory of this Role and Goal, given the User\'s Query.\nUser\'s query: {query}\n\nRole: {role}\nGoal: {goal}\n\nThe backstory should be specific and well aligned with the rest of the information.\nBackstory:""",\n        query=chat_input.message_response,\n        role=role_prompt.build_prompt,\n        goal=goal_prompt.build_prompt,\n    )\n    dynamic_agent.set(\n        tools=[search_api_tool.build_tool, yahoo_search_tool.build_tool],\n        llm=llm.build_model,\n        role=role_prompt.build_prompt,\n        goal=goal_prompt.build_prompt,\n        backstory=backstory_prompt.build_prompt,\n    )\n\n    response_prompt = PromptComponent()\n    response_prompt.set(\n        template="""User\'s query:\n{query}\n\nRespond to the user with as much as information as you can about the topic. Delete if needed.\nIf it is just a general query (e.g a greeting) you can respond them directly.""",\n        query=chat_input.message_response,\n    )\n    manager_agent = CrewAIAgentComponent()\n    manager_agent.set(\n        llm=manager_llm.build_model,\n        role="Manager",\n        goal="You can answer general questions from the User and may call others for help if needed.",\n        backstory="You are polite and helpful. You\'ve always been a beacon of politeness.",\n    )\n    task = HierarchicalTaskComponent()\n    task.set(\n        task_description=response_prompt.build_prompt,\n        expected_output="Succinct response that answers the User\'s query.",\n    )\n    crew_component = HierarchicalCrewComponent()\n    crew_component.set(\n        tasks=task.build_task, agents=[dynamic_agent.build_output], manager_agent=manager_agent.build_output\n    )\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    return Graph(\n        start=chat_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nCreates a complex agent graph with dynamic role assignment and hierarchical task execution.\n\nThis function builds a graph that:\n1. Takes user input via ChatInput\n2. Dynamically determines appropriate role, goal, and backstory based on user query\n3. Creates a specialized agent with search and financial tools\n4. Sets up a manager agent to coordinate responses\n5. Structures tasks in a hierarchical manner\n6. Returns comprehensive responses to user queries\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - OpenAIModelComponent\n    - PromptComponent\n    - CrewAIAgentComponent\n    - HierarchicalTaskComponent\n    - HierarchicalCrewComponent\n    - SearchAPIComponent\n    - YfinanceToolComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph\n    \n    # Create complex agent graph\n    graph = complex_agent_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n### Question\nMake a flow that summarizes PDF documents using Mistral and returns key points\n'}, {'role': 'assistant', 'content': '```python\ndef pdf_summary_flow():\n    """\n    Creates a flow that summarizes PDF documents using Mistral AI and extracts key points.\n    \n    This function builds a flow that:\n    1. Loads a PDF document.\n    2. Parses the document into structured Data objects.\n    3. Summarizes the content using Mistral AI.\n    4. Extracts key points from the summary.\n    \n    Returns:\n        Graph: A Langflow Graph object with file_loader as start and key_points_extractor as end\n    \n    Dependencies:\n        - FileComponent\n        - DocumentsToDataComponent\n        - MistralAIModelComponent\n        - KeyPointsExtractorComponent\n    """\n    # Load the PDF file\n    file_loader = FileComponent()\n    file_loader.set(path="path_to_pdf_document.pdf")\n\n    # Parse the PDF into Data objects\n    document_parser = DocumentsToDataComponent()\n    document_parser.set(data=file_loader.data)\n\n    # Summarize the content using Mistral AI\n    mistral_summarizer = MistralAIModelComponent()\n    mistral_summarizer.set(\n        model_name="mistral-large-latest",\n        max_tokens=512,\n        api_key="your_mistral_api_key"\n    )\n    mistral_summarizer.set(input_data=document_parser.dataframe)\n\n    # Extract key points from the summary\n    key_points_extractor = KeyPointsExtractorComponent()\n    key_points_extractor.set(input_text=mistral_summarizer.model_output)\n\n    # Create the graph\n    return Graph(start=file_loader, end=key_points_extractor)\n```'}, {'role': 'user', 'content': 'Based on the following documentation, components, and sample code, generate a python function. Reply with code only. Do not provide any commentary:\n\n### Documentation:\n### Documentation\nThis component creates a tool for retrieving news from Yahoo Finance.\n### Inputs\nThis component does not have any input parameters.\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| tool | Tool | Yahoo Finance News tool for use in LangChain |\n\nThe [simple agent starter project](/starter-projects-simple-agent) uses an [agent component](#agent-component) connected to URL and Calculator tools to answer a user\'s questions. The OpenAI LLM acts as a brain for the agent to decide which tool to use. Tools are connected to agent components at the **Tools** port.\n![Simple agent starter flow](/img/starter-flow-simple-agent.png)\nFor a multi-agent example, see [Create a problem-solving agent](/agents-tool-calling-agent-component).\n\nThis component creates an agent that can use tools to answer questions and perform tasks based on given instructions.\nThe component includes an LLM model integration, a system message prompt, and a **Tools** port to connect tools to extend its capabilities.\nFor more information on this component, see the [tool calling agent documentation](/agents-tool-calling-agent-component).\n### Inputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent_llm | Dropdown | The provider of the language model that the agent will use to generate responses. Options include OpenAI and other providers, or Custom. |\n| system_prompt | String | System Prompt: Initial instructions and context provided to guide the agent\'s behavior. |\n| tools | List | List of tools available for the agent to use. |\n| input_value | String | The input task or question for the agent to process. |\n| add_current_date_tool | Boolean | If true, adds a tool to the agent that returns the current date. |\n| memory | Memory | Optional memory configuration for maintaining conversation history. |\n| max_iterations | Integer | Maximum number of iterations the agent can perform. |\n| handle_parsing_errors | Boolean | Whether to handle parsing errors during agent execution. |\n| verbose | Boolean | Enables verbose output for detailed logging. |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| response | Message | The agent\'s response to the given input task. |\n\nAgent components in Langflow\nAgent components define the behavior and capabilities of AI agents in your flow.\nAgents use LLMs as a reasoning engine to decide which of the connected tool components to use to solve a problem.\nTools in agentic functions are, essentially, functions that the agent can call to perform tasks or access external resources.\nA function is wrapped as a `Tool` object, with a common interface the agent understands.\nAgents become aware of tools through tool registration, where the agent is provided a list of available tools, typically at agent initialization. The `Tool` object\'s description tells the agent what the tool can do.\nThe agent then uses a connected LLM to reason through the problem to decide which tool is best for the job.\n\nThis component creates an XML Agent using LangChain.\nThe agent uses XML formatting for tool instructions to the Language Model.\n### Inputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | Language model to use for the agent |\n| user_prompt | String | Custom prompt template for the agent (includes XML formatting instructions) |\n| tools | List[Tool] | List of tools available to the agent |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | XML Agent instance |\n\nTools are typically connected to agent components at the **Tools** port.\nThe [simple agent starter project](/starter-projects-simple-agent) uses URL and Calculator tools connected to an [agent component](/components-agents#agent-component) to answer a user\'s questions. The OpenAI LLM acts as a brain for the agent to decide which tool to use.\n![Simple agent starter flow](/img/starter-flow-simple-agent.png)\nTo make a component into a tool that an agent can use, enable **Tool mode** in the component. Enabling **Tool mode** modifies a component input to accept calls from an agent.\nIf the component you want to connect to an agent doesn\'t have a **Tool mode** option, you can modify the component\'s inputs to become a tool.\nFor an example, see [Make any component a tool](/agents-tool-calling-agent-component#make-any-component-a-tool).\n\nThis component creates a Tool Calling Agent using LangChain.\n### Inputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | Language model to use for the agent |\n| system_prompt | String | System prompt for the agent |\n| user_prompt | String | User prompt template (must contain \'input\' key) |\n| chat_history | List[Data] | Optional chat history for the agent |\n| tools | List[Tool] | List of tools available to the agent |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | Tool Calling Agent instance |\n\nThis component creates an OpenAI Tools Agent using LangChain.\nFor more information, see the [LangChain documentation](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/openai_tools/).\n### Inputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | Language model to use for the agent (must be tool-enabled) |\n| system_prompt | String | System prompt for the agent |\n| user_prompt | String | User prompt template (must contain \'input\' key) |\n| chat_history | List[Data] | Optional chat history for the agent |\n| tools | List[Tool] | List of tools available to the agent |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | OpenAI Tools Agent instance |\n\nModel components receive inputs and prompts for generating text, and the generated text is sent to an output component.\nThe model output can also be sent to the **Language Model** port and on to a **Parse Data** component, where the output can be parsed into structured [Data](/concepts-objects) objects.\nThis example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/starter-projects-basic-prompting).\n![](/img/starter-flow-basic-prompting.png)\n\nThis component generates text using Google\'s Generative AI models.\nFor more information, see the [Google Generative AI documentation](https://cloud.google.com/vertex-ai/docs/).\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| Google API Key | Google API Key | Your Google API key to use for the Google Generative AI. |\n| Model | Model | The name of the model to use, such as `"gemini-pro"`. |\n| Max Output Tokens | Max Output Tokens | The maximum number of tokens to generate. |\n| Temperature | Temperature | Run inference with this temperature. |\n| Top K | Top K | Consider the set of top K most probable tokens. |\n| Top P | Top P | The maximum cumulative probability of tokens to consider when sampling. |\n| N | N | Number of chat completions to generate for each prompt. |\n### Outputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatGoogleGenerativeAI configured with the specified parameters. |\n\n### Component Information\n### Component Information\nComponent: ComposioGmailAPIComponent\nName: GmailAPI\nInputs: [*ComposioBaseComponent._base_inputs, MessageTextInput(name=\'recipient_email\', display_name=\'Recipient Email\', info=\'Email address of the recipient\', show=False, required=True, advanced=False), MessageTextInput(name=\'subject\', display_name=\'Subject\', info=\'Subject of the email\', show=False, required=True, advanced=False), MessageTextInput(name=\'body\', display_name=\'Body\', required=True, info=\'Content of the email\', show=False, advanced=False), MessageTextInput(name=\'cc\', display_name=\'CC\', info=\'Email addresses to CC (Carbon Copy) in the email, separated by commas\', show=False, advanced=True), MessageTextInput(name=\'bcc\', display_name=\'BCC\', info=\'Email addresses to BCC (Blind Carbon Copy) in the email, separated by commas\', show=False, advanced=True), BoolInput(name=\'is_html\', display_name=\'Is HTML\', info=\'Specify whether the email body contains HTML content (true/false)\', show=False, value=False, advanced=True), MessageTextInput(name=\'gmail_user_id\', display_name=\'User ID\', info="The user\'s email address or \'me\' for the authenticated user", show=False, advanced=True), IntInput(name=\'max_results\', display_name=\'Max Results\', required=True, info=\'Maximum number of emails to be returned\', show=False, advanced=False), MessageTextInput(name=\'message_id\', display_name=\'Message ID\', info=\'The ID of the specific email message\', show=False, required=True, advanced=False), MessageTextInput(name=\'thread_id\', display_name=\'Thread ID\', info=\'The ID of the email thread\', show=False, required=True, advanced=False), MessageTextInput(name=\'query\', display_name=\'Query\', info="Search query to filter emails (e.g., \'from:someone@email.com\' or \'subject:hello\')", show=False, advanced=False), MessageTextInput(name=\'message_body\', display_name=\'Message Body\', info=\'The body content of the message to be sent\', show=False, advanced=True), MessageTextInput(name=\'label_name\', display_name=\'Label Name\', info=\'Name of the Gmail label to create, modify, or filter by\', show=False, required=True, advanced=False), MessageTextInput(name=\'label_id\', display_name=\'Label ID\', info=\'The ID of the Gmail label\', show=False, advanced=False), MessageTextInput(name=\'label_ids\', display_name=\'Label Ids\', info=\'Comma-separated list of label IDs to filter messages\', show=False, advanced=True), MessageTextInput(name=\'label_list_visibility\', display_name=\'Label List Visibility\', info=\'The visibility of the label in the label list in the Gmail web interface\', show=False, advanced=True), MessageTextInput(name=\'message_list_visibility\', display_name=\'Message List Visibility\', info=\'The visibility of the label in the message list in the Gmail web interface\', show=False, advanced=True), MessageTextInput(name=\'page_token\', display_name=\'Page Token\', info=\'Token for retrieving the next page of results\', show=False, advanced=True), BoolInput(name=\'include_spam_trash\', display_name=\'Include messages from Spam/Trash\', info=\'Include messages from SPAM and TRASH in the results\', show=False, value=False, advanced=True), MessageTextInput(name=\'format\', display_name=\'Format\', info=\'The format to return the message in. Possible values: minimal, full, raw, metadata\', show=False, advanced=True), MessageTextInput(name=\'resource_name\', display_name=\'Resource Name\', info=\'The resource name of the person to provide information about\', show=False, advanced=True), MessageTextInput(name=\'person_fields\', display_name=\'Person fields\', info=\'Fields to return for the person. Multiple fields can be specified by separating them with commas\', show=False, advanced=True), MessageTextInput(name=\'attachment_id\', display_name=\'Attachment ID\', info=\'Id of the attachment\', show=False, required=True, advanced=False), MessageTextInput(name=\'file_name\', display_name=\'File name\', info=\'File name of the attachment file\', show=False, required=True, advanced=False), FileInput(name=\'attachment\', display_name=\'Add Attachment\', file_types=[\'csv\', \'txt\', \'doc\', \'docx\', \'xls\', \'xlsx\', \'pdf\', \'png\', \'jpg\', \'jpeg\', \'gif\', \'zip\', \'rar\', \'ppt\', \'pptx\'], info=\'Add an attachment\', show=False)]\n\nComponent: GmailLoaderComponent\nName: None\nInputs: [SecretStrInput(name=\'json_string\', display_name=\'JSON String of the Service Account Token\', info=\'JSON string containing OAuth 2.0 access token information for service account access\', required=True, value=\'{\\n                "account": "",\\n                "client_id": "",\\n                "client_secret": "",\\n                "expiry": "",\\n                "refresh_token": "",\\n                "scopes": [\\n                    "https://www.googleapis.com/auth/gmail.readonly",\\n                ],\\n                "token": "",\\n                "token_uri": "https://oauth2.googleapis.com/token",\\n                "universe_domain": "googleapis.com"\\n            }\'), MessageTextInput(name=\'label_ids\', display_name=\'Label IDs\', info=\'Comma-separated list of label IDs to filter emails.\', required=True, value=\'INBOX,SENT,UNREAD,IMPORTANT\'), MessageTextInput(name=\'max_results\', display_name=\'Max Results\', info=\'Maximum number of emails to load.\', required=True, value=\'10\')]\n\nComponent: GoogleGenerativeAIComponent\nName: GoogleGenerativeAIModel\nInputs: [*LCModelComponent._base_inputs, IntInput(name=\'max_output_tokens\', display_name=\'Max Output Tokens\', info=\'The maximum number of tokens to generate.\'), DropdownInput(name=\'model_name\', display_name=\'Model\', info=\'The name of the model to use.\', options=GOOGLE_GENERATIVE_AI_MODELS, value=\'gemini-1.5-pro\', refresh_button=True, combobox=True), SecretStrInput(name=\'api_key\', display_name=\'Google API Key\', info=\'The Google API Key to use for the Google Generative AI.\', required=True, real_time_refresh=True), FloatInput(name=\'top_p\', display_name=\'Top P\', info=\'The maximum cumulative probability of tokens to consider when sampling.\', advanced=True), SliderInput(name=\'temperature\', display_name=\'Temperature\', value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01), info=\'Controls randomness. Lower values are more deterministic, higher values are more creative.\'), IntInput(name=\'n\', display_name=\'N\', info=\'Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\', advanced=True), IntInput(name=\'top_k\', display_name=\'Top K\', info=\'Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\', advanced=True), BoolInput(name=\'tool_model_enabled\', display_name=\'Tool Model Enabled\', info=\'Whether to use the tool model.\', value=False)]\n\nComponent: CSVAgentComponent\nName: CSVAgent\nInputs: [*LCAgentComponent._base_inputs, HandleInput(name=\'llm\', display_name=\'Language Model\', input_types=[\'LanguageModel\'], required=True, info=\'An LLM Model Object (It can be found in any LLM Component).\'), FileInput(name=\'path\', display_name=\'File Path\', file_types=[\'csv\'], input_types=[\'str\', \'Message\'], required=True, info=\'A CSV File or File Path.\'), DropdownInput(name=\'agent_type\', display_name=\'Agent Type\', advanced=True, options=[\'zero-shot-react-description\', \'openai-functions\', \'openai-tools\'], value=\'openai-tools\'), MessageTextInput(name=\'input_value\', display_name=\'Text\', info=\'Text to be passed as input and extract info from the CSV File.\', required=True), DictInput(name=\'pandas_kwargs\', display_name=\'Pandas Kwargs\', info=\'Pandas Kwargs to be passed to the agent.\', advanced=True, is_list=True)]\n\nComponent: AgentComponent\nName: Agent\nInputs: [DropdownInput(name=\'agent_llm\', display_name=\'Model Provider\', info=\'The provider of the language model that the agent will use to generate responses.\', options=[*sorted(MODEL_PROVIDERS_DICT.keys()), \'Custom\'], value=\'OpenAI\', real_time_refresh=True, input_types=[], options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())] + [{\'icon\': \'brain\'}]), *MODEL_PROVIDERS_DICT[\'OpenAI\'][\'inputs\'], MultilineInput(name=\'system_prompt\', display_name=\'Agent Instructions\', info="System Prompt: Initial instructions and context provided to guide the agent\'s behavior.", value=\'You are a helpful assistant that can use tools to answer questions and perform tasks.\', advanced=False), *LCToolsAgentComponent._base_inputs, *memory_inputs, BoolInput(name=\'add_current_date_tool\', display_name=\'Current Date\', advanced=True, info=\'If true, will add a tool to the agent that returns the current date.\', value=True)]\n\nComponent: CustomGMailLoader\nName: None\nInputs: {}\n\nComponent: GoogleGenerativeAIEmbeddingsComponent\nName: Google Generative AI Embeddings\nInputs: [SecretStrInput(name=\'api_key\', display_name=\'API Key\', required=True), MessageTextInput(name=\'model_name\', display_name=\'Model Name\', value=\'models/text-embedding-004\')]\n\nComponent: AgentQL\nName: AgentQL\nInputs: [SecretStrInput(name=\'api_key\', display_name=\'API Key\', required=True, password=True, info=\'Your AgentQL API key from dev.agentql.com\'), MessageTextInput(name=\'url\', display_name=\'URL\', required=True, info=\'The URL of the public web page you want to extract data from.\', tool_mode=True), MultilineInput(name=\'query\', display_name=\'AgentQL Query\', required=False, info=\'The AgentQL query to execute. Learn more at https://docs.agentql.com/agentql-query or use a prompt.\', tool_mode=True), MultilineInput(name=\'prompt\', display_name=\'Prompt\', required=False, info=\'A Natural Language description of the data to extract from the page. Alternative to AgentQL query.\', tool_mode=True), BoolInput(name=\'is_stealth_mode_enabled\', display_name=\'Enable Stealth Mode (Beta)\', info=\'Enable experimental anti-bot evasion strategies. May not work for all websites at all times.\', value=False, advanced=True), IntInput(name=\'timeout\', display_name=\'Timeout\', info=\'Seconds to wait for a request.\', value=900, advanced=True), DropdownInput(name=\'mode\', display_name=\'Request Mode\', info="\'standard\' uses deep data analysis, while \'fast\' trades some depth of analysis for speed.", options=[\'fast\', \'standard\'], value=\'fast\', advanced=True), IntInput(name=\'wait_for\', display_name=\'Wait For\', info=\'Seconds to wait for the page to load before extracting data.\', value=0, range_spec=RangeSpec(min=0, max=10, step_type=\'int\'), advanced=True), BoolInput(name=\'is_scroll_to_bottom_enabled\', display_name=\'Enable scroll to bottom\', info=\'Scroll to bottom of the page before extracting data.\', value=False, advanced=True), BoolInput(name=\'is_screenshot_enabled\', display_name=\'Enable screenshot\', info="Take a screenshot before extracting data. Returned in \'metadata\' as a Base64 string.", value=False, advanced=True)]\n\nComponent: SQLAgentComponent\nName: SQLAgent\nInputs: [*LCAgentComponent._base_inputs, HandleInput(name=\'llm\', display_name=\'Language Model\', input_types=[\'LanguageModel\'], required=True), MessageTextInput(name=\'database_uri\', display_name=\'Database URI\', required=True), HandleInput(name=\'extra_tools\', display_name=\'Extra Tools\', input_types=[\'Tool\'], is_list=True, advanced=True)]\n\nComponent: OpenAIToolsAgentComponent\nName: OpenAIToolsAgent\nInputs: [*LCToolsAgentComponent._base_inputs, HandleInput(name=\'llm\', display_name=\'Language Model\', input_types=[\'LanguageModel\', \'ToolEnabledLanguageModel\'], required=True), MultilineInput(name=\'system_prompt\', display_name=\'System Prompt\', info=\'System prompt for the agent.\', value=\'You are a helpful assistant\'), MultilineInput(name=\'user_prompt\', display_name=\'Prompt\', info="This prompt must contain \'input\' key.", value=\'{input}\'), DataInput(name=\'chat_history\', display_name=\'Chat History\', is_list=True, advanced=True)]\n\n### Code Examples\n### Code Examples\n```python\ndef complex_agent_graph():\n    """\n    Creates a complex agent graph with dynamic role assignment and hierarchical task execution.\n    \n    This function builds a graph that:\n    1. Takes user input via ChatInput\n    2. Dynamically determines appropriate role, goal, and backstory based on user query\n    3. Creates a specialized agent with search and financial tools\n    4. Sets up a manager agent to coordinate responses\n    5. Structures tasks in a hierarchical manner\n    6. Returns comprehensive responses to user queries\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - OpenAIModelComponent\n        - PromptComponent\n        - CrewAIAgentComponent\n        - HierarchicalTaskComponent\n        - HierarchicalCrewComponent\n        - SearchAPIComponent\n        - YfinanceToolComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph\n        \n        # Create complex agent graph\n        graph = complex_agent_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    llm = OpenAIModelComponent(model_name="gpt-4o-mini")\n    manager_llm = OpenAIModelComponent(model_name="gpt-4o")\n    search_api_tool = SearchAPIComponent()\n    yahoo_search_tool = YfinanceToolComponent()\n    dynamic_agent = CrewAIAgentComponent()\n    chat_input = ChatInput()\n    role_prompt = PromptComponent(_display_name="Role Prompt")\n    role_prompt.set(\n        template="""Define a Role that could execute or answer well the user\'s query.\n\nUser\'s query: {query}\n\nRole should be two words max. Something like "Researcher" or "Software Developer".\n"""\n    )\n\n    goal_prompt = PromptComponent(_display_name="Goal Prompt")\n    goal_prompt.set(\n        template="""Define the Goal of this Role, given the User\'s Query.\nUser\'s query: {query}\n\nRole: {role}\n\nThe goal should be concise and specific.\nGoal:\n""",\n        query=chat_input.message_response,\n        role=role_prompt.build_prompt,\n    )\n    backstory_prompt = PromptComponent(_display_name="Backstory Prompt")\n    backstory_prompt.set(\n        template="""Define a Backstory of this Role and Goal, given the User\'s Query.\nUser\'s query: {query}\n\nRole: {role}\nGoal: {goal}\n\nThe backstory should be specific and well aligned with the rest of the information.\nBackstory:""",\n        query=chat_input.message_response,\n        role=role_prompt.build_prompt,\n        goal=goal_prompt.build_prompt,\n    )\n    dynamic_agent.set(\n        tools=[search_api_tool.build_tool, yahoo_search_tool.build_tool],\n        llm=llm.build_model,\n        role=role_prompt.build_prompt,\n        goal=goal_prompt.build_prompt,\n        backstory=backstory_prompt.build_prompt,\n    )\n\n    response_prompt = PromptComponent()\n    response_prompt.set(\n        template="""User\'s query:\n{query}\n\nRespond to the user with as much as information as you can about the topic. Delete if needed.\nIf it is just a general query (e.g a greeting) you can respond them directly.""",\n        query=chat_input.message_response,\n    )\n    manager_agent = CrewAIAgentComponent()\n    manager_agent.set(\n        llm=manager_llm.build_model,\n        role="Manager",\n        goal="You can answer general questions from the User and may call others for help if needed.",\n        backstory="You are polite and helpful. You\'ve always been a beacon of politeness.",\n    )\n    task = HierarchicalTaskComponent()\n    task.set(\n        task_description=response_prompt.build_prompt,\n        expected_output="Succinct response that answers the User\'s query.",\n    )\n    crew_component = HierarchicalCrewComponent()\n    crew_component.set(\n        tasks=task.build_task, agents=[dynamic_agent.build_output], manager_agent=manager_agent.build_output\n    )\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    return Graph(\n        start=chat_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nCreates a complex agent graph with dynamic role assignment and hierarchical task execution.\n\nThis function builds a graph that:\n1. Takes user input via ChatInput\n2. Dynamically determines appropriate role, goal, and backstory based on user query\n3. Creates a specialized agent with search and financial tools\n4. Sets up a manager agent to coordinate responses\n5. Structures tasks in a hierarchical manner\n6. Returns comprehensive responses to user queries\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - OpenAIModelComponent\n    - PromptComponent\n    - CrewAIAgentComponent\n    - HierarchicalTaskComponent\n    - HierarchicalCrewComponent\n    - SearchAPIComponent\n    - YfinanceToolComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph\n    \n    # Create complex agent graph\n    graph = complex_agent_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef sequential_tasks_agent_graph():\n    """\n    Creates a sequential task execution agent graph.\n    \n    This function builds a graph that:\n    1. Takes a topic as input via TextInputComponent\n    2. Sets up three specialized agents: Researcher, Editor, and Comedian\n    3. Configures sequential tasks where each agent builds upon previous agent\'s work\n    4. Uses SearchAPI tool for research capabilities\n    5. Produces a final creative blog post about the given topic\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - TextInputComponent\n        - OpenAIModelComponent\n        - PromptComponent\n        - SequentialTaskAgentComponent\n        - SequentialCrewComponent\n        - SearchAPIComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph\n        \n        # Create sequential tasks agent graph\n        graph = sequential_tasks_agent_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    llm = OpenAIModelComponent()\n    search_api_tool = SearchAPIComponent()\n\n    text_input = TextInputComponent(_display_name="Topic")\n    text_input.set(input_value="Agile")\n\n    # Document Prompt for Researcher\n    document_prompt_component = PromptComponent()\n    document_prompt_component.set(\n        template="""Topic: {topic}\n\nBuild a document about this topic.""",\n        topic=text_input.text_response,\n    )\n\n    # Researcher Task Agent\n    researcher_task_agent = SequentialTaskAgentComponent()\n    researcher_task_agent.set(\n        role="Researcher",\n        goal="Search Google to find information to complete the task.",\n        backstory="Research has always been your thing. You can quickly find things on the web because of your skills.",\n        tools=[search_api_tool.build_tool],\n        llm=llm.build_model,\n        task_description=document_prompt_component.build_prompt,\n        expected_output="Bullet points and small phrases about the research topic.",\n    )\n\n    # Revision Prompt for Editor\n    revision_prompt_component = PromptComponent()\n    revision_prompt_component.set(\n        template="""Topic: {topic}\n\nRevise this document.""",\n        topic=text_input.text_response,\n    )\n\n    # Editor Task Agent\n    editor_task_agent = SequentialTaskAgentComponent()\n    editor_task_agent.set(\n        role="Editor",\n        goal="You should edit the information provided by the Researcher to make it more palatable and to not contain "\n        "misleading information.",\n        backstory="You are the editor of the most reputable journal in the world.",\n        llm=llm.build_model,\n        task_description=revision_prompt_component.build_prompt,\n        expected_output="Small paragraphs and bullet points with the corrected content.",\n        previous_task=researcher_task_agent.build_agent_and_task,\n    )\n\n    # Blog Prompt for Comedian\n    blog_prompt_component = PromptComponent()\n    blog_prompt_component.set(\n        template="""Topic: {topic}\n\nBuild a fun blog post about this topic.""",\n        topic=text_input.text_response,\n    )\n\n    # Comedian Task Agent\n    comedian_task_agent = SequentialTaskAgentComponent()\n    comedian_task_agent.set(\n        role="Comedian",\n        goal="You write comedic content based on the information provided by the editor.",\n        backstory="Your formal occupation is Comedian-in-Chief. "\n        "You write jokes, do standup comedy, and write funny articles.",\n        llm=llm.build_model,\n        task_description=blog_prompt_component.build_prompt,\n        expected_output="A small blog about the topic.",\n        previous_task=editor_task_agent.build_agent_and_task,\n    )\n\n    crew_component = SequentialCrewComponent()\n    crew_component.set(\n        tasks=comedian_task_agent.build_agent_and_task,\n    )\n\n    # Set up the output component\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    # Create the graph\n    return Graph(\n        start=text_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nCreates a sequential task execution agent graph.\n\nThis function builds a graph that:\n1. Takes a topic as input via TextInputComponent\n2. Sets up three specialized agents: Researcher, Editor, and Comedian\n3. Configures sequential tasks where each agent builds upon previous agent\'s work\n4. Uses SearchAPI tool for research capabilities\n5. Produces a final creative blog post about the given topic\n\nReturns:\n    Graph: A Langflow Graph object with text_input as start and chat_output as end\n\nDependencies:\n    - TextInputComponent\n    - OpenAIModelComponent\n    - PromptComponent\n    - SequentialTaskAgentComponent\n    - SequentialCrewComponent\n    - SearchAPIComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph\n    \n    # Create sequential tasks agent graph\n    graph = sequential_tasks_agent_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef hierarchical_tasks_agent_graph():\n    llm = OpenAIModelComponent(model_name="gpt-4o-mini")\n    manager_llm = OpenAIModelComponent(model_name="gpt-4o")\n    search_api_tool = SearchAPIComponent()\n    researcher_agent = CrewAIAgentComponent()\n    chat_input = ChatInput()\n    researcher_agent.set(\n        tools=[search_api_tool.build_tool],\n        llm=llm.build_model,\n        role="Researcher",\n        goal="Search for information about the User\'s query and answer as best as you can",\n        backstory="You are a reliable researcher and journalist ",\n    )\n\n    editor_agent = CrewAIAgentComponent()\n\n    editor_agent.set(\n        llm=llm.build_model,\n        role="Editor",\n        goal="Evaluate the information for misleading or biased data.",\n        backstory="You are a reliable researcher and journalist ",\n    )\n\n    response_prompt = PromptComponent()\n    response_prompt.set(\n        template="""User\'s query:\n{query}\n\nRespond to the user with as much as information as you can about the topic. Delete if needed.\nIf it is just a general query (e.g a greeting) you can respond them directly.""",\n        query=chat_input.message_response,\n    )\n    manager_agent = CrewAIAgentComponent()\n    manager_agent.set(\n        llm=manager_llm.build_model,\n        role="Manager",\n        goal="You can answer general questions from the User and may call others for help if needed.",\n        backstory="You are polite and helpful. You\'ve always been a beacon of politeness.",\n    )\n    task = HierarchicalTaskComponent()\n    task.set(\n        task_description=response_prompt.build_prompt,\n        expected_output="Succinct response that answers the User\'s query.",\n    )\n    crew_component = HierarchicalCrewComponent()\n    crew_component.set(\n        tasks=task.build_task,\n        agents=[researcher_agent.build_output, editor_agent.build_output],\n        manager_agent=manager_agent.build_output,\n    )\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    return Graph(\n        start=chat_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nNone\n\n```python\ndef blog_writer_graph(template: str | None = None):\n    """\n    Creates a blog writing graph that generates content based on reference URLs.\n    \n    This function builds a graph that:\n    1. Fetches content from specified URLs\n    2. Parses the fetched content\n    3. Takes user instructions as input\n    4. Combines references and instructions in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the generated blog content\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that combines references and instructions.\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - URLComponent\n        - ParseDataComponent\n        - TextInputComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph\n        \n        # Create blog writer graph with default template\n        graph = blog_writer_graph()\n        \n        # Create graph with custom template\n        custom_template = "Data: {references}\\n\\nWrite a blog post about: {instructions}"\n        custom_graph = blog_writer_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = dedent("""Reference 1:\n\n{references}\n\n---\n\n{instructions}\n\nBlog:\n""")\n    url_component = URLComponent()\n    url_component.set(urls=["https://langflow.org/", "https://docs.langflow.org/"])\n    parse_data_component = ParseDataComponent()\n    parse_data_component.set(data=url_component.fetch_content)\n\n    text_input = TextInputComponent(_display_name="Instructions")\n    text_input.set(\n        input_value="Use the references above for style to write a new blog/tutorial about Langflow and AI. "\n        "Suggest non-covered topics."\n    )\n\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        instructions=text_input.text_response,\n        references=parse_data_component.parse_data,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=text_input, end=chat_output)\n```\n\nDocstring:\n    Creates a blog writing graph that generates content based on reference URLs.\n    \n    This function builds a graph that:\n    1. Fetches content from specified URLs\n    2. Parses the fetched content\n    3. Takes user instructions as input\n    4. Combines references and instructions in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the generated blog content\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that combines references and instructions.\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - URLComponent\n        - ParseDataComponent\n        - TextInputComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph\n        \n        # Create blog writer graph with default template\n        graph = blog_writer_graph()\n        \n        # Create graph with custom template\n        custom_template = "Data: {references}\n\nWrite a blog post about: {instructions}"\n        custom_graph = blog_writer_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef memory_chatbot_graph(template: str | None = None):\n    """\n    Creates a chatbot graph with conversation memory.\n    \n    This function builds a graph that:\n    1. Maintains conversation history using MemoryComponent\n    2. Takes user input via ChatInput\n    3. Combines conversation context and current message in a prompt\n    4. Sends the prompt to an OpenAI model\n    5. Returns the model\'s response while updating conversation memory\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that includes conversation context and user message.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - MemoryComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph\n        \n        # Create memory chatbot with default template\n        graph = memory_chatbot_graph()\n        \n        # Create graph with custom template\n        custom_template = "Previous messages:\\n{context}\\n\\nUser: {user_message}\\nAI:"\n        custom_graph = memory_chatbot_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = """{context}\n\n    User: {user_message}\n    AI: """\n    memory_component = MemoryComponent()\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template, user_message=chat_input.message_response, context=memory_component.retrieve_messages_as_text\n    )\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(chat_input, chat_output)\n```\n\nDocstring:\n    Creates a chatbot graph with conversation memory.\n    \n    This function builds a graph that:\n    1. Maintains conversation history using MemoryComponent\n    2. Takes user input via ChatInput\n    3. Combines conversation context and current message in a prompt\n    4. Sends the prompt to an OpenAI model\n    5. Returns the model\'s response while updating conversation memory\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that includes conversation context and user message.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - MemoryComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph\n        \n        # Create memory chatbot with default template\n        graph = memory_chatbot_graph()\n        \n        # Create graph with custom template\n        custom_template = "Previous messages:\n{context}\n\nUser: {user_message}\nAI:"\n        custom_graph = memory_chatbot_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef document_qa_graph(template: str | None = None):\n    """\n    Creates a document question-answering graph.\n    \n    This function builds a graph that:\n    1. Loads document files\n    2. Parses the document content\n    3. Takes user questions as input\n    4. Combines document content and question in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the model\'s answer\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that presents the document and user question.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - FileComponent\n        - ParseDataComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph\n        \n        # Create document QA graph with default template\n        graph = document_qa_graph()\n        \n        # Create graph with custom template\n        custom_template = "Document content:\\n{Document}\\n\\nAnswer this: {Question}"\n        custom_graph = document_qa_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = """Answer user\'s questions based on the document below:\n\n---\n\n{Document}\n\n---\n\nQuestion:\n{Question}\n\nAnswer:\n"""\n    file_component = FileComponent()\n    parse_data_component = ParseDataComponent()\n    parse_data_component.set(data=file_component.load_files)\n\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        context=parse_data_component.parse_data,\n        question=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\n    Creates a document question-answering graph.\n    \n    This function builds a graph that:\n    1. Loads document files\n    2. Parses the document content\n    3. Takes user questions as input\n    4. Combines document content and question in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the model\'s answer\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that presents the document and user question.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - FileComponent\n        - ParseDataComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph\n        \n        # Create document QA graph with default template\n        graph = document_qa_graph()\n        \n        # Create graph with custom template\n        custom_template = "Document content:\n{Document}\n\nAnswer this: {Question}"\n        custom_graph = document_qa_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef rag_graph():\n    """\n    Creates a Retrieval Augmented Generation (RAG) query graph.\n    \n    This function builds a graph that:\n    1. Takes user input via ChatInput\n    2. Uses the input to query a vector store (AstraDB)\n    3. Processes and parses retrieved documents\n    4. Formats a prompt with retrieved context and user query\n    5. Sends the prompt to OpenAI model\n    6. Returns the model\'s response as output\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - OpenAIEmbeddingsComponent\n        - AstraDBVectorStoreComponent\n        - ParseDataComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph\n        \n        # Create the RAG query graph\n        graph = rag_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    # RAG Graph\n    openai_embeddings = OpenAIEmbeddingsComponent()\n    chat_input = ChatInput()\n    rag_vector_store = AstraDBVectorStoreComponent()\n    rag_vector_store.set(\n        search_query=chat_input.message_response,\n        embedding_model=openai_embeddings.build_embeddings,\n    )\n\n    parse_data = ParseDataComponent()\n    parse_data.set(data=rag_vector_store.search_documents)\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=dedent("""Given the following context, answer the question.\n                         Context:{context}\n\n                         Question: {question}\n                         Answer:"""),\n        context=parse_data.parse_data,\n        question=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\nCreates a Retrieval Augmented Generation (RAG) query graph.\n\nThis function builds a graph that:\n1. Takes user input via ChatInput\n2. Uses the input to query a vector store (AstraDB)\n3. Processes and parses retrieved documents\n4. Formats a prompt with retrieved context and user query\n5. Sends the prompt to OpenAI model\n6. Returns the model\'s response as output\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - OpenAIEmbeddingsComponent\n    - AstraDBVectorStoreComponent\n    - ParseDataComponent\n    - PromptComponent\n    - OpenAIModelComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph\n    \n    # Create the RAG query graph\n    graph = rag_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef basic_prompting_graph(template: str | None = None):\n    """\n    Creates a basic conversation graph with prompt templating.\n    \n    This function builds a simple graph that:\n    1. Takes user input via ChatInput\n    2. Processes the input through a template prompt\n    3. Sends the formatted prompt to an OpenAI model\n    4. Returns the model\'s response as output\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use. \n            Defaults to a pirate-themed response template.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph\n        \n        # Create graph with default pirate template\n        default_graph = basic_prompting_graph()\n        \n        # Create graph with custom template\n        custom_template = "Answer as a helpful assistant: {user_input}"\n        custom_graph = basic_prompting_graph(template=custom_template)\n        \n        # Build and run the graph\n        default_graph.build()\n        ```\n    """\n    if template is None:\n        template = """Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer:\n"""\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        user_input=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\nCreates a basic conversation graph with prompt templating.\n\nThis function builds a simple graph that:\n1. Takes user input via ChatInput\n2. Processes the input through a template prompt\n3. Sends the formatted prompt to an OpenAI model\n4. Returns the model\'s response as output\n\nArgs:\n    template (str | None, optional): Custom prompt template to use. \n        Defaults to a pirate-themed response template.\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - PromptComponent\n    - OpenAIModelComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph\n    \n    # Create graph with default pirate template\n    default_graph = basic_prompting_graph()\n    \n    # Create graph with custom template\n    custom_template = "Answer as a helpful assistant: {user_input}"\n    custom_graph = basic_prompting_graph(template=custom_template)\n    \n    # Build and run the graph\n    default_graph.build()\n    ```\n\n```python\ndef ingestion_graph():\n    """\n    Creates a document ingestion graph for RAG applications.\n    \n    This function builds a graph that:\n    1. Loads files using a FileComponent\n    2. Splits the text into chunks with SplitTextComponent\n    3. Creates embeddings using OpenAI\'s embedding model\n    4. Stores the embeddings in an AstraDB vector store\n    \n    Returns:\n        Graph: A Langflow Graph object with file_component as start and vector_store as end\n    \n    Dependencies:\n        - FileComponent\n        - SplitTextComponent\n        - OpenAIEmbeddingsComponent\n        - AstraDBVectorStoreComponent\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph\n        \n        # Create the ingestion graph\n        graph = ingestion_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    # Ingestion Graph\n    file_component = FileComponent()\n    text_splitter = SplitTextComponent()\n    text_splitter.set(data_inputs=file_component.load_files)\n    openai_embeddings = OpenAIEmbeddingsComponent()\n    vector_store = AstraDBVectorStoreComponent()\n    vector_store.set(\n        embedding_model=openai_embeddings.build_embeddings,\n        ingest_data=text_splitter.split_text,\n    )\n\n    return Graph(file_component, vector_store)\n```\n\nDocstring:\nCreates a document ingestion graph for RAG applications.\n\nThis function builds a graph that:\n1. Loads files using a FileComponent\n2. Splits the text into chunks with SplitTextComponent\n3. Creates embeddings using OpenAI\'s embedding model\n4. Stores the embeddings in an AstraDB vector store\n\nReturns:\n    Graph: A Langflow Graph object with file_component as start and vector_store as end\n\nDependencies:\n    - FileComponent\n    - SplitTextComponent\n    - OpenAIEmbeddingsComponent\n    - AstraDBVectorStoreComponent\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph\n    \n    # Create the ingestion graph\n    graph = ingestion_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef vector_store_rag_graph():\n    """\n    Combines ingestion and RAG graphs into a complete vector store RAG flow.\n    \n    This function:\n    1. Creates an ingestion graph for processing documents into a vector store\n    2. Creates a RAG graph for querying the vector store and generating responses\n    3. Combines them into a single operational graph\n    \n    Returns:\n        Graph: A combined graph representing the complete RAG workflow\n    \n    Dependencies:\n        - All dependencies from ingestion_graph() and rag_graph()\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph\n        \n        # Create the complete RAG workflow\n        graph = vector_store_rag_graph()\n        \n        # Build and run the graph\n        graph.build()\n        # Run ingestion first with documents\n        # Then run queries against the stored vectors\n        ```\n    """\n    return ingestion_graph() + rag_graph()\n```\n\nDocstring:\nCombines ingestion and RAG graphs into a complete vector store RAG flow.\n\nThis function:\n1. Creates an ingestion graph for processing documents into a vector store\n2. Creates a RAG graph for querying the vector store and generating responses\n3. Combines them into a single operational graph\n\nReturns:\n    Graph: A combined graph representing the complete RAG workflow\n\nDependencies:\n    - All dependencies from ingestion_graph() and rag_graph()\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph\n    \n    # Create the complete RAG workflow\n    graph = vector_store_rag_graph()\n    \n    # Build and run the graph\n    graph.build()\n    # Run ingestion first with documents\n    # Then run queries against the stored vectors\n    ```\n\n### Question\nGenerate an agent which can send or access Gmail and also create a latest news news letter.uses relevant tools or components to get the latest news and optimize the prompt to create a beautiful well articulated news letter which could be send via Gmail.\n'}, {'role': 'assistant', 'content': '```python\ndef gmail_news_agent_graph():\n    """\n    Creates an agent graph that generates a newsletter with the latest news and sends it via Gmail.\n    \n    This function builds a graph that:\n    1. Retrieves the latest news using a news tool.\n    2. Formats the news into a newsletter using a prompt.\n    3. Sends the newsletter via Gmail.\n    \n    Returns:\n        Graph: A Langflow Graph object with news_tool as start and gmail_sender as end\n    \n    Dependencies:\n        - NewsToolComponent\n        - PromptComponent\n        - GmailAPIComponent\n    """\n    # Retrieve latest news\n    news_tool = NewsToolComponent()\n\n    # Format the news into a newsletter\n    newsletter_prompt = PromptComponent()\n    newsletter_prompt.set(\n        template="Here are the latest news updates:\\n\\n{news}",\n        news=news_tool.news\n    )\n\n    # Send the newsletter via Gmail\n    gmail_sender = GmailAPIComponent()\n    gmail_sender.set(\n        recipient_email="recipient@example.com",\n        subject="Latest News Newsletter",\n        body=newsletter_prompt.prompt_output\n    )\n\n    return Graph(start=news_tool, end=gmail_sender)\n```'}, {'role': 'user', 'content': 'Based on the following documentation, components, and sample code, generate a python function. Reply with code only. Do not provide any commentary:\n\n### Documentation:\n### Documentation\nThis flow creates a question-and-answer chatbot for documents that are loaded into the flow.\nThe [Unstructured.io](https://unstructured.io/) loader component loads files from your local machine, and then parses them into a list of structured [Data](/concepts-objects) objects.\nThis loaded data informs the **Open AI** component\'s responses to your questions.\n![Sample Flow retrieving data with unstructured](/img/starter-flow-unstructured-qa.png)\n\nTo use the **Chat Input** and **Chat Output** components in a flow, connect them to components that accept or send the [Message](/concepts-objects#message) type.\nFor this example, connect a **Chat Input** component to an **OpenAI** model component\'s **Input** port, and then connect the **OpenAI** model component\'s **Message** port to the **Chat Output** component.\nIn the **OpenAI** model component, in the **OpenAI API Key** field, add your **OpenAI API key**.\nThe flow looks like this:\n![Chat input and output components connected to an OpenAI model](/img/component-chat-io.png)\nTo send a message to your flow, open the **Playground**, and then enter a message.\nThe **OpenAI** model component responds.\nOptionally, in the **OpenAI** model component, enter a **System Message** to control the model\'s response.\nIn the Langflow UI, click your flow name, and then click **Logs**.\nThe **Logs** pane opens.\nHere, you can inspect your component logs.\n![Logs pane](/img/logs.png)\nYour first message was sent by the **Chat Input** component to the **OpenAI** model component.\nClick **Outputs** to view the sent message:\nYour second message was sent by the **OpenAI** model component to the **Chat Output** component.\nThis is the raw text output of the model\'s response.\nThe **Chat Output** component accepts this text as input and presents it as a formatted message.\nClick **Outputs** to view the sent message:\n:::tip\nOptionally, to view the outputs of each component in the flow, click <Icon name="TextSearch" aria-label="Inspect icon" />.\n:::\n### Send chat messages with the API\nThe **Chat Input** component is often the entry point for passing messages to the Langflow API.\nTo send the same example messages programmatically to your Langflow server, do the following:\nTo get your Langflow endpoint, click **Publish**, and then click **API access**.\nCopy the command from the **cURL** tab, and then paste it in your terminal.\nIt looks similar to this:\nModify `input_value` so it contains the question, `What\'s the recommended way to install Docker on Mac M1?`.\nNote the `output_type` and `input_type` parameters that are passed with the message. The `chat` type provides additional configuration options, and the messages appear in the **Playground**. The `text` type returns only text strings, and does not appear in the **Playground**.\nAdd a custom `session_id` to the message\'s `data` object.\nThe custom `session_id` value starts a new chat session between your client and the Langflow server, and can be useful in keeping conversations and AI context separate.\nSend the POST request.\nYour request is answered.\nNavigate to the **Playground**.\nA new chat session called `docker-question-on-m1` has appeared, using your unique `session_id`.\nTo modify additional parameters with **Tweaks** for your **Chat Input** and **Chat Output** components, click **Publish**, and then click **API access**.\nClick **Tweaks** to modify parameters in the component\'s `data` object.\nFor example, disabling storing messages from the **Chat Input** component adds a **Tweak** to your command:\nTo confirm your command is using the tweak, navigate to the **Logs** pane and view the request from the **Chat Input** component.\nThe value for `should_store_message` is `false`.\n\nThis example uses the **Astra DB vector store** component. Your vector store component\'s parameters and authentication may be different, but the document ingestion workflow is the same. A document is loaded from a local machine and chunked. The Astra DB vector store generates embeddings with the connected [model](/components-models) component, and stores them in the connected Astra DB database.\nThis vector data can then be retrieved for workloads like Retrieval Augmented Generation.\n![](/img/vector-store-retrieval.png)\nThe user\'s chat input is embedded and compared to the vectors embedded during document ingestion for a similarity search.\nThe results are output from the vector database component as a [Data](/concepts-objects) object and parsed into text.\nThis text fills the `{context}` variable in the **Prompt** component, which informs the **Open AI model** component\'s responses.\nAlternatively, connect the vector database component\'s **Retriever** port to a [retriever tool](components-tools#retriever-tool), and then to an [agent](/components-agents) component. This enables the agent to use your vector database as a tool and make decisions based on the available data.\n![](/img/vector-store-agent-retrieval-tool.png)\n\nModel components receive inputs and prompts for generating text, and the generated text is sent to an output component.\nThe model output can also be sent to the **Language Model** port and on to a **Parse Data** component, where the output can be parsed into structured [Data](/concepts-objects) objects.\nThis example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/starter-projects-basic-prompting).\n![](/img/starter-flow-basic-prompting.png)\n\nThe **Batch Run** component runs a language model over **each row** of a [DataFrame](/concepts-objects#dataframe-object) text column and returns a new DataFrame with the original text and an LLM response.\nThe response contains the following columns:\n`text_input`: The original text from the input DataFrame.\n`model_response`: The model\'s response for each input.\n`batch_index`: The processing order, with a `0`-based index.\n`metadata` (optional): Additional information about the processing.\nThese columns, when connected to a **Parser** component, can be used as variables within curly braces.\nTo use the Batch Run component with a **Parser** component, do the following:\nConnect a **Model** component to the **Batch Run** component\'s **Language model** port.\nConnect a component that outputs DataFrame, like **File** component, to the **Batch Run** component\'s **DataFrame** input.\nConnect the **Batch Run** component\'s **Batch Results** output to a **Parser** component\'s **DataFrame** input.\nThe flow looks like this:\n![A batch run component connected to OpenAI and a Parser](/img/component-batch-run.png)\nIn the **Column Name** field of the **Batch Run** component, enter a column name based on the data you\'re loading from the **File** loader. For example, to process a column of `name`, enter `name`.\nOptionally, in the **System Message** field of the **Batch Run** component, enter a **System Message** to instruct the connected LLM on how to process your file. For example, `Create a business card for each name.`\nIn the **Template** field of the **Parser** component, enter a template for using the **Batch Run** component\'s new DataFrame columns.\nTo use all three columns from the **Batch Run** component, include them like this:\nTo run the flow, in the **Parser** component, click <Icon name="Play" aria-label="Play icon" />.\nTo view your created DataFrame, in the **Parser** component, click <Icon name="TextSearch" aria-label="Inspect icon" />.\nOptionally, connect a **Chat Output** component, and open the **Playground** to see the output.\n### Inputs\n| Name | Display Name | Type | Info |\n| ------ | -------------- | ------ | ------ |\n| model | Language Model | HandleInput | Connect the \'Language Model\' output from your LLM component here. Required. |\n| system_message | System Message | MultilineInput | Multi-line system instruction for all rows in the DataFrame. |\n| df | DataFrame | DataFrameInput | The DataFrame whose column is treated as text messages, as specified by \'column_name\'. Required. |\n| column_name | Column Name | MessageTextInput | The name of the DataFrame column to treat as text messages. Default=\'text\'. Required. |\n| enable_metadata | Enable Metadata | BoolInput | If True, add metadata to the output DataFrame. |\n### Outputs\n| Name | Display Name | Method | Info |\n| ------ | -------------- | -------- | ------ |\n| batch_results | Batch Results | run_batch | A DataFrame with columns: \'text_input\', \'model_response\', \'batch_index\', and optional \'metadata\' containing processing information. |\n\nThis component fetches content from one or more URLs, processes the content, and returns it in various formats. It supports output in plain text, raw HTML, or JSON, with options for cleaning and separating multiple outputs.\nTo use this component in a flow, connect the **DataFrame** output to a component that accepts the input.\nFor example, connect the **URL** component to a **Chat Output** component.\n![URL request into a chat output component](/img/component-url.png)\nIn the URL component\'s **URLs** field, enter the URL for your request.\nThis example uses `langflow.org`.\nOptionally, in the **Max Depth** field, enter how many pages away from the initial URL you want to crawl.\nSelect `1` to crawl only the page specified in the **URLs** field.\nSelect `2` to crawl all pages linked from that page.\nThe component crawls by link traversal, not by URL path depth.\nClick **Playground**, and then click **Run Flow**.\nThe text contents of the URL are returned to the Playground as a structured DataFrame.\nIn the **URL** component, change the output port to **Message**, and then run the flow again.\nThe text contents of the URL are returned as unstructured raw text, which you can extract patterns from with the **Regex Extractor** tool.\nConnect the **URL** component to a **Regex Extractor** and **Chat Output**.\n![Regex extractor connected to url component](/img/component-url-regex.png)\nIn the **Regex Extractor** tool, enter a pattern to extract text from the **URL** component\'s raw output.\nThis example extracts the first paragraph from the "In the News" section of `https://en.wikipedia.org/wiki/Main_Page`.\nResult:\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| urls | URLs | Enter one or more URLs. URLs are automatically validated and cleaned. |\n| format | Output Format | Output Format. Use **Text** to extract text from the HTML, **Raw HTML** for the raw HTML content, or **JSON** to extract JSON from the HTML. |\n| separator | Separator | Specify the separator to use between multiple outputs. Default for **Text** is `\\n\\n`. Default for **Raw HTML** is `\\n<!-- Separator -->\\n`. |\n| clean_extra_whitespace | Clean Extra Whitespace | Whether to clean excessive blank lines in the text output. Only applies to `Text` format. |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | List of [Data](/concepts-objects) objects containing fetched content and metadata. |\n| text | Text | Fetched content as formatted text, with applied separators and cleaning. |\n| dataframe | DataFrame | Content formatted as a [DataFrame](/concepts-objects#dataframe-object) object. |\n\nThis code defines a custom component that accepts 5 inputs and outputs a Message.\nCopy and paste it into the Custom Component code pane and click **Check & Save.**\nSince the component outputs a `Message`, you can wire it into a chat and pass messages to yourself.\nYour Custom Component accepts the Chat Input message through `MessageTextInput`, fills in the variables with the `process_inputs` method, and finally passes the message `User Username (Age: 49, Gender: Male) sent the following special message: Hello!` to Chat Output.\nBy defining inputs this way, Langflow can automatically handle the validation and display of these fields in the user interface, making it easier to create robust and user-friendly custom components.\nAll of the types detailed above derive from a general class that can also be accessed through the generic\xa0`Input`\xa0class.\n:::tip\nUse `MessageInput` to get the entire Message object instead of just the text.\n:::\n\nThis component implements a Vector Store using Astra DB with search capabilities.\nFor more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html).\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| token | Astra DB Application Token | The authentication token for accessing Astra DB. |\n| environment | Environment | The environment for the Astra DB API Endpoint. For example, `dev` or `prod`. |\n| database_name | Database | The database name for the Astra DB instance. |\n| api_endpoint | Astra DB API Endpoint | The API endpoint for the Astra DB instance. This supersedes the database selection. |\n| collection_name | Collection | The name of the collection within Astra DB where the vectors are stored. |\n| keyspace | Keyspace | An optional keyspace within Astra DB to use for the collection. |\n| embedding_choice | Embedding Model or Astra Vectorize | Choose an embedding model or use Astra vectorize. |\n| embedding_model | Embedding Model | Specify the embedding model. Not required for Astra vectorize collections. |\n| number_of_results | Number of Search Results | The number of search results to return (default: `4`). |\n| search_type | Search Type | The search type to use. The options are `Similarity`, `Similarity with score threshold`, and `MMR (Max Marginal Relevance)`. |\n| search_score_threshold | Search Score Threshold | The minimum similarity score threshold for search results when using the `Similarity with score threshold` option. |\n| advanced_search_filter | Search Metadata Filter | An optional dictionary of filters to apply to the search query. |\n| autodetect_collection | Autodetect Collection | A boolean flag to determine whether to autodetect the collection. |\n| content_field | Content Field | A field to use as the text content field for the vector store. |\n| deletion_field | Deletion Based On Field | When provided, documents in the target collection with metadata field values matching the input metadata field value are deleted before new data is loaded. |\n| ignore_invalid_documents | Ignore Invalid Documents | A boolean flag to determine whether to ignore invalid documents at runtime. |\n| astradb_vectorstore_kwargs | AstraDBVectorStore Parameters | An optional dictionary of additional parameters for the AstraDBVectorStore. |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | Astra DB vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |\n### Generate embeddings\nThe **Astra DB Vector Store** component offers two methods for generating embeddings.\n**Embedding Model**: Use your own embedding model by connecting an [Embeddings](/components-embedding-models) component in Langflow.\n**Astra Vectorize**: Use Astra DB\'s built-in embedding generation service. When creating a new collection, choose the embeddings provider and models, including NVIDIA\'s `NV-Embed-QA` model hosted by Datastax.\n:::important\nThe embedding model selection is made when creating a new collection and cannot be changed later.\n:::\nFor an example of using the **Astra DB Vector Store** component with an embedding model, see the [Vector Store RAG starter project](/starter-projects-vector-store-rag).\nFor more information, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html).\n### Hybrid search\nThe **Astra DB** component includes **hybrid search**, which is enabled by default.\nThe component fields related to hybrid search are **Search Query**, **Lexical Terms**, and **Reranker**.\n**Search Query** finds results by vector similarity.\n**Lexical Terms** is a comma-separated string of keywords, like `features, data, attributes, characteristics`.\n**Reranker** is the re-ranker model used in the hybrid search.\nThe re-ranker model is `nvidia/llama-3.2-nv.reranker`.\n[Hybrid search](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html) performs a vector similarity search and a lexical search, compares the results of both searches, and then returns the most relevant results overall.\n:::important\nTo use hybrid search, your collection must be created with vector, lexical, and rerank capabilities enabled. These capabilities are enabled by default when you create a collection in a database in the AWS us-east-2 region.\nFor more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid).\n:::\nTo use **Hybrid search** in the **Astra DB** component, do the following:\nClick **New Flow** > **RAG** > **Hybrid Search RAG**.\nIn the **OpenAI** model component, add your **OpenAI API key**.\nIn the **Astra DB** vector store component, add your **Astra DB Application Token**.\nIn the **Database** field, select your database.\nIn the **Collection** field, select or create a collection with hybrid search capabilities enabled.\nIn the **Playground**, enter a question about your data, such as `What are the features of my data?`\nYour query is sent to two components: an **OpenAI** model component and the **Astra DB** vector database component.\nThe **OpenAI** component contains a prompt for creating the lexical query from your input:\nTo view the keywords and questions the **OpenAI** component generates from your collection, in the **OpenAI** component, click <Icon name="TextSearch" aria-label="Inspect icon" />.\nTo view the [DataFrame](/concepts-objects#dataframe-object) generated from the **OpenAI** component\'s response, in the **Structured Output** component, click <Icon name="TextSearch" aria-label="Inspect icon" />.\nThe DataFrame is passed to a **Parser** component, which parses the contents of the **Keywords** column into a string.\nThis string of comma-separated words is passed to the **Lexical Terms** port of the **Astra DB** component.\n Note that the **Search Query** port of the Astra DB port is connected to the **Chat Input** component from step 6.\n This **Search Query** is vectorized, and both the **Search Query** and **Lexical Terms** content are sent to the reranker at the `find_and_rerank` endpoint.\nThe reranker compares the vector search results against the string of terms from the lexical search.\n The highest-ranked results of your hybrid search are returned to the **Playground**.\nFor more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html).\n\nThe **Chat Output** component creates a [Message](/concepts-objects#message-object) object that includes the input text, sender information, session ID, and styling properties.\nThe component accepts the following input types.\n[Data](/concepts-objects#data-object)\n[DataFrame](/concepts-objects#dataframe-object)\n[Message](/concepts-objects#message-object)\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Text | The message to be passed as output. |\n| should_store_message | Store Messages | The flag to store the message in the history. |\n| sender | Sender Type | The type of sender. |\n| sender_name | Sender Name | The name of the sender. |\n| session_id | Session ID | The session ID of the chat. If empty, the current session ID parameter is used. |\n| data_template | Data Template | The template to convert Data to Text. If the option is left empty, it is dynamically set to the Data\'s text key. |\n| background_color | Background Color | The background color of the icon. |\n| chat_icon | Icon | The icon of the message. |\n| text_color | Text Color | The text color of the name. |\n| clean_data | Basic Clean Data | When enabled, `DataFrame` inputs are cleaned when converted to text. Cleaning removes empty rows, empty lines in cells, and multiple newlines. |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| message | Message | The resulting chat message object with all specified properties. |\n\nThis component converts one or multiple [Data](/concepts-objects#data-object) objects into a [DataFrame](/concepts-objects#dataframe-object). Each Data object corresponds to one row in the resulting DataFrame. Fields from the `.data` attribute become columns, and the `.text` field (if present) is placed in a \'text\' column.\nTo use this component in a flow, connect a component that outputs [Data](/concepts-objects#data-object) to the **Data to Dataframe** component\'s input.\nThis example connects a **Webhook** component to convert `text` and `data` into a DataFrame.\nTo view the flow\'s output, connect a **Chat Output** component to the **Data to Dataframe** component.\n![A webhook and data to dataframe](/img/component-data-to-dataframe.png)\nSend a POST request to the **Webhook** containing your JSON data.\nReplace `YOUR_FLOW_ID` with your flow ID.\nThis example uses the default Langflow server address.\nIn the **Playground**, view the output of your flow.\nThe **Data to DataFrame** component converts the webhook request into a `DataFrame`, with `text` and `data` fields as columns.\nSend another employee data object.\nIn the **Playground**, this request is also converted to `DataFrame`.\n### Inputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data_list | Data or Data List | One or multiple Data objects to transform into a DataFrame. |\n### Outputs\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| dataframe | DataFrame | A DataFrame built from each Data object\'s fields plus a \'text\' column. |\n\n### Component Information\n### Component Information\nComponent: CSVAgentComponent\nName: CSVAgent\nInputs: [*LCAgentComponent._base_inputs, HandleInput(name=\'llm\', display_name=\'Language Model\', input_types=[\'LanguageModel\'], required=True, info=\'An LLM Model Object (It can be found in any LLM Component).\'), FileInput(name=\'path\', display_name=\'File Path\', file_types=[\'csv\'], input_types=[\'str\', \'Message\'], required=True, info=\'A CSV File or File Path.\'), DropdownInput(name=\'agent_type\', display_name=\'Agent Type\', advanced=True, options=[\'zero-shot-react-description\', \'openai-functions\', \'openai-tools\'], value=\'openai-tools\'), MessageTextInput(name=\'input_value\', display_name=\'Text\', info=\'Text to be passed as input and extract info from the CSV File.\', required=True), DictInput(name=\'pandas_kwargs\', display_name=\'Pandas Kwargs\', info=\'Pandas Kwargs to be passed to the agent.\', advanced=True, is_list=True)]\n\nComponent: CSVToDataComponent\nName: CSVtoData\nInputs: [FileInput(name=\'csv_file\', display_name=\'CSV File\', file_types=[\'csv\'], info=\'Upload a CSV file to convert to a list of Data objects\'), MessageTextInput(name=\'csv_path\', display_name=\'CSV File Path\', info=\'Provide the path to the CSV file as pure text\'), MultilineInput(name=\'csv_string\', display_name=\'CSV String\', info=\'Paste a CSV string directly to convert to a list of Data objects\'), MessageTextInput(name=\'text_key\', display_name=\'Text Key\', info="The key to use for the text column. Defaults to \'text\'.", value=\'text\')]\n\nComponent: ChatOutput\nName: ChatOutput\nInputs: [HandleInput(name=\'input_value\', display_name=\'Text\', info=\'Message to be passed as output.\', input_types=[\'Data\', \'DataFrame\', \'Message\'], required=True), BoolInput(name=\'should_store_message\', display_name=\'Store Messages\', info=\'Store the message in the history.\', value=True, advanced=True), DropdownInput(name=\'sender\', display_name=\'Sender Type\', options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER], value=MESSAGE_SENDER_AI, advanced=True, info=\'Type of sender.\'), MessageTextInput(name=\'sender_name\', display_name=\'Sender Name\', info=\'Name of the sender.\', value=MESSAGE_SENDER_NAME_AI, advanced=True), MessageTextInput(name=\'session_id\', display_name=\'Session ID\', info=\'The session ID of the chat. If empty, the current session ID parameter will be used.\', advanced=True), MessageTextInput(name=\'data_template\', display_name=\'Data Template\', value=\'{text}\', advanced=True, info="Template to convert Data to Text. If left empty, it will be dynamically set to the Data\'s text key."), MessageTextInput(name=\'background_color\', display_name=\'Background Color\', info=\'The background color of the icon.\', advanced=True), MessageTextInput(name=\'chat_icon\', display_name=\'Icon\', info=\'The icon of the message.\', advanced=True), MessageTextInput(name=\'text_color\', display_name=\'Text Color\', info=\'The text color of the name\', advanced=True), BoolInput(name=\'clean_data\', display_name=\'Basic Clean Data\', value=True, info=\'Whether to clean the data\', advanced=True)]\n\nComponent: ChatLiteLLMModelComponent\nName: None\nInputs: [MessageInput(name=\'input_value\', display_name=\'Input\'), StrInput(name=\'model\', display_name=\'Model name\', advanced=False, required=True, info=\'The name of the model to use. For example, `gpt-3.5-turbo`.\'), SecretStrInput(name=\'api_key\', display_name=\'API Key\', advanced=False, required=False), DropdownInput(name=\'provider\', display_name=\'Provider\', info=\'The provider of the API key.\', options=[\'OpenAI\', \'Azure\', \'Anthropic\', \'Replicate\', \'Cohere\', \'OpenRouter\']), FloatInput(name=\'temperature\', display_name=\'Temperature\', advanced=False, required=False, value=0.7), DictInput(name=\'kwargs\', display_name=\'Kwargs\', advanced=True, required=False, is_list=True, value={}), DictInput(name=\'model_kwargs\', display_name=\'Model kwargs\', advanced=True, required=False, is_list=True, value={}), FloatInput(name=\'top_p\', display_name=\'Top p\', advanced=True, required=False, value=0.5), IntInput(name=\'top_k\', display_name=\'Top k\', advanced=True, required=False, value=35), IntInput(name=\'n\', display_name=\'N\', advanced=True, required=False, info=\'Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\', value=1), IntInput(name=\'max_tokens\', display_name=\'Max tokens\', advanced=False, value=256, info=\'The maximum number of tokens to generate for each chat completion.\'), IntInput(name=\'max_retries\', display_name=\'Max retries\', advanced=True, required=False, value=6), BoolInput(name=\'verbose\', display_name=\'Verbose\', advanced=True, required=False, value=False), BoolInput(name=\'stream\', display_name=\'Stream\', info=STREAM_INFO_TEXT, advanced=True), StrInput(name=\'system_message\', display_name=\'System Message\', info=\'System message to pass to the model.\', advanced=True)]\n\nComponent: QianfanChatEndpointComponent\nName: BaiduQianfanChatModel\nInputs: [*LCModelComponent._base_inputs, DropdownInput(name=\'model\', display_name=\'Model Name\', options=[\'EB-turbo-AppBuilder\', \'Llama-2-70b-chat\', \'ERNIE-Bot-turbo-AI\', \'ERNIE-Lite-8K-0308\', \'ERNIE-Speed\', \'Qianfan-Chinese-Llama-2-13B\', \'ERNIE-3.5-8K\', \'BLOOMZ-7B\', \'Qianfan-Chinese-Llama-2-7B\', \'XuanYuan-70B-Chat-4bit\', \'AquilaChat-7B\', \'ERNIE-Bot-4\', \'Llama-2-13b-chat\', \'ChatGLM2-6B-32K\', \'ERNIE-Bot\', \'ERNIE-Speed-128k\', \'ERNIE-4.0-8K\', \'Qianfan-BLOOMZ-7B-compressed\', \'ERNIE Speed\', \'Llama-2-7b-chat\', \'Mixtral-8x7B-Instruct\', \'ERNIE 3.5\', \'ERNIE Speed-AppBuilder\', \'ERNIE-Speed-8K\', \'Yi-34B-Chat\'], info=\'https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint\', value=\'ERNIE-4.0-8K\'), SecretStrInput(name=\'qianfan_ak\', display_name=\'Qianfan Ak\', info=\'which you could get from  https://cloud.baidu.com/product/wenxinworkshop\'), SecretStrInput(name=\'qianfan_sk\', display_name=\'Qianfan Sk\', info=\'which you could get from  https://cloud.baidu.com/product/wenxinworkshop\'), FloatInput(name=\'top_p\', display_name=\'Top p\', info=\'Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo\', value=0.8, advanced=True), FloatInput(name=\'temperature\', display_name=\'Temperature\', info=\'Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo\', value=0.95), FloatInput(name=\'penalty_score\', display_name=\'Penalty Score\', info=\'Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo\', value=1.0, advanced=True), MessageTextInput(name=\'endpoint\', display_name=\'Endpoint\', info=\'Endpoint of the Qianfan LLM, required if custom model used.\')]\n\nComponent: ChatVertexAIComponent\nName: VertexAiModel\nInputs: [*LCModelComponent._base_inputs, FileInput(name=\'credentials\', display_name=\'Credentials\', info=\'JSON credentials file. Leave empty to fallback to environment variables\', file_types=[\'json\']), MessageTextInput(name=\'model_name\', display_name=\'Model Name\', value=\'gemini-1.5-pro\'), StrInput(name=\'project\', display_name=\'Project\', info=\'The project ID.\', advanced=True), StrInput(name=\'location\', display_name=\'Location\', value=\'us-central1\', advanced=True), IntInput(name=\'max_output_tokens\', display_name=\'Max Output Tokens\', advanced=True), IntInput(name=\'max_retries\', display_name=\'Max Retries\', value=1, advanced=True), FloatInput(name=\'temperature\', value=0.0, display_name=\'Temperature\'), IntInput(name=\'top_k\', display_name=\'Top K\', advanced=True), FloatInput(name=\'top_p\', display_name=\'Top P\', value=0.95, advanced=True), BoolInput(name=\'verbose\', display_name=\'Verbose\', value=False, advanced=True)]\n\nComponent: VectaraRagComponent\nName: VectaraRAG\nInputs: [StrInput(name=\'vectara_customer_id\', display_name=\'Vectara Customer ID\', required=True), StrInput(name=\'vectara_corpus_id\', display_name=\'Vectara Corpus ID\', required=True), SecretStrInput(name=\'vectara_api_key\', display_name=\'Vectara API Key\', required=True), MessageTextInput(name=\'search_query\', display_name=\'Search Query\', info=\'The query to receive an answer on.\', tool_mode=True), FloatInput(name=\'lexical_interpolation\', display_name=\'Hybrid Search Factor\', range_spec=RangeSpec(min=0.005, max=0.1, step=0.005), value=0.005, advanced=True, info=\'How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all, and 1 means only lexical search is used.\'), MessageTextInput(name=\'filter\', display_name=\'Metadata Filters\', value=\'\', advanced=True, info=\'The filter string to narrow the search to according to metadata attributes.\'), DropdownInput(name=\'reranker\', display_name=\'Reranker Type\', options=RERANKER_TYPES, value=RERANKER_TYPES[0], info=\'How to rerank the retrieved search results.\'), IntInput(name=\'reranker_k\', display_name=\'Number of Results to Rerank\', value=50, range_spec=RangeSpec(min=1, max=100, step=1), advanced=True), FloatInput(name=\'diversity_bias\', display_name=\'Diversity Bias\', value=0.2, range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True, info=\'Ranges from 0 to 1, with higher values indicating greater diversity (only applies to MMR reranker).\'), IntInput(name=\'max_results\', display_name=\'Max Results to Summarize\', value=7, range_spec=RangeSpec(min=1, max=100, step=1), advanced=True, info=\'The maximum number of search results to be available to the prompt.\'), DropdownInput(name=\'response_lang\', display_name=\'Response Language\', options=RESPONSE_LANGUAGES, value=\'eng\', advanced=True, info=\'Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language.\'), DropdownInput(name=\'prompt\', display_name=\'Prompt Name\', options=SUMMARIZER_PROMPTS, value=SUMMARIZER_PROMPTS[0], advanced=True, info=\'Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only.\')]\n\nComponent: AgentQL\nName: AgentQL\nInputs: [SecretStrInput(name=\'api_key\', display_name=\'API Key\', required=True, password=True, info=\'Your AgentQL API key from dev.agentql.com\'), MessageTextInput(name=\'url\', display_name=\'URL\', required=True, info=\'The URL of the public web page you want to extract data from.\', tool_mode=True), MultilineInput(name=\'query\', display_name=\'AgentQL Query\', required=False, info=\'The AgentQL query to execute. Learn more at https://docs.agentql.com/agentql-query or use a prompt.\', tool_mode=True), MultilineInput(name=\'prompt\', display_name=\'Prompt\', required=False, info=\'A Natural Language description of the data to extract from the page. Alternative to AgentQL query.\', tool_mode=True), BoolInput(name=\'is_stealth_mode_enabled\', display_name=\'Enable Stealth Mode (Beta)\', info=\'Enable experimental anti-bot evasion strategies. May not work for all websites at all times.\', value=False, advanced=True), IntInput(name=\'timeout\', display_name=\'Timeout\', info=\'Seconds to wait for a request.\', value=900, advanced=True), DropdownInput(name=\'mode\', display_name=\'Request Mode\', info="\'standard\' uses deep data analysis, while \'fast\' trades some depth of analysis for speed.", options=[\'fast\', \'standard\'], value=\'fast\', advanced=True), IntInput(name=\'wait_for\', display_name=\'Wait For\', info=\'Seconds to wait for the page to load before extracting data.\', value=0, range_spec=RangeSpec(min=0, max=10, step_type=\'int\'), advanced=True), BoolInput(name=\'is_scroll_to_bottom_enabled\', display_name=\'Enable scroll to bottom\', info=\'Scroll to bottom of the page before extracting data.\', value=False, advanced=True), BoolInput(name=\'is_screenshot_enabled\', display_name=\'Enable screenshot\', info="Take a screenshot before extracting data. Returned in \'metadata\' as a Base64 string.", value=False, advanced=True)]\n\nComponent: CassandraChatMemory\nName: CassandraChatMemory\nInputs: [MessageTextInput(name=\'database_ref\', display_name=\'Contact Points / Astra Database ID\', info=\'Contact points for the database (or AstraDB database ID)\', required=True), MessageTextInput(name=\'username\', display_name=\'Username\', info=\'Username for the database (leave empty for AstraDB).\'), SecretStrInput(name=\'token\', display_name=\'Password / AstraDB Token\', info=\'User password for the database (or AstraDB token).\', required=True), MessageTextInput(name=\'keyspace\', display_name=\'Keyspace\', info=\'Table Keyspace (or AstraDB namespace).\', required=True), MessageTextInput(name=\'table_name\', display_name=\'Table Name\', info=\'The name of the table (or AstraDB collection) where vectors will be stored.\', required=True), MessageTextInput(name=\'session_id\', display_name=\'Session ID\', info=\'Session ID for the message.\', advanced=True), DictInput(name=\'cluster_kwargs\', display_name=\'Cluster arguments\', info=\'Optional dictionary of additional keyword arguments for the Cassandra cluster.\', advanced=True, is_list=True)]\n\nComponent: OpenAIModelComponent\nName: OpenAIModel\nInputs: [*LCModelComponent._base_inputs, IntInput(name=\'max_tokens\', display_name=\'Max Tokens\', advanced=True, info=\'The maximum number of tokens to generate. Set to 0 for unlimited tokens.\', range_spec=RangeSpec(min=0, max=128000)), DictInput(name=\'model_kwargs\', display_name=\'Model Kwargs\', advanced=True, info=\'Additional keyword arguments to pass to the model.\'), BoolInput(name=\'json_mode\', display_name=\'JSON Mode\', advanced=True, info=\'If True, it will output JSON regardless of passing a schema.\'), DropdownInput(name=\'model_name\', display_name=\'Model Name\', advanced=False, options=OPENAI_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES, value=OPENAI_MODEL_NAMES[1], combobox=True, real_time_refresh=True), StrInput(name=\'openai_api_base\', display_name=\'OpenAI API Base\', advanced=True, info=\'The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\'), SecretStrInput(name=\'api_key\', display_name=\'OpenAI API Key\', info=\'The OpenAI API Key to use for the OpenAI model.\', advanced=False, value=\'OPENAI_API_KEY\', required=True), SliderInput(name=\'temperature\', display_name=\'Temperature\', value=0.1, range_spec=RangeSpec(min=0, max=1, step=0.01), show=True), IntInput(name=\'seed\', display_name=\'Seed\', info=\'The seed controls the reproducibility of the job.\', advanced=True, value=1), IntInput(name=\'max_retries\', display_name=\'Max Retries\', info=\'The maximum number of retries to make when generating.\', advanced=True, value=5), IntInput(name=\'timeout\', display_name=\'Timeout\', info=\'The timeout for requests to OpenAI completion API.\', advanced=True, value=700)]\n\n### Code Examples\n### Code Examples\n```python\ndef memory_chatbot_graph(template: str | None = None):\n    """\n    Creates a chatbot graph with conversation memory.\n    \n    This function builds a graph that:\n    1. Maintains conversation history using MemoryComponent\n    2. Takes user input via ChatInput\n    3. Combines conversation context and current message in a prompt\n    4. Sends the prompt to an OpenAI model\n    5. Returns the model\'s response while updating conversation memory\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that includes conversation context and user message.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - MemoryComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph\n        \n        # Create memory chatbot with default template\n        graph = memory_chatbot_graph()\n        \n        # Create graph with custom template\n        custom_template = "Previous messages:\\n{context}\\n\\nUser: {user_message}\\nAI:"\n        custom_graph = memory_chatbot_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = """{context}\n\n    User: {user_message}\n    AI: """\n    memory_component = MemoryComponent()\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template, user_message=chat_input.message_response, context=memory_component.retrieve_messages_as_text\n    )\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(chat_input, chat_output)\n```\n\nDocstring:\n    Creates a chatbot graph with conversation memory.\n    \n    This function builds a graph that:\n    1. Maintains conversation history using MemoryComponent\n    2. Takes user input via ChatInput\n    3. Combines conversation context and current message in a prompt\n    4. Sends the prompt to an OpenAI model\n    5. Returns the model\'s response while updating conversation memory\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that includes conversation context and user message.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - MemoryComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph\n        \n        # Create memory chatbot with default template\n        graph = memory_chatbot_graph()\n        \n        # Create graph with custom template\n        custom_template = "Previous messages:\n{context}\n\nUser: {user_message}\nAI:"\n        custom_graph = memory_chatbot_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef document_qa_graph(template: str | None = None):\n    """\n    Creates a document question-answering graph.\n    \n    This function builds a graph that:\n    1. Loads document files\n    2. Parses the document content\n    3. Takes user questions as input\n    4. Combines document content and question in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the model\'s answer\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that presents the document and user question.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - FileComponent\n        - ParseDataComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph\n        \n        # Create document QA graph with default template\n        graph = document_qa_graph()\n        \n        # Create graph with custom template\n        custom_template = "Document content:\\n{Document}\\n\\nAnswer this: {Question}"\n        custom_graph = document_qa_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = """Answer user\'s questions based on the document below:\n\n---\n\n{Document}\n\n---\n\nQuestion:\n{Question}\n\nAnswer:\n"""\n    file_component = FileComponent()\n    parse_data_component = ParseDataComponent()\n    parse_data_component.set(data=file_component.load_files)\n\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        context=parse_data_component.parse_data,\n        question=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\n    Creates a document question-answering graph.\n    \n    This function builds a graph that:\n    1. Loads document files\n    2. Parses the document content\n    3. Takes user questions as input\n    4. Combines document content and question in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the model\'s answer\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that presents the document and user question.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - FileComponent\n        - ParseDataComponent\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph\n        \n        # Create document QA graph with default template\n        graph = document_qa_graph()\n        \n        # Create graph with custom template\n        custom_template = "Document content:\n{Document}\n\nAnswer this: {Question}"\n        custom_graph = document_qa_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef basic_prompting_graph(template: str | None = None):\n    """\n    Creates a basic conversation graph with prompt templating.\n    \n    This function builds a simple graph that:\n    1. Takes user input via ChatInput\n    2. Processes the input through a template prompt\n    3. Sends the formatted prompt to an OpenAI model\n    4. Returns the model\'s response as output\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use. \n            Defaults to a pirate-themed response template.\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph\n        \n        # Create graph with default pirate template\n        default_graph = basic_prompting_graph()\n        \n        # Create graph with custom template\n        custom_template = "Answer as a helpful assistant: {user_input}"\n        custom_graph = basic_prompting_graph(template=custom_template)\n        \n        # Build and run the graph\n        default_graph.build()\n        ```\n    """\n    if template is None:\n        template = """Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer:\n"""\n    chat_input = ChatInput()\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        user_input=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\nCreates a basic conversation graph with prompt templating.\n\nThis function builds a simple graph that:\n1. Takes user input via ChatInput\n2. Processes the input through a template prompt\n3. Sends the formatted prompt to an OpenAI model\n4. Returns the model\'s response as output\n\nArgs:\n    template (str | None, optional): Custom prompt template to use. \n        Defaults to a pirate-themed response template.\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - PromptComponent\n    - OpenAIModelComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph\n    \n    # Create graph with default pirate template\n    default_graph = basic_prompting_graph()\n    \n    # Create graph with custom template\n    custom_template = "Answer as a helpful assistant: {user_input}"\n    custom_graph = basic_prompting_graph(template=custom_template)\n    \n    # Build and run the graph\n    default_graph.build()\n    ```\n\n```python\ndef complex_agent_graph():\n    """\n    Creates a complex agent graph with dynamic role assignment and hierarchical task execution.\n    \n    This function builds a graph that:\n    1. Takes user input via ChatInput\n    2. Dynamically determines appropriate role, goal, and backstory based on user query\n    3. Creates a specialized agent with search and financial tools\n    4. Sets up a manager agent to coordinate responses\n    5. Structures tasks in a hierarchical manner\n    6. Returns comprehensive responses to user queries\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - OpenAIModelComponent\n        - PromptComponent\n        - CrewAIAgentComponent\n        - HierarchicalTaskComponent\n        - HierarchicalCrewComponent\n        - SearchAPIComponent\n        - YfinanceToolComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph\n        \n        # Create complex agent graph\n        graph = complex_agent_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    llm = OpenAIModelComponent(model_name="gpt-4o-mini")\n    manager_llm = OpenAIModelComponent(model_name="gpt-4o")\n    search_api_tool = SearchAPIComponent()\n    yahoo_search_tool = YfinanceToolComponent()\n    dynamic_agent = CrewAIAgentComponent()\n    chat_input = ChatInput()\n    role_prompt = PromptComponent(_display_name="Role Prompt")\n    role_prompt.set(\n        template="""Define a Role that could execute or answer well the user\'s query.\n\nUser\'s query: {query}\n\nRole should be two words max. Something like "Researcher" or "Software Developer".\n"""\n    )\n\n    goal_prompt = PromptComponent(_display_name="Goal Prompt")\n    goal_prompt.set(\n        template="""Define the Goal of this Role, given the User\'s Query.\nUser\'s query: {query}\n\nRole: {role}\n\nThe goal should be concise and specific.\nGoal:\n""",\n        query=chat_input.message_response,\n        role=role_prompt.build_prompt,\n    )\n    backstory_prompt = PromptComponent(_display_name="Backstory Prompt")\n    backstory_prompt.set(\n        template="""Define a Backstory of this Role and Goal, given the User\'s Query.\nUser\'s query: {query}\n\nRole: {role}\nGoal: {goal}\n\nThe backstory should be specific and well aligned with the rest of the information.\nBackstory:""",\n        query=chat_input.message_response,\n        role=role_prompt.build_prompt,\n        goal=goal_prompt.build_prompt,\n    )\n    dynamic_agent.set(\n        tools=[search_api_tool.build_tool, yahoo_search_tool.build_tool],\n        llm=llm.build_model,\n        role=role_prompt.build_prompt,\n        goal=goal_prompt.build_prompt,\n        backstory=backstory_prompt.build_prompt,\n    )\n\n    response_prompt = PromptComponent()\n    response_prompt.set(\n        template="""User\'s query:\n{query}\n\nRespond to the user with as much as information as you can about the topic. Delete if needed.\nIf it is just a general query (e.g a greeting) you can respond them directly.""",\n        query=chat_input.message_response,\n    )\n    manager_agent = CrewAIAgentComponent()\n    manager_agent.set(\n        llm=manager_llm.build_model,\n        role="Manager",\n        goal="You can answer general questions from the User and may call others for help if needed.",\n        backstory="You are polite and helpful. You\'ve always been a beacon of politeness.",\n    )\n    task = HierarchicalTaskComponent()\n    task.set(\n        task_description=response_prompt.build_prompt,\n        expected_output="Succinct response that answers the User\'s query.",\n    )\n    crew_component = HierarchicalCrewComponent()\n    crew_component.set(\n        tasks=task.build_task, agents=[dynamic_agent.build_output], manager_agent=manager_agent.build_output\n    )\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    return Graph(\n        start=chat_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nCreates a complex agent graph with dynamic role assignment and hierarchical task execution.\n\nThis function builds a graph that:\n1. Takes user input via ChatInput\n2. Dynamically determines appropriate role, goal, and backstory based on user query\n3. Creates a specialized agent with search and financial tools\n4. Sets up a manager agent to coordinate responses\n5. Structures tasks in a hierarchical manner\n6. Returns comprehensive responses to user queries\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - OpenAIModelComponent\n    - PromptComponent\n    - CrewAIAgentComponent\n    - HierarchicalTaskComponent\n    - HierarchicalCrewComponent\n    - SearchAPIComponent\n    - YfinanceToolComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph\n    \n    # Create complex agent graph\n    graph = complex_agent_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef rag_graph():\n    """\n    Creates a Retrieval Augmented Generation (RAG) query graph.\n    \n    This function builds a graph that:\n    1. Takes user input via ChatInput\n    2. Uses the input to query a vector store (AstraDB)\n    3. Processes and parses retrieved documents\n    4. Formats a prompt with retrieved context and user query\n    5. Sends the prompt to OpenAI model\n    6. Returns the model\'s response as output\n    \n    Returns:\n        Graph: A Langflow Graph object with chat_input as start and chat_output as end\n    \n    Dependencies:\n        - ChatInput\n        - OpenAIEmbeddingsComponent\n        - AstraDBVectorStoreComponent\n        - ParseDataComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph\n        \n        # Create the RAG query graph\n        graph = rag_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    # RAG Graph\n    openai_embeddings = OpenAIEmbeddingsComponent()\n    chat_input = ChatInput()\n    rag_vector_store = AstraDBVectorStoreComponent()\n    rag_vector_store.set(\n        search_query=chat_input.message_response,\n        embedding_model=openai_embeddings.build_embeddings,\n    )\n\n    parse_data = ParseDataComponent()\n    parse_data.set(data=rag_vector_store.search_documents)\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=dedent("""Given the following context, answer the question.\n                         Context:{context}\n\n                         Question: {question}\n                         Answer:"""),\n        context=parse_data.parse_data,\n        question=chat_input.message_response,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=chat_input, end=chat_output)\n```\n\nDocstring:\nCreates a Retrieval Augmented Generation (RAG) query graph.\n\nThis function builds a graph that:\n1. Takes user input via ChatInput\n2. Uses the input to query a vector store (AstraDB)\n3. Processes and parses retrieved documents\n4. Formats a prompt with retrieved context and user query\n5. Sends the prompt to OpenAI model\n6. Returns the model\'s response as output\n\nReturns:\n    Graph: A Langflow Graph object with chat_input as start and chat_output as end\n\nDependencies:\n    - ChatInput\n    - OpenAIEmbeddingsComponent\n    - AstraDBVectorStoreComponent\n    - ParseDataComponent\n    - PromptComponent\n    - OpenAIModelComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph\n    \n    # Create the RAG query graph\n    graph = rag_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef hierarchical_tasks_agent_graph():\n    llm = OpenAIModelComponent(model_name="gpt-4o-mini")\n    manager_llm = OpenAIModelComponent(model_name="gpt-4o")\n    search_api_tool = SearchAPIComponent()\n    researcher_agent = CrewAIAgentComponent()\n    chat_input = ChatInput()\n    researcher_agent.set(\n        tools=[search_api_tool.build_tool],\n        llm=llm.build_model,\n        role="Researcher",\n        goal="Search for information about the User\'s query and answer as best as you can",\n        backstory="You are a reliable researcher and journalist ",\n    )\n\n    editor_agent = CrewAIAgentComponent()\n\n    editor_agent.set(\n        llm=llm.build_model,\n        role="Editor",\n        goal="Evaluate the information for misleading or biased data.",\n        backstory="You are a reliable researcher and journalist ",\n    )\n\n    response_prompt = PromptComponent()\n    response_prompt.set(\n        template="""User\'s query:\n{query}\n\nRespond to the user with as much as information as you can about the topic. Delete if needed.\nIf it is just a general query (e.g a greeting) you can respond them directly.""",\n        query=chat_input.message_response,\n    )\n    manager_agent = CrewAIAgentComponent()\n    manager_agent.set(\n        llm=manager_llm.build_model,\n        role="Manager",\n        goal="You can answer general questions from the User and may call others for help if needed.",\n        backstory="You are polite and helpful. You\'ve always been a beacon of politeness.",\n    )\n    task = HierarchicalTaskComponent()\n    task.set(\n        task_description=response_prompt.build_prompt,\n        expected_output="Succinct response that answers the User\'s query.",\n    )\n    crew_component = HierarchicalCrewComponent()\n    crew_component.set(\n        tasks=task.build_task,\n        agents=[researcher_agent.build_output, editor_agent.build_output],\n        manager_agent=manager_agent.build_output,\n    )\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    return Graph(\n        start=chat_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nNone\n\n```python\ndef sequential_tasks_agent_graph():\n    """\n    Creates a sequential task execution agent graph.\n    \n    This function builds a graph that:\n    1. Takes a topic as input via TextInputComponent\n    2. Sets up three specialized agents: Researcher, Editor, and Comedian\n    3. Configures sequential tasks where each agent builds upon previous agent\'s work\n    4. Uses SearchAPI tool for research capabilities\n    5. Produces a final creative blog post about the given topic\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - TextInputComponent\n        - OpenAIModelComponent\n        - PromptComponent\n        - SequentialTaskAgentComponent\n        - SequentialCrewComponent\n        - SearchAPIComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph\n        \n        # Create sequential tasks agent graph\n        graph = sequential_tasks_agent_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    llm = OpenAIModelComponent()\n    search_api_tool = SearchAPIComponent()\n\n    text_input = TextInputComponent(_display_name="Topic")\n    text_input.set(input_value="Agile")\n\n    # Document Prompt for Researcher\n    document_prompt_component = PromptComponent()\n    document_prompt_component.set(\n        template="""Topic: {topic}\n\nBuild a document about this topic.""",\n        topic=text_input.text_response,\n    )\n\n    # Researcher Task Agent\n    researcher_task_agent = SequentialTaskAgentComponent()\n    researcher_task_agent.set(\n        role="Researcher",\n        goal="Search Google to find information to complete the task.",\n        backstory="Research has always been your thing. You can quickly find things on the web because of your skills.",\n        tools=[search_api_tool.build_tool],\n        llm=llm.build_model,\n        task_description=document_prompt_component.build_prompt,\n        expected_output="Bullet points and small phrases about the research topic.",\n    )\n\n    # Revision Prompt for Editor\n    revision_prompt_component = PromptComponent()\n    revision_prompt_component.set(\n        template="""Topic: {topic}\n\nRevise this document.""",\n        topic=text_input.text_response,\n    )\n\n    # Editor Task Agent\n    editor_task_agent = SequentialTaskAgentComponent()\n    editor_task_agent.set(\n        role="Editor",\n        goal="You should edit the information provided by the Researcher to make it more palatable and to not contain "\n        "misleading information.",\n        backstory="You are the editor of the most reputable journal in the world.",\n        llm=llm.build_model,\n        task_description=revision_prompt_component.build_prompt,\n        expected_output="Small paragraphs and bullet points with the corrected content.",\n        previous_task=researcher_task_agent.build_agent_and_task,\n    )\n\n    # Blog Prompt for Comedian\n    blog_prompt_component = PromptComponent()\n    blog_prompt_component.set(\n        template="""Topic: {topic}\n\nBuild a fun blog post about this topic.""",\n        topic=text_input.text_response,\n    )\n\n    # Comedian Task Agent\n    comedian_task_agent = SequentialTaskAgentComponent()\n    comedian_task_agent.set(\n        role="Comedian",\n        goal="You write comedic content based on the information provided by the editor.",\n        backstory="Your formal occupation is Comedian-in-Chief. "\n        "You write jokes, do standup comedy, and write funny articles.",\n        llm=llm.build_model,\n        task_description=blog_prompt_component.build_prompt,\n        expected_output="A small blog about the topic.",\n        previous_task=editor_task_agent.build_agent_and_task,\n    )\n\n    crew_component = SequentialCrewComponent()\n    crew_component.set(\n        tasks=comedian_task_agent.build_agent_and_task,\n    )\n\n    # Set up the output component\n    chat_output = ChatOutput()\n    chat_output.set(input_value=crew_component.build_output)\n\n    # Create the graph\n    return Graph(\n        start=text_input,\n        end=chat_output,\n        flow_name="Sequential Tasks Agent",\n        description="This Agent runs tasks in a predefined sequence.",\n    )\n```\n\nDocstring:\nCreates a sequential task execution agent graph.\n\nThis function builds a graph that:\n1. Takes a topic as input via TextInputComponent\n2. Sets up three specialized agents: Researcher, Editor, and Comedian\n3. Configures sequential tasks where each agent builds upon previous agent\'s work\n4. Uses SearchAPI tool for research capabilities\n5. Produces a final creative blog post about the given topic\n\nReturns:\n    Graph: A Langflow Graph object with text_input as start and chat_output as end\n\nDependencies:\n    - TextInputComponent\n    - OpenAIModelComponent\n    - PromptComponent\n    - SequentialTaskAgentComponent\n    - SequentialCrewComponent\n    - SearchAPIComponent\n    - ChatOutput\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph\n    \n    # Create sequential tasks agent graph\n    graph = sequential_tasks_agent_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef blog_writer_graph(template: str | None = None):\n    """\n    Creates a blog writing graph that generates content based on reference URLs.\n    \n    This function builds a graph that:\n    1. Fetches content from specified URLs\n    2. Parses the fetched content\n    3. Takes user instructions as input\n    4. Combines references and instructions in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the generated blog content\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that combines references and instructions.\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - URLComponent\n        - ParseDataComponent\n        - TextInputComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph\n        \n        # Create blog writer graph with default template\n        graph = blog_writer_graph()\n        \n        # Create graph with custom template\n        custom_template = "Data: {references}\\n\\nWrite a blog post about: {instructions}"\n        custom_graph = blog_writer_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    if template is None:\n        template = dedent("""Reference 1:\n\n{references}\n\n---\n\n{instructions}\n\nBlog:\n""")\n    url_component = URLComponent()\n    url_component.set(urls=["https://langflow.org/", "https://docs.langflow.org/"])\n    parse_data_component = ParseDataComponent()\n    parse_data_component.set(data=url_component.fetch_content)\n\n    text_input = TextInputComponent(_display_name="Instructions")\n    text_input.set(\n        input_value="Use the references above for style to write a new blog/tutorial about Langflow and AI. "\n        "Suggest non-covered topics."\n    )\n\n    prompt_component = PromptComponent()\n    prompt_component.set(\n        template=template,\n        instructions=text_input.text_response,\n        references=parse_data_component.parse_data,\n    )\n\n    openai_component = OpenAIModelComponent()\n    openai_component.set(input_value=prompt_component.build_prompt)\n\n    chat_output = ChatOutput()\n    chat_output.set(input_value=openai_component.text_response)\n\n    return Graph(start=text_input, end=chat_output)\n```\n\nDocstring:\n    Creates a blog writing graph that generates content based on reference URLs.\n    \n    This function builds a graph that:\n    1. Fetches content from specified URLs\n    2. Parses the fetched content\n    3. Takes user instructions as input\n    4. Combines references and instructions in a prompt template\n    5. Sends the prompt to an OpenAI model\n    6. Returns the generated blog content\n    \n    Args:\n        template (str | None, optional): Custom prompt template to use.\n            Defaults to a template that combines references and instructions.\n    \n    Returns:\n        Graph: A Langflow Graph object with text_input as start and chat_output as end\n    \n    Dependencies:\n        - URLComponent\n        - ParseDataComponent\n        - TextInputComponent\n        - PromptComponent\n        - OpenAIModelComponent\n        - ChatOutput\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph\n        \n        # Create blog writer graph with default template\n        graph = blog_writer_graph()\n        \n        # Create graph with custom template\n        custom_template = "Data: {references}\n\nWrite a blog post about: {instructions}"\n        custom_graph = blog_writer_graph(template=custom_template)\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    \n\n```python\ndef ingestion_graph():\n    """\n    Creates a document ingestion graph for RAG applications.\n    \n    This function builds a graph that:\n    1. Loads files using a FileComponent\n    2. Splits the text into chunks with SplitTextComponent\n    3. Creates embeddings using OpenAI\'s embedding model\n    4. Stores the embeddings in an AstraDB vector store\n    \n    Returns:\n        Graph: A Langflow Graph object with file_component as start and vector_store as end\n    \n    Dependencies:\n        - FileComponent\n        - SplitTextComponent\n        - OpenAIEmbeddingsComponent\n        - AstraDBVectorStoreComponent\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph\n        \n        # Create the ingestion graph\n        graph = ingestion_graph()\n        \n        # Build and run the graph\n        graph.build()\n        ```\n    """\n    # Ingestion Graph\n    file_component = FileComponent()\n    text_splitter = SplitTextComponent()\n    text_splitter.set(data_inputs=file_component.load_files)\n    openai_embeddings = OpenAIEmbeddingsComponent()\n    vector_store = AstraDBVectorStoreComponent()\n    vector_store.set(\n        embedding_model=openai_embeddings.build_embeddings,\n        ingest_data=text_splitter.split_text,\n    )\n\n    return Graph(file_component, vector_store)\n```\n\nDocstring:\nCreates a document ingestion graph for RAG applications.\n\nThis function builds a graph that:\n1. Loads files using a FileComponent\n2. Splits the text into chunks with SplitTextComponent\n3. Creates embeddings using OpenAI\'s embedding model\n4. Stores the embeddings in an AstraDB vector store\n\nReturns:\n    Graph: A Langflow Graph object with file_component as start and vector_store as end\n\nDependencies:\n    - FileComponent\n    - SplitTextComponent\n    - OpenAIEmbeddingsComponent\n    - AstraDBVectorStoreComponent\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph\n    \n    # Create the ingestion graph\n    graph = ingestion_graph()\n    \n    # Build and run the graph\n    graph.build()\n    ```\n\n```python\ndef vector_store_rag_graph():\n    """\n    Combines ingestion and RAG graphs into a complete vector store RAG flow.\n    \n    This function:\n    1. Creates an ingestion graph for processing documents into a vector store\n    2. Creates a RAG graph for querying the vector store and generating responses\n    3. Combines them into a single operational graph\n    \n    Returns:\n        Graph: A combined graph representing the complete RAG workflow\n    \n    Dependencies:\n        - All dependencies from ingestion_graph() and rag_graph()\n        \n    Example:\n        ```python\n        from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph\n        \n        # Create the complete RAG workflow\n        graph = vector_store_rag_graph()\n        \n        # Build and run the graph\n        graph.build()\n        # Run ingestion first with documents\n        # Then run queries against the stored vectors\n        ```\n    """\n    return ingestion_graph() + rag_graph()\n```\n\nDocstring:\nCombines ingestion and RAG graphs into a complete vector store RAG flow.\n\nThis function:\n1. Creates an ingestion graph for processing documents into a vector store\n2. Creates a RAG graph for querying the vector store and generating responses\n3. Combines them into a single operational graph\n\nReturns:\n    Graph: A combined graph representing the complete RAG workflow\n\nDependencies:\n    - All dependencies from ingestion_graph() and rag_graph()\n    \nExample:\n    ```python\n    from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph\n    \n    # Create the complete RAG workflow\n    graph = vector_store_rag_graph()\n    \n    # Build and run the graph\n    graph.build()\n    # Run ingestion first with documents\n    # Then run queries against the stored vectors\n    ```\n\n### Question\nBuild a chatbot that responds to questions about uploaded CSV data with statistics\n'}]
Error querying OpenAI:
Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-MUQ2mi84ezqp39BqS0tFhNwr on tokens per min (TPM): Limit 30000, Requested 40820. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
Traceback:
Traceback (most recent call last):
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/src/graphrag_agent/flow_chat_v1.py", line 219, in query_openai
    response = await client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pedropacheco/Projects/dev/graphrag-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-MUQ2mi84ezqp39BqS0tFhNwr on tokens per min (TPM): Limit 30000, Requested 40820. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

MESSAGES:

--- MESSAGE #1: system ---
You are an AI assistant specialized in generating Python code for creating flows in langflow-ai. Use the provided context and documentation to generate accurate and efficient solutions. Flows are composed of components and connections. Each component has a set of inputs and outputs, along with mandatory attributes. Your job is to instantiate the components, populate their attributes, and connect them to other components. The output should strictly adhere to the provided context and consist only of valid Python code and necessary comments. When selecting the class name for components look at Component Information section and extract the class name. Remember that the flow must end with a  Graph(start=<first component>, end=<last component>) statement. If you don't know how to answer, say 'I don't know'.

======================

--- MESSAGE #2: user ---
Based on the following documentation, components, and sample code, generate a python function. Reply with code only. Do not provide any commentary:

### Documentation:
### Documentation
This component fetches content from one or more URLs, processes the content, and returns it in various formats. It supports output in plain text, raw HTML, or JSON, with options for cleaning and separating multiple outputs.
To use this component in a flow, connect the **DataFrame** output to a component that accepts the input.
For example, connect the **URL** component to a **Chat Output** component.
![URL request into a chat output component](/img/component-url.png)
In the URL component's **URLs** field, enter the URL for your request.
This example uses `langflow.org`.
Optionally, in the **Max Depth** field, enter how many pages away from the initial URL you want to crawl.
Select `1` to crawl only the page specified in the **URLs** field.
Select `2` to crawl all pages linked from that page.
The component crawls by link traversal, not by URL path depth.
Click **Playground**, and then click **Run Flow**.
The text contents of the URL are returned to the Playground as a structured DataFrame.
In the **URL** component, change the output port to **Message**, and then run the flow again.
The text contents of the URL are returned as unstructured raw text, which you can extract patterns from with the **Regex Extractor** tool.
Connect the **URL** component to a **Regex Extractor** and **Chat Output**.
![Regex extractor connected to url component](/img/component-url-regex.png)
In the **Regex Extractor** tool, enter a pattern to extract text from the **URL** component's raw output.
This example extracts the first paragraph from the "In the News" section of `https://en.wikipedia.org/wiki/Main_Page`.
Result:
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| urls | URLs | Enter one or more URLs. URLs are automatically validated and cleaned. |
| format | Output Format | Output Format. Use **Text** to extract text from the HTML, **Raw HTML** for the raw HTML content, or **JSON** to extract JSON from the HTML. |
| separator | Separator | Specify the separator to use between multiple outputs. Default for **Text** is `\n\n`. Default for **Raw HTML** is `\n<!-- Separator -->\n`. |
| clean_extra_whitespace | Clean Extra Whitespace | Whether to clean excessive blank lines in the text output. Only applies to `Text` format. |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Data | List of [Data](/concepts-objects) objects containing fetched content and metadata. |
| text | Text | Fetched content as formatted text, with applied separators and cleaning. |
| dataframe | DataFrame | Content formatted as a [DataFrame](/concepts-objects#dataframe-object) object. |

The **URL** data component loads content from a list of URLs.
In the component's **URLs** field, enter a comma-separated list of URLs you want to load. Alternatively, connect a component that outputs the `Message` type, like the **Chat Input** component, to supply your URLs with a component.
To output a `Data` type, in the **Output Format** dropdown, select **Raw HTML**.
To output a `Message` type, in the **Output Format** dropdown, select **Text**. This option applies postprocessing with the `data_to_text` helper function.
In this example of a document ingestion pipeline, the URL component outputs raw HTML to a text splitter, which splits the raw content into chunks for a vector database to ingest.
![URL component in a data ingestion pipeline](/img/url-component.png)

:::important
This component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.3.
:::
This component calls the `searchapi.io` API. It can be used to search the web for information.
For more information, see the [SearchAPI documentation](https://www.searchapi.io/docs/google).
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| engine | Engine | The search engine to use (default: "google") |
| api_key | SearchAPI API Key | The API key for authenticating with SearchAPI |
| input_value | Input | The search query or input for the API call |
| search_params | Search parameters | Additional parameters for customizing the search |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Search Results | List of Data objects containing search results |
| tool | Search API Tool | A Tool object for use in LangChain workflows |

This component allows you to run any flow stored in your Langflow database without opening the flow editor.
The Run Flow component can also be used as a tool when connected to an [Agent](/components-agents). The `name` and `description` metadata that the Agent uses to register the tool are created automatically.
When you select a flow, the component fetches the flow's graph structure and uses it to generate the inputs and outputs for the Run Flow component.
To use the Run Flow component as a tool, do the following:
Add the **Run Flow** component to the [Simple Agent](/starter-projects-simple-agent) flow.
In the **Flow Name** menu, select the sub-flow you want to run.
The appearance of the **Run Flow** component changes to reflect the inputs and outputs of the selected flow.
On the **Run Flow** component, enable **Tool Mode**.
Connect the **Run Flow** component to the **Toolset** input of the Agent.
Your flow should now look like this:
![Run Flow component](/img/component-run-flow.png)
Run the flow. The Agent uses the Run Flow component as a tool to run the selected sub-flow.
### Inputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| flow_name_selected | Dropdown | The name of the flow to run. |
| flow_tweak_data | Dict | Dictionary of tweaks to customize the flow's behavior. |
| dynamic inputs | Various | Additional inputs that are generated based on the selected flow. |
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| run_outputs | A `List` of types `Data`, `Message,` or `DataFrame` | All outputs are generated from running the flow. |

This flow creates a question-and-answer chatbot for documents that are loaded into the flow.
The [Unstructured.io](https://unstructured.io/) loader component loads files from your local machine, and then parses them into a list of structured [Data](/concepts-objects) objects.
This loaded data informs the **Open AI** component's responses to your questions.
![Sample Flow retrieving data with unstructured](/img/starter-flow-unstructured-qa.png)

:::important
This component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.3.
:::
This component allows you to call the Google Search API.
### Inputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| google_api_key | SecretString | Google API key for authentication |
| google_cse_id | SecretString | Google Custom Search Engine ID |
| input_value | String | Search query input |
| k | Integer | Number of search results to return |
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| results | List[Data] | List of search results |
| tool | Tool | Google Search tool for use in LangChain |

This component makes HTTP requests using URLs or cURL commands.
To use this component in a flow, connect the **Data** output to a component that accepts the input.
For example, connect the **API Request** component to a **Chat Output** component.
![API request into a chat output component](/img/component-api-request-chat-output.png)
In the API component's **URLs** field, enter the endpoint for your request.
This example uses `https://dummy-json.mock.beeceptor.com/posts`, which is a list of technology blog posts.
In the **Method** field, enter the type of request.
This example uses GET to retrieve a list of blog posts.
The component also supports POST, PATCH, PUT, and DELETE.
Optionally, enable the **Use cURL** button to create a field for pasting curl requests.
The equivalent call in this example is `curl -v https://dummy-json.mock.beeceptor.com/posts`.
Click **Playground**, and then click **Run Flow**.
Your request returns a list of blog posts in the `result` field.
### Filter API request data
The **API Request** component retrieved a list of JSON objects in the `result` field.
For this example, you will use the **Lambda Filter** to extract the desired data nested within the `result` field.
Connect a **Lambda Filter** to the API request component, and a **Language model** to the **Lambda Filter**. This example connects a **Groq** model component.
In the **Groq** model component, add your **Groq** API key.
To filter the data, in the **Lambda filter** component, in the **Instructions** field, use natural language to describe how the data should be filtered.
For this example, enter:
:::tip
Avoid punctuation in the **Instructions** field, as it can cause errors.
:::
4. To run the flow, in the **Lambda Filter** component, click <Icon name="Play" aria-label="Play icon" />.
5. To inspect the filtered data, in the **Lambda Filter** component, click <Icon name="TextSearch" aria-label="Inspect icon" />.
The result is a structured DataFrame.
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| urls | URLs | Enter one or more URLs, separated by commas. |
| curl | cURL | Paste a curl command to populate the dictionary fields for headers and body. |
| method | Method | The HTTP method to use. |
| use_curl | Use cURL | Enable cURL mode to populate fields from a cURL command. |
| query_params | Query Parameters | The query parameters to append to the URL. |
| body | Body | The body to send with the request as a dictionary (for `POST`, `PATCH`, `PUT`). |
| headers | Headers | The headers to send with the request as a dictionary. |
| timeout | Timeout | The timeout to use for the request. |
| follow_redirects | Follow Redirects | Whether to follow http redirects. |
| save_to_file | Save to File | Save the API response to a temporary file |
| include_httpx_metadata | Include HTTPx Metadata | Include properties such as `headers`, `status_code`, `response_headers`, and `redirection_history` in the output. |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Data | The result of the API requests. Returns a Data object containing source URL and results. |
| dataframe | DataFrame | Converts the API response data into a tabular DataFrame format. |

This component creates a tool for searching using SearXNG, a metasearch engine.
### Inputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| url | String | The URL of the SearXNG instance |
| max_results | Integer | Maximum number of results to return |
| categories | List[String] | Categories to search in |
| language | String | Language for the search results |
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| result_tool | Tool | SearXNG search tool for use in LangChain |

This flow creates a summarizing "for each" loop with the [Loop](/components-logic#loop) component.
The component iterates over a list of [Data](/concepts-objects#data-object) objects until it's completed, and then the **Done** loop aggregates the results.
The **File** component loads text files from your local machine, and then the **Parse Data** component parses them into a list of structured `Data` objects.
The **Loop** component passes each `Data` object to a **Prompt** to be summarized.
When the **Loop** component runs out of `Data`, the **Done** loop activates, which counts the number of pages and summarizes their tone with another **Prompt**.
This is represented in Langflow by connecting the Parse Data component's **Data List** output to the Loop component's `Data` loop input.
![Sample Flow looping summarizer](/img/loop-text-summarizer.png)
The output will look similar to this:

The [simple agent starter project](/starter-projects-simple-agent) uses an [agent component](#agent-component) connected to URL and Calculator tools to answer a user's questions. The OpenAI LLM acts as a brain for the agent to decide which tool to use. Tools are connected to agent components at the **Tools** port.
![Simple agent starter flow](/img/starter-flow-simple-agent.png)
For a multi-agent example, see [Create a problem-solving agent](/agents-tool-calling-agent-component).

### Component Information
### Component Information
Component: ScrapeGraphMarkdownifyApi
Name: ScrapeGraphMarkdownifyApi
Inputs: [SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='url', display_name='URL', tool_mode=True, info='The URL to markdownify.')]

Component: FirecrawlScrapeApi
Name: FirecrawlScrapeApi
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='url', display_name='URL', required=True, info='The URL to scrape.', tool_mode=True), IntInput(name='timeout', display_name='Timeout', info='Timeout in milliseconds for the request.'), DataInput(name='scrapeOptions', display_name='Scrape Options', info='The page options to send with the request.'), DataInput(name='extractorOptions', display_name='Extractor Options', info='The extractor options to send with the request.')]

Component: SpiderTool
Name: None
Inputs: [SecretStrInput(name='spider_api_key', display_name='Spider API Key', required=True, password=True, info='The Spider API Key, get it from https://spider.cloud'), StrInput(name='url', display_name='URL', required=True, info='The URL to scrape or crawl'), DropdownInput(name='mode', display_name='Mode', required=True, options=MODES, value=MODES[0], info='The mode of operation: scrape or crawl'), IntInput(name='limit', display_name='Limit', info='The maximum amount of pages allowed to crawl per website. Set to 0 to crawl all pages.', advanced=True), IntInput(name='depth', display_name='Depth', info='The crawl limit for maximum depth. If 0, no limit will be applied.', advanced=True), StrInput(name='blacklist', display_name='Blacklist', info='Blacklist paths that you do not want to crawl. Use Regex patterns.', advanced=True), StrInput(name='whitelist', display_name='Whitelist', info='Whitelist paths that you want to crawl, ignoring all other routes. Use Regex patterns.', advanced=True), BoolInput(name='readability', display_name='Use Readability', info='Use readability to pre-process the content for reading.', advanced=True), IntInput(name='request_timeout', display_name='Request Timeout', info='Timeout for the request in seconds.', advanced=True), BoolInput(name='metadata', display_name='Metadata', info='Include metadata in the response.', advanced=True), DictInput(name='params', display_name='Additional Parameters', info='Additional parameters to pass to the API. If provided, other inputs will be ignored.')]

Component: ScrapeGraphSmartScraperApi
Name: ScrapeGraphSmartScraperApi
Inputs: [SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='url', display_name='URL', tool_mode=True, info='The URL to scrape.'), MessageTextInput(name='prompt', display_name='Prompt', tool_mode=True, info='The prompt to use for the smart scraper.')]

Component: AgentQL
Name: AgentQL
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='Your AgentQL API key from dev.agentql.com'), MessageTextInput(name='url', display_name='URL', required=True, info='The URL of the public web page you want to extract data from.', tool_mode=True), MultilineInput(name='query', display_name='AgentQL Query', required=False, info='The AgentQL query to execute. Learn more at https://docs.agentql.com/agentql-query or use a prompt.', tool_mode=True), MultilineInput(name='prompt', display_name='Prompt', required=False, info='A Natural Language description of the data to extract from the page. Alternative to AgentQL query.', tool_mode=True), BoolInput(name='is_stealth_mode_enabled', display_name='Enable Stealth Mode (Beta)', info='Enable experimental anti-bot evasion strategies. May not work for all websites at all times.', value=False, advanced=True), IntInput(name='timeout', display_name='Timeout', info='Seconds to wait for a request.', value=900, advanced=True), DropdownInput(name='mode', display_name='Request Mode', info="'standard' uses deep data analysis, while 'fast' trades some depth of analysis for speed.", options=['fast', 'standard'], value='fast', advanced=True), IntInput(name='wait_for', display_name='Wait For', info='Seconds to wait for the page to load before extracting data.', value=0, range_spec=RangeSpec(min=0, max=10, step_type='int'), advanced=True), BoolInput(name='is_scroll_to_bottom_enabled', display_name='Enable scroll to bottom', info='Scroll to bottom of the page before extracting data.', value=False, advanced=True), BoolInput(name='is_screenshot_enabled', display_name='Enable screenshot', info="Take a screenshot before extracting data. Returned in 'metadata' as a Base64 string.", value=False, advanced=True)]

Component: FirecrawlExtractApi
Name: FirecrawlExtractApi
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='urls', display_name='URLs', required=True, info='List of URLs to extract data from (separated by commas or new lines).', tool_mode=True), MultilineInput(name='prompt', display_name='Prompt', required=True, info='Prompt to guide the extraction process.', tool_mode=True), DataInput(name='schema', display_name='Schema', required=False, info='Schema to define the structure of the extracted data.'), BoolInput(name='enable_web_search', display_name='Enable Web Search', info='When true, the extraction will use web search to find additional data.')]

Component: URLComponent
Name: URLComponent
Inputs: [MessageTextInput(name='urls', display_name='URLs', info="Enter one or more URLs to crawl recursively, by clicking the '+' button.", is_list=True, tool_mode=True, placeholder='Enter a URL...', list_add_label='Add URL'), IntInput(name='max_depth', display_name='Max Depth', info="Controls how many 'clicks' away from the initial page the crawler will go:\n- depth 1: only the initial page\n- depth 2: initial page + all pages linked directly from it\n- depth 3: initial page + direct links + links found on those direct link pages\nNote: This is about link traversal, not URL path depth.", value=1, required=False), BoolInput(name='prevent_outside', display_name='Prevent Outside', info='If enabled, only crawls URLs within the same domain as the root URL. This helps prevent the crawler from going to external websites.', value=True, required=False, advanced=True), BoolInput(name='use_async', display_name='Use Async', info='If enabled, uses asynchronous loading which can be significantly faster but might use more system resources.', value=True, required=False, advanced=True), DropdownInput(name='format', display_name='Output Format', info="Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.", options=['Text', 'HTML'], value='Text', advanced=True)]

Component: FirecrawlCrawlApi
Name: FirecrawlCrawlApi
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='url', display_name='URL', required=True, info='The URL to scrape.', tool_mode=True), IntInput(name='timeout', display_name='Timeout', info='Timeout in milliseconds for the request.'), StrInput(name='idempotency_key', display_name='Idempotency Key', info='Optional idempotency key to ensure unique requests.'), DataInput(name='crawlerOptions', display_name='Crawler Options', info='The crawler options to send with the request.'), DataInput(name='scrapeOptions', display_name='Scrape Options', info='The page options to send with the request.')]

Component: FirecrawlMapApi
Name: FirecrawlMapApi
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='urls', display_name='URLs', required=True, info='List of URLs to create maps from (separated by commas or new lines).', tool_mode=True), BoolInput(name='ignore_sitemap', display_name='Ignore Sitemap', info='When true, the sitemap.xml file will be ignored during crawling.'), BoolInput(name='sitemap_only', display_name='Sitemap Only', info='When true, only links found in the sitemap will be returned.'), BoolInput(name='include_subdomains', display_name='Include Subdomains', info='When true, subdomains of the provided URL will also be scanned.')]

Component: ScrapeGraphSearchApi
Name: ScrapeGraphSearchApi
Inputs: [SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='user_prompt', display_name='Search Prompt', tool_mode=True, info='The search prompt to use.')]

### Code Examples
### Code Examples
```python
def blog_writer_graph(template: str | None = None):
    """
    Creates a blog writing graph that generates content based on reference URLs.
    
    This function builds a graph that:
    1. Fetches content from specified URLs
    2. Parses the fetched content
    3. Takes user instructions as input
    4. Combines references and instructions in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the generated blog content
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that combines references and instructions.
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - URLComponent
        - ParseDataComponent
        - TextInputComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph
        
        # Create blog writer graph with default template
        graph = blog_writer_graph()
        
        # Create graph with custom template
        custom_template = "Data: {references}\n\nWrite a blog post about: {instructions}"
        custom_graph = blog_writer_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = dedent("""Reference 1:

{references}

---

{instructions}

Blog:
""")
    url_component = URLComponent()
    url_component.set(urls=["https://langflow.org/", "https://docs.langflow.org/"])
    parse_data_component = ParseDataComponent()
    parse_data_component.set(data=url_component.fetch_content)

    text_input = TextInputComponent(_display_name="Instructions")
    text_input.set(
        input_value="Use the references above for style to write a new blog/tutorial about Langflow and AI. "
        "Suggest non-covered topics."
    )

    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        instructions=text_input.text_response,
        references=parse_data_component.parse_data,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=text_input, end=chat_output)
```

Docstring:
    Creates a blog writing graph that generates content based on reference URLs.
    
    This function builds a graph that:
    1. Fetches content from specified URLs
    2. Parses the fetched content
    3. Takes user instructions as input
    4. Combines references and instructions in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the generated blog content
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that combines references and instructions.
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - URLComponent
        - ParseDataComponent
        - TextInputComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph
        
        # Create blog writer graph with default template
        graph = blog_writer_graph()
        
        # Create graph with custom template
        custom_template = "Data: {references}

Write a blog post about: {instructions}"
        custom_graph = blog_writer_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def sequential_tasks_agent_graph():
    """
    Creates a sequential task execution agent graph.
    
    This function builds a graph that:
    1. Takes a topic as input via TextInputComponent
    2. Sets up three specialized agents: Researcher, Editor, and Comedian
    3. Configures sequential tasks where each agent builds upon previous agent's work
    4. Uses SearchAPI tool for research capabilities
    5. Produces a final creative blog post about the given topic
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - TextInputComponent
        - OpenAIModelComponent
        - PromptComponent
        - SequentialTaskAgentComponent
        - SequentialCrewComponent
        - SearchAPIComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph
        
        # Create sequential tasks agent graph
        graph = sequential_tasks_agent_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    llm = OpenAIModelComponent()
    search_api_tool = SearchAPIComponent()

    text_input = TextInputComponent(_display_name="Topic")
    text_input.set(input_value="Agile")

    # Document Prompt for Researcher
    document_prompt_component = PromptComponent()
    document_prompt_component.set(
        template="""Topic: {topic}

Build a document about this topic.""",
        topic=text_input.text_response,
    )

    # Researcher Task Agent
    researcher_task_agent = SequentialTaskAgentComponent()
    researcher_task_agent.set(
        role="Researcher",
        goal="Search Google to find information to complete the task.",
        backstory="Research has always been your thing. You can quickly find things on the web because of your skills.",
        tools=[search_api_tool.build_tool],
        llm=llm.build_model,
        task_description=document_prompt_component.build_prompt,
        expected_output="Bullet points and small phrases about the research topic.",
    )

    # Revision Prompt for Editor
    revision_prompt_component = PromptComponent()
    revision_prompt_component.set(
        template="""Topic: {topic}

Revise this document.""",
        topic=text_input.text_response,
    )

    # Editor Task Agent
    editor_task_agent = SequentialTaskAgentComponent()
    editor_task_agent.set(
        role="Editor",
        goal="You should edit the information provided by the Researcher to make it more palatable and to not contain "
        "misleading information.",
        backstory="You are the editor of the most reputable journal in the world.",
        llm=llm.build_model,
        task_description=revision_prompt_component.build_prompt,
        expected_output="Small paragraphs and bullet points with the corrected content.",
        previous_task=researcher_task_agent.build_agent_and_task,
    )

    # Blog Prompt for Comedian
    blog_prompt_component = PromptComponent()
    blog_prompt_component.set(
        template="""Topic: {topic}

Build a fun blog post about this topic.""",
        topic=text_input.text_response,
    )

    # Comedian Task Agent
    comedian_task_agent = SequentialTaskAgentComponent()
    comedian_task_agent.set(
        role="Comedian",
        goal="You write comedic content based on the information provided by the editor.",
        backstory="Your formal occupation is Comedian-in-Chief. "
        "You write jokes, do standup comedy, and write funny articles.",
        llm=llm.build_model,
        task_description=blog_prompt_component.build_prompt,
        expected_output="A small blog about the topic.",
        previous_task=editor_task_agent.build_agent_and_task,
    )

    crew_component = SequentialCrewComponent()
    crew_component.set(
        tasks=comedian_task_agent.build_agent_and_task,
    )

    # Set up the output component
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    # Create the graph
    return Graph(
        start=text_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
Creates a sequential task execution agent graph.

This function builds a graph that:
1. Takes a topic as input via TextInputComponent
2. Sets up three specialized agents: Researcher, Editor, and Comedian
3. Configures sequential tasks where each agent builds upon previous agent's work
4. Uses SearchAPI tool for research capabilities
5. Produces a final creative blog post about the given topic

Returns:
    Graph: A Langflow Graph object with text_input as start and chat_output as end

Dependencies:
    - TextInputComponent
    - OpenAIModelComponent
    - PromptComponent
    - SequentialTaskAgentComponent
    - SequentialCrewComponent
    - SearchAPIComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph
    
    # Create sequential tasks agent graph
    graph = sequential_tasks_agent_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def ingestion_graph():
    """
    Creates a document ingestion graph for RAG applications.
    
    This function builds a graph that:
    1. Loads files using a FileComponent
    2. Splits the text into chunks with SplitTextComponent
    3. Creates embeddings using OpenAI's embedding model
    4. Stores the embeddings in an AstraDB vector store
    
    Returns:
        Graph: A Langflow Graph object with file_component as start and vector_store as end
    
    Dependencies:
        - FileComponent
        - SplitTextComponent
        - OpenAIEmbeddingsComponent
        - AstraDBVectorStoreComponent
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph
        
        # Create the ingestion graph
        graph = ingestion_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    # Ingestion Graph
    file_component = FileComponent()
    text_splitter = SplitTextComponent()
    text_splitter.set(data_inputs=file_component.load_files)
    openai_embeddings = OpenAIEmbeddingsComponent()
    vector_store = AstraDBVectorStoreComponent()
    vector_store.set(
        embedding_model=openai_embeddings.build_embeddings,
        ingest_data=text_splitter.split_text,
    )

    return Graph(file_component, vector_store)
```

Docstring:
Creates a document ingestion graph for RAG applications.

This function builds a graph that:
1. Loads files using a FileComponent
2. Splits the text into chunks with SplitTextComponent
3. Creates embeddings using OpenAI's embedding model
4. Stores the embeddings in an AstraDB vector store

Returns:
    Graph: A Langflow Graph object with file_component as start and vector_store as end

Dependencies:
    - FileComponent
    - SplitTextComponent
    - OpenAIEmbeddingsComponent
    - AstraDBVectorStoreComponent
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph
    
    # Create the ingestion graph
    graph = ingestion_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def document_qa_graph(template: str | None = None):
    """
    Creates a document question-answering graph.
    
    This function builds a graph that:
    1. Loads document files
    2. Parses the document content
    3. Takes user questions as input
    4. Combines document content and question in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the model's answer
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that presents the document and user question.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph
        
        # Create document QA graph with default template
        graph = document_qa_graph()
        
        # Create graph with custom template
        custom_template = "Document content:\n{Document}\n\nAnswer this: {Question}"
        custom_graph = document_qa_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = """Answer user's questions based on the document below:

---

{Document}

---

Question:
{Question}

Answer:
"""
    file_component = FileComponent()
    parse_data_component = ParseDataComponent()
    parse_data_component.set(data=file_component.load_files)

    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        context=parse_data_component.parse_data,
        question=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
    Creates a document question-answering graph.
    
    This function builds a graph that:
    1. Loads document files
    2. Parses the document content
    3. Takes user questions as input
    4. Combines document content and question in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the model's answer
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that presents the document and user question.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph
        
        # Create document QA graph with default template
        graph = document_qa_graph()
        
        # Create graph with custom template
        custom_template = "Document content:
{Document}

Answer this: {Question}"
        custom_graph = document_qa_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def complex_agent_graph():
    """
    Creates a complex agent graph with dynamic role assignment and hierarchical task execution.
    
    This function builds a graph that:
    1. Takes user input via ChatInput
    2. Dynamically determines appropriate role, goal, and backstory based on user query
    3. Creates a specialized agent with search and financial tools
    4. Sets up a manager agent to coordinate responses
    5. Structures tasks in a hierarchical manner
    6. Returns comprehensive responses to user queries
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - OpenAIModelComponent
        - PromptComponent
        - CrewAIAgentComponent
        - HierarchicalTaskComponent
        - HierarchicalCrewComponent
        - SearchAPIComponent
        - YfinanceToolComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph
        
        # Create complex agent graph
        graph = complex_agent_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    llm = OpenAIModelComponent(model_name="gpt-4o-mini")
    manager_llm = OpenAIModelComponent(model_name="gpt-4o")
    search_api_tool = SearchAPIComponent()
    yahoo_search_tool = YfinanceToolComponent()
    dynamic_agent = CrewAIAgentComponent()
    chat_input = ChatInput()
    role_prompt = PromptComponent(_display_name="Role Prompt")
    role_prompt.set(
        template="""Define a Role that could execute or answer well the user's query.

User's query: {query}

Role should be two words max. Something like "Researcher" or "Software Developer".
"""
    )

    goal_prompt = PromptComponent(_display_name="Goal Prompt")
    goal_prompt.set(
        template="""Define the Goal of this Role, given the User's Query.
User's query: {query}

Role: {role}

The goal should be concise and specific.
Goal:
""",
        query=chat_input.message_response,
        role=role_prompt.build_prompt,
    )
    backstory_prompt = PromptComponent(_display_name="Backstory Prompt")
    backstory_prompt.set(
        template="""Define a Backstory of this Role and Goal, given the User's Query.
User's query: {query}

Role: {role}
Goal: {goal}

The backstory should be specific and well aligned with the rest of the information.
Backstory:""",
        query=chat_input.message_response,
        role=role_prompt.build_prompt,
        goal=goal_prompt.build_prompt,
    )
    dynamic_agent.set(
        tools=[search_api_tool.build_tool, yahoo_search_tool.build_tool],
        llm=llm.build_model,
        role=role_prompt.build_prompt,
        goal=goal_prompt.build_prompt,
        backstory=backstory_prompt.build_prompt,
    )

    response_prompt = PromptComponent()
    response_prompt.set(
        template="""User's query:
{query}

Respond to the user with as much as information as you can about the topic. Delete if needed.
If it is just a general query (e.g a greeting) you can respond them directly.""",
        query=chat_input.message_response,
    )
    manager_agent = CrewAIAgentComponent()
    manager_agent.set(
        llm=manager_llm.build_model,
        role="Manager",
        goal="You can answer general questions from the User and may call others for help if needed.",
        backstory="You are polite and helpful. You've always been a beacon of politeness.",
    )
    task = HierarchicalTaskComponent()
    task.set(
        task_description=response_prompt.build_prompt,
        expected_output="Succinct response that answers the User's query.",
    )
    crew_component = HierarchicalCrewComponent()
    crew_component.set(
        tasks=task.build_task, agents=[dynamic_agent.build_output], manager_agent=manager_agent.build_output
    )
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    return Graph(
        start=chat_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
Creates a complex agent graph with dynamic role assignment and hierarchical task execution.

This function builds a graph that:
1. Takes user input via ChatInput
2. Dynamically determines appropriate role, goal, and backstory based on user query
3. Creates a specialized agent with search and financial tools
4. Sets up a manager agent to coordinate responses
5. Structures tasks in a hierarchical manner
6. Returns comprehensive responses to user queries

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - OpenAIModelComponent
    - PromptComponent
    - CrewAIAgentComponent
    - HierarchicalTaskComponent
    - HierarchicalCrewComponent
    - SearchAPIComponent
    - YfinanceToolComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph
    
    # Create complex agent graph
    graph = complex_agent_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def hierarchical_tasks_agent_graph():
    llm = OpenAIModelComponent(model_name="gpt-4o-mini")
    manager_llm = OpenAIModelComponent(model_name="gpt-4o")
    search_api_tool = SearchAPIComponent()
    researcher_agent = CrewAIAgentComponent()
    chat_input = ChatInput()
    researcher_agent.set(
        tools=[search_api_tool.build_tool],
        llm=llm.build_model,
        role="Researcher",
        goal="Search for information about the User's query and answer as best as you can",
        backstory="You are a reliable researcher and journalist ",
    )

    editor_agent = CrewAIAgentComponent()

    editor_agent.set(
        llm=llm.build_model,
        role="Editor",
        goal="Evaluate the information for misleading or biased data.",
        backstory="You are a reliable researcher and journalist ",
    )

    response_prompt = PromptComponent()
    response_prompt.set(
        template="""User's query:
{query}

Respond to the user with as much as information as you can about the topic. Delete if needed.
If it is just a general query (e.g a greeting) you can respond them directly.""",
        query=chat_input.message_response,
    )
    manager_agent = CrewAIAgentComponent()
    manager_agent.set(
        llm=manager_llm.build_model,
        role="Manager",
        goal="You can answer general questions from the User and may call others for help if needed.",
        backstory="You are polite and helpful. You've always been a beacon of politeness.",
    )
    task = HierarchicalTaskComponent()
    task.set(
        task_description=response_prompt.build_prompt,
        expected_output="Succinct response that answers the User's query.",
    )
    crew_component = HierarchicalCrewComponent()
    crew_component.set(
        tasks=task.build_task,
        agents=[researcher_agent.build_output, editor_agent.build_output],
        manager_agent=manager_agent.build_output,
    )
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    return Graph(
        start=chat_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
None

```python
def rag_graph():
    """
    Creates a Retrieval Augmented Generation (RAG) query graph.
    
    This function builds a graph that:
    1. Takes user input via ChatInput
    2. Uses the input to query a vector store (AstraDB)
    3. Processes and parses retrieved documents
    4. Formats a prompt with retrieved context and user query
    5. Sends the prompt to OpenAI model
    6. Returns the model's response as output
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - OpenAIEmbeddingsComponent
        - AstraDBVectorStoreComponent
        - ParseDataComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph
        
        # Create the RAG query graph
        graph = rag_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    # RAG Graph
    openai_embeddings = OpenAIEmbeddingsComponent()
    chat_input = ChatInput()
    rag_vector_store = AstraDBVectorStoreComponent()
    rag_vector_store.set(
        search_query=chat_input.message_response,
        embedding_model=openai_embeddings.build_embeddings,
    )

    parse_data = ParseDataComponent()
    parse_data.set(data=rag_vector_store.search_documents)
    prompt_component = PromptComponent()
    prompt_component.set(
        template=dedent("""Given the following context, answer the question.
                         Context:{context}

                         Question: {question}
                         Answer:"""),
        context=parse_data.parse_data,
        question=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
Creates a Retrieval Augmented Generation (RAG) query graph.

This function builds a graph that:
1. Takes user input via ChatInput
2. Uses the input to query a vector store (AstraDB)
3. Processes and parses retrieved documents
4. Formats a prompt with retrieved context and user query
5. Sends the prompt to OpenAI model
6. Returns the model's response as output

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - OpenAIEmbeddingsComponent
    - AstraDBVectorStoreComponent
    - ParseDataComponent
    - PromptComponent
    - OpenAIModelComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph
    
    # Create the RAG query graph
    graph = rag_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def vector_store_rag_graph():
    """
    Combines ingestion and RAG graphs into a complete vector store RAG flow.
    
    This function:
    1. Creates an ingestion graph for processing documents into a vector store
    2. Creates a RAG graph for querying the vector store and generating responses
    3. Combines them into a single operational graph
    
    Returns:
        Graph: A combined graph representing the complete RAG workflow
    
    Dependencies:
        - All dependencies from ingestion_graph() and rag_graph()
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph
        
        # Create the complete RAG workflow
        graph = vector_store_rag_graph()
        
        # Build and run the graph
        graph.build()
        # Run ingestion first with documents
        # Then run queries against the stored vectors
        ```
    """
    return ingestion_graph() + rag_graph()
```

Docstring:
Combines ingestion and RAG graphs into a complete vector store RAG flow.

This function:
1. Creates an ingestion graph for processing documents into a vector store
2. Creates a RAG graph for querying the vector store and generating responses
3. Combines them into a single operational graph

Returns:
    Graph: A combined graph representing the complete RAG workflow

Dependencies:
    - All dependencies from ingestion_graph() and rag_graph()
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph
    
    # Create the complete RAG workflow
    graph = vector_store_rag_graph()
    
    # Build and run the graph
    graph.build()
    # Run ingestion first with documents
    # Then run queries against the stored vectors
    ```

```python
def basic_prompting_graph(template: str | None = None):
    """
    Creates a basic conversation graph with prompt templating.
    
    This function builds a simple graph that:
    1. Takes user input via ChatInput
    2. Processes the input through a template prompt
    3. Sends the formatted prompt to an OpenAI model
    4. Returns the model's response as output
    
    Args:
        template (str | None, optional): Custom prompt template to use. 
            Defaults to a pirate-themed response template.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph
        
        # Create graph with default pirate template
        default_graph = basic_prompting_graph()
        
        # Create graph with custom template
        custom_template = "Answer as a helpful assistant: {user_input}"
        custom_graph = basic_prompting_graph(template=custom_template)
        
        # Build and run the graph
        default_graph.build()
        ```
    """
    if template is None:
        template = """Answer the user as if you were a pirate.

User: {user_input}

Answer:
"""
    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        user_input=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
Creates a basic conversation graph with prompt templating.

This function builds a simple graph that:
1. Takes user input via ChatInput
2. Processes the input through a template prompt
3. Sends the formatted prompt to an OpenAI model
4. Returns the model's response as output

Args:
    template (str | None, optional): Custom prompt template to use. 
        Defaults to a pirate-themed response template.

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - PromptComponent
    - OpenAIModelComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph
    
    # Create graph with default pirate template
    default_graph = basic_prompting_graph()
    
    # Create graph with custom template
    custom_template = "Answer as a helpful assistant: {user_input}"
    custom_graph = basic_prompting_graph(template=custom_template)
    
    # Build and run the graph
    default_graph.build()
    ```

```python
def memory_chatbot_graph(template: str | None = None):
    """
    Creates a chatbot graph with conversation memory.
    
    This function builds a graph that:
    1. Maintains conversation history using MemoryComponent
    2. Takes user input via ChatInput
    3. Combines conversation context and current message in a prompt
    4. Sends the prompt to an OpenAI model
    5. Returns the model's response while updating conversation memory
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that includes conversation context and user message.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - MemoryComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph
        
        # Create memory chatbot with default template
        graph = memory_chatbot_graph()
        
        # Create graph with custom template
        custom_template = "Previous messages:\n{context}\n\nUser: {user_message}\nAI:"
        custom_graph = memory_chatbot_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = """{context}

    User: {user_message}
    AI: """
    memory_component = MemoryComponent()
    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template, user_message=chat_input.message_response, context=memory_component.retrieve_messages_as_text
    )
    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(chat_input, chat_output)
```

Docstring:
    Creates a chatbot graph with conversation memory.
    
    This function builds a graph that:
    1. Maintains conversation history using MemoryComponent
    2. Takes user input via ChatInput
    3. Combines conversation context and current message in a prompt
    4. Sends the prompt to an OpenAI model
    5. Returns the model's response while updating conversation memory
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that includes conversation context and user message.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - MemoryComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph
        
        # Create memory chatbot with default template
        graph = memory_chatbot_graph()
        
        # Create graph with custom template
        custom_template = "Previous messages:
{context}

User: {user_message}
AI:"
        custom_graph = memory_chatbot_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

### Question
Create a flow that takes a website URL, scrapes the content, and generates an SEO report


======================

--- MESSAGE #3: assistant ---
```python
def seo_report_graph():
    """
    Creates a graph to scrape a website URL and generate an SEO report.
    
    This function builds a graph that:
    1. Takes a website URL as input
    2. Scrapes the content of the website
    3. Analyzes the content to generate an SEO report
    
    Returns:
        Graph: A Langflow Graph object with url_input as start and seo_output as end
    
    Dependencies:
        - URLComponent
        - ScrapeGraphSmartScraperApi
        - SEOAnalysisComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.seo_report import seo_report_graph
        
        # Create the SEO report graph
        graph = seo_report_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    url_input = URLComponent()
    scraper = ScrapeGraphSmartScraperApi()
    scraper.set(api_key="your_scrapegraph_api_key", url=url_input.urls)

    seo_analysis = SEOAnalysisComponent()
    seo_analysis.set(content=scraper.data)

    seo_output = ChatOutput()
    seo_output.set(input_value=seo_analysis.report)

    return Graph(start=url_input, end=seo_output)
```

======================

--- MESSAGE #4: user ---
Based on the following documentation, components, and sample code, generate a python function. Reply with code only. Do not provide any commentary:

### Documentation:
### Documentation
This flow creates a summarizing "for each" loop with the [Loop](/components-logic#loop) component.
The component iterates over a list of [Data](/concepts-objects#data-object) objects until it's completed, and then the **Done** loop aggregates the results.
The **File** component loads text files from your local machine, and then the **Parse Data** component parses them into a list of structured `Data` objects.
The **Loop** component passes each `Data` object to a **Prompt** to be summarized.
When the **Loop** component runs out of `Data`, the **Done** loop activates, which counts the number of pages and summarizes their tone with another **Prompt**.
This is represented in Langflow by connecting the Parse Data component's **Data List** output to the Loop component's `Data` loop input.
![Sample Flow looping summarizer](/img/loop-text-summarizer.png)
The output will look similar to this:

This component generates text using MistralAI LLMs.
For more information, see [Mistral AI documentation](https://docs.mistral.ai/).
### Inputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens (advanced). |
| model_name | String | The name of the Mistral AI model to use. Options include `open-mixtral-8x7b`, `open-mixtral-8x22b`, `mistral-small-latest`, `mistral-medium-latest`, `mistral-large-latest`, and `codestral-latest`. Default: `codestral-latest`. |
| mistral_api_base | String | The base URL of the Mistral API. Defaults to `https://api.mistral.ai/v1` (advanced). |
| api_key | SecretString | The Mistral API Key to use for authentication. |
| temperature | Float | Controls randomness in the output. Default: 0.5. |
| max_retries | Integer | Maximum number of retries for API calls. Default: 5 (advanced). |
| timeout | Integer | Timeout for API calls in seconds. Default: 60 (advanced). |
| max_concurrent_requests | Integer | Maximum number of concurrent API requests. Default: 3 (advanced). |
| top_p | Float | Nucleus sampling parameter. Default: 1 (advanced). |
| random_seed | Integer | Seed for random number generation. Default: 1 (advanced). |
| safe_mode | Boolean | Enables safe mode for content generation (advanced). |
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| model | LanguageModel | An instance of ChatMistralAI configured with the specified parameters. |

The **Split Text** processing component in this flow splits the incoming [Data](/concepts-objects) into chunks to be embedded into the vector store component.
The component offers control over chunk size, overlap, and separator, which affect context and granularity in vector store retrieval results.
![A vector store ingesting documents](/img/vector-store-document-ingestion.png)

This flow creates a question-and-answer chatbot for documents that are loaded into the flow.
The [Unstructured.io](https://unstructured.io/) loader component loads files from your local machine, and then parses them into a list of structured [Data](/concepts-objects) objects.
This loaded data informs the **Open AI** component's responses to your questions.
![Sample Flow retrieving data with unstructured](/img/starter-flow-unstructured-qa.png)

This component generates embeddings using [MistralAI](https://docs.mistral.ai/) models.
### Inputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| model | String | The MistralAI model to use (default: "mistral-embed") |
| mistral_api_key | SecretString | API key for authenticating with MistralAI |
| max_concurrent_requests | Integer | Maximum number of concurrent API requests (default: 64) |
| max_retries | Integer | Maximum number of retry attempts for failed requests (default: 5) |
| timeout | Integer | Request timeout in seconds (default: 120) |
| endpoint | String | Custom API endpoint URL (default: `https://api.mistral.ai/v1/`) |
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| embeddings | Embeddings | MistralAIEmbeddings instance for generating embeddings |

This component loads and parses files of various supported formats and converts the content into a [Data](/concepts-objects) object. It supports multiple file types and provides options for parallel processing and error handling.
To load a document, follow these steps:
Click the **Select files** button.
Select a local file or a file loaded with [File management](/concepts-file-management), and then click **Select file**.
The loaded file name appears in the component.
The default maximum supported file size is 100 MB.
To modify this value, see [--max-file-size-upload](/environment-variables#LANGFLOW_MAX_FILE_SIZE_UPLOAD).
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| path | Files | Path to file(s) to load. Supports individual files or bundled archives. |
| file_path | Server File Path | Data object with a `file_path` property pointing to the server file or a Message object with a path to the file. Supersedes 'Path' but supports the same file types. |
| separator | Separator | Specify the separator to use between multiple outputs in Message format. |
| silent_errors | Silent Errors | If true, errors do not raise an exception. |
| delete_server_file_after_processing | Delete Server File After Processing | If true, the Server File Path is deleted after processing. |
| ignore_unsupported_extensions | Ignore Unsupported Extensions | If true, files with unsupported extensions are not processed. |
| ignore_unspecified_files | Ignore Unspecified Files | If true, `Data` with no `file_path` property is ignored. |
| use_multithreading | [Deprecated] Use Multithreading | Set 'Processing Concurrency' greater than `1` to enable multithreading. This option is deprecated. |
| concurrency_multithreading | Processing Concurrency | When multiple files are being processed, the number of files to process concurrently. Default is 1. Values greater than 1 enable parallel processing for 2 or more files. |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Data | Parsed content of the file as a [Data](/concepts-objects) object. |
| dataframe | DataFrame | File content as a [DataFrame](/concepts-objects#dataframe-object) object. |
| message | Message | File content as a [Message](/concepts-objects#message-object) object. |
### Supported File Types
Text files:
`.txt` - Text files
`.md`, `.mdx` - Markdown files
`.csv` - CSV files
`.json` - JSON files
`.yaml`, `.yml` - YAML files
`.xml` - XML files
`.html`, `.htm` - HTML files
`.pdf` - PDF files
`.docx` - Word documents
`.py` - Python files
`.sh` - Shell scripts
`.sql` - SQL files
`.js` - JavaScript files
`.ts`, `.tsx` - TypeScript files
Archive formats (for bundling multiple files):
`.zip` - ZIP archives
`.tar` - TAR archives
`.tgz` - Gzipped TAR archives
`.bz2` - Bzip2 compressed files
`.gz` - Gzip compressed files

This example uses the **Astra DB vector store** component. Your vector store component's parameters and authentication may be different, but the document ingestion workflow is the same. A document is loaded from a local machine and chunked. The Astra DB vector store generates embeddings with the connected [model](/components-models) component, and stores them in the connected Astra DB database.
This vector data can then be retrieved for workloads like Retrieval Augmented Generation.
![](/img/vector-store-retrieval.png)
The user's chat input is embedded and compared to the vectors embedded during document ingestion for a similarity search.
The results are output from the vector database component as a [Data](/concepts-objects) object and parsed into text.
This text fills the `{context}` variable in the **Prompt** component, which informs the **Open AI model** component's responses.
Alternatively, connect the vector database component's **Retriever** port to a [retriever tool](components-tools#retriever-tool), and then to an [agent](/components-agents) component. This enables the agent to use your vector database as a tool and make decisions based on the available data.
![](/img/vector-store-agent-retrieval-tool.png)

This component uses the [Unstructured.io](https://unstructured.io/) Serverless API to load and parse files into a list of structured [Data](/concepts-objects) objects.
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| file | File | The path to the file to be parsed (supported types are listed [here](https://docs.unstructured.io/api-reference/api-services/overview#supported-file-types)) |
| api_key | API Key | Unstructured.io Serverless API Key |
| api_url | Unstructured.io API URL | Optional URL for the Unstructured API |
| chunking_strategy | Chunking Strategy | Strategy for chunking the document (options: "", "basic", "by_title", "by_page", "by_similarity") |
| unstructured_args | Additional Arguments | Optional dictionary of additional arguments for the Unstructured.io API |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Data | List of Data objects containing the parsed content from the input file |

The Confluence component integrates with the Confluence wiki collaboration platform to load and process documents. It utilizes the ConfluenceLoader from LangChain to fetch content from a specified Confluence space.
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| url | Site URL | The base URL of the Confluence Space (e.g., `https://company.atlassian.net/wiki`) |
| username | Username | Atlassian User E-mail (e.g., `email@example.com`) |
| api_key | API Key | Atlassian API Key (Create an API key at: [Atlassian](https://id.atlassian.com/manage-profile/security/api-tokens)) |
| space_key | Space Key | The key of the Confluence space to access |
| cloud | Use Cloud? | Whether to use Confluence Cloud (default: true) |
| content_format | Content Format | Specify content format (default: STORAGE) |
| max_pages | Max Pages | Maximum number of pages to retrieve (default: 1000) |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Data | List of Data objects containing the loaded Confluence documents |

Model components receive inputs and prompts for generating text, and the generated text is sent to an output component.
The model output can also be sent to the **Language Model** port and on to a **Parse Data** component, where the output can be parsed into structured [Data](/concepts-objects) objects.
This example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/starter-projects-basic-prompting).
![](/img/starter-flow-basic-prompting.png)

### Component Information
### Component Information
Component: MistralAIModelComponent
Name: MistralModel
Inputs: [*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=['open-mixtral-8x7b', 'open-mixtral-8x22b', 'mistral-small-latest', 'mistral-medium-latest', 'mistral-large-latest', 'codestral-latest'], value='codestral-latest'), StrInput(name='mistral_api_base', display_name='Mistral API Base', advanced=True, info='The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.'), SecretStrInput(name='api_key', display_name='Mistral API Key', info='The Mistral API Key to use for the Mistral model.', advanced=False, required=True, value='MISTRAL_API_KEY'), FloatInput(name='temperature', display_name='Temperature', value=0.1, advanced=True), IntInput(name='max_retries', display_name='Max Retries', advanced=True, value=5), IntInput(name='timeout', display_name='Timeout', advanced=True, value=60), IntInput(name='max_concurrent_requests', display_name='Max Concurrent Requests', advanced=True, value=3), FloatInput(name='top_p', display_name='Top P', advanced=True, value=1), IntInput(name='random_seed', display_name='Random Seed', value=1, advanced=True), BoolInput(name='safe_mode', display_name='Safe Mode', advanced=True, value=False)]

Component: DocumentsToDataComponent
Name: DocumentsToData
Inputs: {}

Component: MistralAIEmbeddingsComponent
Name: MistalAIEmbeddings
Inputs: [DropdownInput(name='model', display_name='Model', advanced=False, options=['mistral-embed'], value='mistral-embed'), SecretStrInput(name='mistral_api_key', display_name='Mistral API Key', required=True), IntInput(name='max_concurrent_requests', display_name='Max Concurrent Requests', advanced=True, value=64), IntInput(name='max_retries', display_name='Max Retries', advanced=True, value=5), IntInput(name='timeout', display_name='Request Timeout', advanced=True, value=120), MessageTextInput(name='endpoint', display_name='API Endpoint', advanced=True, value='https://api.mistral.ai/v1/')]

Component: ListFlowsComponent
Name: ListFlows
Inputs: {}

Component: RunFlowComponent
Name: RunFlow
Inputs: RunFlowBaseComponent._base_inputs

Component: FlowToolComponent
Name: FlowTool
Inputs: [DropdownInput(name='flow_name', display_name='Flow Name', info='The name of the flow to run.', refresh_button=True), StrInput(name='tool_name', display_name='Name', info='The name of the tool.'), StrInput(name='tool_description', display_name='Description', info="The description of the tool; defaults to the Flow's description."), BoolInput(name='return_direct', display_name='Return Direct', info='Return the result directly from the Tool.', advanced=True)]

Component: ScrapeGraphMarkdownifyApi
Name: ScrapeGraphMarkdownifyApi
Inputs: [SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='url', display_name='URL', tool_mode=True, info='The URL to markdownify.')]

Component: AssemblyAILeMUR
Name: None
Inputs: [SecretStrInput(name='api_key', display_name='Assembly API Key', info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/', advanced=False, required=True), DataInput(name='transcription_result', display_name='Transcription Result', info='The transcription result from AssemblyAI', required=True), MultilineInput(name='prompt', display_name='Input Prompt', info='The text to prompt the model', required=True), DropdownInput(name='final_model', display_name='Final Model', options=['claude3_5_sonnet', 'claude3_opus', 'claude3_haiku', 'claude3_sonnet'], value='claude3_5_sonnet', info='The model that is used for the final prompt after compression is performed', advanced=True), FloatInput(name='temperature', display_name='Temperature', advanced=True, value=0.0, info='The temperature to use for the model'), IntInput(name='max_output_size', display_name=' Max Output Size', advanced=True, value=2000, info='Max output size in tokens, up to 4000'), DropdownInput(name='endpoint', display_name='Endpoint', options=['task', 'summary', 'question-answer'], value='task', info="The LeMUR endpoint to use. For 'summary' and 'question-answer', no prompt input is needed. See https://www.assemblyai.com/docs/api-reference/lemur/ for more info.", advanced=True), MultilineInput(name='questions', display_name='Questions', info="Comma-separated list of your questions. Only used if Endpoint is 'question-answer'", advanced=True), MultilineInput(name='transcript_ids', display_name='Transcript IDs', info='Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided, the Transcription Result is ignored.', advanced=True)]

Component: AstraDBToolComponent
Name: None
Inputs: [StrInput(name='tool_name', display_name='Tool Name', info='The name of the tool to be passed to the LLM.', required=True), StrInput(name='tool_description', display_name='Tool Description', info='Describe the tool to LLM. Add any information that can help the LLM to use the tool.', required=True), StrInput(name='keyspace', display_name='Keyspace Name', info='The name of the keyspace within Astra where the collection is stored.', value='default_keyspace', advanced=True), StrInput(name='collection_name', display_name='Collection Name', info='The name of the collection within Astra DB where the vectors will be stored.', required=True), SecretStrInput(name='token', display_name='Astra DB Application Token', info='Authentication token for accessing Astra DB.', value='ASTRA_DB_APPLICATION_TOKEN', required=True), SecretStrInput(name='api_endpoint', display_name='Database' if os.getenv('ASTRA_ENHANCED', 'false').lower() == 'true' else 'API Endpoint', info='API endpoint URL for the Astra DB service.', value='ASTRA_DB_API_ENDPOINT', required=True), StrInput(name='projection_attributes', display_name='Projection Attributes', info='Attributes to be returned by the tool separated by comma.', required=True, value='*', advanced=True), TableInput(name='tools_params_v2', display_name='Tools Parameters', info='Define the structure for the tool parameters. Describe the parameters in a way the LLM can understand how to use them.', required=False, table_schema=[{'name': 'name', 'display_name': 'Name', 'type': 'str', 'description': 'Specify the name of the output field/parameter for the model.', 'default': 'field', 'edit_mode': EditMode.INLINE}, {'name': 'attribute_name', 'display_name': 'Attribute Name', 'type': 'str', 'description': 'Specify the attribute name to be filtered on the collection. Leave empty if the attribute name is the same as the name of the field.', 'default': '', 'edit_mode': EditMode.INLINE}, {'name': 'description', 'display_name': 'Description', 'type': 'str', 'description': 'Describe the purpose of the output field.', 'default': 'description of field', 'edit_mode': EditMode.POPOVER}, {'name': 'metadata', 'display_name': 'Is Metadata', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is included in the metadata field.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'mandatory', 'display_name': 'Is Mandatory', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is mandatory.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'is_timestamp', 'display_name': 'Is Timestamp', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is a timestamp.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'operator', 'display_name': 'Operator', 'type': 'str', 'description': 'Set the operator for the field. https://docs.datastax.com/en/astra-db-serverless/api-reference/documents.html#operators', 'default': '$eq', 'options': ['$gt', '$gte', '$lt', '$lte', '$eq', '$ne', '$in', '$nin', '$exists', '$all', '$size'], 'edit_mode': EditMode.INLINE}], value=[]), DictInput(name='tool_params', info='DEPRECATED: Attributes to filter and description to the model. Add ! for mandatory (e.g: !customerId)', display_name='Tool params', is_list=True, advanced=True), DictInput(name='static_filters', info='Attributes to filter and correspoding value', display_name='Static filters', advanced=True, is_list=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=5), BoolInput(name='use_search_query', display_name='Semantic Search', info='When this parameter is activated, the search query parameter will be used to search the collection.', advanced=False, value=False), BoolInput(name='use_vectorize', display_name='Use Astra DB Vectorize', info='When this parameter is activated, Astra DB Vectorize method will be used to generate the embeddings.', advanced=False, value=False), HandleInput(name='embedding', display_name='Embedding Model', input_types=['Embeddings']), StrInput(name='semantic_search_instruction', display_name='Semantic Search Instruction', info='The instruction to use for the semantic search.', required=True, value='Search query to find relevant documents.', advanced=True)]

Component: FirecrawlMapApi
Name: FirecrawlMapApi
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='urls', display_name='URLs', required=True, info='List of URLs to create maps from (separated by commas or new lines).', tool_mode=True), BoolInput(name='ignore_sitemap', display_name='Ignore Sitemap', info='When true, the sitemap.xml file will be ignored during crawling.'), BoolInput(name='sitemap_only', display_name='Sitemap Only', info='When true, only links found in the sitemap will be returned.'), BoolInput(name='include_subdomains', display_name='Include Subdomains', info='When true, subdomains of the provided URL will also be scanned.')]

### Code Examples
### Code Examples
```python
def document_qa_graph(template: str | None = None):
    """
    Creates a document question-answering graph.
    
    This function builds a graph that:
    1. Loads document files
    2. Parses the document content
    3. Takes user questions as input
    4. Combines document content and question in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the model's answer
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that presents the document and user question.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph
        
        # Create document QA graph with default template
        graph = document_qa_graph()
        
        # Create graph with custom template
        custom_template = "Document content:\n{Document}\n\nAnswer this: {Question}"
        custom_graph = document_qa_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = """Answer user's questions based on the document below:

---

{Document}

---

Question:
{Question}

Answer:
"""
    file_component = FileComponent()
    parse_data_component = ParseDataComponent()
    parse_data_component.set(data=file_component.load_files)

    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        context=parse_data_component.parse_data,
        question=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
    Creates a document question-answering graph.
    
    This function builds a graph that:
    1. Loads document files
    2. Parses the document content
    3. Takes user questions as input
    4. Combines document content and question in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the model's answer
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that presents the document and user question.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph
        
        # Create document QA graph with default template
        graph = document_qa_graph()
        
        # Create graph with custom template
        custom_template = "Document content:
{Document}

Answer this: {Question}"
        custom_graph = document_qa_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def ingestion_graph():
    """
    Creates a document ingestion graph for RAG applications.
    
    This function builds a graph that:
    1. Loads files using a FileComponent
    2. Splits the text into chunks with SplitTextComponent
    3. Creates embeddings using OpenAI's embedding model
    4. Stores the embeddings in an AstraDB vector store
    
    Returns:
        Graph: A Langflow Graph object with file_component as start and vector_store as end
    
    Dependencies:
        - FileComponent
        - SplitTextComponent
        - OpenAIEmbeddingsComponent
        - AstraDBVectorStoreComponent
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph
        
        # Create the ingestion graph
        graph = ingestion_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    # Ingestion Graph
    file_component = FileComponent()
    text_splitter = SplitTextComponent()
    text_splitter.set(data_inputs=file_component.load_files)
    openai_embeddings = OpenAIEmbeddingsComponent()
    vector_store = AstraDBVectorStoreComponent()
    vector_store.set(
        embedding_model=openai_embeddings.build_embeddings,
        ingest_data=text_splitter.split_text,
    )

    return Graph(file_component, vector_store)
```

Docstring:
Creates a document ingestion graph for RAG applications.

This function builds a graph that:
1. Loads files using a FileComponent
2. Splits the text into chunks with SplitTextComponent
3. Creates embeddings using OpenAI's embedding model
4. Stores the embeddings in an AstraDB vector store

Returns:
    Graph: A Langflow Graph object with file_component as start and vector_store as end

Dependencies:
    - FileComponent
    - SplitTextComponent
    - OpenAIEmbeddingsComponent
    - AstraDBVectorStoreComponent
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph
    
    # Create the ingestion graph
    graph = ingestion_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def sequential_tasks_agent_graph():
    """
    Creates a sequential task execution agent graph.
    
    This function builds a graph that:
    1. Takes a topic as input via TextInputComponent
    2. Sets up three specialized agents: Researcher, Editor, and Comedian
    3. Configures sequential tasks where each agent builds upon previous agent's work
    4. Uses SearchAPI tool for research capabilities
    5. Produces a final creative blog post about the given topic
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - TextInputComponent
        - OpenAIModelComponent
        - PromptComponent
        - SequentialTaskAgentComponent
        - SequentialCrewComponent
        - SearchAPIComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph
        
        # Create sequential tasks agent graph
        graph = sequential_tasks_agent_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    llm = OpenAIModelComponent()
    search_api_tool = SearchAPIComponent()

    text_input = TextInputComponent(_display_name="Topic")
    text_input.set(input_value="Agile")

    # Document Prompt for Researcher
    document_prompt_component = PromptComponent()
    document_prompt_component.set(
        template="""Topic: {topic}

Build a document about this topic.""",
        topic=text_input.text_response,
    )

    # Researcher Task Agent
    researcher_task_agent = SequentialTaskAgentComponent()
    researcher_task_agent.set(
        role="Researcher",
        goal="Search Google to find information to complete the task.",
        backstory="Research has always been your thing. You can quickly find things on the web because of your skills.",
        tools=[search_api_tool.build_tool],
        llm=llm.build_model,
        task_description=document_prompt_component.build_prompt,
        expected_output="Bullet points and small phrases about the research topic.",
    )

    # Revision Prompt for Editor
    revision_prompt_component = PromptComponent()
    revision_prompt_component.set(
        template="""Topic: {topic}

Revise this document.""",
        topic=text_input.text_response,
    )

    # Editor Task Agent
    editor_task_agent = SequentialTaskAgentComponent()
    editor_task_agent.set(
        role="Editor",
        goal="You should edit the information provided by the Researcher to make it more palatable and to not contain "
        "misleading information.",
        backstory="You are the editor of the most reputable journal in the world.",
        llm=llm.build_model,
        task_description=revision_prompt_component.build_prompt,
        expected_output="Small paragraphs and bullet points with the corrected content.",
        previous_task=researcher_task_agent.build_agent_and_task,
    )

    # Blog Prompt for Comedian
    blog_prompt_component = PromptComponent()
    blog_prompt_component.set(
        template="""Topic: {topic}

Build a fun blog post about this topic.""",
        topic=text_input.text_response,
    )

    # Comedian Task Agent
    comedian_task_agent = SequentialTaskAgentComponent()
    comedian_task_agent.set(
        role="Comedian",
        goal="You write comedic content based on the information provided by the editor.",
        backstory="Your formal occupation is Comedian-in-Chief. "
        "You write jokes, do standup comedy, and write funny articles.",
        llm=llm.build_model,
        task_description=blog_prompt_component.build_prompt,
        expected_output="A small blog about the topic.",
        previous_task=editor_task_agent.build_agent_and_task,
    )

    crew_component = SequentialCrewComponent()
    crew_component.set(
        tasks=comedian_task_agent.build_agent_and_task,
    )

    # Set up the output component
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    # Create the graph
    return Graph(
        start=text_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
Creates a sequential task execution agent graph.

This function builds a graph that:
1. Takes a topic as input via TextInputComponent
2. Sets up three specialized agents: Researcher, Editor, and Comedian
3. Configures sequential tasks where each agent builds upon previous agent's work
4. Uses SearchAPI tool for research capabilities
5. Produces a final creative blog post about the given topic

Returns:
    Graph: A Langflow Graph object with text_input as start and chat_output as end

Dependencies:
    - TextInputComponent
    - OpenAIModelComponent
    - PromptComponent
    - SequentialTaskAgentComponent
    - SequentialCrewComponent
    - SearchAPIComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph
    
    # Create sequential tasks agent graph
    graph = sequential_tasks_agent_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def blog_writer_graph(template: str | None = None):
    """
    Creates a blog writing graph that generates content based on reference URLs.
    
    This function builds a graph that:
    1. Fetches content from specified URLs
    2. Parses the fetched content
    3. Takes user instructions as input
    4. Combines references and instructions in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the generated blog content
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that combines references and instructions.
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - URLComponent
        - ParseDataComponent
        - TextInputComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph
        
        # Create blog writer graph with default template
        graph = blog_writer_graph()
        
        # Create graph with custom template
        custom_template = "Data: {references}\n\nWrite a blog post about: {instructions}"
        custom_graph = blog_writer_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = dedent("""Reference 1:

{references}

---

{instructions}

Blog:
""")
    url_component = URLComponent()
    url_component.set(urls=["https://langflow.org/", "https://docs.langflow.org/"])
    parse_data_component = ParseDataComponent()
    parse_data_component.set(data=url_component.fetch_content)

    text_input = TextInputComponent(_display_name="Instructions")
    text_input.set(
        input_value="Use the references above for style to write a new blog/tutorial about Langflow and AI. "
        "Suggest non-covered topics."
    )

    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        instructions=text_input.text_response,
        references=parse_data_component.parse_data,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=text_input, end=chat_output)
```

Docstring:
    Creates a blog writing graph that generates content based on reference URLs.
    
    This function builds a graph that:
    1. Fetches content from specified URLs
    2. Parses the fetched content
    3. Takes user instructions as input
    4. Combines references and instructions in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the generated blog content
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that combines references and instructions.
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - URLComponent
        - ParseDataComponent
        - TextInputComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph
        
        # Create blog writer graph with default template
        graph = blog_writer_graph()
        
        # Create graph with custom template
        custom_template = "Data: {references}

Write a blog post about: {instructions}"
        custom_graph = blog_writer_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def memory_chatbot_graph(template: str | None = None):
    """
    Creates a chatbot graph with conversation memory.
    
    This function builds a graph that:
    1. Maintains conversation history using MemoryComponent
    2. Takes user input via ChatInput
    3. Combines conversation context and current message in a prompt
    4. Sends the prompt to an OpenAI model
    5. Returns the model's response while updating conversation memory
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that includes conversation context and user message.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - MemoryComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph
        
        # Create memory chatbot with default template
        graph = memory_chatbot_graph()
        
        # Create graph with custom template
        custom_template = "Previous messages:\n{context}\n\nUser: {user_message}\nAI:"
        custom_graph = memory_chatbot_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = """{context}

    User: {user_message}
    AI: """
    memory_component = MemoryComponent()
    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template, user_message=chat_input.message_response, context=memory_component.retrieve_messages_as_text
    )
    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(chat_input, chat_output)
```

Docstring:
    Creates a chatbot graph with conversation memory.
    
    This function builds a graph that:
    1. Maintains conversation history using MemoryComponent
    2. Takes user input via ChatInput
    3. Combines conversation context and current message in a prompt
    4. Sends the prompt to an OpenAI model
    5. Returns the model's response while updating conversation memory
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that includes conversation context and user message.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - MemoryComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph
        
        # Create memory chatbot with default template
        graph = memory_chatbot_graph()
        
        # Create graph with custom template
        custom_template = "Previous messages:
{context}

User: {user_message}
AI:"
        custom_graph = memory_chatbot_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def vector_store_rag_graph():
    """
    Combines ingestion and RAG graphs into a complete vector store RAG flow.
    
    This function:
    1. Creates an ingestion graph for processing documents into a vector store
    2. Creates a RAG graph for querying the vector store and generating responses
    3. Combines them into a single operational graph
    
    Returns:
        Graph: A combined graph representing the complete RAG workflow
    
    Dependencies:
        - All dependencies from ingestion_graph() and rag_graph()
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph
        
        # Create the complete RAG workflow
        graph = vector_store_rag_graph()
        
        # Build and run the graph
        graph.build()
        # Run ingestion first with documents
        # Then run queries against the stored vectors
        ```
    """
    return ingestion_graph() + rag_graph()
```

Docstring:
Combines ingestion and RAG graphs into a complete vector store RAG flow.

This function:
1. Creates an ingestion graph for processing documents into a vector store
2. Creates a RAG graph for querying the vector store and generating responses
3. Combines them into a single operational graph

Returns:
    Graph: A combined graph representing the complete RAG workflow

Dependencies:
    - All dependencies from ingestion_graph() and rag_graph()
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph
    
    # Create the complete RAG workflow
    graph = vector_store_rag_graph()
    
    # Build and run the graph
    graph.build()
    # Run ingestion first with documents
    # Then run queries against the stored vectors
    ```

```python
def basic_prompting_graph(template: str | None = None):
    """
    Creates a basic conversation graph with prompt templating.
    
    This function builds a simple graph that:
    1. Takes user input via ChatInput
    2. Processes the input through a template prompt
    3. Sends the formatted prompt to an OpenAI model
    4. Returns the model's response as output
    
    Args:
        template (str | None, optional): Custom prompt template to use. 
            Defaults to a pirate-themed response template.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph
        
        # Create graph with default pirate template
        default_graph = basic_prompting_graph()
        
        # Create graph with custom template
        custom_template = "Answer as a helpful assistant: {user_input}"
        custom_graph = basic_prompting_graph(template=custom_template)
        
        # Build and run the graph
        default_graph.build()
        ```
    """
    if template is None:
        template = """Answer the user as if you were a pirate.

User: {user_input}

Answer:
"""
    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        user_input=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
Creates a basic conversation graph with prompt templating.

This function builds a simple graph that:
1. Takes user input via ChatInput
2. Processes the input through a template prompt
3. Sends the formatted prompt to an OpenAI model
4. Returns the model's response as output

Args:
    template (str | None, optional): Custom prompt template to use. 
        Defaults to a pirate-themed response template.

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - PromptComponent
    - OpenAIModelComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph
    
    # Create graph with default pirate template
    default_graph = basic_prompting_graph()
    
    # Create graph with custom template
    custom_template = "Answer as a helpful assistant: {user_input}"
    custom_graph = basic_prompting_graph(template=custom_template)
    
    # Build and run the graph
    default_graph.build()
    ```

```python
def rag_graph():
    """
    Creates a Retrieval Augmented Generation (RAG) query graph.
    
    This function builds a graph that:
    1. Takes user input via ChatInput
    2. Uses the input to query a vector store (AstraDB)
    3. Processes and parses retrieved documents
    4. Formats a prompt with retrieved context and user query
    5. Sends the prompt to OpenAI model
    6. Returns the model's response as output
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - OpenAIEmbeddingsComponent
        - AstraDBVectorStoreComponent
        - ParseDataComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph
        
        # Create the RAG query graph
        graph = rag_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    # RAG Graph
    openai_embeddings = OpenAIEmbeddingsComponent()
    chat_input = ChatInput()
    rag_vector_store = AstraDBVectorStoreComponent()
    rag_vector_store.set(
        search_query=chat_input.message_response,
        embedding_model=openai_embeddings.build_embeddings,
    )

    parse_data = ParseDataComponent()
    parse_data.set(data=rag_vector_store.search_documents)
    prompt_component = PromptComponent()
    prompt_component.set(
        template=dedent("""Given the following context, answer the question.
                         Context:{context}

                         Question: {question}
                         Answer:"""),
        context=parse_data.parse_data,
        question=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
Creates a Retrieval Augmented Generation (RAG) query graph.

This function builds a graph that:
1. Takes user input via ChatInput
2. Uses the input to query a vector store (AstraDB)
3. Processes and parses retrieved documents
4. Formats a prompt with retrieved context and user query
5. Sends the prompt to OpenAI model
6. Returns the model's response as output

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - OpenAIEmbeddingsComponent
    - AstraDBVectorStoreComponent
    - ParseDataComponent
    - PromptComponent
    - OpenAIModelComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph
    
    # Create the RAG query graph
    graph = rag_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def hierarchical_tasks_agent_graph():
    llm = OpenAIModelComponent(model_name="gpt-4o-mini")
    manager_llm = OpenAIModelComponent(model_name="gpt-4o")
    search_api_tool = SearchAPIComponent()
    researcher_agent = CrewAIAgentComponent()
    chat_input = ChatInput()
    researcher_agent.set(
        tools=[search_api_tool.build_tool],
        llm=llm.build_model,
        role="Researcher",
        goal="Search for information about the User's query and answer as best as you can",
        backstory="You are a reliable researcher and journalist ",
    )

    editor_agent = CrewAIAgentComponent()

    editor_agent.set(
        llm=llm.build_model,
        role="Editor",
        goal="Evaluate the information for misleading or biased data.",
        backstory="You are a reliable researcher and journalist ",
    )

    response_prompt = PromptComponent()
    response_prompt.set(
        template="""User's query:
{query}

Respond to the user with as much as information as you can about the topic. Delete if needed.
If it is just a general query (e.g a greeting) you can respond them directly.""",
        query=chat_input.message_response,
    )
    manager_agent = CrewAIAgentComponent()
    manager_agent.set(
        llm=manager_llm.build_model,
        role="Manager",
        goal="You can answer general questions from the User and may call others for help if needed.",
        backstory="You are polite and helpful. You've always been a beacon of politeness.",
    )
    task = HierarchicalTaskComponent()
    task.set(
        task_description=response_prompt.build_prompt,
        expected_output="Succinct response that answers the User's query.",
    )
    crew_component = HierarchicalCrewComponent()
    crew_component.set(
        tasks=task.build_task,
        agents=[researcher_agent.build_output, editor_agent.build_output],
        manager_agent=manager_agent.build_output,
    )
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    return Graph(
        start=chat_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
None

```python
def complex_agent_graph():
    """
    Creates a complex agent graph with dynamic role assignment and hierarchical task execution.
    
    This function builds a graph that:
    1. Takes user input via ChatInput
    2. Dynamically determines appropriate role, goal, and backstory based on user query
    3. Creates a specialized agent with search and financial tools
    4. Sets up a manager agent to coordinate responses
    5. Structures tasks in a hierarchical manner
    6. Returns comprehensive responses to user queries
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - OpenAIModelComponent
        - PromptComponent
        - CrewAIAgentComponent
        - HierarchicalTaskComponent
        - HierarchicalCrewComponent
        - SearchAPIComponent
        - YfinanceToolComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph
        
        # Create complex agent graph
        graph = complex_agent_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    llm = OpenAIModelComponent(model_name="gpt-4o-mini")
    manager_llm = OpenAIModelComponent(model_name="gpt-4o")
    search_api_tool = SearchAPIComponent()
    yahoo_search_tool = YfinanceToolComponent()
    dynamic_agent = CrewAIAgentComponent()
    chat_input = ChatInput()
    role_prompt = PromptComponent(_display_name="Role Prompt")
    role_prompt.set(
        template="""Define a Role that could execute or answer well the user's query.

User's query: {query}

Role should be two words max. Something like "Researcher" or "Software Developer".
"""
    )

    goal_prompt = PromptComponent(_display_name="Goal Prompt")
    goal_prompt.set(
        template="""Define the Goal of this Role, given the User's Query.
User's query: {query}

Role: {role}

The goal should be concise and specific.
Goal:
""",
        query=chat_input.message_response,
        role=role_prompt.build_prompt,
    )
    backstory_prompt = PromptComponent(_display_name="Backstory Prompt")
    backstory_prompt.set(
        template="""Define a Backstory of this Role and Goal, given the User's Query.
User's query: {query}

Role: {role}
Goal: {goal}

The backstory should be specific and well aligned with the rest of the information.
Backstory:""",
        query=chat_input.message_response,
        role=role_prompt.build_prompt,
        goal=goal_prompt.build_prompt,
    )
    dynamic_agent.set(
        tools=[search_api_tool.build_tool, yahoo_search_tool.build_tool],
        llm=llm.build_model,
        role=role_prompt.build_prompt,
        goal=goal_prompt.build_prompt,
        backstory=backstory_prompt.build_prompt,
    )

    response_prompt = PromptComponent()
    response_prompt.set(
        template="""User's query:
{query}

Respond to the user with as much as information as you can about the topic. Delete if needed.
If it is just a general query (e.g a greeting) you can respond them directly.""",
        query=chat_input.message_response,
    )
    manager_agent = CrewAIAgentComponent()
    manager_agent.set(
        llm=manager_llm.build_model,
        role="Manager",
        goal="You can answer general questions from the User and may call others for help if needed.",
        backstory="You are polite and helpful. You've always been a beacon of politeness.",
    )
    task = HierarchicalTaskComponent()
    task.set(
        task_description=response_prompt.build_prompt,
        expected_output="Succinct response that answers the User's query.",
    )
    crew_component = HierarchicalCrewComponent()
    crew_component.set(
        tasks=task.build_task, agents=[dynamic_agent.build_output], manager_agent=manager_agent.build_output
    )
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    return Graph(
        start=chat_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
Creates a complex agent graph with dynamic role assignment and hierarchical task execution.

This function builds a graph that:
1. Takes user input via ChatInput
2. Dynamically determines appropriate role, goal, and backstory based on user query
3. Creates a specialized agent with search and financial tools
4. Sets up a manager agent to coordinate responses
5. Structures tasks in a hierarchical manner
6. Returns comprehensive responses to user queries

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - OpenAIModelComponent
    - PromptComponent
    - CrewAIAgentComponent
    - HierarchicalTaskComponent
    - HierarchicalCrewComponent
    - SearchAPIComponent
    - YfinanceToolComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph
    
    # Create complex agent graph
    graph = complex_agent_graph()
    
    # Build and run the graph
    graph.build()
    ```

### Question
Make a flow that summarizes PDF documents using Mistral and returns key points


======================

--- MESSAGE #5: assistant ---
```python
def pdf_summary_graph():
    """
    Creates a graph to summarize PDF documents using Mistral AI and extract key points.
    
    This function builds a graph that:
    1. Loads a PDF document
    2. Converts the PDF content into text
    3. Summarizes the text using Mistral AI
    4. Extracts key points from the summary
    
    Returns:
        Graph: A Langflow Graph object with file_input as start and key_points_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - MistralAIModelComponent
        - KeyPointsExtractorComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.pdf_summary import pdf_summary_graph
        
        # Create the PDF summary graph
        graph = pdf_summary_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    file_input = FileComponent()
    file_input.set(path="path_to_pdf_document.pdf")

    parse_data = ParseDataComponent()
    parse_data.set(data=file_input.data)

    mistral_model = MistralAIModelComponent()
    mistral_model.set(
        model_name="mistral-small-latest",
        max_tokens=512,
        api_key="your_mistral_api_key",
        text=parse_data.text
    )

    summary = mistral_model.model

    key_points_extractor = KeyPointsExtractorComponent()
    key_points_extractor.set(text=summary)

    key_points_output = ChatOutput()
    key_points_output.set(input_value=key_points_extractor.key_points)

    return Graph(start=file_input, end=key_points_output)
```

======================

--- MESSAGE #6: user ---
Based on the following documentation, components, and sample code, generate a python function. Reply with code only. Do not provide any commentary:

### Documentation:
### Documentation
In this example of a document ingestion pipeline, the **OpenAI** embeddings model is connected to a vector database. The component converts the text chunks into vectors and stores them in the vector database. The vectorized data can be used to inform AI workloads like chatbots, similarity searches, and agents.
This embeddings component uses an OpenAI API key for authentication. Refer to your specific embeddings component's documentation for more information on authentication.
![URL component in a data ingestion pipeline](/img/url-component.png)

Model components receive inputs and prompts for generating text, and the generated text is sent to an output component.
The model output can also be sent to the **Language Model** port and on to a **Parse Data** component, where the output can be parsed into structured [Data](/concepts-objects) objects.
This example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/starter-projects-basic-prompting).
![](/img/starter-flow-basic-prompting.png)

This example uses the **Astra DB vector store** component. Your vector store component's parameters and authentication may be different, but the document ingestion workflow is the same. A document is loaded from a local machine and chunked. The Astra DB vector store generates embeddings with the connected [model](/components-models) component, and stores them in the connected Astra DB database.
This vector data can then be retrieved for workloads like Retrieval Augmented Generation.
![](/img/vector-store-retrieval.png)
The user's chat input is embedded and compared to the vectors embedded during document ingestion for a similarity search.
The results are output from the vector database component as a [Data](/concepts-objects) object and parsed into text.
This text fills the `{context}` variable in the **Prompt** component, which informs the **Open AI model** component's responses.
Alternatively, connect the vector database component's **Retriever** port to a [retriever tool](components-tools#retriever-tool), and then to an [agent](/components-agents) component. This enables the agent to use your vector database as a tool and make decisions based on the available data.
![](/img/vector-store-agent-retrieval-tool.png)

This flow creates a question-and-answer chatbot for documents that are loaded into the flow.
The [Unstructured.io](https://unstructured.io/) loader component loads files from your local machine, and then parses them into a list of structured [Data](/concepts-objects) objects.
This loaded data informs the **Open AI** component's responses to your questions.
![Sample Flow retrieving data with unstructured](/img/starter-flow-unstructured-qa.png)

This flow creates a summarizing "for each" loop with the [Loop](/components-logic#loop) component.
The component iterates over a list of [Data](/concepts-objects#data-object) objects until it's completed, and then the **Done** loop aggregates the results.
The **File** component loads text files from your local machine, and then the **Parse Data** component parses them into a list of structured `Data` objects.
The **Loop** component passes each `Data` object to a **Prompt** to be summarized.
When the **Loop** component runs out of `Data`, the **Done** loop activates, which counts the number of pages and summarizes their tone with another **Prompt**.
This is represented in Langflow by connecting the Parse Data component's **Data List** output to the Loop component's `Data` loop input.
![Sample Flow looping summarizer](/img/loop-text-summarizer.png)
The output will look similar to this:

The **Split Text** processing component in this flow splits the incoming [Data](/concepts-objects) into chunks to be embedded into the vector store component.
The component offers control over chunk size, overlap, and separator, which affect context and granularity in vector store retrieval results.
![A vector store ingesting documents](/img/vector-store-document-ingestion.png)

The [simple agent starter project](/starter-projects-simple-agent) uses an [agent component](#agent-component) connected to URL and Calculator tools to answer a user's questions. The OpenAI LLM acts as a brain for the agent to decide which tool to use. Tools are connected to agent components at the **Tools** port.
![Simple agent starter flow](/img/starter-flow-simple-agent.png)
For a multi-agent example, see [Create a problem-solving agent](/agents-tool-calling-agent-component).

This component creates a tool for retrieving news from Yahoo Finance.
### Inputs
This component does not have any input parameters.
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| tool | Tool | Yahoo Finance News tool for use in LangChain |

This component allows you to run any flow stored in your Langflow database without opening the flow editor.
The Run Flow component can also be used as a tool when connected to an [Agent](/components-agents). The `name` and `description` metadata that the Agent uses to register the tool are created automatically.
When you select a flow, the component fetches the flow's graph structure and uses it to generate the inputs and outputs for the Run Flow component.
To use the Run Flow component as a tool, do the following:
Add the **Run Flow** component to the [Simple Agent](/starter-projects-simple-agent) flow.
In the **Flow Name** menu, select the sub-flow you want to run.
The appearance of the **Run Flow** component changes to reflect the inputs and outputs of the selected flow.
On the **Run Flow** component, enable **Tool Mode**.
Connect the **Run Flow** component to the **Toolset** input of the Agent.
Your flow should now look like this:
![Run Flow component](/img/component-run-flow.png)
Run the flow. The Agent uses the Run Flow component as a tool to run the selected sub-flow.
### Inputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| flow_name_selected | Dropdown | The name of the flow to run. |
| flow_tweak_data | Dict | Dictionary of tweaks to customize the flow's behavior. |
| dynamic inputs | Various | Additional inputs that are generated based on the selected flow. |
### Outputs
| Name | Type | Description |
| ------ | ------ | ------------- |
| run_outputs | A `List` of types `Data`, `Message,` or `DataFrame` | All outputs are generated from running the flow. |

This component fetches content from one or more URLs, processes the content, and returns it in various formats. It supports output in plain text, raw HTML, or JSON, with options for cleaning and separating multiple outputs.
To use this component in a flow, connect the **DataFrame** output to a component that accepts the input.
For example, connect the **URL** component to a **Chat Output** component.
![URL request into a chat output component](/img/component-url.png)
In the URL component's **URLs** field, enter the URL for your request.
This example uses `langflow.org`.
Optionally, in the **Max Depth** field, enter how many pages away from the initial URL you want to crawl.
Select `1` to crawl only the page specified in the **URLs** field.
Select `2` to crawl all pages linked from that page.
The component crawls by link traversal, not by URL path depth.
Click **Playground**, and then click **Run Flow**.
The text contents of the URL are returned to the Playground as a structured DataFrame.
In the **URL** component, change the output port to **Message**, and then run the flow again.
The text contents of the URL are returned as unstructured raw text, which you can extract patterns from with the **Regex Extractor** tool.
Connect the **URL** component to a **Regex Extractor** and **Chat Output**.
![Regex extractor connected to url component](/img/component-url-regex.png)
In the **Regex Extractor** tool, enter a pattern to extract text from the **URL** component's raw output.
This example extracts the first paragraph from the "In the News" section of `https://en.wikipedia.org/wiki/Main_Page`.
Result:
### Inputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| urls | URLs | Enter one or more URLs. URLs are automatically validated and cleaned. |
| format | Output Format | Output Format. Use **Text** to extract text from the HTML, **Raw HTML** for the raw HTML content, or **JSON** to extract JSON from the HTML. |
| separator | Separator | Specify the separator to use between multiple outputs. Default for **Text** is `\n\n`. Default for **Raw HTML** is `\n<!-- Separator -->\n`. |
| clean_extra_whitespace | Clean Extra Whitespace | Whether to clean excessive blank lines in the text output. Only applies to `Text` format. |
### Outputs
| Name | Display Name | Info |
| ------ | -------------- | ------ |
| data | Data | List of [Data](/concepts-objects) objects containing fetched content and metadata. |
| text | Text | Fetched content as formatted text, with applied separators and cleaning. |
| dataframe | DataFrame | Content formatted as a [DataFrame](/concepts-objects#dataframe-object) object. |

### Component Information
### Component Information
Component: RunFlowComponent
Name: RunFlow
Inputs: RunFlowBaseComponent._base_inputs

Component: ExtractKeyFromDataComponent
Name: ExtractKeyFromData
Inputs: {}

Component: HtmlLinkExtractorComponent
Name: HtmlLinkExtractor
Inputs: [StrInput(name='kind', display_name='Kind of edge', value='hyperlink', required=False), BoolInput(name='drop_fragments', display_name='Drop URL fragments', value=True, required=False), DataInput(name='data_input', display_name='Input', info='The texts from which to extract links.', input_types=['Document', 'Data'], required=True)]

Component: AgentQL
Name: AgentQL
Inputs: [SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='Your AgentQL API key from dev.agentql.com'), MessageTextInput(name='url', display_name='URL', required=True, info='The URL of the public web page you want to extract data from.', tool_mode=True), MultilineInput(name='query', display_name='AgentQL Query', required=False, info='The AgentQL query to execute. Learn more at https://docs.agentql.com/agentql-query or use a prompt.', tool_mode=True), MultilineInput(name='prompt', display_name='Prompt', required=False, info='A Natural Language description of the data to extract from the page. Alternative to AgentQL query.', tool_mode=True), BoolInput(name='is_stealth_mode_enabled', display_name='Enable Stealth Mode (Beta)', info='Enable experimental anti-bot evasion strategies. May not work for all websites at all times.', value=False, advanced=True), IntInput(name='timeout', display_name='Timeout', info='Seconds to wait for a request.', value=900, advanced=True), DropdownInput(name='mode', display_name='Request Mode', info="'standard' uses deep data analysis, while 'fast' trades some depth of analysis for speed.", options=['fast', 'standard'], value='fast', advanced=True), IntInput(name='wait_for', display_name='Wait For', info='Seconds to wait for the page to load before extracting data.', value=0, range_spec=RangeSpec(min=0, max=10, step_type='int'), advanced=True), BoolInput(name='is_scroll_to_bottom_enabled', display_name='Enable scroll to bottom', info='Scroll to bottom of the page before extracting data.', value=False, advanced=True), BoolInput(name='is_screenshot_enabled', display_name='Enable screenshot', info="Take a screenshot before extracting data. Returned in 'metadata' as a Base64 string.", value=False, advanced=True)]

Component: YfinanceComponent
Name: None
Inputs: [MessageTextInput(name='symbol', display_name='Stock Symbol', info='The stock symbol to retrieve data for (e.g., AAPL, GOOG).', tool_mode=True), DropdownInput(name='method', display_name='Data Method', info='The type of data to retrieve.', options=list(YahooFinanceMethod), value='get_news'), IntInput(name='num_news', display_name='Number of News', info='The number of news articles to retrieve (only applicable for get_news).', value=5)]

Component: VertexAIEmbeddingsComponent
Name: VertexAIEmbeddings
Inputs: [FileInput(name='credentials', display_name='Credentials', info='JSON credentials file. Leave empty to fallback to environment variables', value='', file_types=['json'], required=True), MessageTextInput(name='location', display_name='Location', value='us-central1', advanced=True), MessageTextInput(name='project', display_name='Project', info='The project ID.', advanced=True), IntInput(name='max_output_tokens', display_name='Max Output Tokens', advanced=True), IntInput(name='max_retries', display_name='Max Retries', value=1, advanced=True), MessageTextInput(name='model_name', display_name='Model Name', value='textembedding-gecko', required=True), IntInput(name='n', display_name='N', value=1, advanced=True), IntInput(name='request_parallelism', value=5, display_name='Request Parallelism', advanced=True), MessageTextInput(name='stop_sequences', display_name='Stop', advanced=True, is_list=True), BoolInput(name='streaming', display_name='Streaming', value=False, advanced=True), FloatInput(name='temperature', value=0.0, display_name='Temperature'), IntInput(name='top_k', display_name='Top K', advanced=True), FloatInput(name='top_p', display_name='Top P', value=0.95, advanced=True)]

Component: NvidiaIngestComponent
Name: None
Inputs: [*BaseFileComponent._base_inputs, MessageTextInput(name='base_url', display_name='Base URL', info='The URL of the NVIDIA NeMo Retriever Extraction API.', required=True), SecretStrInput(name='api_key', display_name='NVIDIA API Key'), BoolInput(name='extract_text', display_name='Extract Text', info='Extract text from documents', value=True), BoolInput(name='extract_charts', display_name='Extract Charts', info='Extract text from charts', value=False), BoolInput(name='extract_tables', display_name='Extract Tables', info='Extract text from tables', value=False), BoolInput(name='extract_images', display_name='Extract Images', info='Extract images from document', value=True), DropdownInput(name='text_depth', display_name='Text Depth', info="Level at which text is extracted (applies before splitting). Support for 'block', 'line', 'span' varies by document type.", options=['document', 'page', 'block', 'line', 'span'], value='page', advanced=True), BoolInput(name='split_text', display_name='Split Text', info='Split text into smaller chunks', value=True, advanced=True), IntInput(name='chunk_size', display_name='Chunk size', info='The number of tokens per chunk', value=500, advanced=True), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='Number of tokens to overlap from previous chunk', value=150, advanced=True), BoolInput(name='filter_images', display_name='Filter Images', info='Filter images (see advanced options for filtering criteria).', advanced=True, value=True), IntInput(name='min_image_size', display_name='Minimum Image Size Filter', info='Minimum image width/length in pixels', value=128, advanced=True), FloatInput(name='min_aspect_ratio', display_name='Minimum Aspect Ratio Filter', info='Minimum allowed aspect ratio (width / height). Images narrower than this will be filtered out.', value=0.2, advanced=True), FloatInput(name='max_aspect_ratio', display_name='Maximum Aspect Ratio Filter', info='Maximum allowed aspect ratio (width / height). Images taller than this will be filtered out.', value=5.0, advanced=True), BoolInput(name='dedup_images', display_name='Deduplicate Images', info='Filter duplicated images.', advanced=True, value=True), BoolInput(name='caption_images', display_name='Caption Images', info='Generate captions for images using the NVIDIA captioning model.', advanced=True, value=True)]

Component: ChatVertexAIComponent
Name: VertexAiModel
Inputs: [*LCModelComponent._base_inputs, FileInput(name='credentials', display_name='Credentials', info='JSON credentials file. Leave empty to fallback to environment variables', file_types=['json']), MessageTextInput(name='model_name', display_name='Model Name', value='gemini-1.5-pro'), StrInput(name='project', display_name='Project', info='The project ID.', advanced=True), StrInput(name='location', display_name='Location', value='us-central1', advanced=True), IntInput(name='max_output_tokens', display_name='Max Output Tokens', advanced=True), IntInput(name='max_retries', display_name='Max Retries', value=1, advanced=True), FloatInput(name='temperature', value=0.0, display_name='Temperature'), IntInput(name='top_k', display_name='Top K', advanced=True), FloatInput(name='top_p', display_name='Top P', value=0.95, advanced=True), BoolInput(name='verbose', display_name='Verbose', value=False, advanced=True)]

Component: ListFlowsComponent
Name: ListFlows
Inputs: {}

Component: DocumentsToDataComponent
Name: DocumentsToData
Inputs: {}

### Code Examples
### Code Examples
```python
def complex_agent_graph():
    """
    Creates a complex agent graph with dynamic role assignment and hierarchical task execution.
    
    This function builds a graph that:
    1. Takes user input via ChatInput
    2. Dynamically determines appropriate role, goal, and backstory based on user query
    3. Creates a specialized agent with search and financial tools
    4. Sets up a manager agent to coordinate responses
    5. Structures tasks in a hierarchical manner
    6. Returns comprehensive responses to user queries
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - OpenAIModelComponent
        - PromptComponent
        - CrewAIAgentComponent
        - HierarchicalTaskComponent
        - HierarchicalCrewComponent
        - SearchAPIComponent
        - YfinanceToolComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph
        
        # Create complex agent graph
        graph = complex_agent_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    llm = OpenAIModelComponent(model_name="gpt-4o-mini")
    manager_llm = OpenAIModelComponent(model_name="gpt-4o")
    search_api_tool = SearchAPIComponent()
    yahoo_search_tool = YfinanceToolComponent()
    dynamic_agent = CrewAIAgentComponent()
    chat_input = ChatInput()
    role_prompt = PromptComponent(_display_name="Role Prompt")
    role_prompt.set(
        template="""Define a Role that could execute or answer well the user's query.

User's query: {query}

Role should be two words max. Something like "Researcher" or "Software Developer".
"""
    )

    goal_prompt = PromptComponent(_display_name="Goal Prompt")
    goal_prompt.set(
        template="""Define the Goal of this Role, given the User's Query.
User's query: {query}

Role: {role}

The goal should be concise and specific.
Goal:
""",
        query=chat_input.message_response,
        role=role_prompt.build_prompt,
    )
    backstory_prompt = PromptComponent(_display_name="Backstory Prompt")
    backstory_prompt.set(
        template="""Define a Backstory of this Role and Goal, given the User's Query.
User's query: {query}

Role: {role}
Goal: {goal}

The backstory should be specific and well aligned with the rest of the information.
Backstory:""",
        query=chat_input.message_response,
        role=role_prompt.build_prompt,
        goal=goal_prompt.build_prompt,
    )
    dynamic_agent.set(
        tools=[search_api_tool.build_tool, yahoo_search_tool.build_tool],
        llm=llm.build_model,
        role=role_prompt.build_prompt,
        goal=goal_prompt.build_prompt,
        backstory=backstory_prompt.build_prompt,
    )

    response_prompt = PromptComponent()
    response_prompt.set(
        template="""User's query:
{query}

Respond to the user with as much as information as you can about the topic. Delete if needed.
If it is just a general query (e.g a greeting) you can respond them directly.""",
        query=chat_input.message_response,
    )
    manager_agent = CrewAIAgentComponent()
    manager_agent.set(
        llm=manager_llm.build_model,
        role="Manager",
        goal="You can answer general questions from the User and may call others for help if needed.",
        backstory="You are polite and helpful. You've always been a beacon of politeness.",
    )
    task = HierarchicalTaskComponent()
    task.set(
        task_description=response_prompt.build_prompt,
        expected_output="Succinct response that answers the User's query.",
    )
    crew_component = HierarchicalCrewComponent()
    crew_component.set(
        tasks=task.build_task, agents=[dynamic_agent.build_output], manager_agent=manager_agent.build_output
    )
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    return Graph(
        start=chat_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
Creates a complex agent graph with dynamic role assignment and hierarchical task execution.

This function builds a graph that:
1. Takes user input via ChatInput
2. Dynamically determines appropriate role, goal, and backstory based on user query
3. Creates a specialized agent with search and financial tools
4. Sets up a manager agent to coordinate responses
5. Structures tasks in a hierarchical manner
6. Returns comprehensive responses to user queries

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - OpenAIModelComponent
    - PromptComponent
    - CrewAIAgentComponent
    - HierarchicalTaskComponent
    - HierarchicalCrewComponent
    - SearchAPIComponent
    - YfinanceToolComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.complex_agent import complex_agent_graph
    
    # Create complex agent graph
    graph = complex_agent_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def hierarchical_tasks_agent_graph():
    llm = OpenAIModelComponent(model_name="gpt-4o-mini")
    manager_llm = OpenAIModelComponent(model_name="gpt-4o")
    search_api_tool = SearchAPIComponent()
    researcher_agent = CrewAIAgentComponent()
    chat_input = ChatInput()
    researcher_agent.set(
        tools=[search_api_tool.build_tool],
        llm=llm.build_model,
        role="Researcher",
        goal="Search for information about the User's query and answer as best as you can",
        backstory="You are a reliable researcher and journalist ",
    )

    editor_agent = CrewAIAgentComponent()

    editor_agent.set(
        llm=llm.build_model,
        role="Editor",
        goal="Evaluate the information for misleading or biased data.",
        backstory="You are a reliable researcher and journalist ",
    )

    response_prompt = PromptComponent()
    response_prompt.set(
        template="""User's query:
{query}

Respond to the user with as much as information as you can about the topic. Delete if needed.
If it is just a general query (e.g a greeting) you can respond them directly.""",
        query=chat_input.message_response,
    )
    manager_agent = CrewAIAgentComponent()
    manager_agent.set(
        llm=manager_llm.build_model,
        role="Manager",
        goal="You can answer general questions from the User and may call others for help if needed.",
        backstory="You are polite and helpful. You've always been a beacon of politeness.",
    )
    task = HierarchicalTaskComponent()
    task.set(
        task_description=response_prompt.build_prompt,
        expected_output="Succinct response that answers the User's query.",
    )
    crew_component = HierarchicalCrewComponent()
    crew_component.set(
        tasks=task.build_task,
        agents=[researcher_agent.build_output, editor_agent.build_output],
        manager_agent=manager_agent.build_output,
    )
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    return Graph(
        start=chat_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
None

```python
def sequential_tasks_agent_graph():
    """
    Creates a sequential task execution agent graph.
    
    This function builds a graph that:
    1. Takes a topic as input via TextInputComponent
    2. Sets up three specialized agents: Researcher, Editor, and Comedian
    3. Configures sequential tasks where each agent builds upon previous agent's work
    4. Uses SearchAPI tool for research capabilities
    5. Produces a final creative blog post about the given topic
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - TextInputComponent
        - OpenAIModelComponent
        - PromptComponent
        - SequentialTaskAgentComponent
        - SequentialCrewComponent
        - SearchAPIComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph
        
        # Create sequential tasks agent graph
        graph = sequential_tasks_agent_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    llm = OpenAIModelComponent()
    search_api_tool = SearchAPIComponent()

    text_input = TextInputComponent(_display_name="Topic")
    text_input.set(input_value="Agile")

    # Document Prompt for Researcher
    document_prompt_component = PromptComponent()
    document_prompt_component.set(
        template="""Topic: {topic}

Build a document about this topic.""",
        topic=text_input.text_response,
    )

    # Researcher Task Agent
    researcher_task_agent = SequentialTaskAgentComponent()
    researcher_task_agent.set(
        role="Researcher",
        goal="Search Google to find information to complete the task.",
        backstory="Research has always been your thing. You can quickly find things on the web because of your skills.",
        tools=[search_api_tool.build_tool],
        llm=llm.build_model,
        task_description=document_prompt_component.build_prompt,
        expected_output="Bullet points and small phrases about the research topic.",
    )

    # Revision Prompt for Editor
    revision_prompt_component = PromptComponent()
    revision_prompt_component.set(
        template="""Topic: {topic}

Revise this document.""",
        topic=text_input.text_response,
    )

    # Editor Task Agent
    editor_task_agent = SequentialTaskAgentComponent()
    editor_task_agent.set(
        role="Editor",
        goal="You should edit the information provided by the Researcher to make it more palatable and to not contain "
        "misleading information.",
        backstory="You are the editor of the most reputable journal in the world.",
        llm=llm.build_model,
        task_description=revision_prompt_component.build_prompt,
        expected_output="Small paragraphs and bullet points with the corrected content.",
        previous_task=researcher_task_agent.build_agent_and_task,
    )

    # Blog Prompt for Comedian
    blog_prompt_component = PromptComponent()
    blog_prompt_component.set(
        template="""Topic: {topic}

Build a fun blog post about this topic.""",
        topic=text_input.text_response,
    )

    # Comedian Task Agent
    comedian_task_agent = SequentialTaskAgentComponent()
    comedian_task_agent.set(
        role="Comedian",
        goal="You write comedic content based on the information provided by the editor.",
        backstory="Your formal occupation is Comedian-in-Chief. "
        "You write jokes, do standup comedy, and write funny articles.",
        llm=llm.build_model,
        task_description=blog_prompt_component.build_prompt,
        expected_output="A small blog about the topic.",
        previous_task=editor_task_agent.build_agent_and_task,
    )

    crew_component = SequentialCrewComponent()
    crew_component.set(
        tasks=comedian_task_agent.build_agent_and_task,
    )

    # Set up the output component
    chat_output = ChatOutput()
    chat_output.set(input_value=crew_component.build_output)

    # Create the graph
    return Graph(
        start=text_input,
        end=chat_output,
        flow_name="Sequential Tasks Agent",
        description="This Agent runs tasks in a predefined sequence.",
    )
```

Docstring:
Creates a sequential task execution agent graph.

This function builds a graph that:
1. Takes a topic as input via TextInputComponent
2. Sets up three specialized agents: Researcher, Editor, and Comedian
3. Configures sequential tasks where each agent builds upon previous agent's work
4. Uses SearchAPI tool for research capabilities
5. Produces a final creative blog post about the given topic

Returns:
    Graph: A Langflow Graph object with text_input as start and chat_output as end

Dependencies:
    - TextInputComponent
    - OpenAIModelComponent
    - PromptComponent
    - SequentialTaskAgentComponent
    - SequentialCrewComponent
    - SearchAPIComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.sequential_tasks_agent import sequential_tasks_agent_graph
    
    # Create sequential tasks agent graph
    graph = sequential_tasks_agent_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def ingestion_graph():
    """
    Creates a document ingestion graph for RAG applications.
    
    This function builds a graph that:
    1. Loads files using a FileComponent
    2. Splits the text into chunks with SplitTextComponent
    3. Creates embeddings using OpenAI's embedding model
    4. Stores the embeddings in an AstraDB vector store
    
    Returns:
        Graph: A Langflow Graph object with file_component as start and vector_store as end
    
    Dependencies:
        - FileComponent
        - SplitTextComponent
        - OpenAIEmbeddingsComponent
        - AstraDBVectorStoreComponent
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph
        
        # Create the ingestion graph
        graph = ingestion_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    # Ingestion Graph
    file_component = FileComponent()
    text_splitter = SplitTextComponent()
    text_splitter.set(data_inputs=file_component.load_files)
    openai_embeddings = OpenAIEmbeddingsComponent()
    vector_store = AstraDBVectorStoreComponent()
    vector_store.set(
        embedding_model=openai_embeddings.build_embeddings,
        ingest_data=text_splitter.split_text,
    )

    return Graph(file_component, vector_store)
```

Docstring:
Creates a document ingestion graph for RAG applications.

This function builds a graph that:
1. Loads files using a FileComponent
2. Splits the text into chunks with SplitTextComponent
3. Creates embeddings using OpenAI's embedding model
4. Stores the embeddings in an AstraDB vector store

Returns:
    Graph: A Langflow Graph object with file_component as start and vector_store as end

Dependencies:
    - FileComponent
    - SplitTextComponent
    - OpenAIEmbeddingsComponent
    - AstraDBVectorStoreComponent
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import ingestion_graph
    
    # Create the ingestion graph
    graph = ingestion_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def blog_writer_graph(template: str | None = None):
    """
    Creates a blog writing graph that generates content based on reference URLs.
    
    This function builds a graph that:
    1. Fetches content from specified URLs
    2. Parses the fetched content
    3. Takes user instructions as input
    4. Combines references and instructions in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the generated blog content
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that combines references and instructions.
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - URLComponent
        - ParseDataComponent
        - TextInputComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph
        
        # Create blog writer graph with default template
        graph = blog_writer_graph()
        
        # Create graph with custom template
        custom_template = "Data: {references}\n\nWrite a blog post about: {instructions}"
        custom_graph = blog_writer_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = dedent("""Reference 1:

{references}

---

{instructions}

Blog:
""")
    url_component = URLComponent()
    url_component.set(urls=["https://langflow.org/", "https://docs.langflow.org/"])
    parse_data_component = ParseDataComponent()
    parse_data_component.set(data=url_component.fetch_content)

    text_input = TextInputComponent(_display_name="Instructions")
    text_input.set(
        input_value="Use the references above for style to write a new blog/tutorial about Langflow and AI. "
        "Suggest non-covered topics."
    )

    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        instructions=text_input.text_response,
        references=parse_data_component.parse_data,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=text_input, end=chat_output)
```

Docstring:
    Creates a blog writing graph that generates content based on reference URLs.
    
    This function builds a graph that:
    1. Fetches content from specified URLs
    2. Parses the fetched content
    3. Takes user instructions as input
    4. Combines references and instructions in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the generated blog content
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that combines references and instructions.
    
    Returns:
        Graph: A Langflow Graph object with text_input as start and chat_output as end
    
    Dependencies:
        - URLComponent
        - ParseDataComponent
        - TextInputComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.blog_writer import blog_writer_graph
        
        # Create blog writer graph with default template
        graph = blog_writer_graph()
        
        # Create graph with custom template
        custom_template = "Data: {references}

Write a blog post about: {instructions}"
        custom_graph = blog_writer_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def rag_graph():
    """
    Creates a Retrieval Augmented Generation (RAG) query graph.
    
    This function builds a graph that:
    1. Takes user input via ChatInput
    2. Uses the input to query a vector store (AstraDB)
    3. Processes and parses retrieved documents
    4. Formats a prompt with retrieved context and user query
    5. Sends the prompt to OpenAI model
    6. Returns the model's response as output
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - OpenAIEmbeddingsComponent
        - AstraDBVectorStoreComponent
        - ParseDataComponent
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph
        
        # Create the RAG query graph
        graph = rag_graph()
        
        # Build and run the graph
        graph.build()
        ```
    """
    # RAG Graph
    openai_embeddings = OpenAIEmbeddingsComponent()
    chat_input = ChatInput()
    rag_vector_store = AstraDBVectorStoreComponent()
    rag_vector_store.set(
        search_query=chat_input.message_response,
        embedding_model=openai_embeddings.build_embeddings,
    )

    parse_data = ParseDataComponent()
    parse_data.set(data=rag_vector_store.search_documents)
    prompt_component = PromptComponent()
    prompt_component.set(
        template=dedent("""Given the following context, answer the question.
                         Context:{context}

                         Question: {question}
                         Answer:"""),
        context=parse_data.parse_data,
        question=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
Creates a Retrieval Augmented Generation (RAG) query graph.

This function builds a graph that:
1. Takes user input via ChatInput
2. Uses the input to query a vector store (AstraDB)
3. Processes and parses retrieved documents
4. Formats a prompt with retrieved context and user query
5. Sends the prompt to OpenAI model
6. Returns the model's response as output

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - OpenAIEmbeddingsComponent
    - AstraDBVectorStoreComponent
    - ParseDataComponent
    - PromptComponent
    - OpenAIModelComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import rag_graph
    
    # Create the RAG query graph
    graph = rag_graph()
    
    # Build and run the graph
    graph.build()
    ```

```python
def document_qa_graph(template: str | None = None):
    """
    Creates a document question-answering graph.
    
    This function builds a graph that:
    1. Loads document files
    2. Parses the document content
    3. Takes user questions as input
    4. Combines document content and question in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the model's answer
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that presents the document and user question.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph
        
        # Create document QA graph with default template
        graph = document_qa_graph()
        
        # Create graph with custom template
        custom_template = "Document content:\n{Document}\n\nAnswer this: {Question}"
        custom_graph = document_qa_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = """Answer user's questions based on the document below:

---

{Document}

---

Question:
{Question}

Answer:
"""
    file_component = FileComponent()
    parse_data_component = ParseDataComponent()
    parse_data_component.set(data=file_component.load_files)

    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        context=parse_data_component.parse_data,
        question=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
    Creates a document question-answering graph.
    
    This function builds a graph that:
    1. Loads document files
    2. Parses the document content
    3. Takes user questions as input
    4. Combines document content and question in a prompt template
    5. Sends the prompt to an OpenAI model
    6. Returns the model's answer
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that presents the document and user question.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - FileComponent
        - ParseDataComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.document_qa import document_qa_graph
        
        # Create document QA graph with default template
        graph = document_qa_graph()
        
        # Create graph with custom template
        custom_template = "Document content:
{Document}

Answer this: {Question}"
        custom_graph = document_qa_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

```python
def vector_store_rag_graph():
    """
    Combines ingestion and RAG graphs into a complete vector store RAG flow.
    
    This function:
    1. Creates an ingestion graph for processing documents into a vector store
    2. Creates a RAG graph for querying the vector store and generating responses
    3. Combines them into a single operational graph
    
    Returns:
        Graph: A combined graph representing the complete RAG workflow
    
    Dependencies:
        - All dependencies from ingestion_graph() and rag_graph()
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph
        
        # Create the complete RAG workflow
        graph = vector_store_rag_graph()
        
        # Build and run the graph
        graph.build()
        # Run ingestion first with documents
        # Then run queries against the stored vectors
        ```
    """
    return ingestion_graph() + rag_graph()
```

Docstring:
Combines ingestion and RAG graphs into a complete vector store RAG flow.

This function:
1. Creates an ingestion graph for processing documents into a vector store
2. Creates a RAG graph for querying the vector store and generating responses
3. Combines them into a single operational graph

Returns:
    Graph: A combined graph representing the complete RAG workflow

Dependencies:
    - All dependencies from ingestion_graph() and rag_graph()
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.vector_store_rag import vector_store_rag_graph
    
    # Create the complete RAG workflow
    graph = vector_store_rag_graph()
    
    # Build and run the graph
    graph.build()
    # Run ingestion first with documents
    # Then run queries against the stored vectors
    ```

```python
def basic_prompting_graph(template: str | None = None):
    """
    Creates a basic conversation graph with prompt templating.
    
    This function builds a simple graph that:
    1. Takes user input via ChatInput
    2. Processes the input through a template prompt
    3. Sends the formatted prompt to an OpenAI model
    4. Returns the model's response as output
    
    Args:
        template (str | None, optional): Custom prompt template to use. 
            Defaults to a pirate-themed response template.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph
        
        # Create graph with default pirate template
        default_graph = basic_prompting_graph()
        
        # Create graph with custom template
        custom_template = "Answer as a helpful assistant: {user_input}"
        custom_graph = basic_prompting_graph(template=custom_template)
        
        # Build and run the graph
        default_graph.build()
        ```
    """
    if template is None:
        template = """Answer the user as if you were a pirate.

User: {user_input}

Answer:
"""
    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template,
        user_input=chat_input.message_response,
    )

    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(start=chat_input, end=chat_output)
```

Docstring:
Creates a basic conversation graph with prompt templating.

This function builds a simple graph that:
1. Takes user input via ChatInput
2. Processes the input through a template prompt
3. Sends the formatted prompt to an OpenAI model
4. Returns the model's response as output

Args:
    template (str | None, optional): Custom prompt template to use. 
        Defaults to a pirate-themed response template.

Returns:
    Graph: A Langflow Graph object with chat_input as start and chat_output as end

Dependencies:
    - ChatInput
    - PromptComponent
    - OpenAIModelComponent
    - ChatOutput
    
Example:
    ```python
    from langflow.initial_setup.starter_projects.basic_prompting import basic_prompting_graph
    
    # Create graph with default pirate template
    default_graph = basic_prompting_graph()
    
    # Create graph with custom template
    custom_template = "Answer as a helpful assistant: {user_input}"
    custom_graph = basic_prompting_graph(template=custom_template)
    
    # Build and run the graph
    default_graph.build()
    ```

```python
def memory_chatbot_graph(template: str | None = None):
    """
    Creates a chatbot graph with conversation memory.
    
    This function builds a graph that:
    1. Maintains conversation history using MemoryComponent
    2. Takes user input via ChatInput
    3. Combines conversation context and current message in a prompt
    4. Sends the prompt to an OpenAI model
    5. Returns the model's response while updating conversation memory
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that includes conversation context and user message.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - MemoryComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph
        
        # Create memory chatbot with default template
        graph = memory_chatbot_graph()
        
        # Create graph with custom template
        custom_template = "Previous messages:\n{context}\n\nUser: {user_message}\nAI:"
        custom_graph = memory_chatbot_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    """
    if template is None:
        template = """{context}

    User: {user_message}
    AI: """
    memory_component = MemoryComponent()
    chat_input = ChatInput()
    prompt_component = PromptComponent()
    prompt_component.set(
        template=template, user_message=chat_input.message_response, context=memory_component.retrieve_messages_as_text
    )
    openai_component = OpenAIModelComponent()
    openai_component.set(input_value=prompt_component.build_prompt)

    chat_output = ChatOutput()
    chat_output.set(input_value=openai_component.text_response)

    return Graph(chat_input, chat_output)
```

Docstring:
    Creates a chatbot graph with conversation memory.
    
    This function builds a graph that:
    1. Maintains conversation history using MemoryComponent
    2. Takes user input via ChatInput
    3. Combines conversation context and current message in a prompt
    4. Sends the prompt to an OpenAI model
    5. Returns the model's response while updating conversation memory
    
    Args:
        template (str | None, optional): Custom prompt template to use.
            Defaults to a template that includes conversation context and user message.
    
    Returns:
        Graph: A Langflow Graph object with chat_input as start and chat_output as end
    
    Dependencies:
        - MemoryComponent
        - ChatInput
        - PromptComponent
        - OpenAIModelComponent
        - ChatOutput
        
    Example:
        ```python
        from langflow.initial_setup.starter_projects.memory_chatbot import memory_chatbot_graph
        
        # Create memory chatbot with default template
        graph = memory_chatbot_graph()
        
        # Create graph with custom template
        custom_template = "Previous messages:
{context}

User: {user_message}
AI:"
        custom_graph = memory_chatbot_graph(template=custom_template)
        
        # Build and run the graph
        graph.build()
        ```
    

### Question
Make a flow that can extract named entities from news articles and visualize relationships


======================
