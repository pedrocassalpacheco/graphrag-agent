{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/crewai/sequential_task.py", "section": "class::SequentialTaskComponent", "content": "from langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, HandleInput, MultilineInput, Output\n\nclass SequentialTaskComponent(Component):\n    display_name: str = \"Sequential Task\"\n    description: str = \"Each task must have a description, an expected output and an agent responsible for execution.\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        MultilineInput(name='task_description',\n        display_name='Description',\n        info=\"Descriptive text detailing task's purpose and execution.\"),\n        MultilineInput(name='expected_output',\n        display_name='Expected Output',\n        info='Clear definition of expected task outcome.'),\n        HandleInput(name='tools',\n        display_name='Tools',\n        input_types=['Tool'],\n        is_list=True,\n        info='List of tools/resources limited for task execution. Uses the Agent tools by default.',\n        required=False,\n        advanced=True),\n        HandleInput(name='agent',\n        display_name='Agent',\n        input_types=['Agent'],\n        info='CrewAI Agent that will perform the task',\n        required=True),\n        HandleInput(name='task',\n        display_name='Task',\n        input_types=['SequentialTask'],\n        info='CrewAI Task that will perform the task'),\n        BoolInput(name='async_execution',\n        display_name='Async Execution',\n        value=True,\n        advanced=True,\n        info='Boolean flag indicating asynchronous task execution.')\n    ]\n\n    outputs = [\n        Output(display_name='Task',\n        name='task_output',\n        method='build_task')\n    ]\n\n    def build_task(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SequentialTaskComponent", "base_classes": ["Component"], "public_methods": ["def build_task(self)"], "imports": ["from langflow.base.agents.crewai.tasks import SequentialTask", "from langflow.custom import Component", "from langflow.io import BoolInput, HandleInput, MultilineInput, Output"], "inputs": "[MultilineInput(name='task_description', display_name='Description', info=\"Descriptive text detailing task's purpose and execution.\"), MultilineInput(name='expected_output', display_name='Expected Output', info='Clear definition of expected task outcome.'), HandleInput(name='tools', display_name='Tools', input_types=['Tool'], is_list=True, info='List of tools/resources limited for task execution. Uses the Agent tools by default.', required=False, advanced=True), HandleInput(name='agent', display_name='Agent', input_types=['Agent'], info='CrewAI Agent that will perform the task', required=True), HandleInput(name='task', display_name='Task', input_types=['SequentialTask'], info='CrewAI Task that will perform the task'), BoolInput(name='async_execution', display_name='Async Execution', value=True, advanced=True, info='Boolean flag indicating asynchronous task execution.')]", "outputs": "[Output(display_name='Task', name='task_output', method='build_task')]", "display_name": "Sequential Task", "name": "", "description": "Each task must have a description, an expected output and an agent responsible for execution.", "icon": "CrewAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/cohere/cohere_rerank.py", "section": "class::CohereRerankComponent", "content": "from langflow.base.compressors.model import LCCompressorComponent\nfrom langflow.field_typing import BaseDocumentCompressor\nfrom langflow.inputs.inputs import SecretStrInput\nfrom langflow.io import DropdownInput\nfrom langflow.template.field.base import Output\nfrom langchain_cohere import CohereRerank\n\nclass CohereRerankComponent(LCCompressorComponent):\n    display_name: str = \"Cohere Rerank\"\n    description: str = \"Rerank documents using the Cohere API.\"\n    icon = \"Cohere\"\n    name = \"CohereRerank\"\n\n    inputs = [\n        *LCCompressorComponent.inputs,\n        SecretStrInput(name='api_key',\n        display_name='Cohere API Key'),\n        DropdownInput(name='model',\n        display_name='Model',\n        options=['rerank-english-v3.0',\n        'rerank-multilingual-v3.0',\n        'rerank-english-v2.0',\n        'rerank-multilingual-v2.0'],\n        value='rerank-english-v3.0')\n    ]\n\n    outputs = [\n        Output(display_name='Reranked Documents',\n        name='reranked_documents',\n        method='compress_documents')\n    ]\n\n    def build_compressor(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CohereRerankComponent", "base_classes": ["LCCompressorComponent"], "public_methods": ["def build_compressor(self)"], "imports": ["from langflow.base.compressors.model import LCCompressorComponent", "from langflow.field_typing import BaseDocumentCompressor", "from langflow.inputs.inputs import SecretStrInput", "from langflow.io import DropdownInput", "from langflow.template.field.base import Output", "from langchain_cohere import CohereRerank"], "inputs": "[*LCCompressorComponent.inputs, SecretStrInput(name='api_key', display_name='Cohere API Key'), DropdownInput(name='model', display_name='Model', options=['rerank-english-v3.0', 'rerank-multilingual-v3.0', 'rerank-english-v2.0', 'rerank-multilingual-v2.0'], value='rerank-english-v3.0')]", "outputs": "[Output(display_name='Reranked Documents', name='reranked_documents', method='compress_documents')]", "display_name": "Cohere Rerank", "name": "CohereRerank", "description": "Rerank documents using the Cohere API.", "icon": "Cohere"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/retrievers/amazon_kendra.py", "section": "class::AmazonKendraRetrieverComponent", "content": "from typing import cast\nfrom langchain_community.retrievers import AmazonKendraRetriever\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Retriever\n\nclass AmazonKendraRetrieverComponent(CustomComponent):\n    display_name: str = \"Amazon Kendra Retriever\"\n    description: str = \"Retriever that uses the Amazon Kendra API.\"\n    icon = \"Amazon\"\n    name = \"AmazonKendra\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, index_id, top_k, region_name, credentials_profile_name, attribute_filter, user_context):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AmazonKendraRetrieverComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, index_id, top_k, region_name, credentials_profile_name, attribute_filter, user_context)"], "imports": ["from typing import cast", "from langchain_community.retrievers import AmazonKendraRetriever", "from langflow.custom import CustomComponent", "from langflow.field_typing import Retriever"], "inputs": "", "outputs": "", "display_name": "Amazon Kendra Retriever", "name": "AmazonKendra", "description": "Retriever that uses the Amazon Kendra API.", "icon": "Amazon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/glean_search_api.py", "section": "class::GleanSearchAPISchema", "content": "import json\nfrom typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom langchain_core.tools import StructuredTool, ToolException\nfrom pydantic import BaseModel\nfrom pydantic.v1 import Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import IntInput, MultilineInput, NestedDictInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\n\nclass GleanSearchAPISchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "GleanSearchAPISchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "from urllib.parse import urljoin", "import httpx", "from langchain_core.tools import StructuredTool, ToolException", "from pydantic import BaseModel", "from pydantic.v1 import Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import IntInput, MultilineInput, NestedDictInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/glean_search_api.py", "section": "class::GleanAPIWrapper", "content": "import json\nfrom typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom langchain_core.tools import StructuredTool, ToolException\nfrom pydantic import BaseModel\nfrom pydantic.v1 import Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import IntInput, MultilineInput, NestedDictInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\n\nclass GleanAPIWrapper(BaseModel):\n    \"\"\"\n    Wrapper around Glean API.\n    \"\"\"\n\n\n    def results(self, query):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run(self, query):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GleanAPIWrapper", "base_classes": ["BaseModel"], "public_methods": ["def results(self, query)", "def run(self, query)"], "imports": ["import json", "from typing import Any", "from urllib.parse import urljoin", "import httpx", "from langchain_core.tools import StructuredTool, ToolException", "from pydantic import BaseModel", "from pydantic.v1 import Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import IntInput, MultilineInput, NestedDictInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/glean_search_api.py", "section": "class::GleanSearchAPIComponent", "content": "import json\nfrom typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom langchain_core.tools import StructuredTool, ToolException\nfrom pydantic import BaseModel\nfrom pydantic.v1 import Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import IntInput, MultilineInput, NestedDictInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\n\nclass GleanSearchAPIComponent(LCToolComponent):\n    display_name: str = \"Glean Search API\"\n    description: str = \"Search using Glean's API.\"\n    icon = \"Glean\"\n\n    inputs = [\n        StrInput(name='glean_api_url',\n        display_name='Glean API URL',\n        required=True),\n        SecretStrInput(name='glean_access_token',\n        display_name='Glean Access Token',\n        required=True),\n        MultilineInput(name='query',\n        display_name='Query',\n        required=True,\n        tool_mode=True),\n        IntInput(name='page_size',\n        display_name='Page Size',\n        value=10),\n        NestedDictInput(name='request_options',\n        display_name='Request Options',\n        required=False)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='run_model'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GleanSearchAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def build_tool(self)", "def run_model(self)", "def as_dataframe(self)"], "imports": ["import json", "from typing import Any", "from urllib.parse import urljoin", "import httpx", "from langchain_core.tools import StructuredTool, ToolException", "from pydantic import BaseModel", "from pydantic.v1 import Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import IntInput, MultilineInput, NestedDictInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame"], "inputs": "[StrInput(name='glean_api_url', display_name='Glean API URL', required=True), SecretStrInput(name='glean_access_token', display_name='Glean Access Token', required=True), MultilineInput(name='query', display_name='Query', required=True, tool_mode=True), IntInput(name='page_size', display_name='Page Size', value=10), NestedDictInput(name='request_options', display_name='Request Options', required=False)]", "outputs": "[Output(display_name='Data', name='data', method='run_model'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Glean Search API", "name": "", "description": "Search using Glean's API.", "icon": "Glean"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/needle/needle.py", "section": "class::NeedleComponent", "content": "from langchain_community.retrievers.needle import NeedleRetriever\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.io import IntInput, MessageTextInput, Output, SecretStrInput\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI\n\nclass NeedleComponent(Component):\n    display_name: str = \"Needle Retriever\"\n    description: str = \"A retriever that uses the Needle API to search collections.\"\n    icon = \"Needle\"\n    name = \"needle\"\n\n    inputs = [\n        SecretStrInput(name='needle_api_key',\n        display_name='Needle API Key',\n        info='Your Needle API key.',\n        required=True),\n        MessageTextInput(name='collection_id',\n        display_name='Collection ID',\n        info='The ID of the Needle collection.',\n        required=True),\n        MessageTextInput(name='query',\n        display_name='User Query',\n        info='Enter your question here. In tool mode,\n        you can also specify top_k parameter (min: 20).',\n        required=True,\n        tool_mode=True),\n        IntInput(name='top_k',\n        display_name='Top K Results',\n        info='Number of search results to return (min: 20).',\n        value=20,\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Result',\n        name='result',\n        type_='Message',\n        method='run')\n    ]\n\n    def run(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NeedleComponent", "base_classes": ["Component"], "public_methods": ["def run(self)"], "imports": ["from langchain_community.retrievers.needle import NeedleRetriever", "from langflow.custom.custom_component.component import Component", "from langflow.io import IntInput, MessageTextInput, Output, SecretStrInput", "from langflow.schema.message import Message", "from langflow.utils.constants import MESSAGE_SENDER_AI"], "inputs": "[SecretStrInput(name='needle_api_key', display_name='Needle API Key', info='Your Needle API key.', required=True), MessageTextInput(name='collection_id', display_name='Collection ID', info='The ID of the Needle collection.', required=True), MessageTextInput(name='query', display_name='User Query', info='Enter your question here. In tool mode, you can also specify top_k parameter (min: 20).', required=True, tool_mode=True), IntInput(name='top_k', display_name='Top K Results', info='Number of search results to return (min: 20).', value=20, required=True)]", "outputs": "[Output(display_name='Result', name='result', type_='Message', method='run')]", "display_name": "Needle Retriever", "name": "needle", "description": "A retriever that uses the Needle API to search collections.", "icon": "Needle"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/vertexai.py", "section": "class::VertexAIEmbeddingsComponent", "content": "from langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, FileInput, FloatInput, IntInput, MessageTextInput, Output\nfrom google.oauth2 import service_account\nfrom langchain_google_vertexai import VertexAIEmbeddings\n\nclass VertexAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"VertexAI Embeddings\"\n    description: str = \"Generate embeddings using Google Cloud VertexAI models.\"\n    icon = \"VertexAI\"\n    name = \"VertexAIEmbeddings\"\n\n    inputs = [\n        FileInput(name='credentials',\n        display_name='Credentials',\n        info='JSON credentials file. Leave empty to fallback to environment variables',\n        value='',\n        file_types=['json'],\n        required=True),\n        MessageTextInput(name='location',\n        display_name='Location',\n        value='us-central1',\n        advanced=True),\n        MessageTextInput(name='project',\n        display_name='Project',\n        info='The project ID.',\n        advanced=True),\n        IntInput(name='max_output_tokens',\n        display_name='Max Output Tokens',\n        advanced=True),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        value=1,\n        advanced=True),\n        MessageTextInput(name='model_name',\n        display_name='Model Name',\n        value='textembedding-gecko',\n        required=True),\n        IntInput(name='n',\n        display_name='N',\n        value=1,\n        advanced=True),\n        IntInput(name='request_parallelism',\n        value=5,\n        display_name='Request Parallelism',\n        advanced=True),\n        MessageTextInput(name='stop_sequences',\n        display_name='Stop',\n        advanced=True,\n        is_list=True),\n        BoolInput(name='streaming',\n        display_name='Streaming',\n        value=False,\n        advanced=True),\n        FloatInput(name='temperature',\n        value=0.0,\n        display_name='Temperature'),\n        IntInput(name='top_k',\n        display_name='Top K',\n        advanced=True),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        value=0.95,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VertexAIEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import Embeddings", "from langflow.io import BoolInput, FileInput, FloatInput, IntInput, MessageTextInput, Output", "from google.oauth2 import service_account", "from langchain_google_vertexai import VertexAIEmbeddings"], "inputs": "[FileInput(name='credentials', display_name='Credentials', info='JSON credentials file. Leave empty to fallback to environment variables', value='', file_types=['json'], required=True), MessageTextInput(name='location', display_name='Location', value='us-central1', advanced=True), MessageTextInput(name='project', display_name='Project', info='The project ID.', advanced=True), IntInput(name='max_output_tokens', display_name='Max Output Tokens', advanced=True), IntInput(name='max_retries', display_name='Max Retries', value=1, advanced=True), MessageTextInput(name='model_name', display_name='Model Name', value='textembedding-gecko', required=True), IntInput(name='n', display_name='N', value=1, advanced=True), IntInput(name='request_parallelism', value=5, display_name='Request Parallelism', advanced=True), MessageTextInput(name='stop_sequences', display_name='Stop', advanced=True, is_list=True), BoolInput(name='streaming', display_name='Streaming', value=False, advanced=True), FloatInput(name='temperature', value=0.0, display_name='Temperature'), IntInput(name='top_k', display_name='Top K', advanced=True), FloatInput(name='top_p', display_name='Top P', value=0.95, advanced=True)]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "VertexAI Embeddings", "name": "VertexAIEmbeddings", "description": "Generate embeddings using Google Cloud VertexAI models.", "icon": "VertexAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/text_embeddings.py", "section": "class::TwelveLabsTextEmbeddings", "content": "from twelvelabs import TwelveLabs\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput\n\nclass TwelveLabsTextEmbeddings(Embeddings):\n\n    def embed_documents(self, texts):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def embed_query(self, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TwelveLabsTextEmbeddings", "base_classes": ["Embeddings"], "public_methods": ["def embed_documents(self, texts)", "def embed_query(self, text)"], "imports": ["from twelvelabs import TwelveLabs", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/text_embeddings.py", "section": "class::TwelveLabsTextEmbeddingsComponent", "content": "from twelvelabs import TwelveLabs\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput\n\nclass TwelveLabsTextEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"Twelve Labs Text Embeddings\"\n    description: str = \"Generate embeddings using Twelve Labs text embedding models.\"\n    icon = \"TwelveLabs\"\n    name = \"TwelveLabsTextEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Twelve Labs API Key',\n        value='TWELVELABS_API_KEY',\n        required=True),\n        DropdownInput(name='model',\n        display_name='Model',\n        advanced=False,\n        options=['Marengo-retrieval-2.7'],\n        value='Marengo-retrieval-2.7'),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        value=3,\n        advanced=True),\n        FloatInput(name='request_timeout',\n        display_name='Request Timeout',\n        advanced=True)\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TwelveLabsTextEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)"], "imports": ["from twelvelabs import TwelveLabs", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput"], "inputs": "[SecretStrInput(name='api_key', display_name='Twelve Labs API Key', value='TWELVELABS_API_KEY', required=True), DropdownInput(name='model', display_name='Model', advanced=False, options=['Marengo-retrieval-2.7'], value='Marengo-retrieval-2.7'), IntInput(name='max_retries', display_name='Max Retries', value=3, advanced=True), FloatInput(name='request_timeout', display_name='Request Timeout', advanced=True)]", "outputs": "", "display_name": "Twelve Labs Text Embeddings", "name": "TwelveLabsTextEmbeddings", "description": "Generate embeddings using Twelve Labs text embedding models.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/homeassistant/list_home_assistant_states.py", "section": "class::ListHomeAssistantStates", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ListHomeAssistantStates(LCToolComponent):\n    display_name: str = \"List HomeAssistant States\"\n    description: str = \"Retrieve states from Home Assistant. The agent only needs to specify 'filter_domain' (optional). Token and base_url are not exposed to the agent.\"\n    icon = \"HomeAssistant\"\n\n    inputs = [\n        SecretStrInput(name='ha_token',\n        display_name='Home Assistant Token',\n        info='Home Assistant Long-Lived Access Token',\n        required=True),\n        StrInput(name='base_url',\n        display_name='Home Assistant URL',\n        info='e.g.,\n        http://192.168.0.10:8123',\n        required=True),\n        StrInput(name='filter_domain',\n        display_name='Default Filter Domain (Optional)',\n        info='light,\n        switch,\n        sensor,\n        etc. (Leave empty to fetch all)',\n        required=False)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ListHomeAssistantStates", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='ha_token', display_name='Home Assistant Token', info='Home Assistant Long-Lived Access Token', required=True), StrInput(name='base_url', display_name='Home Assistant URL', info='e.g., http://192.168.0.10:8123', required=True), StrInput(name='filter_domain', display_name='Default Filter Domain (Optional)', info='light, switch, sensor, etc. (Leave empty to fetch all)', required=False)]", "outputs": "", "display_name": "List HomeAssistant States", "name": "", "description": "Retrieve states from Home Assistant. The agent only needs to specify 'filter_domain' (optional). Token and base_url are not exposed to the agent.", "icon": "HomeAssistant"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/homeassistant/list_home_assistant_states.py", "section": "class::ToolSchema", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ToolSchema(BaseModel):\n    \"\"\"\n    Parameters to be passed by the agent: filter_domain only.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "ToolSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/language_recursive.py", "section": "class::LanguageRecursiveTextSplitterComponent", "content": "from typing import Any\nfrom langchain_text_splitters import Language, RecursiveCharacterTextSplitter, TextSplitter\nfrom langflow.base.textsplitters.model import LCTextSplitterComponent\nfrom langflow.inputs import DataInput, DropdownInput, IntInput\n\nclass LanguageRecursiveTextSplitterComponent(LCTextSplitterComponent):\n    display_name: str = \"Language Recursive Text Splitter\"\n    description: str = \"Split text into chunks of a specified length based on language.\"\n    icon = \"LangChain\"\n    name = \"LanguageRecursiveTextSplitter\"\n\n    inputs = [\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        info='The maximum length of each chunk.',\n        value=1000),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='The amount of overlap between chunks.',\n        value=200),\n        DataInput(name='data_input',\n        display_name='Input',\n        info='The texts to split.',\n        input_types=['Document',\n        'Data'],\n        required=True),\n        DropdownInput(name='code_language',\n        display_name='Code Language',\n        options=[x.value for x in Language],\n        value='python')\n    ]\n\n    def get_data_input(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_text_splitter(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LanguageRecursiveTextSplitterComponent", "base_classes": ["LCTextSplitterComponent"], "public_methods": ["def get_data_input(self)", "def build_text_splitter(self)"], "imports": ["from typing import Any", "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter, TextSplitter", "from langflow.base.textsplitters.model import LCTextSplitterComponent", "from langflow.inputs import DataInput, DropdownInput, IntInput"], "inputs": "[IntInput(name='chunk_size', display_name='Chunk Size', info='The maximum length of each chunk.', value=1000), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='The amount of overlap between chunks.', value=200), DataInput(name='data_input', display_name='Input', info='The texts to split.', input_types=['Document', 'Data'], required=True), DropdownInput(name='code_language', display_name='Code Language', options=[x.value for x in Language], value='python')]", "outputs": "", "display_name": "Language Recursive Text Splitter", "name": "LanguageRecursiveTextSplitter", "description": "Split text into chunks of a specified length based on language.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/save_to_file.py", "section": "class::SaveToFileComponent", "content": "import json\nfrom collections.abc import AsyncIterator, Iterator\nfrom pathlib import Path\nimport pandas as pd\nfrom langflow.custom import Component\nfrom langflow.io import DataFrameInput, DataInput, DropdownInput, MessageInput, Output, StrInput\nfrom langflow.schema import Data, DataFrame, Message\n\nclass SaveToFileComponent(Component):\n    display_name: str = \"Save to File\"\n    description: str = \"Save DataFrames, Data, or Messages to various file formats.\"\n    icon = \"save\"\n    name = \"SaveToFile\"\n\n    inputs = [\n        DropdownInput(name='input_type',\n        display_name='Input Type',\n        options=['DataFrame',\n        'Data',\n        'Message'],\n        info='Select the type of input to save.',\n        value='DataFrame',\n        real_time_refresh=True),\n        DataFrameInput(name='df',\n        display_name='DataFrame',\n        info='The DataFrame to save.',\n        dynamic=True,\n        show=True),\n        DataInput(name='data',\n        display_name='Data',\n        info='The Data object to save.',\n        dynamic=True,\n        show=False),\n        MessageInput(name='message',\n        display_name='Message',\n        info='The Message to save.',\n        dynamic=True,\n        show=False),\n        DropdownInput(name='file_format',\n        display_name='File Format',\n        options=DATA_FORMAT_CHOICES,\n        info='Select the file format to save the input.',\n        real_time_refresh=True),\n        StrInput(name='file_path',\n        display_name='File Path (including filename)',\n        info='The full file path (including filename and extension).',\n        value='./output')\n    ]\n\n    outputs = [\n        Output(name='confirmation',\n        display_name='Confirmation',\n        method='save_to_file',\n        info='Confirmation message after saving the file.')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def save_to_file(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SaveToFileComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def save_to_file(self)"], "imports": ["import json", "from collections.abc import AsyncIterator, Iterator", "from pathlib import Path", "import pandas as pd", "from langflow.custom import Component", "from langflow.io import DataFrameInput, DataInput, DropdownInput, MessageInput, Output, StrInput", "from langflow.schema import Data, DataFrame, Message"], "inputs": "[DropdownInput(name='input_type', display_name='Input Type', options=['DataFrame', 'Data', 'Message'], info='Select the type of input to save.', value='DataFrame', real_time_refresh=True), DataFrameInput(name='df', display_name='DataFrame', info='The DataFrame to save.', dynamic=True, show=True), DataInput(name='data', display_name='Data', info='The Data object to save.', dynamic=True, show=False), MessageInput(name='message', display_name='Message', info='The Message to save.', dynamic=True, show=False), DropdownInput(name='file_format', display_name='File Format', options=DATA_FORMAT_CHOICES, info='Select the file format to save the input.', real_time_refresh=True), StrInput(name='file_path', display_name='File Path (including filename)', info='The full file path (including filename and extension).', value='./output')]", "outputs": "[Output(name='confirmation', display_name='Confirmation', method='save_to_file', info='Confirmation message after saving the file.')]", "display_name": "Save to File", "name": "SaveToFile", "description": "Save DataFrames, Data, or Messages to various file formats.", "icon": "save"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/firecrawl/firecrawl_map_api.py", "section": "class::FirecrawlMapApi", "content": "from langflow.custom import Component\nfrom langflow.io import BoolInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom firecrawl import FirecrawlApp\n\nclass FirecrawlMapApi(Component):\n    display_name: str = \"FirecrawlMapApi\"\n    description: str = \"Firecrawl Map API.\"\n    name = \"FirecrawlMapApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        password=True,\n        info='The API key to use Firecrawl API.'),\n        MultilineInput(name='urls',\n        display_name='URLs',\n        required=True,\n        info='List of URLs to create maps from (separated by commas or new lines).',\n        tool_mode=True),\n        BoolInput(name='ignore_sitemap',\n        display_name='Ignore Sitemap',\n        info='When true,\n        the sitemap.xml file will be ignored during crawling.'),\n        BoolInput(name='sitemap_only',\n        display_name='Sitemap Only',\n        info='When true,\n        only links found in the sitemap will be returned.'),\n        BoolInput(name='include_subdomains',\n        display_name='Include Subdomains',\n        info='When true,\n        subdomains of the provided URL will also be scanned.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='map')\n    ]\n\n    def map(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FirecrawlMapApi", "base_classes": ["Component"], "public_methods": ["def map(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import BoolInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import Data", "from firecrawl import FirecrawlApp"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='urls', display_name='URLs', required=True, info='List of URLs to create maps from (separated by commas or new lines).', tool_mode=True), BoolInput(name='ignore_sitemap', display_name='Ignore Sitemap', info='When true, the sitemap.xml file will be ignored during crawling.'), BoolInput(name='sitemap_only', display_name='Sitemap Only', info='When true, only links found in the sitemap will be returned.'), BoolInput(name='include_subdomains', display_name='Include Subdomains', info='When true, subdomains of the provided URL will also be scanned.')]", "outputs": "[Output(display_name='Data', name='data', method='map')]", "display_name": "FirecrawlMapApi", "name": "FirecrawlMapApi", "description": "Firecrawl Map API.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/flow_tool.py", "section": "class::FlowToolComponent", "content": "from typing import Any\nfrom loguru import logger\nfrom typing_extensions import override\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.base.tools.flow_tool import FlowTool\nfrom langflow.field_typing import Tool\nfrom langflow.graph.graph.base import Graph\nfrom langflow.helpers.flow import get_flow_inputs\nfrom langflow.io import BoolInput, DropdownInput, Output, StrInput\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\nclass FlowToolComponent(LCToolComponent):\n    display_name: str = \"Flow as Tool [Deprecated]\"\n    description: str = \"Construct a Tool from a function that runs the loaded Flow.\"\n    icon = \"hammer\"\n    name = \"FlowTool\"\n\n    inputs = [\n        DropdownInput(name='flow_name',\n        display_name='Flow Name',\n        info='The name of the flow to run.',\n        refresh_button=True),\n        StrInput(name='tool_name',\n        display_name='Name',\n        info='The name of the tool.'),\n        StrInput(name='tool_description',\n        display_name='Description',\n        info=\"The description of the tool; defaults to the Flow's description.\"),\n        BoolInput(name='return_direct',\n        display_name='Return Direct',\n        info='Return the result directly from the Tool.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='api_build_tool',\n        display_name='Tool',\n        method='build_tool')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "FlowToolComponent", "base_classes": ["LCToolComponent"], "public_methods": [], "imports": ["from typing import Any", "from loguru import logger", "from typing_extensions import override", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.base.tools.flow_tool import FlowTool", "from langflow.field_typing import Tool", "from langflow.graph.graph.base import Graph", "from langflow.helpers.flow import get_flow_inputs", "from langflow.io import BoolInput, DropdownInput, Output, StrInput", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict"], "inputs": "[DropdownInput(name='flow_name', display_name='Flow Name', info='The name of the flow to run.', refresh_button=True), StrInput(name='tool_name', display_name='Name', info='The name of the tool.'), StrInput(name='tool_description', display_name='Description', info=\"The description of the tool; defaults to the Flow's description.\"), BoolInput(name='return_direct', display_name='Return Direct', info='Return the result directly from the Tool.', advanced=True)]", "outputs": "[Output(name='api_build_tool', display_name='Tool', method='build_tool')]", "display_name": "Flow as Tool [Deprecated]", "name": "FlowTool", "description": "Construct a Tool from a function that runs the loaded Flow.", "icon": "hammer"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/vertexai.py", "section": "class::ChatVertexAIComponent", "content": "from typing import cast\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import BoolInput, FileInput, FloatInput, IntInput, StrInput\nfrom langchain_google_vertexai import ChatVertexAI\nfrom google.cloud import aiplatform\nfrom google.oauth2 import service_account\n\nclass ChatVertexAIComponent(LCModelComponent):\n    display_name: str = \"Vertex AI\"\n    description: str = \"Generate text using Vertex AI LLMs.\"\n    icon = \"VertexAI\"\n    name = \"VertexAiModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        FileInput(name='credentials',\n        display_name='Credentials',\n        info='JSON credentials file. Leave empty to fallback to environment variables',\n        file_types=['json']),\n        MessageTextInput(name='model_name',\n        display_name='Model Name',\n        value='gemini-1.5-pro'),\n        StrInput(name='project',\n        display_name='Project',\n        info='The project ID.',\n        advanced=True),\n        StrInput(name='location',\n        display_name='Location',\n        value='us-central1',\n        advanced=True),\n        IntInput(name='max_output_tokens',\n        display_name='Max Output Tokens',\n        advanced=True),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        value=1,\n        advanced=True),\n        FloatInput(name='temperature',\n        value=0.0,\n        display_name='Temperature'),\n        IntInput(name='top_k',\n        display_name='Top K',\n        advanced=True),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        value=0.95,\n        advanced=True),\n        BoolInput(name='verbose',\n        display_name='Verbose',\n        value=False,\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ChatVertexAIComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from typing import cast", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.inputs import MessageTextInput", "from langflow.io import BoolInput, FileInput, FloatInput, IntInput, StrInput", "from langchain_google_vertexai import ChatVertexAI", "from google.cloud import aiplatform", "from google.oauth2 import service_account"], "inputs": "[*LCModelComponent._base_inputs, FileInput(name='credentials', display_name='Credentials', info='JSON credentials file. Leave empty to fallback to environment variables', file_types=['json']), MessageTextInput(name='model_name', display_name='Model Name', value='gemini-1.5-pro'), StrInput(name='project', display_name='Project', info='The project ID.', advanced=True), StrInput(name='location', display_name='Location', value='us-central1', advanced=True), IntInput(name='max_output_tokens', display_name='Max Output Tokens', advanced=True), IntInput(name='max_retries', display_name='Max Retries', value=1, advanced=True), FloatInput(name='temperature', value=0.0, display_name='Temperature'), IntInput(name='top_k', display_name='Top K', advanced=True), FloatInput(name='top_p', display_name='Top P', value=0.95, advanced=True), BoolInput(name='verbose', display_name='Verbose', value=False, advanced=True)]", "outputs": "", "display_name": "Vertex AI", "name": "VertexAiModel", "description": "Generate text using Vertex AI LLMs.", "icon": "VertexAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/trending.py", "section": "class::YouTubeTrendingComponent", "content": "from contextlib import contextmanager\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, SecretStrInput\nfrom langflow.schema import DataFrame\nfrom langflow.template import Output\nimport re\nimport logging\n\nclass YouTubeTrendingComponent(Component):\n    \"\"\"\n    A component that retrieves trending videos from YouTube.\n    \"\"\"\n\n    display_name: str = \"YouTube Trending\"\n    description: str = \"Retrieves trending videos from YouTube with filtering options.\"\n    icon = \"YouTube\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='YouTube API Key',\n        info='Your YouTube Data API key.',\n        required=True),\n        DropdownInput(name='region',\n        display_name='Region',\n        options=list(COUNTRY_CODES.keys()),\n        value='Global',\n        info='The region to get trending videos from.'),\n        DropdownInput(name='category',\n        display_name='Category',\n        options=list(VIDEO_CATEGORIES.keys()),\n        value='All',\n        info='The category of videos to retrieve.'),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=10,\n        info='Maximum number of trending videos to return (1-50).'),\n        BoolInput(name='include_statistics',\n        display_name='Include Statistics',\n        value=True,\n        info='Include video statistics (views,\n        likes,\n        comments).'),\n        BoolInput(name='include_content_details',\n        display_name='Include Content Details',\n        value=True,\n        info='Include video duration and quality info.',\n        advanced=True),\n        BoolInput(name='include_thumbnails',\n        display_name='Include Thumbnails',\n        value=True,\n        info='Include video thumbnail URLs.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='trending_videos',\n        display_name='Trending Videos',\n        method='get_trending_videos')\n    ]\n\n    def youtube_client(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_trending_videos(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubeTrendingComponent", "base_classes": ["Component"], "public_methods": ["def youtube_client(self)", "def get_trending_videos(self)"], "imports": ["from contextlib import contextmanager", "import pandas as pd", "from googleapiclient.discovery import build", "from googleapiclient.errors import HttpError", "from langflow.custom import Component", "from langflow.inputs import BoolInput, DropdownInput, IntInput, SecretStrInput", "from langflow.schema import DataFrame", "from langflow.template import Output", "import re", "import logging"], "inputs": "[SecretStrInput(name='api_key', display_name='YouTube API Key', info='Your YouTube Data API key.', required=True), DropdownInput(name='region', display_name='Region', options=list(COUNTRY_CODES.keys()), value='Global', info='The region to get trending videos from.'), DropdownInput(name='category', display_name='Category', options=list(VIDEO_CATEGORIES.keys()), value='All', info='The category of videos to retrieve.'), IntInput(name='max_results', display_name='Max Results', value=10, info='Maximum number of trending videos to return (1-50).'), BoolInput(name='include_statistics', display_name='Include Statistics', value=True, info='Include video statistics (views, likes, comments).'), BoolInput(name='include_content_details', display_name='Include Content Details', value=True, info='Include video duration and quality info.', advanced=True), BoolInput(name='include_thumbnails', display_name='Include Thumbnails', value=True, info='Include video thumbnail URLs.', advanced=True)]", "outputs": "[Output(name='trending_videos', display_name='Trending Videos', method='get_trending_videos')]", "display_name": "YouTube Trending", "name": "", "description": "Retrieves trending videos from YouTube with filtering options.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/notdiamond/notdiamond.py", "section": "class::NotDiamondComponent", "content": "import warnings\nimport requests\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.chat_result import get_chat_result\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, MessageInput, MessageTextInput, Output, SecretStrInput, StrInput\nfrom langflow.schema.message import Message\n\nclass NotDiamondComponent(Component):\n    display_name: str = \"Not Diamond Router\"\n    description: str = \"Call the right model at the right time with the world's most powerful AI model router.\"\n    icon = \"NotDiamond\"\n    name = \"NotDiamond\"\n\n    inputs = [\n        MessageInput(name='input_value',\n        display_name='Input',\n        required=True),\n        MessageTextInput(name='system_message',\n        display_name='System Message',\n        info='System message to pass to the model.',\n        advanced=False),\n        HandleInput(name='models',\n        display_name='Language Models',\n        input_types=['LanguageModel'],\n        required=True,\n        is_list=True,\n        info='Link the models you want to route between.'),\n        SecretStrInput(name='api_key',\n        display_name='Not Diamond API Key',\n        info='The Not Diamond API Key to use for routing.',\n        advanced=False,\n        value='NOTDIAMOND_API_KEY',\n        required=True),\n        StrInput(name='preference_id',\n        display_name='Preference ID',\n        info='The ID of the router preference that was configured via the Dashboard.',\n        advanced=False),\n        DropdownInput(name='tradeoff',\n        display_name='Tradeoff',\n        info='The tradeoff between cost and latency for the router to determine the best LLM for a given query.',\n        advanced=False,\n        options=['quality',\n        'cost',\n        'latency'],\n        value='quality'),\n        BoolInput(name='hash_content',\n        display_name='Hash Content',\n        info='Whether to hash the content before being sent to the NotDiamond API.',\n        advanced=False,\n        value=False)\n    ]\n\n    outputs = [\n        Output(display_name='Output',\n        name='output',\n        method='model_select'),\n        Output(display_name='Selected Model',\n        name='selected_model',\n        method='get_selected_model',\n        required_inputs=['output'])\n    ]\n\n    def get_selected_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def model_select(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotDiamondComponent", "base_classes": ["Component"], "public_methods": ["def get_selected_model(self)", "def model_select(self)"], "imports": ["import warnings", "import requests", "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage", "from pydantic.v1 import SecretStr", "from langflow.base.models.chat_result import get_chat_result", "from langflow.base.models.model_utils import get_model_name", "from langflow.custom.custom_component.component import Component", "from langflow.io import BoolInput, DropdownInput, HandleInput, MessageInput, MessageTextInput, Output, SecretStrInput, StrInput", "from langflow.schema.message import Message"], "inputs": "[MessageInput(name='input_value', display_name='Input', required=True), MessageTextInput(name='system_message', display_name='System Message', info='System message to pass to the model.', advanced=False), HandleInput(name='models', display_name='Language Models', input_types=['LanguageModel'], required=True, is_list=True, info='Link the models you want to route between.'), SecretStrInput(name='api_key', display_name='Not Diamond API Key', info='The Not Diamond API Key to use for routing.', advanced=False, value='NOTDIAMOND_API_KEY', required=True), StrInput(name='preference_id', display_name='Preference ID', info='The ID of the router preference that was configured via the Dashboard.', advanced=False), DropdownInput(name='tradeoff', display_name='Tradeoff', info='The tradeoff between cost and latency for the router to determine the best LLM for a given query.', advanced=False, options=['quality', 'cost', 'latency'], value='quality'), BoolInput(name='hash_content', display_name='Hash Content', info='Whether to hash the content before being sent to the NotDiamond API.', advanced=False, value=False)]", "outputs": "[Output(display_name='Output', name='output', method='model_select'), Output(display_name='Selected Model', name='selected_model', method='get_selected_model', required_inputs=['output'])]", "display_name": "Not Diamond Router", "name": "NotDiamond", "description": "Call the right model at the right time with the world's most powerful AI model router.", "icon": "NotDiamond"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/page_content_viewer.py", "section": "class::NotionPageContent", "content": "import requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionPageContent(LCToolComponent):\n    display_name: str = \"Page Content Viewer \"\n    description: str = \"Retrieve the content of a Notion page as plain text.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        StrInput(name='page_id',\n        display_name='Page ID',\n        info='The ID of the Notion page to retrieve.'),\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_blocks(self, blocks):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_rich_text(self, rich_text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionPageContent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)", "def parse_blocks(self, blocks)", "def parse_rich_text(self, rich_text)"], "imports": ["import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='page_id', display_name='Page ID', info='The ID of the Notion page to retrieve.'), SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True)]", "outputs": "", "display_name": "Page Content Viewer ", "name": "", "description": "Retrieve the content of a Notion page as plain text.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/page_content_viewer.py", "section": "class::NotionPageContentSchema", "content": "import requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionPageContentSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionPageContentSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/run.py", "section": "class::AssistantsRun", "content": "from typing import Any\nfrom openai.lib.streaming import AssistantEventHandler\nfrom langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema import dotdict\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass AssistantsRun(ComponentWithCache):\n    display_name: str = \"Run Assistant\"\n    description: str = \"Executes an Assistant Run against a thread\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        MultilineInput(name='assistant_id',\n        display_name='Assistant ID',\n        info='The ID of the assistant to run. \\n\\nCan be retrieved using the List Assistants component or created with the Create Assistant component.'),\n        MultilineInput(name='user_message',\n        display_name='User Message',\n        info='User message to pass to the run.'),\n        MultilineInput(name='thread_id',\n        display_name='Thread ID',\n        required=False,\n        info='Thread ID to use with the run. If not provided,\n        a new thread will be created.'),\n        MultilineInput(name='env_set',\n        display_name='Environment Set',\n        info='Dummy input to allow chaining with Dotenv Component.')\n    ]\n\n    outputs = [\n        Output(display_name='Assistant Response',\n        name='assistant_response',\n        method='process_inputs')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssistantsRun", "base_classes": ["ComponentWithCache"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def process_inputs(self)"], "imports": ["from typing import Any", "from openai.lib.streaming import AssistantEventHandler", "from langflow.base.astra_assistants.util import get_patched_openai_client", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.inputs import MultilineInput", "from langflow.schema import dotdict", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[MultilineInput(name='assistant_id', display_name='Assistant ID', info='The ID of the assistant to run. \\n\\nCan be retrieved using the List Assistants component or created with the Create Assistant component.'), MultilineInput(name='user_message', display_name='User Message', info='User message to pass to the run.'), MultilineInput(name='thread_id', display_name='Thread ID', required=False, info='Thread ID to use with the run. If not provided, a new thread will be created.'), MultilineInput(name='env_set', display_name='Environment Set', info='Dummy input to allow chaining with Dotenv Component.')]", "outputs": "[Output(display_name='Assistant Response', name='assistant_response', method='process_inputs')]", "display_name": "Run Assistant", "name": "", "description": "Executes an Assistant Run against a thread", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/run.py", "section": "class::EventHandler", "content": "from typing import Any\nfrom openai.lib.streaming import AssistantEventHandler\nfrom langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema import dotdict\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass EventHandler(AssistantEventHandler):\n\n    def on_exception(self, exception):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "EventHandler", "base_classes": ["AssistantEventHandler"], "public_methods": ["def on_exception(self, exception)"], "imports": ["from typing import Any", "from openai.lib.streaming import AssistantEventHandler", "from langflow.base.astra_assistants.util import get_patched_openai_client", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.inputs import MultilineInput", "from langflow.schema import dotdict", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/scrapegraph/scrapegraph_markdownify_api.py", "section": "class::ScrapeGraphMarkdownifyApi", "content": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom scrapegraph_py import Client\nfrom scrapegraph_py.logger import sgai_logger\n\nclass ScrapeGraphMarkdownifyApi(Component):\n    display_name: str = \"ScrapeGraphMarkdownifyApi\"\n    description: str = \"ScrapeGraph Markdownify API.\n    Given a URL, it will return the markdownified content of the website.\n    More info at https://docs.scrapegraphai.com/services/markdownify\"\n    name = \"ScrapeGraphMarkdownifyApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='ScrapeGraph API Key',\n        required=True,\n        password=True,\n        info='The API key to use ScrapeGraph API.'),\n        MessageTextInput(name='url',\n        display_name='URL',\n        tool_mode=True,\n        info='The URL to markdownify.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='scrape')\n    ]\n\n    def scrape(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ScrapeGraphMarkdownifyApi", "base_classes": ["Component"], "public_methods": ["def scrape(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data", "from scrapegraph_py import Client", "from scrapegraph_py.logger import sgai_logger"], "inputs": "[SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='url', display_name='URL', tool_mode=True, info='The URL to markdownify.')]", "outputs": "[Output(display_name='Data', name='data', method='scrape')]", "display_name": "ScrapeGraphMarkdownifyApi", "name": "ScrapeGraphMarkdownifyApi", "description": "ScrapeGraph Markdownify API.\n    Given a URL, it will return the markdownified content of the website.\n    More info at https://docs.scrapegraphai.com/services/markdownify", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/cassandra.py", "section": "class::CassandraVectorStoreComponent", "content": "from langchain_community.vectorstores import Cassandra\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import BoolInput, DictInput, FloatInput\nfrom langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\nfrom uuid import UUID\nimport cassio\nfrom langchain_community.utilities.cassandra import SetupMode\n\nclass CassandraVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Cassandra\"\n    description: str = \"Cassandra Vector Store with search capabilities\"\n    icon = \"Cassandra\"\n    name = \"Cassandra\"\n\n    inputs = [\n        MessageTextInput(name='database_ref',\n        display_name='Contact Points / Astra Database ID',\n        info='Contact points for the database (or AstraDB database ID)',\n        required=True),\n        MessageTextInput(name='username',\n        display_name='Username',\n        info='Username for the database (leave empty for AstraDB).'),\n        SecretStrInput(name='token',\n        display_name='Password / AstraDB Token',\n        info='User password for the database (or AstraDB token).',\n        required=True),\n        MessageTextInput(name='keyspace',\n        display_name='Keyspace',\n        info='Table Keyspace (or AstraDB namespace).',\n        required=True),\n        MessageTextInput(name='table_name',\n        display_name='Table Name',\n        info='The name of the table (or AstraDB collection) where vectors will be stored.',\n        required=True),\n        IntInput(name='ttl_seconds',\n        display_name='TTL Seconds',\n        info='Optional time-to-live for the added texts.',\n        advanced=True),\n        IntInput(name='batch_size',\n        display_name='Batch Size',\n        info='Optional number of data to process in a single batch.',\n        value=16,\n        advanced=True),\n        DropdownInput(name='setup_mode',\n        display_name='Setup Mode',\n        info=\"Configuration mode for setting up the Cassandra table,\n        with options like 'Sync',\n        'Async',\n        or 'Off'.\",\n        options=['Sync',\n        'Async',\n        'Off'],\n        value='Sync',\n        advanced=True),\n        DictInput(name='cluster_kwargs',\n        display_name='Cluster arguments',\n        info='Optional dictionary of additional keyword arguments for the Cassandra cluster.',\n        advanced=True,\n        list=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        info='Search type to use',\n        options=['Similarity',\n        'Similarity with score threshold',\n        'MMR (Max Marginal Relevance)'],\n        value='Similarity',\n        advanced=True),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n        value=0,\n        advanced=True),\n        DictInput(name='search_filter',\n        display_name='Search Metadata Filter',\n        info='Optional dictionary of filters to apply to the search query.',\n        advanced=True,\n        list=True),\n        MessageTextInput(name='body_search',\n        display_name='Search Body',\n        info='Document textual search terms to apply to the search query.',\n        advanced=True),\n        BoolInput(name='enable_body_search',\n        display_name='Enable Body Search',\n        info='Flag to enable body search. This must be enabled BEFORE the table is created.',\n        value=False,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_retriever_kwargs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CassandraVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)", "def get_retriever_kwargs(self)"], "imports": ["from langchain_community.vectorstores import Cassandra", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.inputs import BoolInput, DictInput, FloatInput", "from langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data", "from uuid import UUID", "import cassio", "from langchain_community.utilities.cassandra import SetupMode"], "inputs": "[MessageTextInput(name='database_ref', display_name='Contact Points / Astra Database ID', info='Contact points for the database (or AstraDB database ID)', required=True), MessageTextInput(name='username', display_name='Username', info='Username for the database (leave empty for AstraDB).'), SecretStrInput(name='token', display_name='Password / AstraDB Token', info='User password for the database (or AstraDB token).', required=True), MessageTextInput(name='keyspace', display_name='Keyspace', info='Table Keyspace (or AstraDB namespace).', required=True), MessageTextInput(name='table_name', display_name='Table Name', info='The name of the table (or AstraDB collection) where vectors will be stored.', required=True), IntInput(name='ttl_seconds', display_name='TTL Seconds', info='Optional time-to-live for the added texts.', advanced=True), IntInput(name='batch_size', display_name='Batch Size', info='Optional number of data to process in a single batch.', value=16, advanced=True), DropdownInput(name='setup_mode', display_name='Setup Mode', info=\"Configuration mode for setting up the Cassandra table, with options like 'Sync', 'Async', or 'Off'.\", options=['Sync', 'Async', 'Off'], value='Sync', advanced=True), DictInput(name='cluster_kwargs', display_name='Cluster arguments', info='Optional dictionary of additional keyword arguments for the Cassandra cluster.', advanced=True, list=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True), DropdownInput(name='search_type', display_name='Search Type', info='Search type to use', options=['Similarity', 'Similarity with score threshold', 'MMR (Max Marginal Relevance)'], value='Similarity', advanced=True), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\", value=0, advanced=True), DictInput(name='search_filter', display_name='Search Metadata Filter', info='Optional dictionary of filters to apply to the search query.', advanced=True, list=True), MessageTextInput(name='body_search', display_name='Search Body', info='Document textual search terms to apply to the search query.', advanced=True), BoolInput(name='enable_body_search', display_name='Enable Body Search', info='Flag to enable body search. This must be enabled BEFORE the table is created.', value=False, advanced=True)]", "outputs": "", "display_name": "Cassandra", "name": "Cassandra", "description": "Cassandra Vector Store with search capabilities", "icon": "Cassandra"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/icosacomputing/combinatorial_reasoner.py", "section": "class::CombinatorialReasonerComponent", "content": "import requests\nfrom requests.auth import HTTPBasicAuth\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, SecretStrInput, StrInput\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass CombinatorialReasonerComponent(Component):\n    display_name: str = \"Combinatorial Reasoner\"\n    description: str = \"Uses Combinatorial Optimization to construct an optimal prompt with embedded reasons. Sign up here:\nhttps://forms.gle/oWNv2NKjBNaqqvCx6\"\n    icon = \"Icosa\"\n    name = \"Combinatorial Reasoner\"\n\n    inputs = [\n        MessageTextInput(name='prompt',\n        display_name='Prompt',\n        required=True),\n        SecretStrInput(name='openai_api_key',\n        display_name='OpenAI API Key',\n        info='The OpenAI API Key to use for the OpenAI model.',\n        advanced=False,\n        value='OPENAI_API_KEY',\n        required=True),\n        StrInput(name='username',\n        display_name='Username',\n        info='Username to authenticate access to Icosa CR API',\n        advanced=False,\n        required=True),\n        SecretStrInput(name='password',\n        display_name='Password',\n        info='Password to authenticate access to Icosa CR API.',\n        advanced=False,\n        required=True),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=OPENAI_MODEL_NAMES,\n        value=OPENAI_MODEL_NAMES[0])\n    ]\n\n    outputs = [\n        Output(display_name='Optimized Prompt',\n        name='optimized_prompt',\n        method='build_prompt'),\n        Output(display_name='Selected Reasons',\n        name='reasons',\n        method='build_reasons')\n    ]\n\n    def build_prompt(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_reasons(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CombinatorialReasonerComponent", "base_classes": ["Component"], "public_methods": ["def build_prompt(self)", "def build_reasons(self)"], "imports": ["import requests", "from requests.auth import HTTPBasicAuth", "from langflow.base.models.openai_constants import OPENAI_MODEL_NAMES", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, SecretStrInput, StrInput", "from langflow.io import MessageTextInput, Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='prompt', display_name='Prompt', required=True), SecretStrInput(name='openai_api_key', display_name='OpenAI API Key', info='The OpenAI API Key to use for the OpenAI model.', advanced=False, value='OPENAI_API_KEY', required=True), StrInput(name='username', display_name='Username', info='Username to authenticate access to Icosa CR API', advanced=False, required=True), SecretStrInput(name='password', display_name='Password', info='Password to authenticate access to Icosa CR API.', advanced=False, required=True), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=OPENAI_MODEL_NAMES, value=OPENAI_MODEL_NAMES[0])]", "outputs": "[Output(display_name='Optimized Prompt', name='optimized_prompt', method='build_prompt'), Output(display_name='Selected Reasons', name='reasons', method='build_reasons')]", "display_name": "Combinatorial Reasoner", "name": "Combinatorial Reasoner", "description": "Uses Combinatorial Optimization to construct an optimal prompt with embedded reasons. Sign up here:\nhttps://forms.gle/oWNv2NKjBNaqqvCx6", "icon": "Icosa"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/prototypes/python_function.py", "section": "class::PythonFunctionComponent", "content": "from collections.abc import Callable\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.custom.utils import get_function\nfrom langflow.io import CodeInput, Output\nfrom langflow.schema import Data, dotdict\nfrom langflow.schema.message import Message\n\nclass PythonFunctionComponent(Component):\n    display_name: str = \"Python Function\"\n    description: str = \"Define and execute a Python function that returns a Data object or a Message.\"\n    icon = \"Python\"\n    name = \"PythonFunction\"\n\n    inputs = [\n        CodeInput(name='function_code',\n        display_name='Function Code',\n        info='The code for the function.')\n    ]\n\n    outputs = [\n        Output(name='function_output',\n        display_name='Function Callable',\n        method='get_function_callable'),\n        Output(name='function_output_data',\n        display_name='Function Output (Data)',\n        method='execute_function_data'),\n        Output(name='function_output_str',\n        display_name='Function Output (Message)',\n        method='execute_function_message')\n    ]\n\n    def get_function_callable(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def execute_function(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def execute_function_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def execute_function_message(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PythonFunctionComponent", "base_classes": ["Component"], "public_methods": ["def get_function_callable(self)", "def execute_function(self)", "def execute_function_data(self)", "def execute_function_message(self)"], "imports": ["from collections.abc import Callable", "from loguru import logger", "from langflow.custom import Component", "from langflow.custom.utils import get_function", "from langflow.io import CodeInput, Output", "from langflow.schema import Data, dotdict", "from langflow.schema.message import Message"], "inputs": "[CodeInput(name='function_code', display_name='Function Code', info='The code for the function.')]", "outputs": "[Output(name='function_output', display_name='Function Callable', method='get_function_callable'), Output(name='function_output_data', display_name='Function Output (Data)', method='execute_function_data'), Output(name='function_output_str', display_name='Function Output (Message)', method='execute_function_message')]", "display_name": "Python Function", "name": "PythonFunction", "description": "Define and execute a Python function that returns a Data object or a Message.", "icon": "Python"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/olivya/olivya.py", "section": "class::OlivyaComponent", "content": "import json\nimport httpx\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\n\nclass OlivyaComponent(Component):\n    display_name: str = \"Place Call\"\n    description: str = \"A component to create an outbound call request from Olivya's platform.\"\n    icon = \"Olivya\"\n    name = \"OlivyaComponent\"\n\n    inputs = [\n        MessageTextInput(name='api_key',\n        display_name='API Key',\n        info='Your API key for authentication',\n        value='',\n        required=True),\n        MessageTextInput(name='from_number',\n        display_name='From Number',\n        info=\"The Agent's phone number\",\n        value='',\n        required=True),\n        MessageTextInput(name='to_number',\n        display_name='To Number',\n        info=\"The recipient's phone number\",\n        value='',\n        required=True),\n        MessageTextInput(name='first_message',\n        display_name='First Message',\n        info=\"The Agent's introductory message\",\n        value='',\n        required=False,\n        tool_mode=True),\n        MessageTextInput(name='system_prompt',\n        display_name='System Prompt',\n        info='The system prompt to guide the interaction',\n        value='',\n        required=False),\n        MessageTextInput(name='conversation_history',\n        display_name='Conversation History',\n        info='The summary of the conversation',\n        value='',\n        required=False,\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Output',\n        name='output',\n        method='build_output')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "OlivyaComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["import json", "import httpx", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import MessageTextInput, Output", "from langflow.schema import Data"], "inputs": "[MessageTextInput(name='api_key', display_name='API Key', info='Your API key for authentication', value='', required=True), MessageTextInput(name='from_number', display_name='From Number', info=\"The Agent's phone number\", value='', required=True), MessageTextInput(name='to_number', display_name='To Number', info=\"The recipient's phone number\", value='', required=True), MessageTextInput(name='first_message', display_name='First Message', info=\"The Agent's introductory message\", value='', required=False, tool_mode=True), MessageTextInput(name='system_prompt', display_name='System Prompt', info='The system prompt to guide the interaction', value='', required=False), MessageTextInput(name='conversation_history', display_name='Conversation History', info='The summary of the conversation', value='', required=False, tool_mode=True)]", "outputs": "[Output(display_name='Output', name='output', method='build_output')]", "display_name": "Place Call", "name": "OlivyaComponent", "description": "A component to create an outbound call request from Olivya's platform.", "icon": "Olivya"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/confluence/confluence.py", "section": "class::ConfluenceComponent", "content": "from langchain_community.document_loaders import ConfluenceLoader\nfrom langchain_community.document_loaders.confluence import ContentFormat\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, IntInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ConfluenceComponent(Component):\n    display_name: str = \"Confluence\"\n    description: str = \"Confluence wiki collaboration platform\"\n    icon = \"Confluence\"\n    name = \"Confluence\"\n\n    inputs = [\n        StrInput(name='url',\n        display_name='Site URL',\n        required=True,\n        info='The base URL of the Confluence Space. Example: https://<company>.atlassian.net/wiki.'),\n        StrInput(name='username',\n        display_name='Username',\n        required=True,\n        info='Atlassian User E-mail. Example: email@example.com'),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        info='Atlassian Key. Create at: https://id.atlassian.com/manage-profile/security/api-tokens'),\n        StrInput(name='space_key',\n        display_name='Space Key',\n        required=True),\n        BoolInput(name='cloud',\n        display_name='Use Cloud?',\n        required=True,\n        value=True,\n        advanced=True),\n        DropdownInput(name='content_format',\n        display_name='Content Format',\n        options=[ContentFormat.EDITOR.value,\n        ContentFormat.EXPORT_VIEW.value,\n        ContentFormat.ANONYMOUS_EXPORT_VIEW.value,\n        ContentFormat.STORAGE.value,\n        ContentFormat.VIEW.value],\n        value=ContentFormat.STORAGE.value,\n        required=True,\n        advanced=True,\n        info='Specify content format,\n        defaults to ContentFormat.STORAGE'),\n        IntInput(name='max_pages',\n        display_name='Max Pages',\n        required=False,\n        value=1000,\n        advanced=True,\n        info='Maximum number of pages to retrieve in total,\n        defaults 1000')\n    ]\n\n    outputs = [\n        Output(name='data',\n        display_name='Data',\n        method='load_documents')\n    ]\n\n    def build_confluence(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def load_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ConfluenceComponent", "base_classes": ["Component"], "public_methods": ["def build_confluence(self)", "def load_documents(self)"], "imports": ["from langchain_community.document_loaders import ConfluenceLoader", "from langchain_community.document_loaders.confluence import ContentFormat", "from langflow.custom import Component", "from langflow.io import BoolInput, DropdownInput, IntInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='url', display_name='Site URL', required=True, info='The base URL of the Confluence Space. Example: https://<company>.atlassian.net/wiki.'), StrInput(name='username', display_name='Username', required=True, info='Atlassian User E-mail. Example: email@example.com'), SecretStrInput(name='api_key', display_name='API Key', required=True, info='Atlassian Key. Create at: https://id.atlassian.com/manage-profile/security/api-tokens'), StrInput(name='space_key', display_name='Space Key', required=True), BoolInput(name='cloud', display_name='Use Cloud?', required=True, value=True, advanced=True), DropdownInput(name='content_format', display_name='Content Format', options=[ContentFormat.EDITOR.value, ContentFormat.EXPORT_VIEW.value, ContentFormat.ANONYMOUS_EXPORT_VIEW.value, ContentFormat.STORAGE.value, ContentFormat.VIEW.value], value=ContentFormat.STORAGE.value, required=True, advanced=True, info='Specify content format, defaults to ContentFormat.STORAGE'), IntInput(name='max_pages', display_name='Max Pages', required=False, value=1000, advanced=True, info='Maximum number of pages to retrieve in total, defaults 1000')]", "outputs": "[Output(name='data', display_name='Data', method='load_documents')]", "display_name": "Confluence", "name": "Confluence", "description": "Confluence wiki collaboration platform", "icon": "Confluence"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/assemblyai/assemblyai_list_transcripts.py", "section": "class::AssemblyAIListTranscripts", "content": "import assemblyai as aai\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\n\nclass AssemblyAIListTranscripts(Component):\n    display_name: str = \"AssemblyAI List Transcripts\"\n    description: str = \"Retrieve a list of transcripts from AssemblyAI with filtering options\"\n    icon = \"AssemblyAI\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Assembly API Key',\n        info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/',\n        required=True),\n        IntInput(name='limit',\n        display_name='Limit',\n        info='Maximum number of transcripts to retrieve (default: 20,\n        use 0 for all)',\n        value=20),\n        DropdownInput(name='status_filter',\n        display_name='Status Filter',\n        options=['all',\n        'queued',\n        'processing',\n        'completed',\n        'error'],\n        value='all',\n        info='Filter by transcript status',\n        advanced=True),\n        MessageTextInput(name='created_on',\n        display_name='Created On',\n        info='Only get transcripts created on this date (YYYY-MM-DD)',\n        advanced=True),\n        BoolInput(name='throttled_only',\n        display_name='Throttled Only',\n        info='Only get throttled transcripts,\n        overrides the status filter',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Transcript List',\n        name='transcript_list',\n        method='list_transcripts')\n    ]\n\n    def list_transcripts(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssemblyAIListTranscripts", "base_classes": ["Component"], "public_methods": ["def list_transcripts(self)"], "imports": ["import assemblyai as aai", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='Assembly API Key', info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/', required=True), IntInput(name='limit', display_name='Limit', info='Maximum number of transcripts to retrieve (default: 20, use 0 for all)', value=20), DropdownInput(name='status_filter', display_name='Status Filter', options=['all', 'queued', 'processing', 'completed', 'error'], value='all', info='Filter by transcript status', advanced=True), MessageTextInput(name='created_on', display_name='Created On', info='Only get transcripts created on this date (YYYY-MM-DD)', advanced=True), BoolInput(name='throttled_only', display_name='Throttled Only', info='Only get throttled transcripts, overrides the status filter', advanced=True)]", "outputs": "[Output(display_name='Transcript List', name='transcript_list', method='list_transcripts')]", "display_name": "AssemblyAI List Transcripts", "name": "", "description": "Retrieve a list of transcripts from AssemblyAI with filtering options", "icon": "AssemblyAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/unstructured/unstructured.py", "section": "class::UnstructuredComponent", "content": "from langchain_unstructured import UnstructuredLoader\nfrom langflow.base.data import BaseFileComponent\nfrom langflow.inputs import DropdownInput, MessageTextInput, NestedDictInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass UnstructuredComponent(BaseFileComponent):\n    display_name: str = \"Unstructured API\"\n    description: str = \"Uses Unstructured.io API to extract clean text from raw source documents. Supports a wide range of file types.\"\n    icon = \"Unstructured\"\n    name = \"Unstructured\"\n\n    inputs = [\n        *BaseFileComponent._base_inputs,\n        SecretStrInput(name='api_key',\n        display_name='Unstructured.io Serverless API Key',\n        required=True,\n        info='Unstructured API Key. Create at: https://app.unstructured.io/'),\n        MessageTextInput(name='api_url',\n        display_name='Unstructured.io API URL',\n        required=False,\n        info='Unstructured API URL.'),\n        DropdownInput(name='chunking_strategy',\n        display_name='Chunking Strategy',\n        info='Chunking strategy to use,\n        see https://docs.unstructured.io/api-reference/api-services/chunking',\n        options=['',\n        'basic',\n        'by_title',\n        'by_page',\n        'by_similarity'],\n        real_time_refresh=False,\n        value=''),\n        NestedDictInput(name='unstructured_args',\n        display_name='Additional Arguments',\n        required=False,\n        info='Optional dictionary of additional arguments to the Loader. See https://docs.unstructured.io/api-reference/api-services/api-parameters for more information.')\n    ]\n\n    outputs = [\n        *BaseFileComponent._base_outputs\n    ]\n\n    def process_files(self, file_list):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "UnstructuredComponent", "base_classes": ["BaseFileComponent"], "public_methods": ["def process_files(self, file_list)"], "imports": ["from langchain_unstructured import UnstructuredLoader", "from langflow.base.data import BaseFileComponent", "from langflow.inputs import DropdownInput, MessageTextInput, NestedDictInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "[*BaseFileComponent._base_inputs, SecretStrInput(name='api_key', display_name='Unstructured.io Serverless API Key', required=True, info='Unstructured API Key. Create at: https://app.unstructured.io/'), MessageTextInput(name='api_url', display_name='Unstructured.io API URL', required=False, info='Unstructured API URL.'), DropdownInput(name='chunking_strategy', display_name='Chunking Strategy', info='Chunking strategy to use, see https://docs.unstructured.io/api-reference/api-services/chunking', options=['', 'basic', 'by_title', 'by_page', 'by_similarity'], real_time_refresh=False, value=''), NestedDictInput(name='unstructured_args', display_name='Additional Arguments', required=False, info='Optional dictionary of additional arguments to the Loader. See https://docs.unstructured.io/api-reference/api-services/api-parameters for more information.')]", "outputs": "[*BaseFileComponent._base_outputs]", "display_name": "Unstructured API", "name": "Unstructured", "description": "Uses Unstructured.io API to extract clean text from raw source documents. Supports a wide range of file types.", "icon": "Unstructured"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/rss.py", "section": "class::RSSReaderComponent", "content": "import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langflow.custom import Component\nfrom langflow.io import IntInput, MessageTextInput, Output\nfrom langflow.logging import logger\nfrom langflow.schema import DataFrame\n\nclass RSSReaderComponent(Component):\n    display_name: str = \"RSS Reader\"\n    description: str = \"Fetches and parses an RSS feed.\"\n    icon = \"rss\"\n    name = \"RSSReaderSimple\"\n\n    inputs = [\n        MessageTextInput(name='rss_url',\n        display_name='RSS Feed URL',\n        info='URL of the RSS feed to parse.',\n        tool_mode=True,\n        input_types=[],\n        required=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='Timeout for the RSS feed request.',\n        value=5,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='articles',\n        display_name='Articles',\n        method='read_rss')\n    ]\n\n    def read_rss(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RSSReaderComponent", "base_classes": ["Component"], "public_methods": ["def read_rss(self)"], "imports": ["import pandas as pd", "import requests", "from bs4 import BeautifulSoup", "from langflow.custom import Component", "from langflow.io import IntInput, MessageTextInput, Output", "from langflow.logging import logger", "from langflow.schema import DataFrame"], "inputs": "[MessageTextInput(name='rss_url', display_name='RSS Feed URL', info='URL of the RSS feed to parse.', tool_mode=True, input_types=[], required=True), IntInput(name='timeout', display_name='Timeout', info='Timeout for the RSS feed request.', value=5, advanced=True)]", "outputs": "[Output(name='articles', display_name='Articles', method='read_rss')]", "display_name": "RSS Reader", "name": "RSSReaderSimple", "description": "Fetches and parses an RSS feed.", "icon": "rss"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/sub_flow.py", "section": "class::SubFlowComponent", "content": "from typing import TYPE_CHECKING, Any\nfrom loguru import logger\nfrom langflow.base.flow_processing.utils import build_data_from_result_data\nfrom langflow.custom import CustomComponent\nfrom langflow.graph.graph.base import Graph\nfrom langflow.graph.vertex.base import Vertex\nfrom langflow.helpers.flow import get_flow_inputs\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.template.field.base import Input\nfrom langflow.graph.schema import RunOutputs\n\nclass SubFlowComponent(CustomComponent):\n    display_name: str = \"Sub Flow\"\n    description: str = \"Dynamically Generates a Component from a Flow. The output is a list of data with keys 'result' and 'message'.\"\n    name = \"SubFlow\"\n\n    def add_inputs_to_build_config(self, inputs, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SubFlowComponent", "base_classes": ["CustomComponent"], "public_methods": ["def add_inputs_to_build_config(self, inputs, build_config)", "def build_config(self)"], "imports": ["from typing import TYPE_CHECKING, Any", "from loguru import logger", "from langflow.base.flow_processing.utils import build_data_from_result_data", "from langflow.custom import CustomComponent", "from langflow.graph.graph.base import Graph", "from langflow.graph.vertex.base import Vertex", "from langflow.helpers.flow import get_flow_inputs", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict", "from langflow.template.field.base import Input", "from langflow.graph.schema import RunOutputs"], "inputs": "", "outputs": "", "display_name": "Sub Flow", "name": "SubFlow", "description": "Dynamically Generates a Component from a Flow. The output is a list of data with keys 'result' and 'message'.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/composio/gmail_composio.py", "section": "class::ComposioGmailAPIComponent", "content": "import json\nfrom typing import Any\nfrom composio import Action\nfrom langflow.base.composio.composio_base import ComposioBaseComponent\nfrom langflow.inputs import BoolInput, FileInput, IntInput, MessageTextInput\nfrom langflow.logging import logger\n\nclass ComposioGmailAPIComponent(ComposioBaseComponent):\n    \"\"\"\n    Gmail API component for interacting with Gmail services.\n    \"\"\"\n\n    display_name: str = \"Gmail\"\n    icon = \"Google\"\n    name = \"GmailAPI\"\n\n    inputs = [\n        *ComposioBaseComponent._base_inputs,\n        MessageTextInput(name='recipient_email',\n        display_name='Recipient Email',\n        info='Email address of the recipient',\n        show=False,\n        required=True,\n        advanced=False),\n        MessageTextInput(name='subject',\n        display_name='Subject',\n        info='Subject of the email',\n        show=False,\n        required=True,\n        advanced=False),\n        MessageTextInput(name='body',\n        display_name='Body',\n        required=True,\n        info='Content of the email',\n        show=False,\n        advanced=False),\n        MessageTextInput(name='cc',\n        display_name='CC',\n        info='Email addresses to CC (Carbon Copy) in the email,\n        separated by commas',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='bcc',\n        display_name='BCC',\n        info='Email addresses to BCC (Blind Carbon Copy) in the email,\n        separated by commas',\n        show=False,\n        advanced=True),\n        BoolInput(name='is_html',\n        display_name='Is HTML',\n        info='Specify whether the email body contains HTML content (true/false)',\n        show=False,\n        value=False,\n        advanced=True),\n        MessageTextInput(name='gmail_user_id',\n        display_name='User ID',\n        info=\"The user's email address or 'me' for the authenticated user\",\n        show=False,\n        advanced=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        required=True,\n        info='Maximum number of emails to be returned',\n        show=False,\n        advanced=False),\n        MessageTextInput(name='message_id',\n        display_name='Message ID',\n        info='The ID of the specific email message',\n        show=False,\n        required=True,\n        advanced=False),\n        MessageTextInput(name='thread_id',\n        display_name='Thread ID',\n        info='The ID of the email thread',\n        show=False,\n        required=True,\n        advanced=False),\n        MessageTextInput(name='query',\n        display_name='Query',\n        info=\"Search query to filter emails (e.g.,\n        'from:someone@email.com' or 'subject:hello')\",\n        show=False,\n        advanced=False),\n        MessageTextInput(name='message_body',\n        display_name='Message Body',\n        info='The body content of the message to be sent',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='label_name',\n        display_name='Label Name',\n        info='Name of the Gmail label to create,\n        modify,\n        or filter by',\n        show=False,\n        required=True,\n        advanced=False),\n        MessageTextInput(name='label_id',\n        display_name='Label ID',\n        info='The ID of the Gmail label',\n        show=False,\n        advanced=False),\n        MessageTextInput(name='label_ids',\n        display_name='Label Ids',\n        info='Comma-separated list of label IDs to filter messages',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='label_list_visibility',\n        display_name='Label List Visibility',\n        info='The visibility of the label in the label list in the Gmail web interface',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='message_list_visibility',\n        display_name='Message List Visibility',\n        info='The visibility of the label in the message list in the Gmail web interface',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='page_token',\n        display_name='Page Token',\n        info='Token for retrieving the next page of results',\n        show=False,\n        advanced=True),\n        BoolInput(name='include_spam_trash',\n        display_name='Include messages from Spam/Trash',\n        info='Include messages from SPAM and TRASH in the results',\n        show=False,\n        value=False,\n        advanced=True),\n        MessageTextInput(name='format',\n        display_name='Format',\n        info='The format to return the message in. Possible values: minimal,\n        full,\n        raw,\n        metadata',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='resource_name',\n        display_name='Resource Name',\n        info='The resource name of the person to provide information about',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='person_fields',\n        display_name='Person fields',\n        info='Fields to return for the person. Multiple fields can be specified by separating them with commas',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='attachment_id',\n        display_name='Attachment ID',\n        info='Id of the attachment',\n        show=False,\n        required=True,\n        advanced=False),\n        MessageTextInput(name='file_name',\n        display_name='File name',\n        info='File name of the attachment file',\n        show=False,\n        required=True,\n        advanced=False),\n        FileInput(name='attachment',\n        display_name='Add Attachment',\n        file_types=['csv',\n        'txt',\n        'doc',\n        'docx',\n        'xls',\n        'xlsx',\n        'pdf',\n        'png',\n        'jpg',\n        'jpeg',\n        'gif',\n        'zip',\n        'rar',\n        'ppt',\n        'pptx'],\n        info='Add an attachment',\n        show=False)\n    ]\n\n    def execute_action(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def set_default_tools(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ComposioGmailAPIComponent", "base_classes": ["ComposioBaseComponent"], "public_methods": ["def execute_action(self)", "def update_build_config(self, build_config, field_value, field_name)", "def set_default_tools(self)"], "imports": ["import json", "from typing import Any", "from composio import Action", "from langflow.base.composio.composio_base import ComposioBaseComponent", "from langflow.inputs import BoolInput, FileInput, IntInput, MessageTextInput", "from langflow.logging import logger"], "inputs": "[*ComposioBaseComponent._base_inputs, MessageTextInput(name='recipient_email', display_name='Recipient Email', info='Email address of the recipient', show=False, required=True, advanced=False), MessageTextInput(name='subject', display_name='Subject', info='Subject of the email', show=False, required=True, advanced=False), MessageTextInput(name='body', display_name='Body', required=True, info='Content of the email', show=False, advanced=False), MessageTextInput(name='cc', display_name='CC', info='Email addresses to CC (Carbon Copy) in the email, separated by commas', show=False, advanced=True), MessageTextInput(name='bcc', display_name='BCC', info='Email addresses to BCC (Blind Carbon Copy) in the email, separated by commas', show=False, advanced=True), BoolInput(name='is_html', display_name='Is HTML', info='Specify whether the email body contains HTML content (true/false)', show=False, value=False, advanced=True), MessageTextInput(name='gmail_user_id', display_name='User ID', info=\"The user's email address or 'me' for the authenticated user\", show=False, advanced=True), IntInput(name='max_results', display_name='Max Results', required=True, info='Maximum number of emails to be returned', show=False, advanced=False), MessageTextInput(name='message_id', display_name='Message ID', info='The ID of the specific email message', show=False, required=True, advanced=False), MessageTextInput(name='thread_id', display_name='Thread ID', info='The ID of the email thread', show=False, required=True, advanced=False), MessageTextInput(name='query', display_name='Query', info=\"Search query to filter emails (e.g., 'from:someone@email.com' or 'subject:hello')\", show=False, advanced=False), MessageTextInput(name='message_body', display_name='Message Body', info='The body content of the message to be sent', show=False, advanced=True), MessageTextInput(name='label_name', display_name='Label Name', info='Name of the Gmail label to create, modify, or filter by', show=False, required=True, advanced=False), MessageTextInput(name='label_id', display_name='Label ID', info='The ID of the Gmail label', show=False, advanced=False), MessageTextInput(name='label_ids', display_name='Label Ids', info='Comma-separated list of label IDs to filter messages', show=False, advanced=True), MessageTextInput(name='label_list_visibility', display_name='Label List Visibility', info='The visibility of the label in the label list in the Gmail web interface', show=False, advanced=True), MessageTextInput(name='message_list_visibility', display_name='Message List Visibility', info='The visibility of the label in the message list in the Gmail web interface', show=False, advanced=True), MessageTextInput(name='page_token', display_name='Page Token', info='Token for retrieving the next page of results', show=False, advanced=True), BoolInput(name='include_spam_trash', display_name='Include messages from Spam/Trash', info='Include messages from SPAM and TRASH in the results', show=False, value=False, advanced=True), MessageTextInput(name='format', display_name='Format', info='The format to return the message in. Possible values: minimal, full, raw, metadata', show=False, advanced=True), MessageTextInput(name='resource_name', display_name='Resource Name', info='The resource name of the person to provide information about', show=False, advanced=True), MessageTextInput(name='person_fields', display_name='Person fields', info='Fields to return for the person. Multiple fields can be specified by separating them with commas', show=False, advanced=True), MessageTextInput(name='attachment_id', display_name='Attachment ID', info='Id of the attachment', show=False, required=True, advanced=False), MessageTextInput(name='file_name', display_name='File name', info='File name of the attachment file', show=False, required=True, advanced=False), FileInput(name='attachment', display_name='Add Attachment', file_types=['csv', 'txt', 'doc', 'docx', 'xls', 'xlsx', 'pdf', 'png', 'jpg', 'jpeg', 'gif', 'zip', 'rar', 'ppt', 'pptx'], info='Add an attachment', show=False)]", "outputs": "", "display_name": "Gmail", "name": "GmailAPI", "description": "", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/structured_output.py", "section": "class::StructuredOutputComponent", "content": "from pydantic import BaseModel, Field, create_model\nfrom trustcall import create_extractor\nfrom langflow.base.models.chat_result import get_chat_result\nfrom langflow.custom import Component\nfrom langflow.helpers.base_model import build_model_from_schema\nfrom langflow.io import BoolInput, HandleInput, MessageTextInput, MultilineInput, Output, TableInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.table import EditMode\n\nclass StructuredOutputComponent(Component):\n    display_name: str = \"Structured Output\"\n    description: str = \"Transforms LLM responses into **structured data formats**. Ideal for extracting specific information or creating consistent outputs.\"\n    icon = \"braces\"\n    name = \"StructuredOutput\"\n\n    inputs = [\n        HandleInput(name='llm',\n        display_name='Language Model',\n        info='The language model to use to generate the structured output.',\n        input_types=['LanguageModel'],\n        required=True),\n        MessageTextInput(name='input_value',\n        display_name='Input Message',\n        info='The input message to the language model.',\n        tool_mode=True,\n        required=True),\n        MultilineInput(name='system_prompt',\n        display_name='Format Instructions',\n        info='The instructions to the language model for formatting the output.',\n        value=\"You are an AI system designed to extract structured information from unstructured text.Given the input_text,\n        return a JSON object with predefined keys based on the expected structure.Extract values accurately and format them according to the specified type (e.g.,\n        string,\n        integer,\n        float,\n        date).If a value is missing or cannot be determined,\n        return a default (e.g.,\n        null,\n        0,\n        or 'N/A').If multiple instances of the expected structure exist within the input_text,\n        stream each as a separate JSON object.\",\n        required=True,\n        advanced=True),\n        MessageTextInput(name='schema_name',\n        display_name='Schema Name',\n        info='Provide a name for the output data schema.',\n        advanced=True),\n        TableInput(name='output_schema',\n        display_name='Output Schema',\n        info=\"Define the structure and data types for the model's output.\",\n        required=True,\n        table_schema=[{'name': 'name',\n        'display_name': 'Name',\n        'type': 'str',\n        'description': 'Specify the name of the output field.',\n        'default': 'field',\n        'edit_mode': EditMode.INLINE},\n        {'name': 'description',\n        'display_name': 'Description',\n        'type': 'str',\n        'description': 'Describe the purpose of the output field.',\n        'default': 'description of field',\n        'edit_mode': EditMode.POPOVER},\n        {'name': 'type',\n        'display_name': 'Type',\n        'type': 'str',\n        'edit_mode': EditMode.INLINE,\n        'description': 'Indicate the data type of the output field (e.g.,\n        str,\n        int,\n        float,\n        bool,\n        dict).',\n        'options': ['str',\n        'int',\n        'float',\n        'bool',\n        'dict'],\n        'default': 'str'}],\n        value=[{'name': 'field',\n        'description': 'description of field',\n        'type': 'str',\n        'multiple': 'False'}]),\n        BoolInput(name='multiple',\n        advanced=True,\n        display_name='Generate Multiple',\n        info='[Deprecated] Always set to True',\n        value=True)\n    ]\n\n    outputs = [\n        Output(name='structured_output',\n        display_name='Structured Output',\n        method='build_structured_output'),\n        Output(name='structured_output_dataframe',\n        display_name='DataFrame',\n        method='as_dataframe')\n    ]\n\n    def build_structured_output_base(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_structured_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "StructuredOutputComponent", "base_classes": ["Component"], "public_methods": ["def build_structured_output_base(self)", "def build_structured_output(self)", "def as_dataframe(self)"], "imports": ["from pydantic import BaseModel, Field, create_model", "from trustcall import create_extractor", "from langflow.base.models.chat_result import get_chat_result", "from langflow.custom import Component", "from langflow.helpers.base_model import build_model_from_schema", "from langflow.io import BoolInput, HandleInput, MessageTextInput, MultilineInput, Output, TableInput", "from langflow.schema.data import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.schema.table import EditMode"], "inputs": "[HandleInput(name='llm', display_name='Language Model', info='The language model to use to generate the structured output.', input_types=['LanguageModel'], required=True), MessageTextInput(name='input_value', display_name='Input Message', info='The input message to the language model.', tool_mode=True, required=True), MultilineInput(name='system_prompt', display_name='Format Instructions', info='The instructions to the language model for formatting the output.', value=\"You are an AI system designed to extract structured information from unstructured text.Given the input_text, return a JSON object with predefined keys based on the expected structure.Extract values accurately and format them according to the specified type (e.g., string, integer, float, date).If a value is missing or cannot be determined, return a default (e.g., null, 0, or 'N/A').If multiple instances of the expected structure exist within the input_text, stream each as a separate JSON object.\", required=True, advanced=True), MessageTextInput(name='schema_name', display_name='Schema Name', info='Provide a name for the output data schema.', advanced=True), TableInput(name='output_schema', display_name='Output Schema', info=\"Define the structure and data types for the model's output.\", required=True, table_schema=[{'name': 'name', 'display_name': 'Name', 'type': 'str', 'description': 'Specify the name of the output field.', 'default': 'field', 'edit_mode': EditMode.INLINE}, {'name': 'description', 'display_name': 'Description', 'type': 'str', 'description': 'Describe the purpose of the output field.', 'default': 'description of field', 'edit_mode': EditMode.POPOVER}, {'name': 'type', 'display_name': 'Type', 'type': 'str', 'edit_mode': EditMode.INLINE, 'description': 'Indicate the data type of the output field (e.g., str, int, float, bool, dict).', 'options': ['str', 'int', 'float', 'bool', 'dict'], 'default': 'str'}], value=[{'name': 'field', 'description': 'description of field', 'type': 'str', 'multiple': 'False'}]), BoolInput(name='multiple', advanced=True, display_name='Generate Multiple', info='[Deprecated] Always set to True', value=True)]", "outputs": "[Output(name='structured_output', display_name='Structured Output', method='build_structured_output'), Output(name='structured_output_dataframe', display_name='DataFrame', method='as_dataframe')]", "display_name": "Structured Output", "name": "StructuredOutput", "description": "Transforms LLM responses into **structured data formats**. Ideal for extracting specific information or creating consistent outputs.", "icon": "braces"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/memories/cassandra.py", "section": "class::CassandraChatMemory", "content": "from langflow.base.memory.model import LCChatMemoryComponent\nfrom langflow.field_typing.constants import Memory\nfrom langflow.inputs import DictInput, MessageTextInput, SecretStrInput\nfrom langchain_community.chat_message_histories import CassandraChatMessageHistory\nfrom uuid import UUID\nimport cassio\n\nclass CassandraChatMemory(LCChatMemoryComponent):\n    display_name: str = \"Cassandra Chat Memory\"\n    description: str = \"Retrieves and store chat messages from Apache Cassandra.\"\n    icon = \"Cassandra\"\n    name = \"CassandraChatMemory\"\n\n    inputs = [\n        MessageTextInput(name='database_ref',\n        display_name='Contact Points / Astra Database ID',\n        info='Contact points for the database (or AstraDB database ID)',\n        required=True),\n        MessageTextInput(name='username',\n        display_name='Username',\n        info='Username for the database (leave empty for AstraDB).'),\n        SecretStrInput(name='token',\n        display_name='Password / AstraDB Token',\n        info='User password for the database (or AstraDB token).',\n        required=True),\n        MessageTextInput(name='keyspace',\n        display_name='Keyspace',\n        info='Table Keyspace (or AstraDB namespace).',\n        required=True),\n        MessageTextInput(name='table_name',\n        display_name='Table Name',\n        info='The name of the table (or AstraDB collection) where vectors will be stored.',\n        required=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='Session ID for the message.',\n        advanced=True),\n        DictInput(name='cluster_kwargs',\n        display_name='Cluster arguments',\n        info='Optional dictionary of additional keyword arguments for the Cassandra cluster.',\n        advanced=True,\n        is_list=True)\n    ]\n\n    def build_message_history(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CassandraChatMemory", "base_classes": ["LCChatMemoryComponent"], "public_methods": ["def build_message_history(self)"], "imports": ["from langflow.base.memory.model import LCChatMemoryComponent", "from langflow.field_typing.constants import Memory", "from langflow.inputs import DictInput, MessageTextInput, SecretStrInput", "from langchain_community.chat_message_histories import CassandraChatMessageHistory", "from uuid import UUID", "import cassio"], "inputs": "[MessageTextInput(name='database_ref', display_name='Contact Points / Astra Database ID', info='Contact points for the database (or AstraDB database ID)', required=True), MessageTextInput(name='username', display_name='Username', info='Username for the database (leave empty for AstraDB).'), SecretStrInput(name='token', display_name='Password / AstraDB Token', info='User password for the database (or AstraDB token).', required=True), MessageTextInput(name='keyspace', display_name='Keyspace', info='Table Keyspace (or AstraDB namespace).', required=True), MessageTextInput(name='table_name', display_name='Table Name', info='The name of the table (or AstraDB collection) where vectors will be stored.', required=True), MessageTextInput(name='session_id', display_name='Session ID', info='Session ID for the message.', advanced=True), DictInput(name='cluster_kwargs', display_name='Cluster arguments', info='Optional dictionary of additional keyword arguments for the Cassandra cluster.', advanced=True, is_list=True)]", "outputs": "", "display_name": "Cassandra Chat Memory", "name": "CassandraChatMemory", "description": "Retrieves and store chat messages from Apache Cassandra.", "icon": "Cassandra"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/git/git.py", "section": "class::GitLoaderComponent", "content": "import re\nimport tempfile\nfrom contextlib import asynccontextmanager\nfrom fnmatch import fnmatch\nfrom pathlib import Path\nimport anyio\nfrom langchain_community.document_loaders.git import GitLoader\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data\n\nclass GitLoaderComponent(Component):\n    display_name: str = \"Git\"\n    description: str = \"Load and filter documents from a local or remote Git repository. Use a local repo path or clone from a remote URL.\"\n    icon = \"GitLoader\"\n\n    inputs = [\n        DropdownInput(name='repo_source',\n        display_name='Repository Source',\n        options=['Local',\n        'Remote'],\n        required=True,\n        info='Select whether to use a local repo path or clone from a remote URL.',\n        real_time_refresh=True),\n        MessageTextInput(name='repo_path',\n        display_name='Local Repository Path',\n        required=False,\n        info=\"The local path to the existing Git repository (used if 'Local' is selected).\",\n        dynamic=True,\n        show=False),\n        MessageTextInput(name='clone_url',\n        display_name='Clone URL',\n        required=False,\n        info=\"The URL of the Git repository to clone (used if 'Clone' is selected).\",\n        dynamic=True,\n        show=False),\n        MessageTextInput(name='branch',\n        display_name='Branch',\n        required=False,\n        value='main',\n        info=\"The branch to load files from. Defaults to 'main'.\"),\n        MessageTextInput(name='file_filter',\n        display_name='File Filter',\n        required=False,\n        advanced=True,\n        info=\"Patterns to filter files. For example:\\nInclude only .py files: '*.py'\\nExclude .py files: '!*.py'\\nMultiple patterns can be separated by commas.\"),\n        MessageTextInput(name='content_filter',\n        display_name='Content Filter',\n        required=False,\n        advanced=True,\n        info='A regex pattern to filter files based on their content.')\n    ]\n\n    outputs = [\n        Output(name='data',\n        display_name='Data',\n        method='load_documents')\n    ]\n\n    def is_binary(file_path):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def check_file_patterns(file_path, patterns):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def check_content_pattern(file_path, pattern):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_combined_filter(self, file_filter_patterns, content_filter_pattern):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GitLoaderComponent", "base_classes": ["Component"], "public_methods": ["def is_binary(file_path)", "def check_file_patterns(file_path, patterns)", "def check_content_pattern(file_path, pattern)", "def build_combined_filter(self, file_filter_patterns, content_filter_pattern)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["import re", "import tempfile", "from contextlib import asynccontextmanager", "from fnmatch import fnmatch", "from pathlib import Path", "import anyio", "from langchain_community.document_loaders.git import GitLoader", "from langflow.custom import Component", "from langflow.io import DropdownInput, MessageTextInput, Output", "from langflow.schema import Data"], "inputs": "[DropdownInput(name='repo_source', display_name='Repository Source', options=['Local', 'Remote'], required=True, info='Select whether to use a local repo path or clone from a remote URL.', real_time_refresh=True), MessageTextInput(name='repo_path', display_name='Local Repository Path', required=False, info=\"The local path to the existing Git repository (used if 'Local' is selected).\", dynamic=True, show=False), MessageTextInput(name='clone_url', display_name='Clone URL', required=False, info=\"The URL of the Git repository to clone (used if 'Clone' is selected).\", dynamic=True, show=False), MessageTextInput(name='branch', display_name='Branch', required=False, value='main', info=\"The branch to load files from. Defaults to 'main'.\"), MessageTextInput(name='file_filter', display_name='File Filter', required=False, advanced=True, info=\"Patterns to filter files. For example:\\nInclude only .py files: '*.py'\\nExclude .py files: '!*.py'\\nMultiple patterns can be separated by commas.\"), MessageTextInput(name='content_filter', display_name='Content Filter', required=False, advanced=True, info='A regex pattern to filter files based on their content.')]", "outputs": "[Output(name='data', display_name='Data', method='load_documents')]", "display_name": "Git", "name": "", "description": "Load and filter documents from a local or remote Git repository. Use a local repo path or clone from a remote URL.", "icon": "GitLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langwatch/langwatch.py", "section": "class::LangWatchComponent", "content": "import json\nimport os\nfrom typing import Any\nimport httpx\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import MultilineInput\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, NestedDictInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\nclass LangWatchComponent(Component):\n    display_name: str = \"LangWatch Evaluator\"\n    description: str = \"Evaluates various aspects of language models using LangWatch's evaluation endpoints.\"\n    icon = \"Langwatch\"\n    name = \"LangWatchEvaluator\"\n\n    inputs = [\n        DropdownInput(name='evaluator_name',\n        display_name='Evaluator Name',\n        options=[],\n        required=True,\n        info='Select an evaluator.',\n        refresh_button=True,\n        real_time_refresh=True),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        info='Enter your LangWatch API key.'),\n        MessageTextInput(name='input',\n        display_name='Input',\n        required=False,\n        info='The input text for evaluation.'),\n        MessageTextInput(name='output',\n        display_name='Output',\n        required=False,\n        info='The output text for evaluation.'),\n        MessageTextInput(name='expected_output',\n        display_name='Expected Output',\n        required=False,\n        info='The expected output for evaluation.'),\n        MessageTextInput(name='contexts',\n        display_name='Contexts',\n        required=False,\n        info='The contexts for evaluation (comma-separated).'),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='The maximum time (in seconds) allowed for the server to respond before timing out.',\n        value=30,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='evaluation_result',\n        display_name='Evaluation Result',\n        method='evaluate')\n    ]\n\n    def get_evaluators(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_dynamic_inputs(self, evaluator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LangWatchComponent", "base_classes": ["Component"], "public_methods": ["def get_evaluators(self)", "def update_build_config(self, build_config, field_value, field_name)", "def get_dynamic_inputs(self, evaluator)"], "imports": ["import json", "import os", "from typing import Any", "import httpx", "from loguru import logger", "from langflow.custom import Component", "from langflow.inputs.inputs import MultilineInput", "from langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, NestedDictInput, Output, SecretStrInput", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict"], "inputs": "[DropdownInput(name='evaluator_name', display_name='Evaluator Name', options=[], required=True, info='Select an evaluator.', refresh_button=True, real_time_refresh=True), SecretStrInput(name='api_key', display_name='API Key', required=True, info='Enter your LangWatch API key.'), MessageTextInput(name='input', display_name='Input', required=False, info='The input text for evaluation.'), MessageTextInput(name='output', display_name='Output', required=False, info='The output text for evaluation.'), MessageTextInput(name='expected_output', display_name='Expected Output', required=False, info='The expected output for evaluation.'), MessageTextInput(name='contexts', display_name='Contexts', required=False, info='The contexts for evaluation (comma-separated).'), IntInput(name='timeout', display_name='Timeout', info='The maximum time (in seconds) allowed for the server to respond before timing out.', value=30, advanced=True)]", "outputs": "[Output(name='evaluation_result', display_name='Evaluation Result', method='evaluate')]", "display_name": "LangWatch Evaluator", "name": "LangWatchEvaluator", "description": "Evaluates various aspects of language models using LangWatch's evaluation endpoints.", "icon": "Langwatch"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/crewai/crewai.py", "section": "class::CrewAIAgentComponent", "content": "from crewai import Agent\nfrom langflow.base.agents.crewai.crew import convert_llm, convert_tools\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\nclass CrewAIAgentComponent(Component):\n    \"\"\"\n    Component for creating a CrewAI agent.\n    \n    This component allows you to create a CrewAI agent with the specified role, goal, backstory, tools,\n    and language model.\n    \n    Args:\n        Component (Component): Base class for all components.\n    \n    Returns:\n        Agent: CrewAI agent.\n    \"\"\"\n\n    display_name: str = \"CrewAI Agent\"\n    description: str = \"Represents an agent of CrewAI.\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        MultilineInput(name='role',\n        display_name='Role',\n        info='The role of the agent.'),\n        MultilineInput(name='goal',\n        display_name='Goal',\n        info='The objective of the agent.'),\n        MultilineInput(name='backstory',\n        display_name='Backstory',\n        info='The backstory of the agent.'),\n        HandleInput(name='tools',\n        display_name='Tools',\n        input_types=['Tool'],\n        is_list=True,\n        info='Tools at agents disposal',\n        value=[]),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        info='Language model that will run the agent.',\n        input_types=['LanguageModel']),\n        BoolInput(name='memory',\n        display_name='Memory',\n        info='Whether the agent should have memory or not',\n        advanced=True,\n        value=True),\n        BoolInput(name='verbose',\n        display_name='Verbose',\n        advanced=True,\n        value=False),\n        BoolInput(name='allow_delegation',\n        display_name='Allow Delegation',\n        info='Whether the agent is allowed to delegate tasks to other agents.',\n        value=True),\n        BoolInput(name='allow_code_execution',\n        display_name='Allow Code Execution',\n        info='Whether the agent is allowed to execute code.',\n        value=False,\n        advanced=True),\n        DictInput(name='kwargs',\n        display_name='kwargs',\n        info='kwargs of agent.',\n        is_list=True,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Agent',\n        name='output',\n        method='build_output')\n    ]\n\n    def build_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CrewAIAgentComponent", "base_classes": ["Component"], "public_methods": ["def build_output(self)"], "imports": ["from crewai import Agent", "from langflow.base.agents.crewai.crew import convert_llm, convert_tools", "from langflow.custom import Component", "from langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output"], "inputs": "[MultilineInput(name='role', display_name='Role', info='The role of the agent.'), MultilineInput(name='goal', display_name='Goal', info='The objective of the agent.'), MultilineInput(name='backstory', display_name='Backstory', info='The backstory of the agent.'), HandleInput(name='tools', display_name='Tools', input_types=['Tool'], is_list=True, info='Tools at agents disposal', value=[]), HandleInput(name='llm', display_name='Language Model', info='Language model that will run the agent.', input_types=['LanguageModel']), BoolInput(name='memory', display_name='Memory', info='Whether the agent should have memory or not', advanced=True, value=True), BoolInput(name='verbose', display_name='Verbose', advanced=True, value=False), BoolInput(name='allow_delegation', display_name='Allow Delegation', info='Whether the agent is allowed to delegate tasks to other agents.', value=True), BoolInput(name='allow_code_execution', display_name='Allow Code Execution', info='Whether the agent is allowed to execute code.', value=False, advanced=True), DictInput(name='kwargs', display_name='kwargs', info='kwargs of agent.', is_list=True, advanced=True)]", "outputs": "[Output(display_name='Agent', name='output', method='build_output')]", "display_name": "CrewAI Agent", "name": "", "description": "Represents an agent of CrewAI.", "icon": "CrewAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/astradb_cql.py", "section": "class::AstraDBCQLToolComponent", "content": "import json\nimport urllib\nfrom datetime import datetime, timezone\nfrom http import HTTPStatus\nfrom typing import Any\nimport requests\nfrom langchain.pydantic_v1 import BaseModel, Field, create_model\nfrom langchain_core.tools import StructuredTool, Tool\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.io import DictInput, IntInput, SecretStrInput, StrInput, TableInput\nfrom langflow.logging import logger\nfrom langflow.schema import Data\nfrom langflow.schema.table import EditMode\n\nclass AstraDBCQLToolComponent(LCToolComponent):\n    display_name: str = \"Astra DB CQL\"\n    description: str = \"Create a tool to get transactional data from DataStax Astra DB CQL Table\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        StrInput(name='tool_name',\n        display_name='Tool Name',\n        info='The name of the tool.',\n        required=True),\n        StrInput(name='tool_description',\n        display_name='Tool Description',\n        info='The tool description to be passed to the model.',\n        required=True),\n        StrInput(name='keyspace',\n        display_name='Keyspace',\n        value='default_keyspace',\n        info='The keyspace name within Astra DB where the data is stored.',\n        required=True,\n        advanced=True),\n        StrInput(name='table_name',\n        display_name='Table Name',\n        info='The name of the table within Astra DB where the data is stored.',\n        required=True),\n        SecretStrInput(name='token',\n        display_name='Astra DB Application Token',\n        info='Authentication token for accessing Astra DB.',\n        value='ASTRA_DB_APPLICATION_TOKEN',\n        required=True),\n        StrInput(name='api_endpoint',\n        display_name='API Endpoint',\n        info='API endpoint URL for the Astra DB service.',\n        value='ASTRA_DB_API_ENDPOINT',\n        required=True),\n        StrInput(name='projection_fields',\n        display_name='Projection fields',\n        info='Attributes to return separated by comma.',\n        required=True,\n        value='*',\n        advanced=True),\n        TableInput(name='tools_params',\n        display_name='Tools Parameters',\n        info='Define the structure for the tool parameters. Describe the parameters in a way the LLM can understand how to use them. Add the parameters respecting the table schema (Partition Keys,\n        Clustering Keys and Indexed Fields).',\n        required=False,\n        table_schema=[{'name': 'name',\n        'display_name': 'Name',\n        'type': 'str',\n        'description': 'Name of the field/parameter to be used by the model.',\n        'default': 'field',\n        'edit_mode': EditMode.INLINE},\n        {'name': 'field_name',\n        'display_name': 'Field Name',\n        'type': 'str',\n        'description': 'Specify the column name to be filtered on the table. Leave empty if the attribute name is the same as the name of the field.',\n        'default': '',\n        'edit_mode': EditMode.INLINE},\n        {'name': 'description',\n        'display_name': 'Description',\n        'type': 'str',\n        'description': 'Describe the purpose of the parameter.',\n        'default': 'description of tool parameter',\n        'edit_mode': EditMode.POPOVER},\n        {'name': 'mandatory',\n        'display_name': 'Is Mandatory',\n        'type': 'boolean',\n        'edit_mode': EditMode.INLINE,\n        'description': 'Indicate if the field is mandatory.',\n        'options': ['True',\n        'False'],\n        'default': 'False'},\n        {'name': 'is_timestamp',\n        'display_name': 'Is Timestamp',\n        'type': 'boolean',\n        'edit_mode': EditMode.INLINE,\n        'description': 'Indicate if the field is a timestamp.',\n        'options': ['True',\n        'False'],\n        'default': 'False'},\n        {'name': 'operator',\n        'display_name': 'Operator',\n        'type': 'str',\n        'description': 'Set the operator for the field. https://docs.datastax.com/en/astra-db-serverless/api-reference/documents.html#operators',\n        'default': '$eq',\n        'options': ['$gt',\n        '$gte',\n        '$lt',\n        '$lte',\n        '$eq',\n        '$ne',\n        '$in',\n        '$nin',\n        '$exists',\n        '$all',\n        '$size'],\n        'edit_mode': EditMode.INLINE}],\n        value=[]),\n        DictInput(name='partition_keys',\n        display_name='DEPRECATED: Partition Keys',\n        is_list=True,\n        info='Field name and description to the model',\n        required=False,\n        advanced=True),\n        DictInput(name='clustering_keys',\n        display_name='DEPRECATED: Clustering Keys',\n        is_list=True,\n        info='Field name and description to the model',\n        required=False,\n        advanced=True),\n        DictInput(name='static_filters',\n        display_name='Static Filters',\n        is_list=True,\n        advanced=True,\n        info='Field name and value. When filled,\n        it will not be generated by the LLM.'),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=5)\n    ]\n\n    def parse_timestamp(self, timestamp_str):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def astra_rest(self, args):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_args_schema(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def projection_args(self, input_str):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AstraDBCQLToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def parse_timestamp(self, timestamp_str)", "def astra_rest(self, args)", "def create_args_schema(self)", "def build_tool(self)", "def projection_args(self, input_str)", "def run_model(self)"], "imports": ["import json", "import urllib", "from datetime import datetime, timezone", "from http import HTTPStatus", "from typing import Any", "import requests", "from langchain.pydantic_v1 import BaseModel, Field, create_model", "from langchain_core.tools import StructuredTool, Tool", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.io import DictInput, IntInput, SecretStrInput, StrInput, TableInput", "from langflow.logging import logger", "from langflow.schema import Data", "from langflow.schema.table import EditMode"], "inputs": "[StrInput(name='tool_name', display_name='Tool Name', info='The name of the tool.', required=True), StrInput(name='tool_description', display_name='Tool Description', info='The tool description to be passed to the model.', required=True), StrInput(name='keyspace', display_name='Keyspace', value='default_keyspace', info='The keyspace name within Astra DB where the data is stored.', required=True, advanced=True), StrInput(name='table_name', display_name='Table Name', info='The name of the table within Astra DB where the data is stored.', required=True), SecretStrInput(name='token', display_name='Astra DB Application Token', info='Authentication token for accessing Astra DB.', value='ASTRA_DB_APPLICATION_TOKEN', required=True), StrInput(name='api_endpoint', display_name='API Endpoint', info='API endpoint URL for the Astra DB service.', value='ASTRA_DB_API_ENDPOINT', required=True), StrInput(name='projection_fields', display_name='Projection fields', info='Attributes to return separated by comma.', required=True, value='*', advanced=True), TableInput(name='tools_params', display_name='Tools Parameters', info='Define the structure for the tool parameters. Describe the parameters in a way the LLM can understand how to use them. Add the parameters respecting the table schema (Partition Keys, Clustering Keys and Indexed Fields).', required=False, table_schema=[{'name': 'name', 'display_name': 'Name', 'type': 'str', 'description': 'Name of the field/parameter to be used by the model.', 'default': 'field', 'edit_mode': EditMode.INLINE}, {'name': 'field_name', 'display_name': 'Field Name', 'type': 'str', 'description': 'Specify the column name to be filtered on the table. Leave empty if the attribute name is the same as the name of the field.', 'default': '', 'edit_mode': EditMode.INLINE}, {'name': 'description', 'display_name': 'Description', 'type': 'str', 'description': 'Describe the purpose of the parameter.', 'default': 'description of tool parameter', 'edit_mode': EditMode.POPOVER}, {'name': 'mandatory', 'display_name': 'Is Mandatory', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is mandatory.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'is_timestamp', 'display_name': 'Is Timestamp', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is a timestamp.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'operator', 'display_name': 'Operator', 'type': 'str', 'description': 'Set the operator for the field. https://docs.datastax.com/en/astra-db-serverless/api-reference/documents.html#operators', 'default': '$eq', 'options': ['$gt', '$gte', '$lt', '$lte', '$eq', '$ne', '$in', '$nin', '$exists', '$all', '$size'], 'edit_mode': EditMode.INLINE}], value=[]), DictInput(name='partition_keys', display_name='DEPRECATED: Partition Keys', is_list=True, info='Field name and description to the model', required=False, advanced=True), DictInput(name='clustering_keys', display_name='DEPRECATED: Clustering Keys', is_list=True, info='Field name and description to the model', required=False, advanced=True), DictInput(name='static_filters', display_name='Static Filters', is_list=True, advanced=True, info='Field name and value. When filled, it will not be generated by the LLM.'), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=5)]", "outputs": "", "display_name": "Astra DB CQL", "name": "", "description": "Create a tool to get transactional data from DataStax Astra DB CQL Table", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/huggingface_inference_api.py", "section": "class::HuggingFaceInferenceAPIEmbeddingsComponent", "content": "from urllib.parse import urlparse\nimport requests\nfrom langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\nfrom pydantic import SecretStr\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\nclass HuggingFaceInferenceAPIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"HuggingFace Embeddings Inference\"\n    description: str = \"Generate embeddings using HuggingFace Text Embeddings Inference (TEI)\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceInferenceAPIEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        advanced=False,\n        info='Required for non-local inference endpoints. Local inference does not require an API Key.'),\n        MessageTextInput(name='inference_endpoint',\n        display_name='Inference Endpoint',\n        required=True,\n        value='https://api-inference.huggingface.co/models/',\n        info='Custom inference endpoint URL.'),\n        MessageTextInput(name='model_name',\n        display_name='Model Name',\n        value='BAAI/bge-large-en-v1.5',\n        info='The name of the model to use for text embeddings.',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def validate_inference_endpoint(self, inference_endpoint):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_api_url(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_huggingface_embeddings(self, api_key, api_url, model_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HuggingFaceInferenceAPIEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def validate_inference_endpoint(self, inference_endpoint)", "def get_api_url(self)", "def create_huggingface_embeddings(self, api_key, api_url, model_name)", "def build_embeddings(self)"], "imports": ["from urllib.parse import urlparse", "import requests", "from langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings", "from pydantic import SecretStr", "from tenacity import retry, stop_after_attempt, wait_fixed", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import MessageTextInput, Output, SecretStrInput"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', advanced=False, info='Required for non-local inference endpoints. Local inference does not require an API Key.'), MessageTextInput(name='inference_endpoint', display_name='Inference Endpoint', required=True, value='https://api-inference.huggingface.co/models/', info='Custom inference endpoint URL.'), MessageTextInput(name='model_name', display_name='Model Name', value='BAAI/bge-large-en-v1.5', info='The name of the model to use for text embeddings.', required=True)]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "HuggingFace Embeddings Inference", "name": "HuggingFaceInferenceAPIEmbeddings", "description": "Generate embeddings using HuggingFace Text Embeddings Inference (TEI)", "icon": "HuggingFace"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/twelvelabs_pegasus.py", "section": "class::TaskError", "content": "import json\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\n\nclass TaskError(Exception):\n    \"\"\"\n    Error raised when a task fails.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "TaskError", "base_classes": ["Exception"], "public_methods": [], "imports": ["import json", "import subprocess", "import time", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput", "from langflow.io import Output", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/twelvelabs_pegasus.py", "section": "class::TaskTimeoutError", "content": "import json\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\n\nclass TaskTimeoutError(Exception):\n    \"\"\"\n    Error raised when a task times out.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "TaskTimeoutError", "base_classes": ["Exception"], "public_methods": [], "imports": ["import json", "import subprocess", "import time", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput", "from langflow.io import Output", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/twelvelabs_pegasus.py", "section": "class::IndexCreationError", "content": "import json\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\n\nclass IndexCreationError(Exception):\n    \"\"\"\n    Error raised when there's an issue with an index.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "IndexCreationError", "base_classes": ["Exception"], "public_methods": [], "imports": ["import json", "import subprocess", "import time", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput", "from langflow.io import Output", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/twelvelabs_pegasus.py", "section": "class::ApiRequestError", "content": "import json\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\n\nclass ApiRequestError(Exception):\n    \"\"\"\n    Error raised when an API request fails.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "ApiRequestError", "base_classes": ["Exception"], "public_methods": [], "imports": ["import json", "import subprocess", "import time", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput", "from langflow.io import Output", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/twelvelabs_pegasus.py", "section": "class::VideoValidationError", "content": "import json\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\n\nclass VideoValidationError(Exception):\n    \"\"\"\n    Error raised when video validation fails.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "VideoValidationError", "base_classes": ["Exception"], "public_methods": [], "imports": ["import json", "import subprocess", "import time", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput", "from langflow.io import Output", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/twelvelabs_pegasus.py", "section": "class::TwelveLabsPegasus", "content": "import json\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\n\nclass TwelveLabsPegasus(Component):\n    display_name: str = \"Twelve Labs Pegasus\"\n    description: str = \"Chat with videos using Twelve Labs Pegasus API.\"\n    icon = \"TwelveLabs\"\n    name = \"TwelveLabsPegasus\"\n\n    inputs = [\n        DataInput(name='videodata',\n        display_name='Video Data',\n        info='Video Data',\n        is_list=True),\n        SecretStrInput(name='api_key',\n        display_name='Twelve Labs API Key',\n        info='Enter your Twelve Labs API Key.',\n        required=True),\n        MessageInput(name='video_id',\n        display_name='Pegasus Video ID',\n        info='Enter a Video ID for a previously indexed video.'),\n        MessageInput(name='index_name',\n        display_name='Index Name',\n        info=\"Name of the index to use. If the index doesn't exist,\n        it will be created.\",\n        required=False),\n        MessageInput(name='index_id',\n        display_name='Index ID',\n        info='ID of an existing index to use. If provided,\n        index_name will be ignored.',\n        required=False),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        info='Pegasus model to use for indexing',\n        options=['pegasus1.2'],\n        value='pegasus1.2',\n        advanced=False),\n        MultilineInput(name='message',\n        display_name='Prompt',\n        info='Message to chat with the video.',\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.7,\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        info='Controls randomness in responses. Lower values are more deterministic,\n        higher values are more creative.')\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='response',\n        method='process_video',\n        type_=Message),\n        Output(display_name='Video ID',\n        name='processed_video_id',\n        method='get_video_id',\n        type_=Message)\n    ]\n\n    def wait_for_task_completion(self, client, task_id, max_retries, sleep_time):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def validate_video_file(self, filepath):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def on_task_update(self, task):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_video(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_video_id(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TwelveLabsPegasus", "base_classes": ["Component"], "public_methods": ["def wait_for_task_completion(self, client, task_id, max_retries, sleep_time)", "def validate_video_file(self, filepath)", "def on_task_update(self, task)", "def process_video(self)", "def get_video_id(self)"], "imports": ["import json", "import subprocess", "import time", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DataInput, DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput", "from langflow.io import Output", "from langflow.schema.message import Message"], "inputs": "[DataInput(name='videodata', display_name='Video Data', info='Video Data', is_list=True), SecretStrInput(name='api_key', display_name='Twelve Labs API Key', info='Enter your Twelve Labs API Key.', required=True), MessageInput(name='video_id', display_name='Pegasus Video ID', info='Enter a Video ID for a previously indexed video.'), MessageInput(name='index_name', display_name='Index Name', info=\"Name of the index to use. If the index doesn't exist, it will be created.\", required=False), MessageInput(name='index_id', display_name='Index ID', info='ID of an existing index to use. If provided, index_name will be ignored.', required=False), DropdownInput(name='model_name', display_name='Model', info='Pegasus model to use for indexing', options=['pegasus1.2'], value='pegasus1.2', advanced=False), MultilineInput(name='message', display_name='Prompt', info='Message to chat with the video.', required=True), SliderInput(name='temperature', display_name='Temperature', value=0.7, range_spec=RangeSpec(min=0, max=1, step=0.01), info='Controls randomness in responses. Lower values are more deterministic, higher values are more creative.')]", "outputs": "[Output(display_name='Message', name='response', method='process_video', type_=Message), Output(display_name='Video ID', name='processed_video_id', method='get_video_id', type_=Message)]", "display_name": "Twelve Labs Pegasus", "name": "TwelveLabsPegasus", "description": "Chat with videos using Twelve Labs Pegasus API.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/homeassistant/home_assistant_control.py", "section": "class::HomeAssistantControl", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass HomeAssistantControl(LCToolComponent):\n    \"\"\"\n    This tool is used to control Home Assistant devices.\n    \n    A very simple tool to control Home Assistant devices.\n    - The agent only needs to provide action (turn_on, turn_off, toggle) + entity_id (e.g., switch.xxx, light.xxx).\n    - The domain (e.g., 'switch', 'light') is automatically extracted from entity_id.\n    \"\"\"\n\n    display_name: str = \"Home Assistant Control\"\n    description: str = \"A very simple tool to control Home Assistant devices. Only action (turn_on, turn_off, toggle) and entity_id need to be provided.\"\n    icon = \"HomeAssistant\"\n\n    inputs = [\n        SecretStrInput(name='ha_token',\n        display_name='Home Assistant Token',\n        info='Home Assistant Long-Lived Access Token',\n        required=True),\n        StrInput(name='base_url',\n        display_name='Home Assistant URL',\n        info='e.g.,\n        http://192.168.0.10:8123',\n        required=True),\n        StrInput(name='default_action',\n        display_name='Default Action (Optional)',\n        info='One of turn_on,\n        turn_off,\n        toggle',\n        required=False),\n        StrInput(name='default_entity_id',\n        display_name='Default Entity ID (Optional)',\n        info='Default entity ID to control (e.g.,\n        switch.unknown_switch_3)',\n        required=False)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HomeAssistantControl", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='ha_token', display_name='Home Assistant Token', info='Home Assistant Long-Lived Access Token', required=True), StrInput(name='base_url', display_name='Home Assistant URL', info='e.g., http://192.168.0.10:8123', required=True), StrInput(name='default_action', display_name='Default Action (Optional)', info='One of turn_on, turn_off, toggle', required=False), StrInput(name='default_entity_id', display_name='Default Entity ID (Optional)', info='Default entity ID to control (e.g., switch.unknown_switch_3)', required=False)]", "outputs": "", "display_name": "Home Assistant Control", "name": "", "description": "A very simple tool to control Home Assistant devices. Only action (turn_on, turn_off, toggle) and entity_id need to be provided.", "icon": "HomeAssistant"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/homeassistant/home_assistant_control.py", "section": "class::ToolSchema", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ToolSchema(BaseModel):\n    \"\"\"\n    Parameters to be passed by the agent: action, entity_id only.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "ToolSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/amazon/amazon_bedrock_embedding.py", "section": "class::AmazonBedrockEmbeddingsComponent", "content": "from langflow.base.models.aws_constants import AWS_EMBEDDING_MODEL_IDS, AWS_REGIONS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs import SecretStrInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langchain_aws import BedrockEmbeddings\nimport boto3\n\nclass AmazonBedrockEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Amazon Bedrock Embeddings\"\n    description: str = \"Generate embeddings using Amazon Bedrock models.\"\n    icon = \"Amazon\"\n    name = \"AmazonBedrockEmbeddings\"\n\n    inputs = [\n        DropdownInput(name='model_id',\n        display_name='Model Id',\n        options=AWS_EMBEDDING_MODEL_IDS,\n        value='amazon.titan-embed-text-v1'),\n        SecretStrInput(name='aws_access_key_id',\n        display_name='AWS Access Key ID',\n        info=\"The access key for your AWS account.Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.\",\n        value='AWS_ACCESS_KEY_ID',\n        required=True),\n        SecretStrInput(name='aws_secret_access_key',\n        display_name='AWS Secret Access Key',\n        info=\"The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.\",\n        value='AWS_SECRET_ACCESS_KEY',\n        required=True),\n        SecretStrInput(name='aws_session_token',\n        display_name='AWS Session Token',\n        advanced=False,\n        info=\"The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.\",\n        value='AWS_SESSION_TOKEN'),\n        SecretStrInput(name='credentials_profile_name',\n        display_name='Credentials Profile Name',\n        advanced=True,\n        info='The name of the profile to use from your ~/.aws/credentials file. If not provided,\n        the default profile will be used.',\n        value='AWS_CREDENTIALS_PROFILE_NAME'),\n        DropdownInput(name='region_name',\n        display_name='Region Name',\n        value='us-east-1',\n        options=AWS_REGIONS,\n        info='The AWS region where your Bedrock resources are located.'),\n        MessageTextInput(name='endpoint_url',\n        display_name='Endpoint URL',\n        advanced=True,\n        info='The URL of the AWS Bedrock endpoint to use.')\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AmazonBedrockEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langflow.base.models.aws_constants import AWS_EMBEDDING_MODEL_IDS, AWS_REGIONS", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import Embeddings", "from langflow.inputs import SecretStrInput", "from langflow.io import DropdownInput, MessageTextInput, Output", "from langchain_aws import BedrockEmbeddings", "import boto3"], "inputs": "[DropdownInput(name='model_id', display_name='Model Id', options=AWS_EMBEDDING_MODEL_IDS, value='amazon.titan-embed-text-v1'), SecretStrInput(name='aws_access_key_id', display_name='AWS Access Key ID', info=\"The access key for your AWS account.Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.\", value='AWS_ACCESS_KEY_ID', required=True), SecretStrInput(name='aws_secret_access_key', display_name='AWS Secret Access Key', info=\"The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.\", value='AWS_SECRET_ACCESS_KEY', required=True), SecretStrInput(name='aws_session_token', display_name='AWS Session Token', advanced=False, info=\"The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.\", value='AWS_SESSION_TOKEN'), SecretStrInput(name='credentials_profile_name', display_name='Credentials Profile Name', advanced=True, info='The name of the profile to use from your ~/.aws/credentials file. If not provided, the default profile will be used.', value='AWS_CREDENTIALS_PROFILE_NAME'), DropdownInput(name='region_name', display_name='Region Name', value='us-east-1', options=AWS_REGIONS, info='The AWS region where your Bedrock resources are located.'), MessageTextInput(name='endpoint_url', display_name='Endpoint URL', advanced=True, info='The URL of the AWS Bedrock endpoint to use.')]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "Amazon Bedrock Embeddings", "name": "AmazonBedrockEmbeddings", "description": "Generate embeddings using Amazon Bedrock models.", "icon": "Amazon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/llm_math.py", "section": "class::LLMMathChainComponent", "content": "from langchain.chains import LLMMathChain\nfrom langflow.base.chains.model import LCChainComponent\nfrom langflow.field_typing import Message\nfrom langflow.inputs import HandleInput, MultilineInput\nfrom langflow.template import Output\n\nclass LLMMathChainComponent(LCChainComponent):\n    display_name: str = \"LLMMathChain\"\n    description: str = \"Chain that interprets a prompt and executes python code to do math.\"\n    icon = \"LangChain\"\n    name = \"LLMMathChain\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input',\n        info='The input value to pass to the chain.',\n        required=True),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='text',\n        method='invoke_chain')\n    ]\n\n    def invoke_chain(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LLMMathChainComponent", "base_classes": ["LCChainComponent"], "public_methods": ["def invoke_chain(self)"], "imports": ["from langchain.chains import LLMMathChain", "from langflow.base.chains.model import LCChainComponent", "from langflow.field_typing import Message", "from langflow.inputs import HandleInput, MultilineInput", "from langflow.template import Output"], "inputs": "[MultilineInput(name='input_value', display_name='Input', info='The input value to pass to the chain.', required=True), HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True)]", "outputs": "[Output(display_name='Message', name='text', method='invoke_chain')]", "display_name": "LLMMathChain", "name": "LLMMathChain", "description": "Chain that interprets a prompt and executes python code to do math.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/lambda_filter.py", "section": "class::LambdaFilterComponent", "content": "from __future__ import annotations\nimport json\nimport re\nfrom typing import TYPE_CHECKING, Any\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, HandleInput, IntInput, MultilineInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.utils.data_structure import get_data_structure\nfrom collections.abc import Callable\n\nclass LambdaFilterComponent(Component):\n    display_name: str = \"Lambda Filter\"\n    description: str = \"Uses an LLM to generate a lambda function for filtering or transforming structured data.\"\n    icon = \"filter\"\n    name = \"LambdaFilter\"\n\n    inputs = [\n        DataInput(name='data',\n        display_name='Data',\n        info='The structured data to filter or transform using a lambda function.',\n        is_list=True,\n        required=True),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        info=\"Connect the 'Language Model' output from your LLM component here.\",\n        input_types=['LanguageModel'],\n        required=True),\n        MultilineInput(name='filter_instruction',\n        display_name='Instructions',\n        info=\"Natural language instructions for how to filter or transform the data using a lambda function. Example: Filter the data to only include items where the 'status' is 'active'.\",\n        value='Filter the data to...',\n        required=True),\n        IntInput(name='sample_size',\n        display_name='Sample Size',\n        info='For large datasets,\n        number of items to sample from head/tail.',\n        value=1000,\n        advanced=True),\n        IntInput(name='max_size',\n        display_name='Max Size',\n        info='Number of characters for the data to be considered large.',\n        value=30000,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Filtered Data',\n        name='filtered_data',\n        method='filter_data'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def get_data_structure(self, data):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LambdaFilterComponent", "base_classes": ["Component"], "public_methods": ["def get_data_structure(self, data)"], "imports": ["from __future__ import annotations", "import json", "import re", "from typing import TYPE_CHECKING, Any", "from langflow.custom import Component", "from langflow.io import DataInput, HandleInput, IntInput, MultilineInput, Output", "from langflow.schema import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.utils.data_structure import get_data_structure", "from collections.abc import Callable"], "inputs": "[DataInput(name='data', display_name='Data', info='The structured data to filter or transform using a lambda function.', is_list=True, required=True), HandleInput(name='llm', display_name='Language Model', info=\"Connect the 'Language Model' output from your LLM component here.\", input_types=['LanguageModel'], required=True), MultilineInput(name='filter_instruction', display_name='Instructions', info=\"Natural language instructions for how to filter or transform the data using a lambda function. Example: Filter the data to only include items where the 'status' is 'active'.\", value='Filter the data to...', required=True), IntInput(name='sample_size', display_name='Sample Size', info='For large datasets, number of items to sample from head/tail.', value=1000, advanced=True), IntInput(name='max_size', display_name='Max Size', info='Number of characters for the data to be considered large.', value=30000, advanced=True)]", "outputs": "[Output(display_name='Filtered Data', name='filtered_data', method='filter_data'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Lambda Filter", "name": "LambdaFilter", "description": "Uses an LLM to generate a lambda function for filtering or transforming structured data.", "icon": "filter"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/agentql/agentql_api.py", "section": "class::AgentQL", "content": "import httpx\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import Data\n\nclass AgentQL(Component):\n    display_name: str = \"Extract Web Data\"\n    description: str = \"Extracts structured data from a web page using an AgentQL query or a Natural Language description.\"\n    icon = \"AgentQL\"\n    name = \"AgentQL\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        password=True,\n        info='Your AgentQL API key from dev.agentql.com'),\n        MessageTextInput(name='url',\n        display_name='URL',\n        required=True,\n        info='The URL of the public web page you want to extract data from.',\n        tool_mode=True),\n        MultilineInput(name='query',\n        display_name='AgentQL Query',\n        required=False,\n        info='The AgentQL query to execute. Learn more at https://docs.agentql.com/agentql-query or use a prompt.',\n        tool_mode=True),\n        MultilineInput(name='prompt',\n        display_name='Prompt',\n        required=False,\n        info='A Natural Language description of the data to extract from the page. Alternative to AgentQL query.',\n        tool_mode=True),\n        BoolInput(name='is_stealth_mode_enabled',\n        display_name='Enable Stealth Mode (Beta)',\n        info='Enable experimental anti-bot evasion strategies. May not work for all websites at all times.',\n        value=False,\n        advanced=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='Seconds to wait for a request.',\n        value=900,\n        advanced=True),\n        DropdownInput(name='mode',\n        display_name='Request Mode',\n        info=\"'standard' uses deep data analysis,\n        while 'fast' trades some depth of analysis for speed.\",\n        options=['fast',\n        'standard'],\n        value='fast',\n        advanced=True),\n        IntInput(name='wait_for',\n        display_name='Wait For',\n        info='Seconds to wait for the page to load before extracting data.',\n        value=0,\n        range_spec=RangeSpec(min=0,\n        max=10,\n        step_type='int'),\n        advanced=True),\n        BoolInput(name='is_scroll_to_bottom_enabled',\n        display_name='Enable scroll to bottom',\n        info='Scroll to bottom of the page before extracting data.',\n        value=False,\n        advanced=True),\n        BoolInput(name='is_screenshot_enabled',\n        display_name='Enable screenshot',\n        info=\"Take a screenshot before extracting data. Returned in 'metadata' as a Base64 string.\",\n        value=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='build_output')\n    ]\n\n    def build_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AgentQL", "base_classes": ["Component"], "public_methods": ["def build_output(self)"], "imports": ["import httpx", "from loguru import logger", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='Your AgentQL API key from dev.agentql.com'), MessageTextInput(name='url', display_name='URL', required=True, info='The URL of the public web page you want to extract data from.', tool_mode=True), MultilineInput(name='query', display_name='AgentQL Query', required=False, info='The AgentQL query to execute. Learn more at https://docs.agentql.com/agentql-query or use a prompt.', tool_mode=True), MultilineInput(name='prompt', display_name='Prompt', required=False, info='A Natural Language description of the data to extract from the page. Alternative to AgentQL query.', tool_mode=True), BoolInput(name='is_stealth_mode_enabled', display_name='Enable Stealth Mode (Beta)', info='Enable experimental anti-bot evasion strategies. May not work for all websites at all times.', value=False, advanced=True), IntInput(name='timeout', display_name='Timeout', info='Seconds to wait for a request.', value=900, advanced=True), DropdownInput(name='mode', display_name='Request Mode', info=\"'standard' uses deep data analysis, while 'fast' trades some depth of analysis for speed.\", options=['fast', 'standard'], value='fast', advanced=True), IntInput(name='wait_for', display_name='Wait For', info='Seconds to wait for the page to load before extracting data.', value=0, range_spec=RangeSpec(min=0, max=10, step_type='int'), advanced=True), BoolInput(name='is_scroll_to_bottom_enabled', display_name='Enable scroll to bottom', info='Scroll to bottom of the page before extracting data.', value=False, advanced=True), BoolInput(name='is_screenshot_enabled', display_name='Enable screenshot', info=\"Take a screenshot before extracting data. Returned in 'metadata' as a Base64 string.\", value=False, advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='build_output')]", "display_name": "Extract Web Data", "name": "AgentQL", "description": "Extracts structured data from a web page using an AgentQL query or a Natural Language description.", "icon": "AgentQL"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/sub_flow.py", "section": "class::SubFlowComponent", "content": "from typing import Any\nfrom loguru import logger\nfrom langflow.base.flow_processing.utils import build_data_from_result_data\nfrom langflow.custom import Component\nfrom langflow.graph.graph.base import Graph\nfrom langflow.graph.vertex.base import Vertex\nfrom langflow.helpers.flow import get_flow_inputs\nfrom langflow.io import DropdownInput, Output\nfrom langflow.schema import Data, dotdict\n\nclass SubFlowComponent(Component):\n    display_name: str = \"Sub Flow [Deprecated]\"\n    description: str = \"Generates a Component from a Flow, with all of its inputs, and \"\n    icon = \"Workflow\"\n    name = \"SubFlow\"\n\n    inputs = [\n        DropdownInput(name='flow_name',\n        display_name='Flow Name',\n        info='The name of the flow to run.',\n        options=[],\n        refresh_button=True,\n        real_time_refresh=True)\n    ]\n\n    outputs = [\n        Output(name='flow_outputs',\n        display_name='Flow Outputs',\n        method='generate_results')\n    ]\n\n    def add_inputs_to_build_config(self, inputs_vertex, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SubFlowComponent", "base_classes": ["Component"], "public_methods": ["def add_inputs_to_build_config(self, inputs_vertex, build_config)"], "imports": ["from typing import Any", "from loguru import logger", "from langflow.base.flow_processing.utils import build_data_from_result_data", "from langflow.custom import Component", "from langflow.graph.graph.base import Graph", "from langflow.graph.vertex.base import Vertex", "from langflow.helpers.flow import get_flow_inputs", "from langflow.io import DropdownInput, Output", "from langflow.schema import Data, dotdict"], "inputs": "[DropdownInput(name='flow_name', display_name='Flow Name', info='The name of the flow to run.', options=[], refresh_button=True, real_time_refresh=True)]", "outputs": "[Output(name='flow_outputs', display_name='Flow Outputs', method='generate_results')]", "display_name": "Sub Flow [Deprecated]", "name": "SubFlow", "description": "Generates a Component from a Flow, with all of its inputs, and ", "icon": "Workflow"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/google_bq_sql_executor.py", "section": "class::BigQueryExecutorComponent", "content": "import json\nimport re\nfrom pathlib import Path\nfrom google.auth.exceptions import RefreshError\nfrom google.cloud import bigquery\nfrom google.oauth2.service_account import Credentials\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, MessageTextInput, Output\nfrom langflow.schema.dataframe import DataFrame\n\nclass BigQueryExecutorComponent(Component):\n    display_name: str = \"BigQuery\"\n    description: str = \"Execute SQL queries on Google BigQuery.\"\n    icon = \"Google\"\n    name = \"BigQueryExecutor\"\n\n    inputs = [\n        FileInput(name='service_account_json_file',\n        display_name='Upload Service Account JSON',\n        info='Upload the JSON file containing Google Cloud service account credentials.',\n        file_types=['json'],\n        required=True),\n        MessageTextInput(name='query',\n        display_name='SQL Query',\n        info='The SQL query to execute on BigQuery.',\n        required=True,\n        tool_mode=True),\n        BoolInput(name='clean_query',\n        display_name='Clean Query',\n        info='When enabled,\n        this will automatically clean up your SQL query.',\n        value=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Query Results',\n        name='query_results',\n        method='execute_sql')\n    ]\n\n    def execute_sql(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "BigQueryExecutorComponent", "base_classes": ["Component"], "public_methods": ["def execute_sql(self)"], "imports": ["import json", "import re", "from pathlib import Path", "from google.auth.exceptions import RefreshError", "from google.cloud import bigquery", "from google.oauth2.service_account import Credentials", "from langflow.custom import Component", "from langflow.io import BoolInput, FileInput, MessageTextInput, Output", "from langflow.schema.dataframe import DataFrame"], "inputs": "[FileInput(name='service_account_json_file', display_name='Upload Service Account JSON', info='Upload the JSON file containing Google Cloud service account credentials.', file_types=['json'], required=True), MessageTextInput(name='query', display_name='SQL Query', info='The SQL query to execute on BigQuery.', required=True, tool_mode=True), BoolInput(name='clean_query', display_name='Clean Query', info='When enabled, this will automatically clean up your SQL query.', value=False, advanced=True)]", "outputs": "[Output(display_name='Query Results', name='query_results', method='execute_sql')]", "display_name": "BigQuery", "name": "BigQueryExecutor", "description": "Execute SQL queries on Google BigQuery.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/agents/agent.py", "section": "class::AgentComponent", "content": "from langchain_core.tools import StructuredTool\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import ALL_PROVIDER_FIELDS, MODEL_DYNAMIC_UPDATE_FIELDS, MODEL_PROVIDERS_DICT, MODELS_METADATA\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.io import BoolInput, DropdownInput, MultilineInput, Output\nfrom langflow.logging import logger\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    icon = \"bot\"\n    name = \"Agent\"\n\n    inputs = [\n        DropdownInput(name='agent_llm',\n        display_name='Model Provider',\n        info='The provider of the language model that the agent will use to generate responses.',\n        options=[*sorted(MODEL_PROVIDERS_DICT.keys()),\n        'Custom'],\n        value='OpenAI',\n        real_time_refresh=True,\n        input_types=[],\n        options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())] + [{'icon': 'brain'}]),\n        *MODEL_PROVIDERS_DICT['OpenAI']['inputs'],\n        MultilineInput(name='system_prompt',\n        display_name='Agent Instructions',\n        info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n        value='You are a helpful assistant that can use tools to answer questions and perform tasks.',\n        advanced=False),\n        *LCToolsAgentComponent._base_inputs,\n        *memory_inputs,\n        BoolInput(name='add_current_date_tool',\n        display_name='Current Date',\n        advanced=True,\n        info='If true,\n        will add a tool to the agent that returns the current date.',\n        value=True)\n    ]\n\n    outputs = [\n        Output(name='response',\n        display_name='Response',\n        method='message_response')\n    ]\n\n    def get_llm(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def set_component_params(self, component):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def delete_fields(self, build_config, fields):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_input_types(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AgentComponent", "base_classes": ["ToolCallingAgentComponent"], "public_methods": ["def get_llm(self)", "def set_component_params(self, component)", "def delete_fields(self, build_config, fields)", "def update_input_types(self, build_config)"], "imports": ["from langchain_core.tools import StructuredTool", "from langflow.base.agents.agent import LCToolsAgentComponent", "from langflow.base.agents.events import ExceptionWithMessageError", "from langflow.base.models.model_input_constants import ALL_PROVIDER_FIELDS, MODEL_DYNAMIC_UPDATE_FIELDS, MODEL_PROVIDERS_DICT, MODELS_METADATA", "from langflow.base.models.model_utils import get_model_name", "from langflow.components.helpers import CurrentDateComponent", "from langflow.components.helpers.memory import MemoryComponent", "from langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent", "from langflow.custom.utils import update_component_build_config", "from langflow.io import BoolInput, DropdownInput, MultilineInput, Output", "from langflow.logging import logger", "from langflow.schema.dotdict import dotdict", "from langflow.schema.message import Message"], "inputs": "[DropdownInput(name='agent_llm', display_name='Model Provider', info='The provider of the language model that the agent will use to generate responses.', options=[*sorted(MODEL_PROVIDERS_DICT.keys()), 'Custom'], value='OpenAI', real_time_refresh=True, input_types=[], options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())] + [{'icon': 'brain'}]), *MODEL_PROVIDERS_DICT['OpenAI']['inputs'], MultilineInput(name='system_prompt', display_name='Agent Instructions', info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\", value='You are a helpful assistant that can use tools to answer questions and perform tasks.', advanced=False), *LCToolsAgentComponent._base_inputs, *memory_inputs, BoolInput(name='add_current_date_tool', display_name='Current Date', advanced=True, info='If true, will add a tool to the agent that returns the current date.', value=True)]", "outputs": "[Output(name='response', display_name='Response', method='message_response')]", "display_name": "Agent", "name": "Agent", "description": "Define the agent's instructions, then enter a task to complete using tools.", "icon": "bot"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/baidu_qianfan_chat.py", "section": "class::QianfanChatEndpointComponent", "content": "from langchain_community.chat_models.baidu_qianfan_endpoint import QianfanChatEndpoint\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing.constants import LanguageModel\nfrom langflow.io import DropdownInput, FloatInput, MessageTextInput, SecretStrInput\n\nclass QianfanChatEndpointComponent(LCModelComponent):\n    display_name: str = \"Qianfan\"\n    description: str = \"Generate text using Baidu Qianfan LLMs.\"\n    icon = \"BaiduQianfan\"\n    name = \"BaiduQianfanChatModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(name='model',\n        display_name='Model Name',\n        options=['EB-turbo-AppBuilder',\n        'Llama-2-70b-chat',\n        'ERNIE-Bot-turbo-AI',\n        'ERNIE-Lite-8K-0308',\n        'ERNIE-Speed',\n        'Qianfan-Chinese-Llama-2-13B',\n        'ERNIE-3.5-8K',\n        'BLOOMZ-7B',\n        'Qianfan-Chinese-Llama-2-7B',\n        'XuanYuan-70B-Chat-4bit',\n        'AquilaChat-7B',\n        'ERNIE-Bot-4',\n        'Llama-2-13b-chat',\n        'ChatGLM2-6B-32K',\n        'ERNIE-Bot',\n        'ERNIE-Speed-128k',\n        'ERNIE-4.0-8K',\n        'Qianfan-BLOOMZ-7B-compressed',\n        'ERNIE Speed',\n        'Llama-2-7b-chat',\n        'Mixtral-8x7B-Instruct',\n        'ERNIE 3.5',\n        'ERNIE Speed-AppBuilder',\n        'ERNIE-Speed-8K',\n        'Yi-34B-Chat'],\n        info='https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint',\n        value='ERNIE-4.0-8K'),\n        SecretStrInput(name='qianfan_ak',\n        display_name='Qianfan Ak',\n        info='which you could get from  https://cloud.baidu.com/product/wenxinworkshop'),\n        SecretStrInput(name='qianfan_sk',\n        display_name='Qianfan Sk',\n        info='which you could get from  https://cloud.baidu.com/product/wenxinworkshop'),\n        FloatInput(name='top_p',\n        display_name='Top p',\n        info='Model params,\n        only supported in ERNIE-Bot and ERNIE-Bot-turbo',\n        value=0.8,\n        advanced=True),\n        FloatInput(name='temperature',\n        display_name='Temperature',\n        info='Model params,\n        only supported in ERNIE-Bot and ERNIE-Bot-turbo',\n        value=0.95),\n        FloatInput(name='penalty_score',\n        display_name='Penalty Score',\n        info='Model params,\n        only supported in ERNIE-Bot and ERNIE-Bot-turbo',\n        value=1.0,\n        advanced=True),\n        MessageTextInput(name='endpoint',\n        display_name='Endpoint',\n        info='Endpoint of the Qianfan LLM,\n        required if custom model used.')\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "QianfanChatEndpointComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_community.chat_models.baidu_qianfan_endpoint import QianfanChatEndpoint", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing.constants import LanguageModel", "from langflow.io import DropdownInput, FloatInput, MessageTextInput, SecretStrInput"], "inputs": "[*LCModelComponent._base_inputs, DropdownInput(name='model', display_name='Model Name', options=['EB-turbo-AppBuilder', 'Llama-2-70b-chat', 'ERNIE-Bot-turbo-AI', 'ERNIE-Lite-8K-0308', 'ERNIE-Speed', 'Qianfan-Chinese-Llama-2-13B', 'ERNIE-3.5-8K', 'BLOOMZ-7B', 'Qianfan-Chinese-Llama-2-7B', 'XuanYuan-70B-Chat-4bit', 'AquilaChat-7B', 'ERNIE-Bot-4', 'Llama-2-13b-chat', 'ChatGLM2-6B-32K', 'ERNIE-Bot', 'ERNIE-Speed-128k', 'ERNIE-4.0-8K', 'Qianfan-BLOOMZ-7B-compressed', 'ERNIE Speed', 'Llama-2-7b-chat', 'Mixtral-8x7B-Instruct', 'ERNIE 3.5', 'ERNIE Speed-AppBuilder', 'ERNIE-Speed-8K', 'Yi-34B-Chat'], info='https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint', value='ERNIE-4.0-8K'), SecretStrInput(name='qianfan_ak', display_name='Qianfan Ak', info='which you could get from  https://cloud.baidu.com/product/wenxinworkshop'), SecretStrInput(name='qianfan_sk', display_name='Qianfan Sk', info='which you could get from  https://cloud.baidu.com/product/wenxinworkshop'), FloatInput(name='top_p', display_name='Top p', info='Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo', value=0.8, advanced=True), FloatInput(name='temperature', display_name='Temperature', info='Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo', value=0.95), FloatInput(name='penalty_score', display_name='Penalty Score', info='Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo', value=1.0, advanced=True), MessageTextInput(name='endpoint', display_name='Endpoint', info='Endpoint of the Qianfan LLM, required if custom model used.')]", "outputs": "", "display_name": "Qianfan", "name": "BaiduQianfanChatModel", "description": "Generate text using Baidu Qianfan LLMs.", "icon": "BaiduQianfan"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/channel.py", "section": "class::YouTubeChannelComponent", "content": "from typing import Any\nfrom urllib.error import HTTPError\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import DataFrame\nfrom langflow.template import Output\nimport re\n\nclass YouTubeChannelComponent(Component):\n    \"\"\"\n    A component that retrieves detailed information about YouTube channels.\n    \"\"\"\n\n    display_name: str = \"YouTube Channel\"\n    description: str = \"Retrieves detailed information and statistics about YouTube channels as a DataFrame.\"\n    icon = \"YouTube\"\n\n    inputs = [\n        MessageTextInput(name='channel_url',\n        display_name='Channel URL or ID',\n        info='The URL or ID of the YouTube channel.',\n        tool_mode=True,\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='YouTube API Key',\n        info='Your YouTube Data API key.',\n        required=True),\n        BoolInput(name='include_statistics',\n        display_name='Include Statistics',\n        value=True,\n        info='Include channel statistics (views,\n        subscribers,\n        videos).'),\n        BoolInput(name='include_branding',\n        display_name='Include Branding',\n        value=True,\n        info='Include channel branding settings (banner,\n        thumbnails).',\n        advanced=True),\n        BoolInput(name='include_playlists',\n        display_name='Include Playlists',\n        value=False,\n        info=\"Include channel's public playlists.\",\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='channel_df',\n        display_name='Channel Info',\n        method='get_channel_info')\n    ]\n\n    def get_channel_info(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubeChannelComponent", "base_classes": ["Component"], "public_methods": ["def get_channel_info(self)"], "imports": ["from typing import Any", "from urllib.error import HTTPError", "import pandas as pd", "from googleapiclient.discovery import build", "from googleapiclient.errors import HttpError", "from langflow.custom import Component", "from langflow.inputs import BoolInput, MessageTextInput, SecretStrInput", "from langflow.schema import DataFrame", "from langflow.template import Output", "import re"], "inputs": "[MessageTextInput(name='channel_url', display_name='Channel URL or ID', info='The URL or ID of the YouTube channel.', tool_mode=True, required=True), SecretStrInput(name='api_key', display_name='YouTube API Key', info='Your YouTube Data API key.', required=True), BoolInput(name='include_statistics', display_name='Include Statistics', value=True, info='Include channel statistics (views, subscribers, videos).'), BoolInput(name='include_branding', display_name='Include Branding', value=True, info='Include channel branding settings (banner, thumbnails).', advanced=True), BoolInput(name='include_playlists', display_name='Include Playlists', value=False, info=\"Include channel's public playlists.\", advanced=True)]", "outputs": "[Output(name='channel_df', display_name='Channel Info', method='get_channel_info')]", "display_name": "YouTube Channel", "name": "", "description": "Retrieves detailed information and statistics about YouTube channels as a DataFrame.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/update_page_property.py", "section": "class::NotionPageUpdate", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionPageUpdate(LCToolComponent):\n    display_name: str = \"Update Page Property \"\n    description: str = \"Update the properties of a Notion page.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        StrInput(name='page_id',\n        display_name='Page ID',\n        info='The ID of the Notion page to update.'),\n        MultilineInput(name='properties',\n        display_name='Properties',\n        info='The properties to update on the page (as a JSON string or a dictionary).'),\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionPageUpdate", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='page_id', display_name='Page ID', info='The ID of the Notion page to update.'), MultilineInput(name='properties', display_name='Properties', info='The properties to update on the page (as a JSON string or a dictionary).'), SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True)]", "outputs": "", "display_name": "Update Page Property ", "name": "", "description": "Update the properties of a Notion page.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/update_page_property.py", "section": "class::NotionPageUpdateSchema", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionPageUpdateSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionPageUpdateSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/create_assistant.py", "section": "class::AssistantsCreateAssistant", "content": "from loguru import logger\nfrom langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput, StrInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass AssistantsCreateAssistant(ComponentWithCache):\n    display_name: str = \"Create Assistant\"\n    description: str = \"Creates an Assistant and returns it's id\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        StrInput(name='assistant_name',\n        display_name='Assistant Name',\n        info='Name for the assistant being created'),\n        StrInput(name='instructions',\n        display_name='Instructions',\n        info='Instructions for the assistant,\n        think of these as the system prompt.'),\n        StrInput(name='model',\n        display_name='Model name',\n        info='Model for the assistant.\\n\\nEnvironment variables for provider credentials can be set with the Dotenv Component.\\n\\nModels are supported via LiteLLM,\n        see (https://docs.litellm.ai/docs/providers) for supported model names and env vars.'),\n        MultilineInput(name='env_set',\n        display_name='Environment Set',\n        info='Dummy input to allow chaining with Dotenv Component.')\n    ]\n\n    outputs = [\n        Output(display_name='Assistant ID',\n        name='assistant_id',\n        method='process_inputs')\n    ]\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssistantsCreateAssistant", "base_classes": ["ComponentWithCache"], "public_methods": ["def process_inputs(self)"], "imports": ["from loguru import logger", "from langflow.base.astra_assistants.util import get_patched_openai_client", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.inputs import MultilineInput, StrInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[StrInput(name='assistant_name', display_name='Assistant Name', info='Name for the assistant being created'), StrInput(name='instructions', display_name='Instructions', info='Instructions for the assistant, think of these as the system prompt.'), StrInput(name='model', display_name='Model name', info='Model for the assistant.\\n\\nEnvironment variables for provider credentials can be set with the Dotenv Component.\\n\\nModels are supported via LiteLLM, see (https://docs.litellm.ai/docs/providers) for supported model names and env vars.'), MultilineInput(name='env_set', display_name='Environment Set', info='Dummy input to allow chaining with Dotenv Component.')]", "outputs": "[Output(display_name='Assistant ID', name='assistant_id', method='process_inputs')]", "display_name": "Create Assistant", "name": "", "description": "Creates an Assistant and returns it's id", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/vectara_rag.py", "section": "class::VectaraRagComponent", "content": "from langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput, StrInput\nfrom langflow.schema.message import Message\nfrom langchain_community.vectorstores import Vectara\nfrom langchain_community.vectorstores.vectara import RerankConfig, SummaryConfig, VectaraQueryConfig\n\nclass VectaraRagComponent(Component):\n    display_name: str = \"Vectara RAG\"\n    description: str = \"Vectara's full end to end RAG\"\n    icon = \"Vectara\"\n    name = \"VectaraRAG\"\n\n    inputs = [\n        StrInput(name='vectara_customer_id',\n        display_name='Vectara Customer ID',\n        required=True),\n        StrInput(name='vectara_corpus_id',\n        display_name='Vectara Corpus ID',\n        required=True),\n        SecretStrInput(name='vectara_api_key',\n        display_name='Vectara API Key',\n        required=True),\n        MessageTextInput(name='search_query',\n        display_name='Search Query',\n        info='The query to receive an answer on.',\n        tool_mode=True),\n        FloatInput(name='lexical_interpolation',\n        display_name='Hybrid Search Factor',\n        range_spec=RangeSpec(min=0.005,\n        max=0.1,\n        step=0.005),\n        value=0.005,\n        advanced=True,\n        info='How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all,\n        and 1 means only lexical search is used.'),\n        MessageTextInput(name='filter',\n        display_name='Metadata Filters',\n        value='',\n        advanced=True,\n        info='The filter string to narrow the search to according to metadata attributes.'),\n        DropdownInput(name='reranker',\n        display_name='Reranker Type',\n        options=RERANKER_TYPES,\n        value=RERANKER_TYPES[0],\n        info='How to rerank the retrieved search results.'),\n        IntInput(name='reranker_k',\n        display_name='Number of Results to Rerank',\n        value=50,\n        range_spec=RangeSpec(min=1,\n        max=100,\n        step=1),\n        advanced=True),\n        FloatInput(name='diversity_bias',\n        display_name='Diversity Bias',\n        value=0.2,\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True,\n        info='Ranges from 0 to 1,\n        with higher values indicating greater diversity (only applies to MMR reranker).'),\n        IntInput(name='max_results',\n        display_name='Max Results to Summarize',\n        value=7,\n        range_spec=RangeSpec(min=1,\n        max=100,\n        step=1),\n        advanced=True,\n        info='The maximum number of search results to be available to the prompt.'),\n        DropdownInput(name='response_lang',\n        display_name='Response Language',\n        options=RESPONSE_LANGUAGES,\n        value='eng',\n        advanced=True,\n        info='Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language.'),\n        DropdownInput(name='prompt',\n        display_name='Prompt Name',\n        options=SUMMARIZER_PROMPTS,\n        value=SUMMARIZER_PROMPTS[0],\n        advanced=True,\n        info='Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only.')\n    ]\n\n    outputs = [\n        Output(name='answer',\n        display_name='Answer',\n        method='generate_response')\n    ]\n\n    def generate_response(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VectaraRagComponent", "base_classes": ["Component"], "public_methods": ["def generate_response(self)"], "imports": ["from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput, StrInput", "from langflow.schema.message import Message", "from langchain_community.vectorstores import Vectara", "from langchain_community.vectorstores.vectara import RerankConfig, SummaryConfig, VectaraQueryConfig"], "inputs": "[StrInput(name='vectara_customer_id', display_name='Vectara Customer ID', required=True), StrInput(name='vectara_corpus_id', display_name='Vectara Corpus ID', required=True), SecretStrInput(name='vectara_api_key', display_name='Vectara API Key', required=True), MessageTextInput(name='search_query', display_name='Search Query', info='The query to receive an answer on.', tool_mode=True), FloatInput(name='lexical_interpolation', display_name='Hybrid Search Factor', range_spec=RangeSpec(min=0.005, max=0.1, step=0.005), value=0.005, advanced=True, info='How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all, and 1 means only lexical search is used.'), MessageTextInput(name='filter', display_name='Metadata Filters', value='', advanced=True, info='The filter string to narrow the search to according to metadata attributes.'), DropdownInput(name='reranker', display_name='Reranker Type', options=RERANKER_TYPES, value=RERANKER_TYPES[0], info='How to rerank the retrieved search results.'), IntInput(name='reranker_k', display_name='Number of Results to Rerank', value=50, range_spec=RangeSpec(min=1, max=100, step=1), advanced=True), FloatInput(name='diversity_bias', display_name='Diversity Bias', value=0.2, range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True, info='Ranges from 0 to 1, with higher values indicating greater diversity (only applies to MMR reranker).'), IntInput(name='max_results', display_name='Max Results to Summarize', value=7, range_spec=RangeSpec(min=1, max=100, step=1), advanced=True, info='The maximum number of search results to be available to the prompt.'), DropdownInput(name='response_lang', display_name='Response Language', options=RESPONSE_LANGUAGES, value='eng', advanced=True, info='Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language.'), DropdownInput(name='prompt', display_name='Prompt Name', options=SUMMARIZER_PROMPTS, value=SUMMARIZER_PROMPTS[0], advanced=True, info='Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only.')]", "outputs": "[Output(name='answer', display_name='Answer', method='generate_response')]", "display_name": "Vectara RAG", "name": "VectaraRAG", "description": "Vectara's full end to end RAG", "icon": "Vectara"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/nvidia/nvidia_rerank.py", "section": "class::NvidiaRerankComponent", "content": "from typing import Any\nfrom langflow.base.compressors.model import LCCompressorComponent\nfrom langflow.field_typing import BaseDocumentCompressor\nfrom langflow.inputs.inputs import SecretStrInput\nfrom langflow.io import DropdownInput, StrInput\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.template.field.base import Output\nfrom langchain_nvidia_ai_endpoints import NVIDIARerank\n\nclass NvidiaRerankComponent(LCCompressorComponent):\n    display_name: str = \"NVIDIA Rerank\"\n    description: str = \"Rerank documents using the NVIDIA API.\"\n    icon = \"NVIDIA\"\n\n    inputs = [\n        *LCCompressorComponent.inputs,\n        SecretStrInput(name='api_key',\n        display_name='NVIDIA API Key'),\n        StrInput(name='base_url',\n        display_name='Base URL',\n        value='https://integrate.api.nvidia.com/v1',\n        refresh_button=True,\n        info='The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.'),\n        DropdownInput(name='model',\n        display_name='Model',\n        options=['nv-rerank-qa-mistral-4b:1'],\n        value='nv-rerank-qa-mistral-4b:1')\n    ]\n\n    outputs = [\n        Output(display_name='Reranked Documents',\n        name='reranked_documents',\n        method='compress_documents')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_compressor(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NvidiaRerankComponent", "base_classes": ["LCCompressorComponent"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def build_compressor(self)"], "imports": ["from typing import Any", "from langflow.base.compressors.model import LCCompressorComponent", "from langflow.field_typing import BaseDocumentCompressor", "from langflow.inputs.inputs import SecretStrInput", "from langflow.io import DropdownInput, StrInput", "from langflow.schema.dotdict import dotdict", "from langflow.template.field.base import Output", "from langchain_nvidia_ai_endpoints import NVIDIARerank"], "inputs": "[*LCCompressorComponent.inputs, SecretStrInput(name='api_key', display_name='NVIDIA API Key'), StrInput(name='base_url', display_name='Base URL', value='https://integrate.api.nvidia.com/v1', refresh_button=True, info='The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.'), DropdownInput(name='model', display_name='Model', options=['nv-rerank-qa-mistral-4b:1'], value='nv-rerank-qa-mistral-4b:1')]", "outputs": "[Output(display_name='Reranked Documents', name='reranked_documents', method='compress_documents')]", "display_name": "NVIDIA Rerank", "name": "", "description": "Rerank documents using the NVIDIA API.", "icon": "NVIDIA"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/custom_component/custom_component.py", "section": "class::CustomComponent", "content": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\n\nclass CustomComponent(Component):\n    display_name: str = \"Custom Component\"\n    description: str = \"Use as a template to create your own component.\"\n    icon = \"code\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name='input_value',\n        display_name='Input Value',\n        info='This is a custom component Input',\n        value='Hello,\n        World!',\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Output',\n        name='output',\n        method='build_output')\n    ]\n\n    def build_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CustomComponent", "base_classes": ["Component"], "public_methods": ["def build_output(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import MessageTextInput, Output", "from langflow.schema import Data"], "inputs": "[MessageTextInput(name='input_value', display_name='Input Value', info='This is a custom component Input', value='Hello, World!', tool_mode=True)]", "outputs": "[Output(display_name='Output', name='output', method='build_output')]", "display_name": "Custom Component", "name": "CustomComponent", "description": "Use as a template to create your own component.", "icon": "code"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/assemblyai/assemblyai_lemur.py", "section": "class::AssemblyAILeMUR", "content": "import assemblyai as aai\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, FloatInput, IntInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import Data\n\nclass AssemblyAILeMUR(Component):\n    display_name: str = \"AssemblyAI LeMUR\"\n    description: str = \"Apply Large Language Models to spoken data using the AssemblyAI LeMUR framework\"\n    icon = \"AssemblyAI\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Assembly API Key',\n        info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/',\n        advanced=False,\n        required=True),\n        DataInput(name='transcription_result',\n        display_name='Transcription Result',\n        info='The transcription result from AssemblyAI',\n        required=True),\n        MultilineInput(name='prompt',\n        display_name='Input Prompt',\n        info='The text to prompt the model',\n        required=True),\n        DropdownInput(name='final_model',\n        display_name='Final Model',\n        options=['claude3_5_sonnet',\n        'claude3_opus',\n        'claude3_haiku',\n        'claude3_sonnet'],\n        value='claude3_5_sonnet',\n        info='The model that is used for the final prompt after compression is performed',\n        advanced=True),\n        FloatInput(name='temperature',\n        display_name='Temperature',\n        advanced=True,\n        value=0.0,\n        info='The temperature to use for the model'),\n        IntInput(name='max_output_size',\n        display_name=' Max Output Size',\n        advanced=True,\n        value=2000,\n        info='Max output size in tokens,\n        up to 4000'),\n        DropdownInput(name='endpoint',\n        display_name='Endpoint',\n        options=['task',\n        'summary',\n        'question-answer'],\n        value='task',\n        info=\"The LeMUR endpoint to use. For 'summary' and 'question-answer',\n        no prompt input is needed. See https://www.assemblyai.com/docs/api-reference/lemur/ for more info.\",\n        advanced=True),\n        MultilineInput(name='questions',\n        display_name='Questions',\n        info=\"Comma-separated list of your questions. Only used if Endpoint is 'question-answer'\",\n        advanced=True),\n        MultilineInput(name='transcript_ids',\n        display_name='Transcript IDs',\n        info='Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided,\n        the Transcription Result is ignored.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='LeMUR Response',\n        name='lemur_response',\n        method='run_lemur')\n    ]\n\n    def run_lemur(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def perform_lemur_action(self, transcript_group, endpoint):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_final_model(self, model_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssemblyAILeMUR", "base_classes": ["Component"], "public_methods": ["def run_lemur(self)", "def perform_lemur_action(self, transcript_group, endpoint)", "def get_final_model(self, model_name)"], "imports": ["import assemblyai as aai", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, FloatInput, IntInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='Assembly API Key', info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/', advanced=False, required=True), DataInput(name='transcription_result', display_name='Transcription Result', info='The transcription result from AssemblyAI', required=True), MultilineInput(name='prompt', display_name='Input Prompt', info='The text to prompt the model', required=True), DropdownInput(name='final_model', display_name='Final Model', options=['claude3_5_sonnet', 'claude3_opus', 'claude3_haiku', 'claude3_sonnet'], value='claude3_5_sonnet', info='The model that is used for the final prompt after compression is performed', advanced=True), FloatInput(name='temperature', display_name='Temperature', advanced=True, value=0.0, info='The temperature to use for the model'), IntInput(name='max_output_size', display_name=' Max Output Size', advanced=True, value=2000, info='Max output size in tokens, up to 4000'), DropdownInput(name='endpoint', display_name='Endpoint', options=['task', 'summary', 'question-answer'], value='task', info=\"The LeMUR endpoint to use. For 'summary' and 'question-answer', no prompt input is needed. See https://www.assemblyai.com/docs/api-reference/lemur/ for more info.\", advanced=True), MultilineInput(name='questions', display_name='Questions', info=\"Comma-separated list of your questions. Only used if Endpoint is 'question-answer'\", advanced=True), MultilineInput(name='transcript_ids', display_name='Transcript IDs', info='Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided, the Transcription Result is ignored.', advanced=True)]", "outputs": "[Output(display_name='LeMUR Response', name='lemur_response', method='run_lemur')]", "display_name": "AssemblyAI LeMUR", "name": "", "description": "Apply Large Language Models to spoken data using the AssemblyAI LeMUR framework", "icon": "AssemblyAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/prompts/prompt.py", "section": "class::PromptComponent", "content": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name='template',\n        display_name='Template'),\n        MessageTextInput(name='tool_placeholder',\n        display_name='Tool Placeholder',\n        tool_mode=True,\n        advanced=True,\n        info='A placeholder input for tool mode.')\n    ]\n\n    outputs = [\n        Output(display_name='Prompt Message',\n        name='prompt',\n        method='build_prompt')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "PromptComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["from langflow.base.prompts.api_utils import process_prompt_template", "from langflow.custom import Component", "from langflow.inputs.inputs import DefaultPromptField", "from langflow.io import MessageTextInput, Output, PromptInput", "from langflow.schema.message import Message", "from langflow.template.utils import update_template_values"], "inputs": "[PromptInput(name='template', display_name='Template'), MessageTextInput(name='tool_placeholder', display_name='Tool Placeholder', tool_mode=True, advanced=True, info='A placeholder input for tool mode.')]", "outputs": "[Output(display_name='Prompt Message', name='prompt', method='build_prompt')]", "display_name": "Prompt", "name": "Prompt", "description": "Create a prompt template with dynamic variables.", "icon": "prompts"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/sql_executor.py", "section": "class::SQLComponent", "content": "from typing import TYPE_CHECKING, Any\nfrom langchain_community.utilities import SQLDatabase\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.io import BoolInput, MessageTextInput, Output\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.services.cache.utils import CacheMiss\nfrom sqlalchemy.engine import Result\n\nclass SQLComponent(ComponentWithCache):\n    \"\"\"\n    A sql component.\n    \"\"\"\n\n    display_name: str = \"SQL Query\"\n    description: str = \"Execute SQL Query\"\n    icon = \"database\"\n    name = \"SQLComponent\"\n\n    inputs = [\n        MessageTextInput(name='database_url',\n        display_name='Database URL',\n        required=True),\n        MessageTextInput(name='query',\n        display_name='SQL Query',\n        tool_mode=True,\n        required=True),\n        BoolInput(name='include_columns',\n        display_name='Include Columns',\n        value=True,\n        tool_mode=True,\n        advanced=True),\n        BoolInput(name='add_error',\n        display_name='Add Error',\n        value=False,\n        tool_mode=True,\n        info='If True,\n        the error will be added to the result',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Query Results',\n        name='sql_query_results',\n        method='sql_query_results')\n    ]\n\n    def maybe_create_db(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_component(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def sql_query_results(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SQLComponent", "base_classes": ["ComponentWithCache"], "public_methods": ["def maybe_create_db(self)", "def build_component(self)", "def sql_query_results(self)"], "imports": ["from typing import TYPE_CHECKING, Any", "from langchain_community.utilities import SQLDatabase", "from sqlalchemy.exc import SQLAlchemyError", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.io import BoolInput, MessageTextInput, Output", "from langflow.schema.dataframe import DataFrame", "from langflow.schema.message import Message", "from langflow.services.cache.utils import CacheMiss", "from sqlalchemy.engine import Result"], "inputs": "[MessageTextInput(name='database_url', display_name='Database URL', required=True), MessageTextInput(name='query', display_name='SQL Query', tool_mode=True, required=True), BoolInput(name='include_columns', display_name='Include Columns', value=True, tool_mode=True, advanced=True), BoolInput(name='add_error', display_name='Add Error', value=False, tool_mode=True, info='If True, the error will be added to the result', advanced=True)]", "outputs": "[Output(display_name='Query Results', name='sql_query_results', method='sql_query_results')]", "display_name": "SQL Query", "name": "SQLComponent", "description": "Execute SQL Query", "icon": "database"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/extract_key_from_data.py", "section": "class::ExtractKeyFromDataComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.schema import Data\n\nclass ExtractKeyFromDataComponent(CustomComponent):\n    display_name: str = \"Extract Key From Data\"\n    description: str = \"Extracts a key from a data.\"\n    name = \"ExtractKeyFromData\"\n\n    def build(self, data, keys):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ExtractKeyFromDataComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build(self, data, keys)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "Extract Key From Data", "name": "ExtractKeyFromData", "description": "Extracts a key from a data.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/outputs/chat.py", "section": "class::ChatOutput", "content": "from collections.abc import Generator\nfrom typing import Any\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\nclass ChatOutput(ChatComponent):\n    display_name: str = \"Chat Output\"\n    description: str = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        HandleInput(name='input_value',\n        display_name='Text',\n        info='Message to be passed as output.',\n        input_types=['Data',\n        'DataFrame',\n        'Message'],\n        required=True),\n        BoolInput(name='should_store_message',\n        display_name='Store Messages',\n        info='Store the message in the history.',\n        value=True,\n        advanced=True),\n        DropdownInput(name='sender',\n        display_name='Sender Type',\n        options=[MESSAGE_SENDER_AI,\n        MESSAGE_SENDER_USER],\n        value=MESSAGE_SENDER_AI,\n        advanced=True,\n        info='Type of sender.'),\n        MessageTextInput(name='sender_name',\n        display_name='Sender Name',\n        info='Name of the sender.',\n        value=MESSAGE_SENDER_NAME_AI,\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='The session ID of the chat. If empty,\n        the current session ID parameter will be used.',\n        advanced=True),\n        MessageTextInput(name='data_template',\n        display_name='Data Template',\n        value='{text}',\n        advanced=True,\n        info=\"Template to convert Data to Text. If left empty,\n        it will be dynamically set to the Data's text key.\"),\n        MessageTextInput(name='background_color',\n        display_name='Background Color',\n        info='The background color of the icon.',\n        advanced=True),\n        MessageTextInput(name='chat_icon',\n        display_name='Icon',\n        info='The icon of the message.',\n        advanced=True),\n        MessageTextInput(name='text_color',\n        display_name='Text Color',\n        info='The text color of the name',\n        advanced=True),\n        BoolInput(name='clean_data',\n        display_name='Basic Clean Data',\n        value=True,\n        info='Whether to clean the data',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='message',\n        method='message_response')\n    ]\n\n    def convert_to_string(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ChatOutput", "base_classes": ["ChatComponent"], "public_methods": ["def convert_to_string(self)"], "imports": ["from collections.abc import Generator", "from typing import Any", "import orjson", "from fastapi.encoders import jsonable_encoder", "from langflow.base.io.chat import ChatComponent", "from langflow.inputs import BoolInput", "from langflow.inputs.inputs import HandleInput", "from langflow.io import DropdownInput, MessageTextInput, Output", "from langflow.schema.data import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.schema.message import Message", "from langflow.schema.properties import Source", "from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER"], "inputs": "[HandleInput(name='input_value', display_name='Text', info='Message to be passed as output.', input_types=['Data', 'DataFrame', 'Message'], required=True), BoolInput(name='should_store_message', display_name='Store Messages', info='Store the message in the history.', value=True, advanced=True), DropdownInput(name='sender', display_name='Sender Type', options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER], value=MESSAGE_SENDER_AI, advanced=True, info='Type of sender.'), MessageTextInput(name='sender_name', display_name='Sender Name', info='Name of the sender.', value=MESSAGE_SENDER_NAME_AI, advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='The session ID of the chat. If empty, the current session ID parameter will be used.', advanced=True), MessageTextInput(name='data_template', display_name='Data Template', value='{text}', advanced=True, info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\"), MessageTextInput(name='background_color', display_name='Background Color', info='The background color of the icon.', advanced=True), MessageTextInput(name='chat_icon', display_name='Icon', info='The icon of the message.', advanced=True), MessageTextInput(name='text_color', display_name='Text Color', info='The text color of the name', advanced=True), BoolInput(name='clean_data', display_name='Basic Clean Data', value=True, info='Whether to clean the data', advanced=True)]", "outputs": "[Output(display_name='Message', name='message', method='message_response')]", "display_name": "Chat Output", "name": "ChatOutput", "description": "Display a chat message in the Playground.", "icon": "MessagesSquare"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/composio/composio_api.py", "section": "class::ComposioAPIComponent", "content": "from collections.abc import Sequence\nfrom typing import Any\nfrom composio import Action, App\nfrom composio_langchain import ComposioToolSet\nfrom langchain_core.tools import Tool\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import ConnectionInput, MessageTextInput, SecretStrInput, SortableListInput\nfrom langflow.io import Output\n\nclass ComposioAPIComponent(LCToolComponent):\n    display_name: str = \"Composio Tools\"\n    description: str = \"Use Composio toolset to run actions with your agent\"\n    icon = \"Composio\"\n    name = \"ComposioAPI\"\n\n    inputs = [\n        MessageTextInput(name='entity_id',\n        display_name='Entity ID',\n        value='default',\n        advanced=True),\n        SecretStrInput(name='api_key',\n        display_name='Composio API Key',\n        required=True,\n        info='Refer to https://docs.composio.dev/faq/api_key/api_key',\n        real_time_refresh=True),\n        ConnectionInput(name='tool_name',\n        display_name='Tool Name',\n        placeholder='Select a tool...',\n        button_metadata={'icon': 'unplug',\n        'variant': 'destructive'},\n        options=[],\n        search_category=[],\n        value='',\n        connection_link='',\n        info='The name of the tool to use',\n        real_time_refresh=True),\n        SortableListInput(name='actions',\n        display_name='Actions',\n        placeholder='Select action',\n        helper_text='Please connect before selecting actions.',\n        helper_text_metadata={'icon': 'OctagonAlert',\n        'variant': 'destructive'},\n        options=[],\n        value='',\n        info='The actions to use',\n        limit=1,\n        show=False)\n    ]\n\n    outputs = [\n        Output(name='tools',\n        display_name='Tools',\n        method='build_tool')\n    ]\n\n    def sanitize_action_name(self, action_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def desanitize_action_name(self, action_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def validate_tool(self, build_config, field_value, connected_app_names):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ComposioAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def sanitize_action_name(self, action_name)", "def desanitize_action_name(self, action_name)", "def validate_tool(self, build_config, field_value, connected_app_names)", "def update_build_config(self, build_config, field_value, field_name)", "def build_tool(self)"], "imports": ["from collections.abc import Sequence", "from typing import Any", "from composio import Action, App", "from composio_langchain import ComposioToolSet", "from langchain_core.tools import Tool", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.inputs import ConnectionInput, MessageTextInput, SecretStrInput, SortableListInput", "from langflow.io import Output"], "inputs": "[MessageTextInput(name='entity_id', display_name='Entity ID', value='default', advanced=True), SecretStrInput(name='api_key', display_name='Composio API Key', required=True, info='Refer to https://docs.composio.dev/faq/api_key/api_key', real_time_refresh=True), ConnectionInput(name='tool_name', display_name='Tool Name', placeholder='Select a tool...', button_metadata={'icon': 'unplug', 'variant': 'destructive'}, options=[], search_category=[], value='', connection_link='', info='The name of the tool to use', real_time_refresh=True), SortableListInput(name='actions', display_name='Actions', placeholder='Select action', helper_text='Please connect before selecting actions.', helper_text_metadata={'icon': 'OctagonAlert', 'variant': 'destructive'}, options=[], value='', info='The actions to use', limit=1, show=False)]", "outputs": "[Output(name='tools', display_name='Tools', method='build_tool')]", "display_name": "Composio Tools", "name": "ComposioAPI", "description": "Use Composio toolset to run actions with your agent", "icon": "Composio"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/id_generator.py", "section": "class::IDGeneratorComponent", "content": "import uuid\nfrom typing import Any\nfrom typing_extensions import override\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import dotdict\nfrom langflow.schema.message import Message\n\nclass IDGeneratorComponent(Component):\n    display_name: str = \"ID Generator\"\n    description: str = \"Generates a unique ID.\"\n    icon = \"fingerprint\"\n    name = \"IDGenerator\"\n\n    inputs = [\n        MessageTextInput(name='unique_id',\n        display_name='Value',\n        info='The generated unique ID.',\n        refresh_button=True,\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='ID',\n        name='id',\n        method='generate_id')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def generate_id(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "IDGeneratorComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def generate_id(self)"], "imports": ["import uuid", "from typing import Any", "from typing_extensions import override", "from langflow.custom import Component", "from langflow.io import MessageTextInput, Output", "from langflow.schema import dotdict", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='unique_id', display_name='Value', info='The generated unique ID.', refresh_button=True, tool_mode=True)]", "outputs": "[Output(display_name='ID', name='id', method='generate_id')]", "display_name": "ID Generator", "name": "IDGenerator", "description": "Generates a unique ID.", "icon": "fingerprint"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/memories/astra_db.py", "section": "class::AstraDBChatMemory", "content": "import os\nfrom astrapy.admin import parse_api_endpoint\nfrom langflow.base.memory.model import LCChatMemoryComponent\nfrom langflow.field_typing.constants import Memory\nfrom langflow.inputs import MessageTextInput, SecretStrInput, StrInput\nfrom langchain_astradb.chat_message_histories import AstraDBChatMessageHistory\n\nclass AstraDBChatMemory(LCChatMemoryComponent):\n    display_name: str = \"Astra DB Chat Memory\"\n    description: str = \"Retrieves and store chat messages from Astra DB.\"\n    icon = \"AstraDB\"\n    name = \"AstraDBChatMemory\"\n\n    inputs = [\n        SecretStrInput(name='token',\n        display_name='Astra DB Application Token',\n        info='Authentication token for accessing Astra DB.',\n        value='ASTRA_DB_APPLICATION_TOKEN',\n        required=True,\n        advanced=os.getenv('ASTRA_ENHANCED',\n        'false').lower() == 'true'),\n        SecretStrInput(name='api_endpoint',\n        display_name='API Endpoint',\n        info='API endpoint URL for the Astra DB service.',\n        value='ASTRA_DB_API_ENDPOINT',\n        required=True),\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        info='The name of the collection within Astra DB where the vectors will be stored.',\n        required=True),\n        StrInput(name='namespace',\n        display_name='Namespace',\n        info='Optional namespace within Astra DB to use for the collection.',\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='The session ID of the chat. If empty,\n        the current session ID parameter will be used.',\n        advanced=True)\n    ]\n\n    def build_message_history(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AstraDBChatMemory", "base_classes": ["LCChatMemoryComponent"], "public_methods": ["def build_message_history(self)"], "imports": ["import os", "from astrapy.admin import parse_api_endpoint", "from langflow.base.memory.model import LCChatMemoryComponent", "from langflow.field_typing.constants import Memory", "from langflow.inputs import MessageTextInput, SecretStrInput, StrInput", "from langchain_astradb.chat_message_histories import AstraDBChatMessageHistory"], "inputs": "[SecretStrInput(name='token', display_name='Astra DB Application Token', info='Authentication token for accessing Astra DB.', value='ASTRA_DB_APPLICATION_TOKEN', required=True, advanced=os.getenv('ASTRA_ENHANCED', 'false').lower() == 'true'), SecretStrInput(name='api_endpoint', display_name='API Endpoint', info='API endpoint URL for the Astra DB service.', value='ASTRA_DB_API_ENDPOINT', required=True), StrInput(name='collection_name', display_name='Collection Name', info='The name of the collection within Astra DB where the vectors will be stored.', required=True), StrInput(name='namespace', display_name='Namespace', info='Optional namespace within Astra DB to use for the collection.', advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='The session ID of the chat. If empty, the current session ID parameter will be used.', advanced=True)]", "outputs": "", "display_name": "Astra DB Chat Memory", "name": "AstraDBChatMemory", "description": "Retrieves and store chat messages from Astra DB.", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/apify/apify_actor.py", "section": "class::ApifyActorsComponent", "content": "import json\nimport string\nfrom typing import Any, cast\nfrom apify_client import ApifyClient\nfrom langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field, field_serializer\nfrom langflow.custom import Component\nfrom langflow.field_typing import Tool\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import MultilineInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ApifyActorsComponent(Component):\n    display_name: str = \"Apify Actors\"\n    description: str = \"Use Apify Actors to extract data from hundreds of places fast. This component can be used in a flow to retrieve data or as a tool with an agent.\"\n    icon = \"Apify\"\n    name = \"ApifyActors\"\n\n    inputs = [\n        SecretStrInput(name='apify_token',\n        display_name='Apify Token',\n        info='The API token for the Apify account.',\n        required=True,\n        password=True),\n        StrInput(name='actor_id',\n        display_name='Actor',\n        info=\"Actor name from Apify store to run. For example 'apify/website-content-crawler' to use the Website Content Crawler Actor.\",\n        value='apify/website-content-crawler',\n        required=True),\n        MultilineInput(name='run_input',\n        display_name='Run input',\n        info='The JSON input for the Actor run. For example for the \"apify/website-content-crawler\" Actor: {\"startUrls\":[{\"url\":\"https://docs.apify.com/academy/web-scraping-for-beginners\"}],\"maxCrawlDepth\":0}',\n        value='{\"startUrls\":[{\"url\":\"https://docs.apify.com/academy/web-scraping-for-beginners\"}],\"maxCrawlDepth\":0}',\n        required=True),\n        MultilineInput(name='dataset_fields',\n        display_name='Output fields',\n        info=\"Fields to extract from the dataset,\n        split by commas. Other fields will be ignored. Dots in nested structures will be replaced by underscores. Sample input: 'text,\n        metadata.title'. Sample output: {'text': 'page content here',\n        'metadata_title': 'page title here'}. For example,\n        for the 'apify/website-content-crawler' Actor,\n        you can extract the 'markdown' field,\n        which is the content of the website in markdown format.\"),\n        BoolInput(name='flatten_dataset',\n        display_name='Flatten output',\n        info=\"The output dataset will be converted from a nested format to a flat structure. Dots in nested structure will be replaced by underscores. This is useful for further processing of the Data object. For example,\n        {'a': {'b': 1}} will be flattened to {'a_b': 1}.\")\n    ]\n\n    outputs = [\n        Output(display_name='Output',\n        name='output',\n        type_=list[Data],\n        method='run_model'),\n        Output(display_name='Tool',\n        name='tool',\n        type_=Tool,\n        method='build_tool')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_tool_class(parent, readme, input_model, actor_id):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_input_model_class(description):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_actor_input_schema_from_build(input_schema):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def dict_to_json_str(d):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def actor_id_to_tool_name(actor_id):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_nested_value(data, key):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_dataset_fields(dataset_fields):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def flatten(d):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ApifyActorsComponent", "base_classes": ["Component"], "public_methods": ["def run_model(self)", "def build_tool(self)", "def create_tool_class(parent, readme, input_model, actor_id)", "def create_input_model_class(description)", "def get_actor_input_schema_from_build(input_schema)", "def dict_to_json_str(d)", "def actor_id_to_tool_name(actor_id)", "def get_nested_value(data, key)", "def parse_dataset_fields(dataset_fields)", "def flatten(d)"], "imports": ["import json", "import string", "from typing import Any, cast", "from apify_client import ApifyClient", "from langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader", "from langchain_core.tools import BaseTool", "from pydantic import BaseModel, Field, field_serializer", "from langflow.custom import Component", "from langflow.field_typing import Tool", "from langflow.inputs.inputs import BoolInput", "from langflow.io import MultilineInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='apify_token', display_name='Apify Token', info='The API token for the Apify account.', required=True, password=True), StrInput(name='actor_id', display_name='Actor', info=\"Actor name from Apify store to run. For example 'apify/website-content-crawler' to use the Website Content Crawler Actor.\", value='apify/website-content-crawler', required=True), MultilineInput(name='run_input', display_name='Run input', info='The JSON input for the Actor run. For example for the \"apify/website-content-crawler\" Actor: {\"startUrls\":[{\"url\":\"https://docs.apify.com/academy/web-scraping-for-beginners\"}],\"maxCrawlDepth\":0}', value='{\"startUrls\":[{\"url\":\"https://docs.apify.com/academy/web-scraping-for-beginners\"}],\"maxCrawlDepth\":0}', required=True), MultilineInput(name='dataset_fields', display_name='Output fields', info=\"Fields to extract from the dataset, split by commas. Other fields will be ignored. Dots in nested structures will be replaced by underscores. Sample input: 'text, metadata.title'. Sample output: {'text': 'page content here', 'metadata_title': 'page title here'}. For example, for the 'apify/website-content-crawler' Actor, you can extract the 'markdown' field, which is the content of the website in markdown format.\"), BoolInput(name='flatten_dataset', display_name='Flatten output', info=\"The output dataset will be converted from a nested format to a flat structure. Dots in nested structure will be replaced by underscores. This is useful for further processing of the Data object. For example, {'a': {'b': 1}} will be flattened to {'a_b': 1}.\")]", "outputs": "[Output(display_name='Output', name='output', type_=list[Data], method='run_model'), Output(display_name='Tool', name='tool', type_=Tool, method='build_tool')]", "display_name": "Apify Actors", "name": "ApifyActors", "description": "Use Apify Actors to extract data from hundreds of places fast. This component can be used in a flow to retrieve data or as a tool with an agent.", "icon": "Apify"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/apify/apify_actor.py", "section": "class::ApifyActorRun", "content": "import json\nimport string\nfrom typing import Any, cast\nfrom apify_client import ApifyClient\nfrom langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field, field_serializer\nfrom langflow.custom import Component\nfrom langflow.field_typing import Tool\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import MultilineInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ApifyActorRun(BaseTool):\n    \"\"\"\n    Tool that runs Apify Actors.\n    \"\"\"\n\n    description: str = \"f'Run an Apify Actor with the given input. Here is a part of the currently loaded Actor README:\\n\\n{readme}\\n\\n'\"\n    name = \"f'apify_actor_{ApifyActorsComponent.actor_id_to_tool_name(actor_id)}'\"\n\n    def serialize_args_schema(self, args_schema):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ApifyActorRun", "base_classes": ["BaseTool"], "public_methods": ["def serialize_args_schema(self, args_schema)"], "imports": ["import json", "import string", "from typing import Any, cast", "from apify_client import ApifyClient", "from langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader", "from langchain_core.tools import BaseTool", "from pydantic import BaseModel, Field, field_serializer", "from langflow.custom import Component", "from langflow.field_typing import Tool", "from langflow.inputs.inputs import BoolInput", "from langflow.io import MultilineInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "f'apify_actor_{ApifyActorsComponent.actor_id_to_tool_name(actor_id)}'", "description": "f'Run an Apify Actor with the given input. Here is a part of the currently loaded Actor README:\\n\\n{readme}\\n\\n'", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/apify/apify_actor.py", "section": "class::ActorInput", "content": "import json\nimport string\nfrom typing import Any, cast\nfrom apify_client import ApifyClient\nfrom langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field, field_serializer\nfrom langflow.custom import Component\nfrom langflow.field_typing import Tool\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import MultilineInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ActorInput(BaseModel):\n    \"\"\"\n    Input for the Apify Actor tool.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "ActorInput", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "import string", "from typing import Any, cast", "from apify_client import ApifyClient", "from langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader", "from langchain_core.tools import BaseTool", "from pydantic import BaseModel, Field, field_serializer", "from langflow.custom import Component", "from langflow.field_typing import Tool", "from langflow.inputs.inputs import BoolInput", "from langflow.io import MultilineInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/inputs/chat.py", "section": "class::ChatInput", "content": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\nclass ChatInput(ChatComponent):\n    display_name: str = \"Chat Input\"\n    description: str = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Text',\n        value='',\n        info='Message to be passed as input.',\n        input_types=[]),\n        BoolInput(name='should_store_message',\n        display_name='Store Messages',\n        info='Store the message in the history.',\n        value=True,\n        advanced=True),\n        DropdownInput(name='sender',\n        display_name='Sender Type',\n        options=[MESSAGE_SENDER_AI,\n        MESSAGE_SENDER_USER],\n        value=MESSAGE_SENDER_USER,\n        info='Type of sender.',\n        advanced=True),\n        MessageTextInput(name='sender_name',\n        display_name='Sender Name',\n        info='Name of the sender.',\n        value=MESSAGE_SENDER_NAME_USER,\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='The session ID of the chat. If empty,\n        the current session ID parameter will be used.',\n        advanced=True),\n        FileInput(name='files',\n        display_name='Files',\n        file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n        info='Files to be sent with the message.',\n        advanced=True,\n        is_list=True,\n        temp_file=True),\n        MessageTextInput(name='background_color',\n        display_name='Background Color',\n        info='The background color of the icon.',\n        advanced=True),\n        MessageTextInput(name='chat_icon',\n        display_name='Icon',\n        info='The icon of the message.',\n        advanced=True),\n        MessageTextInput(name='text_color',\n        display_name='Text Color',\n        info='The text color of the name',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='message',\n        method='message_response')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "ChatInput", "base_classes": ["ChatComponent"], "public_methods": [], "imports": ["from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES", "from langflow.base.io.chat import ChatComponent", "from langflow.inputs import BoolInput", "from langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output", "from langflow.schema.message import Message", "from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER"], "inputs": "[MultilineInput(name='input_value', display_name='Text', value='', info='Message to be passed as input.', input_types=[]), BoolInput(name='should_store_message', display_name='Store Messages', info='Store the message in the history.', value=True, advanced=True), DropdownInput(name='sender', display_name='Sender Type', options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER], value=MESSAGE_SENDER_USER, info='Type of sender.', advanced=True), MessageTextInput(name='sender_name', display_name='Sender Name', info='Name of the sender.', value=MESSAGE_SENDER_NAME_USER, advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='The session ID of the chat. If empty, the current session ID parameter will be used.', advanced=True), FileInput(name='files', display_name='Files', file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES, info='Files to be sent with the message.', advanced=True, is_list=True, temp_file=True), MessageTextInput(name='background_color', display_name='Background Color', info='The background color of the icon.', advanced=True), MessageTextInput(name='chat_icon', display_name='Icon', info='The icon of the message.', advanced=True), MessageTextInput(name='text_color', display_name='Text Color', info='The text color of the name', advanced=True)]", "outputs": "[Output(display_name='Message', name='message', method='message_response')]", "display_name": "Chat Input", "name": "ChatInput", "description": "Get chat inputs from the Playground.", "icon": "MessagesSquare"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/retrievers/metal.py", "section": "class::MetalRetrieverComponent", "content": "from typing import cast\nfrom langchain_community.retrievers import MetalRetriever\nfrom metal_sdk.metal import Metal\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Retriever\n\nclass MetalRetrieverComponent(CustomComponent):\n    display_name: str = \"Metal Retriever\"\n    description: str = \"Retriever that uses the Metal API.\"\n    name = \"MetalRetriever\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, api_key, client_id, index_id, params):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MetalRetrieverComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, api_key, client_id, index_id, params)"], "imports": ["from typing import cast", "from langchain_community.retrievers import MetalRetriever", "from metal_sdk.metal import Metal", "from langflow.custom import CustomComponent", "from langflow.field_typing import Retriever"], "inputs": "", "outputs": "", "display_name": "Metal Retriever", "name": "MetalRetriever", "description": "Retriever that uses the Metal API.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/bing_search_api.py", "section": "class::BingSearchAPIComponent", "content": "from typing import cast\nfrom langchain_community.tools.bing_search import BingSearchResults\nfrom langchain_community.utilities import BingSearchAPIWrapper\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import IntInput, MessageTextInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass BingSearchAPIComponent(LCToolComponent):\n    display_name: str = \"Bing Search API\"\n    description: str = \"Call the Bing Search API.\"\n    icon = \"Bing\"\n    name = \"BingSearchAPI\"\n\n    inputs = [\n        SecretStrInput(name='bing_subscription_key',\n        display_name='Bing Subscription Key'),\n        MultilineInput(name='input_value',\n        display_name='Input'),\n        MessageTextInput(name='bing_search_url',\n        display_name='Bing Search URL',\n        advanced=True),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "BingSearchAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["from typing import cast", "from langchain_community.tools.bing_search import BingSearchResults", "from langchain_community.utilities import BingSearchAPIWrapper", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import IntInput, MessageTextInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='bing_subscription_key', display_name='Bing Subscription Key'), MultilineInput(name='input_value', display_name='Input'), MessageTextInput(name='bing_search_url', display_name='Bing Search URL', advanced=True), IntInput(name='k', display_name='Number of results', value=4, required=True)]", "outputs": "", "display_name": "Bing Search API", "name": "BingSearchAPI", "description": "Call the Bing Search API.", "icon": "Bing"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/nvidia.py", "section": "class::NVIDIAEmbeddingsComponent", "content": "from typing import Any\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\nfrom langflow.schema.dotdict import dotdict\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\nclass NVIDIAEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"NVIDIA Embeddings\"\n    description: str = \"Generate embeddings using NVIDIA models.\"\n    icon = \"NVIDIA\"\n\n    inputs = [\n        DropdownInput(name='model',\n        display_name='Model',\n        options=['nvidia/nv-embed-v1',\n        'snowflake/arctic-embed-I'],\n        value='nvidia/nv-embed-v1',\n        required=True),\n        MessageTextInput(name='base_url',\n        display_name='NVIDIA Base URL',\n        refresh_button=True,\n        value='https://integrate.api.nvidia.com/v1',\n        required=True),\n        SecretStrInput(name='nvidia_api_key',\n        display_name='NVIDIA API Key',\n        info='The NVIDIA API Key.',\n        advanced=False,\n        value='NVIDIA_API_KEY',\n        required=True),\n        FloatInput(name='temperature',\n        display_name='Model Temperature',\n        value=0.1,\n        advanced=True)\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NVIDIAEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def build_embeddings(self)"], "imports": ["from typing import Any", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.inputs.inputs import DropdownInput, SecretStrInput", "from langflow.io import FloatInput, MessageTextInput", "from langflow.schema.dotdict import dotdict", "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings"], "inputs": "[DropdownInput(name='model', display_name='Model', options=['nvidia/nv-embed-v1', 'snowflake/arctic-embed-I'], value='nvidia/nv-embed-v1', required=True), MessageTextInput(name='base_url', display_name='NVIDIA Base URL', refresh_button=True, value='https://integrate.api.nvidia.com/v1', required=True), SecretStrInput(name='nvidia_api_key', display_name='NVIDIA API Key', info='The NVIDIA API Key.', advanced=False, value='NVIDIA_API_KEY', required=True), FloatInput(name='temperature', display_name='Model Temperature', value=0.1, advanced=True)]", "outputs": "", "display_name": "NVIDIA Embeddings", "name": "", "description": "Generate embeddings using NVIDIA models.", "icon": "NVIDIA"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/video_embeddings.py", "section": "class::TwelveLabsVideoEmbeddings", "content": "import time\nfrom pathlib import Path\nfrom typing import Any, cast\nfrom twelvelabs import TwelveLabs\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput\n\nclass TwelveLabsVideoEmbeddings(Embeddings):\n\n    def embed_documents(self, texts):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def embed_query(self, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def embed_video(self, video_path):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TwelveLabsVideoEmbeddings", "base_classes": ["Embeddings"], "public_methods": ["def embed_documents(self, texts)", "def embed_query(self, text)", "def embed_video(self, video_path)"], "imports": ["import time", "from pathlib import Path", "from typing import Any, cast", "from twelvelabs import TwelveLabs", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, IntInput, SecretStrInput"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/video_embeddings.py", "section": "class::TwelveLabsVideoEmbeddingsComponent", "content": "import time\nfrom pathlib import Path\nfrom typing import Any, cast\nfrom twelvelabs import TwelveLabs\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput\n\nclass TwelveLabsVideoEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"Twelve Labs Video Embeddings\"\n    description: str = \"Generate embeddings from videos using Twelve Labs video embedding models.\"\n    icon = \"TwelveLabs\"\n    name = \"TwelveLabsVideoEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        advanced=False,\n        options=['Marengo-retrieval-2.7'],\n        value='Marengo-retrieval-2.7'),\n        IntInput(name='request_timeout',\n        display_name='Request Timeout',\n        advanced=True)\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TwelveLabsVideoEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)"], "imports": ["import time", "from pathlib import Path", "from typing import Any, cast", "from twelvelabs import TwelveLabs", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, IntInput, SecretStrInput"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True), DropdownInput(name='model_name', display_name='Model', advanced=False, options=['Marengo-retrieval-2.7'], value='Marengo-retrieval-2.7'), IntInput(name='request_timeout', display_name='Request Timeout', advanced=True)]", "outputs": "", "display_name": "Twelve Labs Video Embeddings", "name": "TwelveLabsVideoEmbeddings", "description": "Generate embeddings from videos using Twelve Labs video embedding models.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/vector_store.py", "section": "class::VectoStoreRetrieverComponent", "content": "from langchain_core.vectorstores import VectorStoreRetriever\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import VectorStore\n\nclass VectoStoreRetrieverComponent(CustomComponent):\n    display_name: str = \"VectorStore Retriever\"\n    description: str = \"A vector store retriever\"\n    icon = \"LangChain\"\n    name = \"VectorStoreRetriever\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, vectorstore):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VectoStoreRetrieverComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, vectorstore)"], "imports": ["from langchain_core.vectorstores import VectorStoreRetriever", "from langflow.custom import CustomComponent", "from langflow.field_typing import VectorStore"], "inputs": "", "outputs": "", "display_name": "VectorStore Retriever", "name": "VectorStoreRetriever", "description": "A vector store retriever", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/update_data.py", "section": "class::UpdateDataComponent", "content": "from typing import Any\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput, DataInput, DictInput, IntInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\nclass UpdateDataComponent(Component):\n    display_name: str = \"Update Data\"\n    description: str = \"Dynamically update or append data with the specified fields.\"\n    icon = \"FolderSync\"\n    name = \"UpdateData\"\n\n    inputs = [\n        DataInput(name='old_data',\n        display_name='Data',\n        info='The record to update.',\n        is_list=True,\n        required=True),\n        IntInput(name='number_of_fields',\n        display_name='Number of Fields',\n        info='Number of fields to be added to the record.',\n        real_time_refresh=True,\n        value=0,\n        range_spec=RangeSpec(min=1,\n        max=MAX_FIELDS,\n        step=1,\n        step_type='int')),\n        MessageTextInput(name='text_key',\n        display_name='Text Key',\n        info='Key that identifies the field to be used as the text content.',\n        advanced=True),\n        BoolInput(name='text_key_validator',\n        display_name='Text Key Validator',\n        advanced=True,\n        info=\"If enabled,\n        checks if the given 'Text Key' is present in the given 'Data'.\")\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='build_data')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def validate_text_key(self, data):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "UpdateDataComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def get_data(self)", "def validate_text_key(self, data)"], "imports": ["from typing import Any", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs.inputs import BoolInput, DataInput, DictInput, IntInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict"], "inputs": "[DataInput(name='old_data', display_name='Data', info='The record to update.', is_list=True, required=True), IntInput(name='number_of_fields', display_name='Number of Fields', info='Number of fields to be added to the record.', real_time_refresh=True, value=0, range_spec=RangeSpec(min=1, max=MAX_FIELDS, step=1, step_type='int')), MessageTextInput(name='text_key', display_name='Text Key', info='Key that identifies the field to be used as the text content.', advanced=True), BoolInput(name='text_key_validator', display_name='Text Key Validator', advanced=True, info=\"If enabled, checks if the given 'Text Key' is present in the given 'Data'.\")]", "outputs": "[Output(display_name='Data', name='data', method='build_data')]", "display_name": "Update Data", "name": "UpdateData", "description": "Dynamically update or append data with the specified fields.", "icon": "FolderSync"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/firecrawl/firecrawl_crawl_api.py", "section": "class::FirecrawlCrawlApi", "content": "import uuid\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, IntInput, MultilineInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom firecrawl import FirecrawlApp\n\nclass FirecrawlCrawlApi(Component):\n    display_name: str = \"FirecrawlCrawlApi\"\n    description: str = \"Firecrawl Crawl API.\"\n    name = \"FirecrawlCrawlApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        password=True,\n        info='The API key to use Firecrawl API.'),\n        MultilineInput(name='url',\n        display_name='URL',\n        required=True,\n        info='The URL to scrape.',\n        tool_mode=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='Timeout in milliseconds for the request.'),\n        StrInput(name='idempotency_key',\n        display_name='Idempotency Key',\n        info='Optional idempotency key to ensure unique requests.'),\n        DataInput(name='crawlerOptions',\n        display_name='Crawler Options',\n        info='The crawler options to send with the request.'),\n        DataInput(name='scrapeOptions',\n        display_name='Scrape Options',\n        info='The page options to send with the request.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='crawl')\n    ]\n\n    def crawl(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FirecrawlCrawlApi", "base_classes": ["Component"], "public_methods": ["def crawl(self)"], "imports": ["import uuid", "from langflow.custom import Component", "from langflow.io import DataInput, IntInput, MultilineInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data", "from firecrawl import FirecrawlApp"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='url', display_name='URL', required=True, info='The URL to scrape.', tool_mode=True), IntInput(name='timeout', display_name='Timeout', info='Timeout in milliseconds for the request.'), StrInput(name='idempotency_key', display_name='Idempotency Key', info='Optional idempotency key to ensure unique requests.'), DataInput(name='crawlerOptions', display_name='Crawler Options', info='The crawler options to send with the request.'), DataInput(name='scrapeOptions', display_name='Scrape Options', info='The page options to send with the request.')]", "outputs": "[Output(display_name='Data', name='data', method='crawl')]", "display_name": "FirecrawlCrawlApi", "name": "FirecrawlCrawlApi", "description": "Firecrawl Crawl API.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/data_conditional_router.py", "section": "class::DataConditionalRouterComponent", "content": "from typing import Any\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data, dotdict\n\nclass DataConditionalRouterComponent(Component):\n    display_name: str = \"Condition\"\n    description: str = \"Route Data object(s) based on a condition applied to a specified key, including boolean validation.\"\n    icon = \"split\"\n    name = \"DataConditionalRouter\"\n\n    inputs = [\n        DataInput(name='data_input',\n        display_name='Data Input',\n        info='The Data object or list of Data objects to process',\n        is_list=True),\n        MessageTextInput(name='key_name',\n        display_name='Key Name',\n        info='The name of the key in the Data object(s) to check'),\n        DropdownInput(name='operator',\n        display_name='Operator',\n        options=['equals',\n        'not equals',\n        'contains',\n        'starts with',\n        'ends with',\n        'boolean validator'],\n        info=\"The operator to apply for comparing the values. 'boolean validator' treats the value as a boolean.\",\n        value='equals'),\n        MessageTextInput(name='compare_value',\n        display_name='Match Text',\n        info='The value to compare against (not used for boolean validator)')\n    ]\n\n    outputs = [\n        Output(display_name='True Output',\n        name='true_output',\n        method='process_data'),\n        Output(display_name='False Output',\n        name='false_output',\n        method='process_data')\n    ]\n\n    def compare_values(self, item_value, compare_value, operator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_boolean(self, value):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def validate_input(self, data_item):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_single_data(self, data_item):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DataConditionalRouterComponent", "base_classes": ["Component"], "public_methods": ["def compare_values(self, item_value, compare_value, operator)", "def parse_boolean(self, value)", "def validate_input(self, data_item)", "def process_data(self)", "def process_single_data(self, data_item)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, MessageTextInput, Output", "from langflow.schema import Data, dotdict"], "inputs": "[DataInput(name='data_input', display_name='Data Input', info='The Data object or list of Data objects to process', is_list=True), MessageTextInput(name='key_name', display_name='Key Name', info='The name of the key in the Data object(s) to check'), DropdownInput(name='operator', display_name='Operator', options=['equals', 'not equals', 'contains', 'starts with', 'ends with', 'boolean validator'], info=\"The operator to apply for comparing the values. 'boolean validator' treats the value as a boolean.\", value='equals'), MessageTextInput(name='compare_value', display_name='Match Text', info='The value to compare against (not used for boolean validator)')]", "outputs": "[Output(display_name='True Output', name='true_output', method='process_data'), Output(display_name='False Output', name='false_output', method='process_data')]", "display_name": "Condition", "name": "DataConditionalRouter", "description": "Route Data object(s) based on a condition applied to a specified key, including boolean validation.", "icon": "split"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/google_drive_search.py", "section": "class::GoogleDriveSearchComponent", "content": "import json\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, MessageTextInput\nfrom langflow.io import SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass GoogleDriveSearchComponent(Component):\n    display_name: str = \"Google Drive Search\"\n    description: str = \"Searches Google Drive files using provided credentials and query parameters.\"\n    icon = \"Google\"\n\n    inputs = [\n        SecretStrInput(name='token_string',\n        display_name='Token String',\n        info='JSON string containing OAuth 2.0 access token information for service account access',\n        required=True),\n        DropdownInput(name='query_item',\n        display_name='Query Item',\n        options=['name',\n        'fullText',\n        'mimeType',\n        'modifiedTime',\n        'viewedByMeTime',\n        'trashed',\n        'starred',\n        'parents',\n        'owners',\n        'writers',\n        'readers',\n        'sharedWithMe',\n        'createdTime',\n        'properties',\n        'appProperties',\n        'visibility',\n        'shortcutDetails.targetId'],\n        info='The field to query.',\n        required=True),\n        DropdownInput(name='valid_operator',\n        display_name='Valid Operator',\n        options=['contains',\n        '=',\n        '!=',\n        '<=',\n        '<',\n        '>',\n        '>=',\n        'in',\n        'has'],\n        info='Operator to use in the query.',\n        required=True),\n        MessageTextInput(name='search_term',\n        display_name='Search Term',\n        info='The value to search for in the specified query item.',\n        required=True),\n        MessageTextInput(name='query_string',\n        display_name='Query String',\n        info='The query string used for searching. You can edit this manually.',\n        value='')\n    ]\n\n    outputs = [\n        Output(display_name='Document URLs',\n        name='doc_urls',\n        method='search_doc_urls'),\n        Output(display_name='Document IDs',\n        name='doc_ids',\n        method='search_doc_ids'),\n        Output(display_name='Document Titles',\n        name='doc_titles',\n        method='search_doc_titles'),\n        Output(display_name='Data',\n        name='Data',\n        method='search_data')\n    ]\n\n    def generate_query_string(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def on_inputs_changed(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def generate_file_url(self, file_id, mime_type):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_files(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_doc_ids(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_doc_urls(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_doc_titles(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleDriveSearchComponent", "base_classes": ["Component"], "public_methods": ["def generate_query_string(self)", "def on_inputs_changed(self)", "def generate_file_url(self, file_id, mime_type)", "def search_files(self)", "def search_doc_ids(self)", "def search_doc_urls(self)", "def search_doc_titles(self)", "def search_data(self)"], "imports": ["import json", "from google.oauth2.credentials import Credentials", "from googleapiclient.discovery import build", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, MessageTextInput", "from langflow.io import SecretStrInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "[SecretStrInput(name='token_string', display_name='Token String', info='JSON string containing OAuth 2.0 access token information for service account access', required=True), DropdownInput(name='query_item', display_name='Query Item', options=['name', 'fullText', 'mimeType', 'modifiedTime', 'viewedByMeTime', 'trashed', 'starred', 'parents', 'owners', 'writers', 'readers', 'sharedWithMe', 'createdTime', 'properties', 'appProperties', 'visibility', 'shortcutDetails.targetId'], info='The field to query.', required=True), DropdownInput(name='valid_operator', display_name='Valid Operator', options=['contains', '=', '!=', '<=', '<', '>', '>=', 'in', 'has'], info='Operator to use in the query.', required=True), MessageTextInput(name='search_term', display_name='Search Term', info='The value to search for in the specified query item.', required=True), MessageTextInput(name='query_string', display_name='Query String', info='The query string used for searching. You can edit this manually.', value='')]", "outputs": "[Output(display_name='Document URLs', name='doc_urls', method='search_doc_urls'), Output(display_name='Document IDs', name='doc_ids', method='search_doc_ids'), Output(display_name='Document Titles', name='doc_titles', method='search_doc_titles'), Output(display_name='Data', name='Data', method='search_data')]", "display_name": "Google Drive Search", "name": "", "description": "Searches Google Drive files using provided credentials and query parameters.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/language_model.py", "section": "class::LanguageModelComponent", "content": "from typing import Any\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import ChatOpenAI\nfrom langflow.base.models.anthropic_constants import ANTHROPIC_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langflow.schema.dotdict import dotdict\n\nclass LanguageModelComponent(LCModelComponent):\n    display_name: str = \"Language Model\"\n    description: str = \"Runs a language model given a specified provider. \"\n    icon = \"brain-circuit\"\n\n    inputs = [\n        DropdownInput(name='provider',\n        display_name='Model Provider',\n        options=['OpenAI',\n        'Anthropic'],\n        value='OpenAI',\n        info='Select the model provider',\n        real_time_refresh=True,\n        options_metadata=[{'icon': 'OpenAI'},\n        {'icon': 'Anthropic'}]),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        options=OPENAI_MODEL_NAMES,\n        value=OPENAI_MODEL_NAMES[0],\n        info='Select the model to use'),\n        SecretStrInput(name='api_key',\n        display_name='OpenAI API Key',\n        info='Model Provider API key',\n        required=False,\n        show=True,\n        real_time_refresh=True),\n        MessageTextInput(name='input_value',\n        display_name='Input',\n        info='The input text to send to the model'),\n        MessageTextInput(name='system_message',\n        display_name='System Message',\n        info='A system message that helps set the behavior of the assistant',\n        advanced=True),\n        BoolInput(name='stream',\n        display_name='Stream',\n        info='Whether to stream the response',\n        value=False,\n        advanced=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        info='Controls randomness in responses',\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LanguageModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any", "from langchain_anthropic import ChatAnthropic", "from langchain_openai import ChatOpenAI", "from langflow.base.models.anthropic_constants import ANTHROPIC_MODELS", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.openai_constants import OPENAI_MODEL_NAMES", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs.inputs import BoolInput", "from langflow.io import DropdownInput, MessageTextInput, SecretStrInput, SliderInput", "from langflow.schema.dotdict import dotdict"], "inputs": "[DropdownInput(name='provider', display_name='Model Provider', options=['OpenAI', 'Anthropic'], value='OpenAI', info='Select the model provider', real_time_refresh=True, options_metadata=[{'icon': 'OpenAI'}, {'icon': 'Anthropic'}]), DropdownInput(name='model_name', display_name='Model Name', options=OPENAI_MODEL_NAMES, value=OPENAI_MODEL_NAMES[0], info='Select the model to use'), SecretStrInput(name='api_key', display_name='OpenAI API Key', info='Model Provider API key', required=False, show=True, real_time_refresh=True), MessageTextInput(name='input_value', display_name='Input', info='The input text to send to the model'), MessageTextInput(name='system_message', display_name='System Message', info='A system message that helps set the behavior of the assistant', advanced=True), BoolInput(name='stream', display_name='Stream', info='Whether to stream the response', value=False, advanced=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, info='Controls randomness in responses', range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True)]", "outputs": "", "display_name": "Language Model", "name": "", "description": "Runs a language model given a specified provider. ", "icon": "brain-circuit"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/add_content_to_page.py", "section": "class::AddContentToPage", "content": "import json\nfrom typing import Any\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom markdown import markdown\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass AddContentToPage(LCToolComponent):\n    display_name: str = \"Add Content to Page \"\n    description: str = \"Convert markdown text to Notion blocks and append them to a Notion page.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        MultilineInput(name='markdown_text',\n        display_name='Markdown Text',\n        info='The markdown text to convert to Notion blocks.'),\n        StrInput(name='block_id',\n        display_name='Page/Block ID',\n        info='The ID of the page/block to add the content.'),\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_node(self, node):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def extract_language_and_code(self, code_text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def is_code_block(self, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def extract_code_block(self, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def is_table(self, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_list(self, node, list_type):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_table(self, node):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_block(self, block_type, content):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AddContentToPage", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)", "def process_node(self, node)", "def extract_language_and_code(self, code_text)", "def is_code_block(self, text)", "def extract_code_block(self, text)", "def is_table(self, text)", "def process_list(self, node, list_type)", "def process_table(self, node)", "def create_block(self, block_type, content)"], "imports": ["import json", "from typing import Any", "import requests", "from bs4 import BeautifulSoup", "from langchain.tools import StructuredTool", "from loguru import logger", "from markdown import markdown", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[MultilineInput(name='markdown_text', display_name='Markdown Text', info='The markdown text to convert to Notion blocks.'), StrInput(name='block_id', display_name='Page/Block ID', info='The ID of the page/block to add the content.'), SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True)]", "outputs": "", "display_name": "Add Content to Page ", "name": "", "description": "Convert markdown text to Notion blocks and append them to a Notion page.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/add_content_to_page.py", "section": "class::AddContentToPageSchema", "content": "import json\nfrom typing import Any\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom markdown import markdown\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass AddContentToPageSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "AddContentToPageSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "import requests", "from bs4 import BeautifulSoup", "from langchain.tools import StructuredTool", "from loguru import logger", "from markdown import markdown", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/scrapegraph/scrapegraph_smart_scraper_api.py", "section": "class::ScrapeGraphSmartScraperApi", "content": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom scrapegraph_py import Client\nfrom scrapegraph_py.logger import sgai_logger\n\nclass ScrapeGraphSmartScraperApi(Component):\n    display_name: str = \"ScrapeGraphSmartScraperApi\"\n    description: str = \"ScrapeGraph Smart Scraper API.\n    Given a URL, it will return the structured data of the website.\n    More info at https://docs.scrapegraphai.com/services/smartscraper\"\n    name = \"ScrapeGraphSmartScraperApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='ScrapeGraph API Key',\n        required=True,\n        password=True,\n        info='The API key to use ScrapeGraph API.'),\n        MessageTextInput(name='url',\n        display_name='URL',\n        tool_mode=True,\n        info='The URL to scrape.'),\n        MessageTextInput(name='prompt',\n        display_name='Prompt',\n        tool_mode=True,\n        info='The prompt to use for the smart scraper.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='scrape')\n    ]\n\n    def scrape(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ScrapeGraphSmartScraperApi", "base_classes": ["Component"], "public_methods": ["def scrape(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data", "from scrapegraph_py import Client", "from scrapegraph_py.logger import sgai_logger"], "inputs": "[SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='url', display_name='URL', tool_mode=True, info='The URL to scrape.'), MessageTextInput(name='prompt', display_name='Prompt', tool_mode=True, info='The prompt to use for the smart scraper.')]", "outputs": "[Output(display_name='Data', name='data', method='scrape')]", "display_name": "ScrapeGraphSmartScraperApi", "name": "ScrapeGraphSmartScraperApi", "description": "ScrapeGraph Smart Scraper API.\n    Given a URL, it will return the structured data of the website.\n    More info at https://docs.scrapegraphai.com/services/smartscraper", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/elasticsearch.py", "section": "class::ElasticsearchVectorStoreComponent", "content": "from typing import Any\nfrom langchain.schema import Document\nfrom langchain_elasticsearch import ElasticsearchStore\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.io import DropdownInput, FloatInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass ElasticsearchVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    Elasticsearch Vector Store with with advanced, customizable search capabilities.\n    \"\"\"\n\n    display_name: str = \"Elasticsearch\"\n    description: str = \"Elasticsearch Vector Store with with advanced, customizable search capabilities.\"\n    icon = \"ElasticsearchStore\"\n    name = \"Elasticsearch\"\n\n    inputs = [\n        StrInput(name='elasticsearch_url',\n        display_name='Elasticsearch URL',\n        value='http://localhost:9200',\n        info='URL for self-managed Elasticsearch deployments (e.g.,\n        http://localhost:9200). Do not use with Elastic Cloud deployments,\n        use Elastic Cloud ID instead.'),\n        SecretStrInput(name='cloud_id',\n        display_name='Elastic Cloud ID',\n        value='',\n        info=\"Use this for Elastic Cloud deployments. Do not use together with 'Elasticsearch URL'.\"),\n        StrInput(name='index_name',\n        display_name='Index Name',\n        value='langflow',\n        info='The index name where the vectors will be stored in Elasticsearch cluster.'),\n        *LCVectorStoreComponent.inputs,\n        StrInput(name='username',\n        display_name='Username',\n        value='',\n        advanced=False,\n        info=\"Elasticsearch username (e.g.,\n        'elastic'). Required for both local and Elastic Cloud setups unless API keys are used.\"),\n        SecretStrInput(name='password',\n        display_name='Password',\n        value='',\n        advanced=False,\n        info='Elasticsearch password for the specified user. Required for both local and Elastic Cloud setups unless API keys are used.'),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        options=['similarity',\n        'mmr'],\n        value='similarity',\n        advanced=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=4),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info='Minimum similarity score threshold for search results.',\n        value=0.0,\n        advanced=True),\n        SecretStrInput(name='api_key',\n        display_name='Elastic API Key',\n        value='',\n        advanced=True,\n        info=\"API Key for Elastic Cloud authentication. If used,\n        'username' and 'password' are not required.\")\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search(self, query):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_all_documents(self, vector_store):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_retriever_kwargs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ElasticsearchVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search(self, query)", "def get_all_documents(self, vector_store)", "def search_documents(self)", "def get_retriever_kwargs(self)"], "imports": ["from typing import Any", "from langchain.schema import Document", "from langchain_elasticsearch import ElasticsearchStore", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.io import DropdownInput, FloatInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='elasticsearch_url', display_name='Elasticsearch URL', value='http://localhost:9200', info='URL for self-managed Elasticsearch deployments (e.g., http://localhost:9200). Do not use with Elastic Cloud deployments, use Elastic Cloud ID instead.'), SecretStrInput(name='cloud_id', display_name='Elastic Cloud ID', value='', info=\"Use this for Elastic Cloud deployments. Do not use together with 'Elasticsearch URL'.\"), StrInput(name='index_name', display_name='Index Name', value='langflow', info='The index name where the vectors will be stored in Elasticsearch cluster.'), *LCVectorStoreComponent.inputs, StrInput(name='username', display_name='Username', value='', advanced=False, info=\"Elasticsearch username (e.g., 'elastic'). Required for both local and Elastic Cloud setups unless API keys are used.\"), SecretStrInput(name='password', display_name='Password', value='', advanced=False, info='Elasticsearch password for the specified user. Required for both local and Elastic Cloud setups unless API keys are used.'), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), DropdownInput(name='search_type', display_name='Search Type', options=['similarity', 'mmr'], value='similarity', advanced=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=4), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info='Minimum similarity score threshold for search results.', value=0.0, advanced=True), SecretStrInput(name='api_key', display_name='Elastic API Key', value='', advanced=True, info=\"API Key for Elastic Cloud authentication. If used, 'username' and 'password' are not required.\")]", "outputs": "", "display_name": "Elasticsearch", "name": "Elasticsearch", "description": "Elasticsearch Vector Store with with advanced, customizable search capabilities.", "icon": "ElasticsearchStore"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/nvidia/nvidia_ingest.py", "section": "class::NvidiaIngestComponent", "content": "from urllib.parse import urlparse\nfrom langflow.base.data import BaseFileComponent\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\nfrom nv_ingest_client.util.file_processing.extract import EXTENSION_TO_DOCUMENT_TYPE\nfrom nv_ingest_client.client import Ingestor\n\nclass NvidiaIngestComponent(BaseFileComponent):\n    display_name: str = \"NVIDIA Retriever Extraction\"\n    description: str = \"Multi-modal data extraction from documents using NVIDIA's NeMo API.\"\n    icon = \"NVIDIA\"\n\n    inputs = [\n        *BaseFileComponent._base_inputs,\n        MessageTextInput(name='base_url',\n        display_name='Base URL',\n        info='The URL of the NVIDIA NeMo Retriever Extraction API.',\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='NVIDIA API Key'),\n        BoolInput(name='extract_text',\n        display_name='Extract Text',\n        info='Extract text from documents',\n        value=True),\n        BoolInput(name='extract_charts',\n        display_name='Extract Charts',\n        info='Extract text from charts',\n        value=False),\n        BoolInput(name='extract_tables',\n        display_name='Extract Tables',\n        info='Extract text from tables',\n        value=False),\n        BoolInput(name='extract_images',\n        display_name='Extract Images',\n        info='Extract images from document',\n        value=True),\n        DropdownInput(name='text_depth',\n        display_name='Text Depth',\n        info=\"Level at which text is extracted (applies before splitting). Support for 'block',\n        'line',\n        'span' varies by document type.\",\n        options=['document',\n        'page',\n        'block',\n        'line',\n        'span'],\n        value='page',\n        advanced=True),\n        BoolInput(name='split_text',\n        display_name='Split Text',\n        info='Split text into smaller chunks',\n        value=True,\n        advanced=True),\n        IntInput(name='chunk_size',\n        display_name='Chunk size',\n        info='The number of tokens per chunk',\n        value=500,\n        advanced=True),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='Number of tokens to overlap from previous chunk',\n        value=150,\n        advanced=True),\n        BoolInput(name='filter_images',\n        display_name='Filter Images',\n        info='Filter images (see advanced options for filtering criteria).',\n        advanced=True,\n        value=True),\n        IntInput(name='min_image_size',\n        display_name='Minimum Image Size Filter',\n        info='Minimum image width/length in pixels',\n        value=128,\n        advanced=True),\n        FloatInput(name='min_aspect_ratio',\n        display_name='Minimum Aspect Ratio Filter',\n        info='Minimum allowed aspect ratio (width / height). Images narrower than this will be filtered out.',\n        value=0.2,\n        advanced=True),\n        FloatInput(name='max_aspect_ratio',\n        display_name='Maximum Aspect Ratio Filter',\n        info='Maximum allowed aspect ratio (width / height). Images taller than this will be filtered out.',\n        value=5.0,\n        advanced=True),\n        BoolInput(name='dedup_images',\n        display_name='Deduplicate Images',\n        info='Filter duplicated images.',\n        advanced=True,\n        value=True),\n        BoolInput(name='caption_images',\n        display_name='Caption Images',\n        info='Generate captions for images using the NVIDIA captioning model.',\n        advanced=True,\n        value=True)\n    ]\n\n    outputs = [\n        *BaseFileComponent._base_outputs\n    ]\n\n    def process_files(self, file_list):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NvidiaIngestComponent", "base_classes": ["BaseFileComponent"], "public_methods": ["def process_files(self, file_list)"], "imports": ["from urllib.parse import urlparse", "from langflow.base.data import BaseFileComponent", "from langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data", "from nv_ingest_client.util.file_processing.extract import EXTENSION_TO_DOCUMENT_TYPE", "from nv_ingest_client.client import Ingestor"], "inputs": "[*BaseFileComponent._base_inputs, MessageTextInput(name='base_url', display_name='Base URL', info='The URL of the NVIDIA NeMo Retriever Extraction API.', required=True), SecretStrInput(name='api_key', display_name='NVIDIA API Key'), BoolInput(name='extract_text', display_name='Extract Text', info='Extract text from documents', value=True), BoolInput(name='extract_charts', display_name='Extract Charts', info='Extract text from charts', value=False), BoolInput(name='extract_tables', display_name='Extract Tables', info='Extract text from tables', value=False), BoolInput(name='extract_images', display_name='Extract Images', info='Extract images from document', value=True), DropdownInput(name='text_depth', display_name='Text Depth', info=\"Level at which text is extracted (applies before splitting). Support for 'block', 'line', 'span' varies by document type.\", options=['document', 'page', 'block', 'line', 'span'], value='page', advanced=True), BoolInput(name='split_text', display_name='Split Text', info='Split text into smaller chunks', value=True, advanced=True), IntInput(name='chunk_size', display_name='Chunk size', info='The number of tokens per chunk', value=500, advanced=True), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='Number of tokens to overlap from previous chunk', value=150, advanced=True), BoolInput(name='filter_images', display_name='Filter Images', info='Filter images (see advanced options for filtering criteria).', advanced=True, value=True), IntInput(name='min_image_size', display_name='Minimum Image Size Filter', info='Minimum image width/length in pixels', value=128, advanced=True), FloatInput(name='min_aspect_ratio', display_name='Minimum Aspect Ratio Filter', info='Minimum allowed aspect ratio (width / height). Images narrower than this will be filtered out.', value=0.2, advanced=True), FloatInput(name='max_aspect_ratio', display_name='Maximum Aspect Ratio Filter', info='Maximum allowed aspect ratio (width / height). Images taller than this will be filtered out.', value=5.0, advanced=True), BoolInput(name='dedup_images', display_name='Deduplicate Images', info='Filter duplicated images.', advanced=True, value=True), BoolInput(name='caption_images', display_name='Caption Images', info='Generate captions for images using the NVIDIA captioning model.', advanced=True, value=True)]", "outputs": "[*BaseFileComponent._base_outputs]", "display_name": "NVIDIA Retriever Extraction", "name": "", "description": "Multi-modal data extraction from documents using NVIDIA's NeMo API.", "icon": "NVIDIA"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/json_to_data.py", "section": "class::JSONToDataComponent", "content": "import json\nfrom pathlib import Path\nfrom json_repair import repair_json\nfrom langflow.custom import Component\nfrom langflow.io import FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema import Data\n\nclass JSONToDataComponent(Component):\n    display_name: str = \"Load JSON\"\n    description: str = \"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects\"\n    icon = \"braces\"\n    name = \"JSONtoData\"\n\n    inputs = [\n        FileInput(name='json_file',\n        display_name='JSON File',\n        file_types=['json'],\n        info='Upload a JSON file to convert to a Data object or list of Data objects'),\n        MessageTextInput(name='json_path',\n        display_name='JSON File Path',\n        info='Provide the path to the JSON file as pure text'),\n        MultilineInput(name='json_string',\n        display_name='JSON String',\n        info='Enter a valid JSON string (object or array) to convert to a Data object or list of Data objects')\n    ]\n\n    outputs = [\n        Output(name='data',\n        display_name='Data',\n        method='convert_json_to_data')\n    ]\n\n    def convert_json_to_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "JSONToDataComponent", "base_classes": ["Component"], "public_methods": ["def convert_json_to_data(self)"], "imports": ["import json", "from pathlib import Path", "from json_repair import repair_json", "from langflow.custom import Component", "from langflow.io import FileInput, MessageTextInput, MultilineInput, Output", "from langflow.schema import Data"], "inputs": "[FileInput(name='json_file', display_name='JSON File', file_types=['json'], info='Upload a JSON file to convert to a Data object or list of Data objects'), MessageTextInput(name='json_path', display_name='JSON File Path', info='Provide the path to the JSON file as pure text'), MultilineInput(name='json_string', display_name='JSON String', info='Enter a valid JSON string (object or array) to convert to a Data object or list of Data objects')]", "outputs": "[Output(name='data', display_name='Data', method='convert_json_to_data')]", "display_name": "Load JSON", "name": "JSONtoData", "description": "Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects", "icon": "braces"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/embed.py", "section": "class::EmbedComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.schema import Data\n\nclass EmbedComponent(CustomComponent):\n    display_name: str = \"Embed Texts\"\n    name = \"Embed\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, texts, embbedings):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "EmbedComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, texts, embbedings)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.field_typing import Embeddings", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "Embed Texts", "name": "Embed", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/outputs/text.py", "section": "class::TextOutputComponent", "content": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\nclass TextOutputComponent(TextComponent):\n    display_name: str = \"Text Output\"\n    description: str = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Text',\n        info='Text to be passed as output.')\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='text',\n        method='text_response')\n    ]\n\n    def text_response(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TextOutputComponent", "base_classes": ["TextComponent"], "public_methods": ["def text_response(self)"], "imports": ["from langflow.base.io.text import TextComponent", "from langflow.io import MultilineInput, Output", "from langflow.schema.message import Message"], "inputs": "[MultilineInput(name='input_value', display_name='Text', info='Text to be passed as output.')]", "outputs": "[Output(display_name='Message', name='text', method='text_response')]", "display_name": "Text Output", "name": "TextOutput", "description": "Display a text output in the Playground.", "icon": "type"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/create_list.py", "section": "class::CreateListComponent", "content": "from langflow.custom import Component\nfrom langflow.inputs import StrInput\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.template import Output\n\nclass CreateListComponent(Component):\n    display_name: str = \"Create List\"\n    description: str = \"Creates a list of texts.\"\n    icon = \"list\"\n    name = \"CreateList\"\n\n    inputs = [\n        StrInput(name='texts',\n        display_name='Texts',\n        info='Enter one or more texts.',\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data List',\n        name='list',\n        method='create_list'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def create_list(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CreateListComponent", "base_classes": ["Component"], "public_methods": ["def create_list(self)", "def as_dataframe(self)"], "imports": ["from langflow.custom import Component", "from langflow.inputs import StrInput", "from langflow.schema import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.template import Output"], "inputs": "[StrInput(name='texts', display_name='Texts', info='Enter one or more texts.', is_list=True)]", "outputs": "[Output(display_name='Data List', name='list', method='create_list'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Create List", "name": "CreateList", "description": "Creates a list of texts.", "icon": "list"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/git/gitextractor.py", "section": "class::GitExtractorComponent", "content": "import os\nimport shutil\nimport tempfile\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\nimport aiofiles\nimport git\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass GitExtractorComponent(Component):\n    display_name: str = \"GitExtractor\"\n    description: str = \"Analyzes a Git repository and returns file contents and complete repository information\"\n    icon = \"GitLoader\"\n\n    inputs = [\n        MessageTextInput(name='repository_url',\n        display_name='Repository URL',\n        info='URL of the Git repository (e.g.,\n        https://github.com/username/repo)',\n        value='')\n    ]\n\n    outputs = [\n        Output(display_name='Text-Based File Contents',\n        name='text_based_file_contents',\n        method='get_text_based_file_contents'),\n        Output(display_name='Directory Structure',\n        name='directory_structure',\n        method='get_directory_structure'),\n        Output(display_name='Repository Info',\n        name='repository_info',\n        method='get_repository_info'),\n        Output(display_name='Statistics',\n        name='statistics',\n        method='get_statistics'),\n        Output(display_name='Files Content',\n        name='files_content',\n        method='get_files_content')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "GitExtractorComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["import os", "import shutil", "import tempfile", "from contextlib import asynccontextmanager", "from pathlib import Path", "import aiofiles", "import git", "from langflow.custom import Component", "from langflow.io import MessageTextInput, Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='repository_url', display_name='Repository URL', info='URL of the Git repository (e.g., https://github.com/username/repo)', value='')]", "outputs": "[Output(display_name='Text-Based File Contents', name='text_based_file_contents', method='get_text_based_file_contents'), Output(display_name='Directory Structure', name='directory_structure', method='get_directory_structure'), Output(display_name='Repository Info', name='repository_info', method='get_repository_info'), Output(display_name='Statistics', name='statistics', method='get_statistics'), Output(display_name='Files Content', name='files_content', method='get_files_content')]", "display_name": "GitExtractor", "name": "", "description": "Analyzes a Git repository and returns file contents and complete repository information", "icon": "GitLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/inputs/text.py", "section": "class::TextInputComponent", "content": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\nclass TextInputComponent(TextComponent):\n    display_name: str = \"Text Input\"\n    description: str = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Text',\n        info='Text to be passed as input.')\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='text',\n        method='text_response')\n    ]\n\n    def text_response(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TextInputComponent", "base_classes": ["TextComponent"], "public_methods": ["def text_response(self)"], "imports": ["from langflow.base.io.text import TextComponent", "from langflow.io import MultilineInput, Output", "from langflow.schema.message import Message"], "inputs": "[MultilineInput(name='input_value', display_name='Text', info='Text to be passed as input.')]", "outputs": "[Output(display_name='Message', name='text', method='text_response')]", "display_name": "Text Input", "name": "TextInput", "description": "Get text inputs from the Playground.", "icon": "type"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/crewai/hierarchical_crew.py", "section": "class::HierarchicalCrewComponent", "content": "from crewai import Crew, Process\nfrom langflow.base.agents.crewai.crew import BaseCrewComponent\nfrom langflow.io import HandleInput\n\nclass HierarchicalCrewComponent(BaseCrewComponent):\n    display_name: str = \"Hierarchical Crew\"\n    description: str = \"Represents a group of agents, defining how they should collaborate and the tasks they should perform.\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        *BaseCrewComponent._base_inputs,\n        HandleInput(name='agents',\n        display_name='Agents',\n        input_types=['Agent'],\n        is_list=True),\n        HandleInput(name='tasks',\n        display_name='Tasks',\n        input_types=['HierarchicalTask'],\n        is_list=True),\n        HandleInput(name='manager_llm',\n        display_name='Manager LLM',\n        input_types=['LanguageModel'],\n        required=False),\n        HandleInput(name='manager_agent',\n        display_name='Manager Agent',\n        input_types=['Agent'],\n        required=False)\n    ]\n\n    def build_crew(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HierarchicalCrewComponent", "base_classes": ["BaseCrewComponent"], "public_methods": ["def build_crew(self)"], "imports": ["from crewai import Crew, Process", "from langflow.base.agents.crewai.crew import BaseCrewComponent", "from langflow.io import HandleInput"], "inputs": "[*BaseCrewComponent._base_inputs, HandleInput(name='agents', display_name='Agents', input_types=['Agent'], is_list=True), HandleInput(name='tasks', display_name='Tasks', input_types=['HierarchicalTask'], is_list=True), HandleInput(name='manager_llm', display_name='Manager LLM', input_types=['LanguageModel'], required=False), HandleInput(name='manager_agent', display_name='Manager Agent', input_types=['Agent'], required=False)]", "outputs": "", "display_name": "Hierarchical Crew", "name": "", "description": "Represents a group of agents, defining how they should collaborate and the tasks they should perform.", "icon": "CrewAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/retrievers/multi_query.py", "section": "class::MultiQueryRetrieverComponent", "content": "from langchain.retrievers import MultiQueryRetriever\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import BaseRetriever, LanguageModel, PromptTemplate, Text\n\nclass MultiQueryRetrieverComponent(CustomComponent):\n    display_name: str = \"MultiQueryRetriever\"\n    description: str = \"Initialize from llm using default template.\"\n    name = \"MultiQueryRetriever\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, llm, retriever, prompt, parser_key):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MultiQueryRetrieverComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, llm, retriever, prompt, parser_key)"], "imports": ["from langchain.retrievers import MultiQueryRetriever", "from langflow.custom import CustomComponent", "from langflow.field_typing import BaseRetriever, LanguageModel, PromptTemplate, Text"], "inputs": "", "outputs": "", "display_name": "MultiQueryRetriever", "name": "MultiQueryRetriever", "description": "Initialize from llm using default template.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/serp.py", "section": "class::SerpAPISchema", "content": "from typing import Any\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.custom import Component\nfrom langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass SerpAPISchema(BaseModel):\n    \"\"\"\n    Schema for SerpAPI search parameters.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "SerpAPISchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from typing import Any", "from langchain_community.utilities.serpapi import SerpAPIWrapper", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.custom import Component", "from langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/serp.py", "section": "class::SerpComponent", "content": "from typing import Any\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.custom import Component\nfrom langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass SerpComponent(Component):\n    display_name: str = \"Serp Search API\"\n    description: str = \"Call Serp Search API with result limiting\"\n    icon = \"SerpSearch\"\n    name = \"Serp\"\n\n    inputs = [\n        SecretStrInput(name='serpapi_api_key',\n        display_name='SerpAPI API Key',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input',\n        tool_mode=True),\n        DictInput(name='search_params',\n        display_name='Parameters',\n        advanced=True,\n        is_list=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=5,\n        advanced=True),\n        IntInput(name='max_snippet_length',\n        display_name='Max Snippet Length',\n        value=100,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Text',\n        name='text',\n        method='fetch_content_text')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SerpComponent", "base_classes": ["Component"], "public_methods": ["def run_model(self)", "def fetch_content(self)", "def fetch_content_text(self)"], "imports": ["from typing import Any", "from langchain_community.utilities.serpapi import SerpAPIWrapper", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.custom import Component", "from langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[SecretStrInput(name='serpapi_api_key', display_name='SerpAPI API Key', required=True), MultilineInput(name='input_value', display_name='Input', tool_mode=True), DictInput(name='search_params', display_name='Parameters', advanced=True, is_list=True), IntInput(name='max_results', display_name='Max Results', value=5, advanced=True), IntInput(name='max_snippet_length', display_name='Max Snippet Length', value=100, advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Text', name='text', method='fetch_content_text')]", "display_name": "Serp Search API", "name": "Serp", "description": "Call Serp Search API with result limiting", "icon": "SerpSearch"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/embedding_model.py", "section": "class::EmbeddingModelComponent", "content": "from typing import Any\nfrom langchain_openai import OpenAIEmbeddings\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema.dotdict import dotdict\n\nclass EmbeddingModelComponent(LCEmbeddingsModel):\n    display_name: str = \"Embedding Model\"\n    description: str = \"Generate embeddings using a specified provider.\"\n    icon = \"binary\"\n    name = \"EmbeddingModel\"\n\n    inputs = [\n        DropdownInput(name='provider',\n        display_name='Model Provider',\n        options=['OpenAI'],\n        value='OpenAI',\n        info='Select the embedding model provider',\n        real_time_refresh=True,\n        options_metadata=[{'icon': 'OpenAI'}]),\n        DropdownInput(name='model',\n        display_name='Model Name',\n        options=OPENAI_EMBEDDING_MODEL_NAMES,\n        value=OPENAI_EMBEDDING_MODEL_NAMES[0],\n        info='Select the embedding model to use'),\n        SecretStrInput(name='api_key',\n        display_name='OpenAI API Key',\n        info='Model Provider API key',\n        required=True,\n        show=True,\n        real_time_refresh=True),\n        MessageTextInput(name='api_base',\n        display_name='API Base URL',\n        info='Base URL for the API. Leave empty for default.',\n        advanced=True),\n        IntInput(name='dimensions',\n        display_name='Dimensions',\n        info='The number of dimensions the resulting output embeddings should have. Only supported by certain models.',\n        advanced=True),\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        advanced=True,\n        value=1000),\n        FloatInput(name='request_timeout',\n        display_name='Request Timeout',\n        advanced=True),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        advanced=True,\n        value=3),\n        BoolInput(name='show_progress_bar',\n        display_name='Show Progress Bar',\n        advanced=True),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True,\n        info='Additional keyword arguments to pass to the model.')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "EmbeddingModelComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any", "from langchain_openai import OpenAIEmbeddings", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES", "from langflow.field_typing import Embeddings", "from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema.dotdict import dotdict"], "inputs": "[DropdownInput(name='provider', display_name='Model Provider', options=['OpenAI'], value='OpenAI', info='Select the embedding model provider', real_time_refresh=True, options_metadata=[{'icon': 'OpenAI'}]), DropdownInput(name='model', display_name='Model Name', options=OPENAI_EMBEDDING_MODEL_NAMES, value=OPENAI_EMBEDDING_MODEL_NAMES[0], info='Select the embedding model to use'), SecretStrInput(name='api_key', display_name='OpenAI API Key', info='Model Provider API key', required=True, show=True, real_time_refresh=True), MessageTextInput(name='api_base', display_name='API Base URL', info='Base URL for the API. Leave empty for default.', advanced=True), IntInput(name='dimensions', display_name='Dimensions', info='The number of dimensions the resulting output embeddings should have. Only supported by certain models.', advanced=True), IntInput(name='chunk_size', display_name='Chunk Size', advanced=True, value=1000), FloatInput(name='request_timeout', display_name='Request Timeout', advanced=True), IntInput(name='max_retries', display_name='Max Retries', advanced=True, value=3), BoolInput(name='show_progress_bar', display_name='Show Progress Bar', advanced=True), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True, info='Additional keyword arguments to pass to the model.')]", "outputs": "", "display_name": "Embedding Model", "name": "EmbeddingModel", "description": "Generate embeddings using a specified provider.", "icon": "binary"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/split_video.py", "section": "class::SplitVideoComponent", "content": "import hashlib\nimport math\nimport subprocess\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, DropdownInput, HandleInput, IntInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass SplitVideoComponent(Component):\n    \"\"\"\n    A component that splits a video into multiple clips of specified duration using FFmpeg.\n    \"\"\"\n\n    display_name: str = \"Split Video\"\n    description: str = \"Split a video into multiple clips of specified duration.\"\n    icon = \"TwelveLabs\"\n    name = \"SplitVideo\"\n\n    inputs = [\n        HandleInput(name='videodata',\n        display_name='Video Data',\n        info='Input video data from VideoFile component',\n        required=True,\n        input_types=['Data']),\n        IntInput(name='clip_duration',\n        display_name='Clip Duration (seconds)',\n        info='Duration of each clip in seconds',\n        required=True,\n        value=30),\n        DropdownInput(name='last_clip_handling',\n        display_name='Last Clip Handling',\n        info=\"How to handle the final clip when it would be shorter than the specified duration:\\n- Truncate: Skip the final clip entirely if it's shorter than the specified duration\\n- Overlap Previous: Start the final clip earlier to maintain full duration,\n        overlapping with previous clip\\n- Keep Short: Keep the final clip at its natural length,\n        even if shorter than specified duration\",\n        options=['Truncate',\n        'Overlap Previous',\n        'Keep Short'],\n        value='Overlap Previous',\n        required=True),\n        BoolInput(name='include_original',\n        display_name='Include Original Video',\n        info='Whether to include the original video in the output',\n        value=False)\n    ]\n\n    outputs = [\n        Output(name='clips',\n        display_name='Video Clips',\n        method='process',\n        output_types=['Data'])\n    ]\n\n    def get_video_duration(self, video_path):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_output_dir(self, video_path):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_video(self, video_path, clip_duration):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SplitVideoComponent", "base_classes": ["Component"], "public_methods": ["def get_video_duration(self, video_path)", "def get_output_dir(self, video_path)", "def process_video(self, video_path, clip_duration)", "def process(self)"], "imports": ["import hashlib", "import math", "import subprocess", "from datetime import datetime, timezone", "from pathlib import Path", "from typing import Any", "from langflow.custom import Component", "from langflow.inputs import BoolInput, DropdownInput, HandleInput, IntInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "[HandleInput(name='videodata', display_name='Video Data', info='Input video data from VideoFile component', required=True, input_types=['Data']), IntInput(name='clip_duration', display_name='Clip Duration (seconds)', info='Duration of each clip in seconds', required=True, value=30), DropdownInput(name='last_clip_handling', display_name='Last Clip Handling', info=\"How to handle the final clip when it would be shorter than the specified duration:\\n- Truncate: Skip the final clip entirely if it's shorter than the specified duration\\n- Overlap Previous: Start the final clip earlier to maintain full duration, overlapping with previous clip\\n- Keep Short: Keep the final clip at its natural length, even if shorter than specified duration\", options=['Truncate', 'Overlap Previous', 'Keep Short'], value='Overlap Previous', required=True), BoolInput(name='include_original', display_name='Include Original Video', info='Whether to include the original video in the output', value=False)]", "outputs": "[Output(name='clips', display_name='Video Clips', method='process', output_types=['Data'])]", "display_name": "Split Video", "name": "SplitVideo", "description": "Split a video into multiple clips of specified duration.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/amazon/s3_bucket_uploader.py", "section": "class::S3BucketUploaderComponent", "content": "from pathlib import Path\nfrom typing import Any\nimport boto3\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, Output, SecretStrInput, StrInput\n\nclass S3BucketUploaderComponent(Component):\n    \"\"\"\n    S3BucketUploaderComponent is a component responsible for uploading files to an S3 bucket.\n    \n    It provides two strategies for file upload: \"By Data\" and \"By File Name\". The component\n    requires AWS credentials and bucket details as inputs and processes files accordingly.\n    \n    Attributes:\n        display_name (str): The display name of the component.\n        description (str): A brief description of the components functionality.\n        icon (str): The icon representing the component.\n        name (str): The internal name of the component.\n        inputs (list): A list of input configurations required by the component.\n        outputs (list): A list of output configurations provided by the component.\n    \n    Methods:\n        process_files() -> None:\n            Processes files based on the selected strategy. Calls the appropriate method\n            based on the strategy attribute.\n        process_files_by_data() -> None:\n            Processes and uploads files to an S3 bucket based on the data inputs. Iterates\n            over the data inputs, logs the file path and text content, and uploads each file\n            to the specified S3 bucket if both file path and text content are available.\n        process_files_by_name() -> None:\n            Processes and uploads files to an S3 bucket based on their names. Iterates through\n            the list of data inputs, retrieves the file path from each data item, and uploads\n            the file to the specified S3 bucket if the file path is available. Logs the file\n            path being uploaded.\n        _s3_client() -> Any:\n            Creates and returns an S3 client using the provided AWS access key ID and secret\n            access key.\n    \n        Please note that this component requires the boto3 library to be installed. It is designed\n        to work with File and Director components as inputs\n    \"\"\"\n\n    display_name: str = \"S3 Bucket Uploader\"\n    description: str = \"Uploads files to S3 bucket.\"\n    icon = \"Amazon\"\n    name = \"s3bucketuploader\"\n\n    inputs = [\n        SecretStrInput(name='aws_access_key_id',\n        display_name='AWS Access Key ID',\n        required=True,\n        password=True,\n        info='AWS Access key ID.'),\n        SecretStrInput(name='aws_secret_access_key',\n        display_name='AWS Secret Key',\n        required=True,\n        password=True,\n        info='AWS Secret Key.'),\n        StrInput(name='bucket_name',\n        display_name='Bucket Name',\n        info='Enter the name of the bucket.',\n        advanced=False),\n        DropdownInput(name='strategy',\n        display_name='Strategy for file upload',\n        options=['Store Data',\n        'Store Original File'],\n        value='By Data',\n        info='Choose the strategy to upload the file. By Data means that the source file is parsed and stored as LangFlow data. By File Name means that the source file is uploaded as is.'),\n        HandleInput(name='data_inputs',\n        display_name='Data Inputs',\n        info='The data to split.',\n        input_types=['Data'],\n        is_list=True,\n        required=True),\n        StrInput(name='s3_prefix',\n        display_name='S3 Prefix',\n        info='Prefix for all files.',\n        advanced=True),\n        BoolInput(name='strip_path',\n        display_name='Strip Path',\n        info='Removes path from file path.',\n        required=True,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Writes to AWS Bucket',\n        name='data',\n        method='process_files')\n    ]\n\n    def process_files(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_files_by_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def process_files_by_name(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "S3BucketUploaderComponent", "base_classes": ["Component"], "public_methods": ["def process_files(self)", "def process_files_by_data(self)", "def process_files_by_name(self)"], "imports": ["from pathlib import Path", "from typing import Any", "import boto3", "from langflow.custom import Component", "from langflow.io import BoolInput, DropdownInput, HandleInput, Output, SecretStrInput, StrInput"], "inputs": "[SecretStrInput(name='aws_access_key_id', display_name='AWS Access Key ID', required=True, password=True, info='AWS Access key ID.'), SecretStrInput(name='aws_secret_access_key', display_name='AWS Secret Key', required=True, password=True, info='AWS Secret Key.'), StrInput(name='bucket_name', display_name='Bucket Name', info='Enter the name of the bucket.', advanced=False), DropdownInput(name='strategy', display_name='Strategy for file upload', options=['Store Data', 'Store Original File'], value='By Data', info='Choose the strategy to upload the file. By Data means that the source file is parsed and stored as LangFlow data. By File Name means that the source file is uploaded as is.'), HandleInput(name='data_inputs', display_name='Data Inputs', info='The data to split.', input_types=['Data'], is_list=True, required=True), StrInput(name='s3_prefix', display_name='S3 Prefix', info='Prefix for all files.', advanced=True), BoolInput(name='strip_path', display_name='Strip Path', info='Removes path from file path.', required=True, advanced=True)]", "outputs": "[Output(display_name='Writes to AWS Bucket', name='data', method='process_files')]", "display_name": "S3 Bucket Uploader", "name": "s3bucketuploader", "description": "Uploads files to S3 bucket.", "icon": "Amazon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/conversation.py", "section": "class::ConversationChainComponent", "content": "from langchain.chains import ConversationChain\nfrom langflow.base.chains.model import LCChainComponent\nfrom langflow.field_typing import Message\nfrom langflow.inputs import HandleInput, MultilineInput\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name: str = \"ConversationChain\"\n    description: str = \"Chain to have a conversation and load context from memory.\"\n    icon = \"LangChain\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input',\n        info='The input value to pass to the chain.',\n        required=True),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        HandleInput(name='memory',\n        display_name='Memory',\n        input_types=['BaseChatMemory'])\n    ]\n\n    def invoke_chain(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ConversationChainComponent", "base_classes": ["LCChainComponent"], "public_methods": ["def invoke_chain(self)"], "imports": ["from langchain.chains import ConversationChain", "from langflow.base.chains.model import LCChainComponent", "from langflow.field_typing import Message", "from langflow.inputs import HandleInput, MultilineInput"], "inputs": "[MultilineInput(name='input_value', display_name='Input', info='The input value to pass to the chain.', required=True), HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), HandleInput(name='memory', display_name='Memory', input_types=['BaseChatMemory'])]", "outputs": "", "display_name": "ConversationChain", "name": "ConversationChain", "description": "Chain to have a conversation and load context from memory.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/combine_text.py", "section": "class::CombineTextComponent", "content": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\nclass CombineTextComponent(Component):\n    display_name: str = \"Combine Text\"\n    description: str = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(name='text1',\n        display_name='First Text',\n        info='The first text input to concatenate.'),\n        MessageTextInput(name='text2',\n        display_name='Second Text',\n        info='The second text input to concatenate.'),\n        MessageTextInput(name='delimiter',\n        display_name='Delimiter',\n        info='A string used to separate the two text inputs. Defaults to a whitespace.',\n        value=' ')\n    ]\n\n    outputs = [\n        Output(display_name='Combined Text',\n        name='combined_text',\n        method='combine_texts')\n    ]\n\n    def combine_texts(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CombineTextComponent", "base_classes": ["Component"], "public_methods": ["def combine_texts(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import MessageTextInput, Output", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='text1', display_name='First Text', info='The first text input to concatenate.'), MessageTextInput(name='text2', display_name='Second Text', info='The second text input to concatenate.'), MessageTextInput(name='delimiter', display_name='Delimiter', info='A string used to separate the two text inputs. Defaults to a whitespace.', value=' ')]", "outputs": "[Output(display_name='Combined Text', name='combined_text', method='combine_texts')]", "display_name": "Combine Text", "name": "CombineText", "description": "Concatenate two text sources into a single text chunk using a specified delimiter.", "icon": "merge"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/firecrawl/firecrawl_extract_api.py", "section": "class::FirecrawlExtractApi", "content": "from loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DataInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom firecrawl import FirecrawlApp\n\nclass FirecrawlExtractApi(Component):\n    display_name: str = \"FirecrawlExtractApi\"\n    description: str = \"Firecrawl Extract API.\"\n    name = \"FirecrawlExtractApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        password=True,\n        info='The API key to use Firecrawl API.'),\n        MultilineInput(name='urls',\n        display_name='URLs',\n        required=True,\n        info='List of URLs to extract data from (separated by commas or new lines).',\n        tool_mode=True),\n        MultilineInput(name='prompt',\n        display_name='Prompt',\n        required=True,\n        info='Prompt to guide the extraction process.',\n        tool_mode=True),\n        DataInput(name='schema',\n        display_name='Schema',\n        required=False,\n        info='Schema to define the structure of the extracted data.'),\n        BoolInput(name='enable_web_search',\n        display_name='Enable Web Search',\n        info='When true,\n        the extraction will use web search to find additional data.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='extract')\n    ]\n\n    def extract(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FirecrawlExtractApi", "base_classes": ["Component"], "public_methods": ["def extract(self)"], "imports": ["from loguru import logger", "from langflow.custom import Component", "from langflow.io import BoolInput, DataInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import Data", "from firecrawl import FirecrawlApp"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='urls', display_name='URLs', required=True, info='List of URLs to extract data from (separated by commas or new lines).', tool_mode=True), MultilineInput(name='prompt', display_name='Prompt', required=True, info='Prompt to guide the extraction process.', tool_mode=True), DataInput(name='schema', display_name='Schema', required=False, info='Schema to define the structure of the extracted data.'), BoolInput(name='enable_web_search', display_name='Enable Web Search', info='When true, the extraction will use web search to find additional data.')]", "outputs": "[Output(display_name='Data', name='data', method='extract')]", "display_name": "FirecrawlExtractApi", "name": "FirecrawlExtractApi", "description": "Firecrawl Extract API.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/gmail.py", "section": "class::GmailLoaderComponent", "content": "import base64\nimport json\nimport re\nfrom collections.abc import Iterator\nfrom json.decoder import JSONDecodeError\nfrom typing import Any\nfrom google.auth.exceptions import RefreshError\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom langchain_core.chat_sessions import ChatSession\nfrom langchain_core.messages import HumanMessage\nfrom langchain_google_community.gmail.loader import GMailLoader\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass GmailLoaderComponent(Component):\n    display_name: str = \"Gmail Loader\"\n    description: str = \"Loads emails from Gmail using provided credentials.\"\n    icon = \"Google\"\n\n    inputs = [\n        SecretStrInput(name='json_string',\n        display_name='JSON String of the Service Account Token',\n        info='JSON string containing OAuth 2.0 access token information for service account access',\n        required=True,\n        value='{\\n                \"account\": \"\",\\n                \"client_id\": \"\",\\n                \"client_secret\": \"\",\\n                \"expiry\": \"\",\\n                \"refresh_token\": \"\",\\n                \"scopes\": [\\n                    \"https://www.googleapis.com/auth/gmail.readonly\",\\n                ],\\n                \"token\": \"\",\\n                \"token_uri\": \"https://oauth2.googleapis.com/token\",\\n                \"universe_domain\": \"googleapis.com\"\\n            }'),\n        MessageTextInput(name='label_ids',\n        display_name='Label IDs',\n        info='Comma-separated list of label IDs to filter emails.',\n        required=True,\n        value='INBOX,SENT,UNREAD,IMPORTANT'),\n        MessageTextInput(name='max_results',\n        display_name='Max Results',\n        info='Maximum number of emails to load.',\n        required=True,\n        value='10')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='load_emails')\n    ]\n\n    def load_emails(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GmailLoaderComponent", "base_classes": ["Component"], "public_methods": ["def load_emails(self)"], "imports": ["import base64", "import json", "import re", "from collections.abc import Iterator", "from json.decoder import JSONDecodeError", "from typing import Any", "from google.auth.exceptions import RefreshError", "from google.oauth2.credentials import Credentials", "from googleapiclient.discovery import build", "from langchain_core.chat_sessions import ChatSession", "from langchain_core.messages import HumanMessage", "from langchain_google_community.gmail.loader import GMailLoader", "from loguru import logger", "from langflow.custom import Component", "from langflow.inputs import MessageTextInput", "from langflow.io import SecretStrInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "[SecretStrInput(name='json_string', display_name='JSON String of the Service Account Token', info='JSON string containing OAuth 2.0 access token information for service account access', required=True, value='{\\n                \"account\": \"\",\\n                \"client_id\": \"\",\\n                \"client_secret\": \"\",\\n                \"expiry\": \"\",\\n                \"refresh_token\": \"\",\\n                \"scopes\": [\\n                    \"https://www.googleapis.com/auth/gmail.readonly\",\\n                ],\\n                \"token\": \"\",\\n                \"token_uri\": \"https://oauth2.googleapis.com/token\",\\n                \"universe_domain\": \"googleapis.com\"\\n            }'), MessageTextInput(name='label_ids', display_name='Label IDs', info='Comma-separated list of label IDs to filter emails.', required=True, value='INBOX,SENT,UNREAD,IMPORTANT'), MessageTextInput(name='max_results', display_name='Max Results', info='Maximum number of emails to load.', required=True, value='10')]", "outputs": "[Output(display_name='Data', name='data', method='load_emails')]", "display_name": "Gmail Loader", "name": "", "description": "Loads emails from Gmail using provided credentials.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/gmail.py", "section": "class::CustomGMailLoader", "content": "import base64\nimport json\nimport re\nfrom collections.abc import Iterator\nfrom json.decoder import JSONDecodeError\nfrom typing import Any\nfrom google.auth.exceptions import RefreshError\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom langchain_core.chat_sessions import ChatSession\nfrom langchain_core.messages import HumanMessage\nfrom langchain_google_community.gmail.loader import GMailLoader\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass CustomGMailLoader(GMailLoader):\n\n    def clean_message_content(self, message):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def lazy_load(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CustomGMailLoader", "base_classes": ["GMailLoader"], "public_methods": ["def clean_message_content(self, message)", "def lazy_load(self)"], "imports": ["import base64", "import json", "import re", "from collections.abc import Iterator", "from json.decoder import JSONDecodeError", "from typing import Any", "from google.auth.exceptions import RefreshError", "from google.oauth2.credentials import Credentials", "from googleapiclient.discovery import build", "from langchain_core.chat_sessions import ChatSession", "from langchain_core.messages import HumanMessage", "from langchain_google_community.gmail.loader import GMailLoader", "from loguru import logger", "from langflow.custom import Component", "from langflow.inputs import MessageTextInput", "from langflow.io import SecretStrInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/maritalk.py", "section": "class::MaritalkModelComponent", "content": "from langchain_community.chat_models import ChatMaritalk\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\n\nclass MaritalkModelComponent(LCModelComponent):\n    display_name: str = \"Maritalk\"\n    description: str = \"Generates text using Maritalk LLMs.\"\n    icon = \"Maritalk\"\n    name = \"Maritalk\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        value=512,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=['sabia-2-small',\n        'sabia-2-medium'],\n        value=['sabia-2-small']),\n        SecretStrInput(name='api_key',\n        display_name='Maritalk API Key',\n        info='The Maritalk API Key to use for the OpenAI model.',\n        advanced=False),\n        FloatInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=1))\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MaritalkModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_community.chat_models import ChatMaritalk", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, value=512, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=['sabia-2-small', 'sabia-2-medium'], value=['sabia-2-small']), SecretStrInput(name='api_key', display_name='Maritalk API Key', info='The Maritalk API Key to use for the OpenAI model.', advanced=False), FloatInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=1))]", "outputs": "", "display_name": "Maritalk", "name": "Maritalk", "description": "Generates text using Maritalk LLMs.", "icon": "Maritalk"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/youtube_transcripts.py", "section": "class::YouTubeTranscriptsComponent", "content": "import pandas as pd\nimport youtube_transcript_api\nfrom langchain_community.document_loaders import YoutubeLoader\nfrom langchain_community.document_loaders.youtube import TranscriptFormat\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, IntInput, MultilineInput\nfrom langflow.schema import Data, DataFrame, Message\nfrom langflow.template import Output\n\nclass YouTubeTranscriptsComponent(Component):\n    \"\"\"\n    A component that extracts spoken content from YouTube videos as transcripts.\n    \"\"\"\n\n    display_name: str = \"YouTube Transcripts\"\n    description: str = \"Extracts spoken content from YouTube videos with multiple output options.\"\n    icon = \"YouTube\"\n    name = \"YouTubeTranscripts\"\n\n    inputs = [\n        MultilineInput(name='url',\n        display_name='Video URL',\n        info='Enter the YouTube video URL to get transcripts from.',\n        tool_mode=True,\n        required=True),\n        IntInput(name='chunk_size_seconds',\n        display_name='Chunk Size (seconds)',\n        value=60,\n        info='The size of each transcript chunk in seconds.'),\n        DropdownInput(name='translation',\n        display_name='Translation Language',\n        advanced=True,\n        options=['',\n        'en',\n        'es',\n        'fr',\n        'de',\n        'it',\n        'pt',\n        'ru',\n        'ja',\n        'ko',\n        'hi',\n        'ar',\n        'id'],\n        info='Translate the transcripts to the specified language. Leave empty for no translation.')\n    ]\n\n    outputs = [\n        Output(name='dataframe',\n        display_name='Chunks',\n        method='get_dataframe_output'),\n        Output(name='message',\n        display_name='Transcript',\n        method='get_message_output'),\n        Output(name='data_output',\n        display_name='Transcript + Source',\n        method='get_data_output')\n    ]\n\n    def get_dataframe_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_message_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_data_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubeTranscriptsComponent", "base_classes": ["Component"], "public_methods": ["def get_dataframe_output(self)", "def get_message_output(self)", "def get_data_output(self)"], "imports": ["import pandas as pd", "import youtube_transcript_api", "from langchain_community.document_loaders import YoutubeLoader", "from langchain_community.document_loaders.youtube import TranscriptFormat", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, IntInput, MultilineInput", "from langflow.schema import Data, DataFrame, Message", "from langflow.template import Output"], "inputs": "[MultilineInput(name='url', display_name='Video URL', info='Enter the YouTube video URL to get transcripts from.', tool_mode=True, required=True), IntInput(name='chunk_size_seconds', display_name='Chunk Size (seconds)', value=60, info='The size of each transcript chunk in seconds.'), DropdownInput(name='translation', display_name='Translation Language', advanced=True, options=['', 'en', 'es', 'fr', 'de', 'it', 'pt', 'ru', 'ja', 'ko', 'hi', 'ar', 'id'], info='Translate the transcripts to the specified language. Leave empty for no translation.')]", "outputs": "[Output(name='dataframe', display_name='Chunks', method='get_dataframe_output'), Output(name='message', display_name='Transcript', method='get_message_output'), Output(name='data_output', display_name='Transcript + Source', method='get_data_output')]", "display_name": "YouTube Transcripts", "name": "YouTubeTranscripts", "description": "Extracts spoken content from YouTube videos with multiple output options.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/list_pages.py", "section": "class::NotionListPages", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionListPages(LCToolComponent):\n    display_name: str = \"List Pages \"\n    description: str = \"Query a Notion database with filtering and sorting. The input should be a JSON string containing the 'filter' and 'sorts' objects. Example input:\n{\"filter\": {\"property\": \"Status\", \"select\": {\"equals\": \"Done\"}}, \"sorts\": [{\"timestamp\": \"created_time\", \"direction\": \"descending\"}]}\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True),\n        StrInput(name='database_id',\n        display_name='Database ID',\n        info='The ID of the Notion database to query.'),\n        MultilineInput(name='query_json',\n        display_name='Database query (JSON)',\n        info='A JSON string containing the filters and sorts that will be used for querying the database. Leave empty for no filters or sorts.')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionListPages", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True), StrInput(name='database_id', display_name='Database ID', info='The ID of the Notion database to query.'), MultilineInput(name='query_json', display_name='Database query (JSON)', info='A JSON string containing the filters and sorts that will be used for querying the database. Leave empty for no filters or sorts.')]", "outputs": "", "display_name": "List Pages ", "name": "", "description": "Query a Notion database with filtering and sorting. The input should be a JSON string containing the 'filter' and 'sorts' objects. Example input:\n{\"filter\": {\"property\": \"Status\", \"select\": {\"equals\": \"Done\"}}, \"sorts\": [{\"timestamp\": \"created_time\", \"direction\": \"descending\"}]}", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/list_pages.py", "section": "class::NotionListPagesSchema", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionListPagesSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionListPagesSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/astra_assistant_manager.py", "section": "class::AstraAssistantManager", "content": "import asyncio\nfrom asyncio import to_thread\nfrom typing import TYPE_CHECKING, Any, cast\nfrom astra_assistants.astra_assistants_manager import AssistantManager\nfrom langchain_core.agents import AgentFinish\nfrom loguru import logger\nfrom langflow.base.agents.events import ExceptionWithMessageError, process_agent_events\nfrom langflow.base.astra_assistants.util import get_patched_openai_client, litellm_model_names, sync_upload, wrap_base_tool_as_tool_interface\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import DropdownInput, FileInput, HandleInput, MultilineInput\nfrom langflow.memory import delete_message\nfrom langflow.schema.content_block import ContentBlock\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\nfrom langflow.utils.constants import MESSAGE_SENDER_AI\nfrom langflow.schema.log import SendMessageFunctionType\n\nclass AstraAssistantManager(ComponentWithCache):\n    display_name: str = \"Astra Assistant Agent\"\n    description: str = \"Manages Assistant Interactions\"\n    icon = \"AstraDB\"\n    name = \"Astra Assistant Agent\"\n\n    inputs = [\n        DropdownInput(name='model_name',\n        display_name='Model',\n        advanced=False,\n        options=litellm_model_names,\n        value='gpt-4o-mini'),\n        MultilineInput(name='instructions',\n        display_name='Agent Instructions',\n        info='Instructions for the assistant,\n        think of these as the system prompt.'),\n        HandleInput(name='input_tools',\n        display_name='Tools',\n        input_types=['Tool'],\n        is_list=True,\n        required=False,\n        info='These are the tools that the agent can use to help with tasks.'),\n        MultilineInput(name='user_message',\n        display_name='User Message',\n        info='User message to pass to the run.',\n        tool_mode=True),\n        FileInput(name='file',\n        display_name='File(s) for retrieval',\n        list=True,\n        info='Files to be sent with the message.',\n        required=False,\n        show=True,\n        file_types=['txt',\n        'md',\n        'mdx',\n        'csv',\n        'json',\n        'yaml',\n        'yml',\n        'xml',\n        'html',\n        'htm',\n        'pdf',\n        'docx',\n        'py',\n        'sh',\n        'sql',\n        'js',\n        'ts',\n        'tsx',\n        'jpg',\n        'jpeg',\n        'png',\n        'bmp',\n        'image',\n        'zip',\n        'tar',\n        'tgz',\n        'bz2',\n        'gz',\n        'c',\n        'cpp',\n        'cs',\n        'css',\n        'go',\n        'java',\n        'php',\n        'rb',\n        'tex',\n        'doc',\n        'docx',\n        'ppt',\n        'pptx',\n        'xls',\n        'xlsx',\n        'jsonl']),\n        MultilineInput(name='input_thread_id',\n        display_name='Thread ID (optional)',\n        info='ID of the thread',\n        advanced=True),\n        MultilineInput(name='input_assistant_id',\n        display_name='Assistant ID (optional)',\n        info='ID of the assistant',\n        advanced=True),\n        MultilineInput(name='env_set',\n        display_name='Environment Set',\n        info='Dummy input to allow chaining with Dotenv Component.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Assistant Response',\n        name='assistant_response',\n        method='get_assistant_response'),\n        Output(display_name='Tool output',\n        name='tool_output',\n        method='get_tool_output',\n        hidden=True),\n        Output(display_name='Thread Id',\n        name='output_thread_id',\n        method='get_thread_id',\n        hidden=True),\n        Output(display_name='Assistant Id',\n        name='output_assistant_id',\n        method='get_assistant_id',\n        hidden=True),\n        Output(display_name='Vector Store Id',\n        name='output_vs_id',\n        method='get_vs_id',\n        hidden=True)\n    ]\n", "metadata": {"parser": "python_component", "class_name": "AstraAssistantManager", "base_classes": ["ComponentWithCache"], "public_methods": [], "imports": ["import asyncio", "from asyncio import to_thread", "from typing import TYPE_CHECKING, Any, cast", "from astra_assistants.astra_assistants_manager import AssistantManager", "from langchain_core.agents import AgentFinish", "from loguru import logger", "from langflow.base.agents.events import ExceptionWithMessageError, process_agent_events", "from langflow.base.astra_assistants.util import get_patched_openai_client, litellm_model_names, sync_upload, wrap_base_tool_as_tool_interface", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.inputs import DropdownInput, FileInput, HandleInput, MultilineInput", "from langflow.memory import delete_message", "from langflow.schema.content_block import ContentBlock", "from langflow.schema.message import Message", "from langflow.template import Output", "from langflow.utils.constants import MESSAGE_SENDER_AI", "from langflow.schema.log import SendMessageFunctionType"], "inputs": "[DropdownInput(name='model_name', display_name='Model', advanced=False, options=litellm_model_names, value='gpt-4o-mini'), MultilineInput(name='instructions', display_name='Agent Instructions', info='Instructions for the assistant, think of these as the system prompt.'), HandleInput(name='input_tools', display_name='Tools', input_types=['Tool'], is_list=True, required=False, info='These are the tools that the agent can use to help with tasks.'), MultilineInput(name='user_message', display_name='User Message', info='User message to pass to the run.', tool_mode=True), FileInput(name='file', display_name='File(s) for retrieval', list=True, info='Files to be sent with the message.', required=False, show=True, file_types=['txt', 'md', 'mdx', 'csv', 'json', 'yaml', 'yml', 'xml', 'html', 'htm', 'pdf', 'docx', 'py', 'sh', 'sql', 'js', 'ts', 'tsx', 'jpg', 'jpeg', 'png', 'bmp', 'image', 'zip', 'tar', 'tgz', 'bz2', 'gz', 'c', 'cpp', 'cs', 'css', 'go', 'java', 'php', 'rb', 'tex', 'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'jsonl']), MultilineInput(name='input_thread_id', display_name='Thread ID (optional)', info='ID of the thread', advanced=True), MultilineInput(name='input_assistant_id', display_name='Assistant ID (optional)', info='ID of the assistant', advanced=True), MultilineInput(name='env_set', display_name='Environment Set', info='Dummy input to allow chaining with Dotenv Component.', advanced=True)]", "outputs": "[Output(display_name='Assistant Response', name='assistant_response', method='get_assistant_response'), Output(display_name='Tool output', name='tool_output', method='get_tool_output', hidden=True), Output(display_name='Thread Id', name='output_thread_id', method='get_thread_id', hidden=True), Output(display_name='Assistant Id', name='output_assistant_id', method='get_assistant_id', hidden=True), Output(display_name='Vector Store Id', name='output_vs_id', method='get_vs_id', hidden=True)]", "display_name": "Astra Assistant Agent", "name": "Astra Assistant Agent", "description": "Manages Assistant Interactions", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/scrapegraph/scrapegraph_search_api.py", "section": "class::ScrapeGraphSearchApi", "content": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom scrapegraph_py import Client\nfrom scrapegraph_py.logger import sgai_logger\n\nclass ScrapeGraphSearchApi(Component):\n    display_name: str = \"ScrapeGraphSearchApi\"\n    description: str = \"ScrapeGraph Search API.\n    Given a search prompt, it will return search results using ScrapeGraph's search functionality.\n    More info at https://docs.scrapegraphai.com/services/searchscraper\"\n    icon = \"ScrapeGraph\"\n    name = \"ScrapeGraphSearchApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='ScrapeGraph API Key',\n        required=True,\n        password=True,\n        info='The API key to use ScrapeGraph API.'),\n        MessageTextInput(name='user_prompt',\n        display_name='Search Prompt',\n        tool_mode=True,\n        info='The search prompt to use.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='search')\n    ]\n\n    def search(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ScrapeGraphSearchApi", "base_classes": ["Component"], "public_methods": ["def search(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data", "from scrapegraph_py import Client", "from scrapegraph_py.logger import sgai_logger"], "inputs": "[SecretStrInput(name='api_key', display_name='ScrapeGraph API Key', required=True, password=True, info='The API key to use ScrapeGraph API.'), MessageTextInput(name='user_prompt', display_name='Search Prompt', tool_mode=True, info='The search prompt to use.')]", "outputs": "[Output(display_name='Data', name='data', method='search')]", "display_name": "ScrapeGraphSearchApi", "name": "ScrapeGraphSearchApi", "description": "ScrapeGraph Search API.\n    Given a search prompt, it will return search results using ScrapeGraph's search functionality.\n    More info at https://docs.scrapegraphai.com/services/searchscraper", "icon": "ScrapeGraph"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/cassandra_graph.py", "section": "class::CassandraGraphVectorStoreComponent", "content": "from uuid import UUID\nfrom langchain_community.graph_vectorstores import CassandraGraphVectorStore\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import DictInput, FloatInput\nfrom langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\nimport cassio\nfrom langchain_community.utilities.cassandra import SetupMode\n\nclass CassandraGraphVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Cassandra Graph\"\n    description: str = \"Cassandra Graph Vector Store\"\n    icon = \"Cassandra\"\n    name = \"CassandraGraph\"\n\n    inputs = [\n        MessageTextInput(name='database_ref',\n        display_name='Contact Points / Astra Database ID',\n        info='Contact points for the database (or AstraDB database ID)',\n        required=True),\n        MessageTextInput(name='username',\n        display_name='Username',\n        info='Username for the database (leave empty for AstraDB).'),\n        SecretStrInput(name='token',\n        display_name='Password / AstraDB Token',\n        info='User password for the database (or AstraDB token).',\n        required=True),\n        MessageTextInput(name='keyspace',\n        display_name='Keyspace',\n        info='Table Keyspace (or AstraDB namespace).',\n        required=True),\n        MessageTextInput(name='table_name',\n        display_name='Table Name',\n        info='The name of the table (or AstraDB collection) where vectors will be stored.',\n        required=True),\n        DropdownInput(name='setup_mode',\n        display_name='Setup Mode',\n        info=\"Configuration mode for setting up the Cassandra table,\n        with options like 'Sync' or 'Off'.\",\n        options=['Sync',\n        'Off'],\n        value='Sync',\n        advanced=True),\n        DictInput(name='cluster_kwargs',\n        display_name='Cluster arguments',\n        info='Optional dictionary of additional keyword arguments for the Cassandra cluster.',\n        advanced=True,\n        list=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        info='Search type to use',\n        options=['Traversal',\n        'MMR traversal',\n        'Similarity',\n        'Similarity with score threshold',\n        'MMR (Max Marginal Relevance)'],\n        value='Traversal',\n        advanced=True),\n        IntInput(name='depth',\n        display_name='Depth of traversal',\n        info=\"The maximum depth of edges to traverse. (when using 'Traversal' or 'MMR traversal')\",\n        value=1,\n        advanced=True),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n        value=0,\n        advanced=True),\n        DictInput(name='search_filter',\n        display_name='Search Metadata Filter',\n        info='Optional dictionary of filters to apply to the search query.',\n        advanced=True,\n        list=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_retriever_kwargs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CassandraGraphVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)", "def get_retriever_kwargs(self)"], "imports": ["from uuid import UUID", "from langchain_community.graph_vectorstores import CassandraGraphVectorStore", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.inputs import DictInput, FloatInput", "from langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data", "import cassio", "from langchain_community.utilities.cassandra import SetupMode"], "inputs": "[MessageTextInput(name='database_ref', display_name='Contact Points / Astra Database ID', info='Contact points for the database (or AstraDB database ID)', required=True), MessageTextInput(name='username', display_name='Username', info='Username for the database (leave empty for AstraDB).'), SecretStrInput(name='token', display_name='Password / AstraDB Token', info='User password for the database (or AstraDB token).', required=True), MessageTextInput(name='keyspace', display_name='Keyspace', info='Table Keyspace (or AstraDB namespace).', required=True), MessageTextInput(name='table_name', display_name='Table Name', info='The name of the table (or AstraDB collection) where vectors will be stored.', required=True), DropdownInput(name='setup_mode', display_name='Setup Mode', info=\"Configuration mode for setting up the Cassandra table, with options like 'Sync' or 'Off'.\", options=['Sync', 'Off'], value='Sync', advanced=True), DictInput(name='cluster_kwargs', display_name='Cluster arguments', info='Optional dictionary of additional keyword arguments for the Cassandra cluster.', advanced=True, list=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True), DropdownInput(name='search_type', display_name='Search Type', info='Search type to use', options=['Traversal', 'MMR traversal', 'Similarity', 'Similarity with score threshold', 'MMR (Max Marginal Relevance)'], value='Traversal', advanced=True), IntInput(name='depth', display_name='Depth of traversal', info=\"The maximum depth of edges to traverse. (when using 'Traversal' or 'MMR traversal')\", value=1, advanced=True), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\", value=0, advanced=True), DictInput(name='search_filter', display_name='Search Metadata Filter', info='Optional dictionary of filters to apply to the search query.', advanced=True, list=True)]", "outputs": "", "display_name": "Cassandra Graph", "name": "CassandraGraph", "description": "Cassandra Graph Vector Store", "icon": "Cassandra"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/nvidia/system_assist.py", "section": "class::NvidiaSystemAssistComponent", "content": "import asyncio\nimport contextlib\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Message\nfrom langflow.services.cache.utils import CacheMiss\nfrom gassist.rise import send_rise_command\n\nclass NvidiaSystemAssistComponent(ComponentWithCache):\n    display_name: str = \"NVIDIA System-Assist\"\n    description: str = \"Prompts NVIDIA System-Assist to interact with the NVIDIA GPU Driver. The user may query GPU specifications, state, and ask the NV-API to perform several GPU-editing acations. The prompt must be human-readable language.(Windows only)\"\n    icon = \"NVIDIA\"\n\n    inputs = [\n        MessageTextInput(name='prompt',\n        display_name='System-Assist Prompt',\n        info='Enter a prompt for NVIDIA System-Assist to process.',\n        value='',\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Response',\n        name='response',\n        method='sys_assist_prompt')\n    ]\n\n    def maybe_register_rise_client(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NvidiaSystemAssistComponent", "base_classes": ["ComponentWithCache"], "public_methods": ["def maybe_register_rise_client(self)"], "imports": ["import asyncio", "import contextlib", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.io import MessageTextInput, Output", "from langflow.schema import Message", "from langflow.services.cache.utils import CacheMiss", "from gassist.rise import send_rise_command"], "inputs": "[MessageTextInput(name='prompt', display_name='System-Assist Prompt', info='Enter a prompt for NVIDIA System-Assist to process.', value='', tool_mode=True)]", "outputs": "[Output(display_name='Response', name='response', method='sys_assist_prompt')]", "display_name": "NVIDIA System-Assist", "name": "", "description": "Prompts NVIDIA System-Assist to interact with the NVIDIA GPU Driver. The user may query GPU specifications, state, and ask the NV-API to perform several GPU-editing acations. The prompt must be human-readable language.(Windows only)", "icon": "NVIDIA"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/assemblyai/assemblyai_get_subtitles.py", "section": "class::AssemblyAIGetSubtitles", "content": "import assemblyai as aai\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, Output, SecretStrInput\nfrom langflow.schema import Data\n\nclass AssemblyAIGetSubtitles(Component):\n    display_name: str = \"AssemblyAI Get Subtitles\"\n    description: str = \"Export your transcript in SRT or VTT format for subtitles and closed captions\"\n    icon = \"AssemblyAI\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Assembly API Key',\n        info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/',\n        required=True),\n        DataInput(name='transcription_result',\n        display_name='Transcription Result',\n        info='The transcription result from AssemblyAI',\n        required=True),\n        DropdownInput(name='subtitle_format',\n        display_name='Subtitle Format',\n        options=['srt',\n        'vtt'],\n        value='srt',\n        info='The format of the captions (SRT or VTT)'),\n        IntInput(name='chars_per_caption',\n        display_name='Characters per Caption',\n        info='The maximum number of characters per caption (0 for no limit)',\n        value=0,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Subtitles',\n        name='subtitles',\n        method='get_subtitles')\n    ]\n\n    def get_subtitles(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssemblyAIGetSubtitles", "base_classes": ["Component"], "public_methods": ["def get_subtitles(self)"], "imports": ["import assemblyai as aai", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, IntInput, Output, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='Assembly API Key', info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/', required=True), DataInput(name='transcription_result', display_name='Transcription Result', info='The transcription result from AssemblyAI', required=True), DropdownInput(name='subtitle_format', display_name='Subtitle Format', options=['srt', 'vtt'], value='srt', info='The format of the captions (SRT or VTT)'), IntInput(name='chars_per_caption', display_name='Characters per Caption', info='The maximum number of characters per caption (0 for no limit)', value=0, advanced=True)]", "outputs": "[Output(display_name='Subtitles', name='subtitles', method='get_subtitles')]", "display_name": "AssemblyAI Get Subtitles", "name": "", "description": "Export your transcript in SRT or VTT format for subtitles and closed captions", "icon": "AssemblyAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/webhook.py", "section": "class::WebhookComponent", "content": "import json\nfrom langflow.custom import Component\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema import Data\n\nclass WebhookComponent(Component):\n    display_name: str = \"Webhook\"\n    icon = \"webhook\"\n    name = \"Webhook\"\n\n    inputs = [\n        MultilineInput(name='data',\n        display_name='Payload',\n        info='Receives a payload from external systems via HTTP POST.',\n        advanced=True),\n        MultilineInput(name='curl',\n        display_name='cURL',\n        value='CURL_WEBHOOK',\n        advanced=True,\n        input_types=[]),\n        MultilineInput(name='endpoint',\n        display_name='Endpoint',\n        value='BACKEND_URL',\n        advanced=False,\n        copy_field=True,\n        input_types=[])\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='output_data',\n        method='build_data')\n    ]\n\n    def build_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WebhookComponent", "base_classes": ["Component"], "public_methods": ["def build_data(self)"], "imports": ["import json", "from langflow.custom import Component", "from langflow.io import MultilineInput, Output", "from langflow.schema import Data"], "inputs": "[MultilineInput(name='data', display_name='Payload', info='Receives a payload from external systems via HTTP POST.', advanced=True), MultilineInput(name='curl', display_name='cURL', value='CURL_WEBHOOK', advanced=True, input_types=[]), MultilineInput(name='endpoint', display_name='Endpoint', value='BACKEND_URL', advanced=False, copy_field=True, input_types=[])]", "outputs": "[Output(display_name='Data', name='output_data', method='build_data')]", "display_name": "Webhook", "name": "Webhook", "description": "", "icon": "webhook"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/documents_to_data.py", "section": "class::DocumentsToDataComponent", "content": "from langchain_core.documents import Document\nfrom langflow.custom import CustomComponent\nfrom langflow.schema import Data\n\nclass DocumentsToDataComponent(CustomComponent):\n    display_name: str = \"Documents ⇢ Data\"\n    description: str = \"Convert LangChain Documents into Data.\"\n    icon = \"LangChain\"\n    name = \"DocumentsToData\"\n\n    def build(self, documents):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DocumentsToDataComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build(self, documents)"], "imports": ["from langchain_core.documents import Document", "from langflow.custom import CustomComponent", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "Documents ⇢ Data", "name": "DocumentsToData", "description": "Convert LangChain Documents into Data.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/composio/googlecalendar_composio.py", "section": "class::ComposioGoogleCalendarAPIComponent", "content": "from typing import Any\nfrom composio import Action\nfrom langflow.base.composio.composio_base import ComposioBaseComponent\nfrom langflow.inputs import BoolInput, IntInput, MessageTextInput\nfrom langflow.logging import logger\n\nclass ComposioGoogleCalendarAPIComponent(ComposioBaseComponent):\n    \"\"\"\n    Google Calendar API component for interacting with Google Calendar services.\n    \"\"\"\n\n    display_name: str = \"Google Calendar\"\n    description: str = \"Google Calendar API\"\n    icon = \"Googlecalendar\"\n\n    inputs = [\n        *ComposioBaseComponent._base_inputs,\n        IntInput(name='GOOGLECALENDAR_LIST_CALENDARS_max_results',\n        display_name='Max Results',\n        info='Maximum number of entries returned on one result page. The page size can never be larger than 250 entries.',\n        show=False,\n        value=10),\n        MessageTextInput(name='GOOGLECALENDAR_LIST_CALENDARS_min_access_role',\n        display_name='Min Access Role',\n        info=\"The minimum access role for the user in the returned entries. Accepted values are 'owner' & 'reader'\",\n        show=False),\n        MessageTextInput(name='GOOGLECALENDAR_LIST_CALENDARS_page_token',\n        display_name='Page Token',\n        info='Token specifying which result page to return.',\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_LIST_CALENDARS_show_deleted',\n        display_name='Show Deleted',\n        info='Whether to include deleted calendar list entries in the result.',\n        show=False,\n        value=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_LIST_CALENDARS_show_hidden',\n        display_name='Show Hidden',\n        info='Whether to show hidden entries.',\n        show=False,\n        value=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_LIST_CALENDARS_sync_token',\n        display_name='Sync Token',\n        info='Token obtained from the nextSyncToken field returned on the last page of results from the previous list request.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_calendar_id',\n        display_name='Calendar Id',\n        info=\"Identifier of the Google Calendar. Use 'primary' for the currently logged in user's primary calendar.\",\n        show=False,\n        value='primary'),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_query',\n        display_name='Query',\n        info=\"Search term to find events that match these terms in the event's summary,\n        description,\n        location,\n        attendee's displayName,\n        attendee's email,\n        organizer's displayName,\n        organizer's email,\n        etc if needed.\",\n        show=False),\n        IntInput(name='GOOGLECALENDAR_FIND_EVENT_max_results',\n        display_name='Max Results',\n        info='Maximum number of events returned on one result page. The page size can never be larger than 2500 events. The default value is 10.',\n        show=False,\n        value=10),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_order_by',\n        display_name='Order By',\n        info=\"The order of the events returned in the result. Acceptable values are 'startTime' and 'updated'.\",\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_FIND_EVENT_show_deleted',\n        display_name='Show Deleted',\n        info=\"Whether to include deleted events (with status equals 'cancelled') in the result.\",\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_FIND_EVENT_single_events',\n        display_name='Single Events',\n        info='Whether to expand recurring events into instances and only return single one-off events and instances of recurring events,\n        but not the underlying recurring events themselves.',\n        show=False,\n        value=True,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_timeMax',\n        display_name='Timemax',\n        info=\"Upper bound (exclusive) for an event's start time to filter by. Accepts multiple formats:,\n        1. ISO format with timezone (e.g.,\n        2024-12-06T13:00:00Z),\n        2. Comma-separated format (e.g.,\n        2024,12,06,13,00,00),\n        3. Simple datetime format (e.g.,\n        2024-12-06 13:00:00)\",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_timeMin',\n        display_name='Timemin',\n        info=\"Lower bound (exclusive) for an event's end time to filter by. Accepts multiple formats:,\n        1. ISO format with timezone (e.g.,\n        2024-12-06T13:00:00Z),\n        2. Comma-separated format (e.g.,\n        2024,12,06,13,00,00),\n        3. Simple datetime format (e.g.,\n        2024-12-06 13:00:00)\",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_updated_min',\n        display_name='Updated Min',\n        info=\"Lower bound for an event's last modification time to filter by. Accepts multiple formats:,\n        1. ISO format with timezone (e.g.,\n        2024-12-06T13:00:00Z),\n        2. Comma-separated format (e.g.,\n        2024,12,06,13,00,00),\n        3. Simple datetime format (e.g.,\n        2024-12-06 13:00:00)\",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_event_types',\n        display_name='Event Types',\n        info='List of event types to return. Possible values are: default,\n        outOfOffice,\n        focusTime,\n        workingLocation.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_page_token',\n        display_name='Page Token',\n        info='Token specifying which result page to return. Optional.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_DUPLICATE_CALENDAR_summary',\n        display_name='Summary/Title',\n        info='Title of the calendar to be duplicated.',\n        show=False,\n        value=''),\n        MessageTextInput(name='GOOGLECALENDAR_REMOVE_ATTENDEE_calendar_id',\n        display_name='Calendar Id',\n        info='ID of the Google Calendar',\n        show=False,\n        value='primary'),\n        MessageTextInput(name='GOOGLECALENDAR_REMOVE_ATTENDEE_event_id',\n        display_name='Event Id',\n        info='ID of the event',\n        show=False,\n        required=True),\n        MessageTextInput(name='GOOGLECALENDAR_REMOVE_ATTENDEE_attendee_email',\n        display_name='Attendee Email',\n        info='Email address of the attendee to be removed',\n        show=False,\n        required=True),\n        MessageTextInput(name='GOOGLECALENDAR_GET_CALENDAR_calendar_id',\n        display_name='Calendar Id',\n        info=\"The ID of the Google Calendar that needs to be fetched. Default is 'primary'.\",\n        show=False,\n        value='primary'),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_description',\n        display_name='Description',\n        info='Description of the event. Can contain HTML. Optional.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_eventType',\n        display_name='Event Type',\n        info=\"Type of the event,\n        immutable post-creation. Currently,\n        only 'default'\",\n        show=False,\n        value='default',\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_create_meeting_room',\n        display_name='Create Meeting Room',\n        info='If true,\n        a Google Meet link is created and added to the event.',\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_guestsCanSeeOtherGuests',\n        display_name='Guests Can See Other Guests',\n        info=\"Whether attendees other than the organizer can see who the event's attendees are.\",\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_guestsCanInviteOthers',\n        display_name='Guests Can Invite Others',\n        info='Whether attendees other than the organizer can invite others to the event.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_location',\n        display_name='Location',\n        info='Geographic location of the event as free-form text.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_summary',\n        display_name='Summary/Title',\n        info='Summary (title) of the event.',\n        show=False),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_transparency',\n        display_name='Event Transparency',\n        info=\"'opaque' (busy) or 'transparent' (available).\",\n        show=False,\n        value='opaque',\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_visibility',\n        display_name='Event Visibility',\n        info=\"Event visibility: 'default',\n        'public',\n        'private',\n        or 'confidential'.\",\n        show=False,\n        value='default',\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_timezone',\n        display_name='Timezone',\n        info=\"IANA timezone name (e.g.,\n        'America/New_York'). Required if datetime is naive. If datetime includes timezone info (Z or offset),\n        this field is optional and defaults to UTC.\",\n        show=False),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_recurrence',\n        display_name='Recurrence',\n        info='List of RRULE,\n        EXRULE,\n        RDATE,\n        EXDATE lines for recurring events.',\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_guests_can_modify',\n        display_name='Guests Can Modify',\n        info='If True,\n        guests can modify the event.',\n        show=False,\n        value=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_attendees',\n        display_name='Attendees',\n        info='List of attendee emails (strings).',\n        show=False),\n        BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_send_updates',\n        display_name='Send Updates',\n        info='Defaults to True. Whether to send updates to the attendees.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_start_datetime',\n        display_name='Start Datetime',\n        info=\"Naive date/time (YYYY-MM-DDTHH:MM:SS) with NO offsets or Z. e.g. '2025-01-16T13:00:00'\",\n        show=False,\n        required=True),\n        IntInput(name='GOOGLECALENDAR_CREATE_EVENT_event_duration_hour',\n        display_name='Event Duration Hour',\n        info='Number of hours (0-24).',\n        show=False,\n        value=0,\n        advanced=True),\n        IntInput(name='GOOGLECALENDAR_CREATE_EVENT_event_duration_minutes',\n        display_name='Event Duration Minutes',\n        info='Number of minutes (0-59).',\n        show=False,\n        value=30,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_calendar_id',\n        display_name='Calendar Id',\n        info='The ID of the Google Calendar. `primary` for interacting with the primary calendar.',\n        show=False,\n        value='primary',\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_DELETE_EVENT_calendar_id',\n        display_name='Calendar Id',\n        info='ID of the Google Calendar',\n        show=False,\n        value='primary'),\n        MessageTextInput(name='GOOGLECALENDAR_DELETE_EVENT_event_id',\n        display_name='Event Id',\n        info='ID of the event to be deleted',\n        show=False,\n        required=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_time_min',\n        display_name='Time Min',\n        info='The start datetime of the interval for the query. Supports multiple formats:,\n        1. ISO format with timezone (e.g.,\n        2024-12-06T13:00:00Z),\n        2. Comma-separated format (e.g.,\n        2024,12,06,13,00,00),\n        3. Simple datetime format (e.g.,\n        2024-12-06 13:00:00)',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_time_max',\n        display_name='Time Max',\n        info='The end datetime of the interval for the query. Supports multiple formats:,\n        1. ISO format with timezone (e.g.,\n        2024-12-06T13:00:00Z),\n        2. Comma-separated format (e.g.,\n        2024,12,06,13,00,00),\n        3. Simple datetime format (e.g.,\n        2024-12-06 13:00:00)',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_timezone',\n        display_name='Timezone',\n        info='Time zone used in the response. Optional. The default is UTC.',\n        show=False,\n        value='UTC',\n        advanced=True),\n        IntInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_group_expansion_max',\n        display_name='Group Expansion Max',\n        info='Maximal number of calendar identifiers to be provided for a single group. Optional. An error is returned for a group with more members than this value. Maximum value is 100.',\n        show=False,\n        value=100,\n        advanced=True),\n        IntInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_calendar_expansion_max',\n        display_name='Calendar Expansion Max',\n        info='Maximal number of calendars for which FreeBusy information is to be provided. Optional. Maximum value is 50.',\n        show=False,\n        value=50,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_items',\n        display_name='Items',\n        info='List of calendars ids for which to fetch',\n        show=False),\n        MessageTextInput(name='GOOGLECALENDAR_QUICK_ADD_calendar_id',\n        display_name='Calendar Id',\n        info=\"Calendar identifier. To list calendars to retrieve calendar IDs use relevant tools. To access the primary calendar of the currently logged in user,\n        use the 'primary' keyword.\",\n        show=False,\n        value='primary'),\n        MessageTextInput(name='GOOGLECALENDAR_QUICK_ADD_text',\n        display_name='Text',\n        info='The text describing the event to be created.',\n        show=False,\n        value=''),\n        MessageTextInput(name='GOOGLECALENDAR_QUICK_ADD_send_updates',\n        display_name='Send Updates',\n        info=\"Guests who should receive notifications about the creation of the new event. Accepted fields include 'all',\n        'none',\n        'externalOnly'\",\n        show=False,\n        value='none',\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_calendar_id',\n        display_name='Calendar Id',\n        info='The ID of the Google Calendar that needs to be updated.',\n        show=False,\n        required=True),\n        MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_description',\n        display_name='Description',\n        info='Description of the calendar. Optional.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_location',\n        display_name='Location',\n        info='Geographic location of the calendar as free-form text.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_summary',\n        display_name='Title/Summary',\n        info='Title of the calendar. This field is required and cannot be left blank as per the Google Calendar API requirements.',\n        show=False,\n        required=True),\n        MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_timezone',\n        display_name='Timezone',\n        info=\"The time zone of the calendar. (Formatted as an IANA Time Zone Database name,\n        e.g. 'Europe/Zurich').\",\n        show=False,\n        advanced=True),\n        IntInput(name='GOOGLECALENDAR_GET_CURRENT_DATE_TIME_timezone',\n        display_name='Timezone',\n        info='The timezone offset from UTC to retrieve current date and time,\n        like for location of UTC+6,\n        you give 6,\n        for UTC -9,\n        your give -9.',\n        show=False,\n        value=0,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_description',\n        display_name='Description',\n        info='Description of the event. Can contain HTML. Optional.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_eventType',\n        display_name='EventType',\n        info=\"Type of the event,\n        immutable post-creation. Currently,\n        only 'default' and 'workingLocation' can be created.\",\n        show=False,\n        value='default',\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_create_meeting_room',\n        display_name='Create Meeting Room',\n        info='If true,\n        a Google Meet link is created and added to the event.',\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_guestsCanSeeOtherGuests',\n        display_name='Guests Can See Other Guests',\n        info=\"Whether attendees other than the organizer can see who the event's attendees are.\",\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_guestsCanInviteOthers',\n        display_name='Guests Can Invite Others',\n        info='Whether attendees other than the organizer can invite others to the event.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_location',\n        display_name='Location',\n        info='Geographic location of the event as free-form text.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_summary',\n        display_name='Summary/Title',\n        info='Summary (title) of the event.',\n        show=False),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_transparency',\n        display_name='Event Transparency',\n        info=\"'opaque' (busy) or 'transparent' (available).\",\n        show=False,\n        value='opaque',\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_visibility',\n        display_name='Event Visibility',\n        info=\"Event visibility: 'default',\n        'public',\n        'private',\n        or 'confidential'.\",\n        show=False,\n        value='default',\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_timezone',\n        display_name='Timezone',\n        info=\"IANA timezone name (e.g.,\n        'America/New_York'). Required if datetime is naive. If datetime includes timezone info (Z or offset),\n        this field is optional and defaults to UTC.\",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_recurrence',\n        display_name='Recurrence',\n        info='List of RRULE,\n        EXRULE,\n        RDATE,\n        EXDATE lines for recurring events.',\n        show=False,\n        advanced=True),\n        BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_guests_can_modify',\n        display_name='Guests Can Modify',\n        info='If True,\n        guests can modify the event.',\n        show=False,\n        value=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_attendees',\n        display_name='Attendees',\n        info='List of attendee emails (strings).',\n        show=False),\n        BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_send_updates',\n        display_name='Send Updates',\n        info='Defaults to True. Whether to send updates to the attendees.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_start_datetime',\n        display_name='Start Datetime',\n        info=\"Naive date/time (YYYY-MM-DDTHH:MM:SS) with NO offsets or Z. e.g. '2025-01-16T13:00:00'\",\n        show=False,\n        required=True),\n        IntInput(name='GOOGLECALENDAR_UPDATE_EVENT_event_duration_hour',\n        display_name='Event Duration Hour',\n        info='Number of hours (0-24).',\n        show=False,\n        value=0,\n        advanced=True),\n        IntInput(name='GOOGLECALENDAR_UPDATE_EVENT_event_duration_minutes',\n        display_name='Event Duration Minutes',\n        info='Number of minutes (0-59).',\n        show=False,\n        value=30,\n        advanced=True),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_calendar_id',\n        display_name='Calendar Id',\n        info='ID of the Google Calendar',\n        show=False,\n        value='primary'),\n        MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_event_id',\n        display_name='Event Id',\n        info='ID of the event to be updated',\n        show=False,\n        required=True)\n    ]\n\n    def execute_action(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ComposioGoogleCalendarAPIComponent", "base_classes": ["ComposioBaseComponent"], "public_methods": ["def execute_action(self)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any", "from composio import Action", "from langflow.base.composio.composio_base import ComposioBaseComponent", "from langflow.inputs import BoolInput, IntInput, MessageTextInput", "from langflow.logging import logger"], "inputs": "[*ComposioBaseComponent._base_inputs, IntInput(name='GOOGLECALENDAR_LIST_CALENDARS_max_results', display_name='Max Results', info='Maximum number of entries returned on one result page. The page size can never be larger than 250 entries.', show=False, value=10), MessageTextInput(name='GOOGLECALENDAR_LIST_CALENDARS_min_access_role', display_name='Min Access Role', info=\"The minimum access role for the user in the returned entries. Accepted values are 'owner' & 'reader'\", show=False), MessageTextInput(name='GOOGLECALENDAR_LIST_CALENDARS_page_token', display_name='Page Token', info='Token specifying which result page to return.', show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_LIST_CALENDARS_show_deleted', display_name='Show Deleted', info='Whether to include deleted calendar list entries in the result.', show=False, value=False, advanced=True), BoolInput(name='GOOGLECALENDAR_LIST_CALENDARS_show_hidden', display_name='Show Hidden', info='Whether to show hidden entries.', show=False, value=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_LIST_CALENDARS_sync_token', display_name='Sync Token', info='Token obtained from the nextSyncToken field returned on the last page of results from the previous list request.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_calendar_id', display_name='Calendar Id', info=\"Identifier of the Google Calendar. Use 'primary' for the currently logged in user's primary calendar.\", show=False, value='primary'), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_query', display_name='Query', info=\"Search term to find events that match these terms in the event's summary, description, location, attendee's displayName, attendee's email, organizer's displayName, organizer's email, etc if needed.\", show=False), IntInput(name='GOOGLECALENDAR_FIND_EVENT_max_results', display_name='Max Results', info='Maximum number of events returned on one result page. The page size can never be larger than 2500 events. The default value is 10.', show=False, value=10), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_order_by', display_name='Order By', info=\"The order of the events returned in the result. Acceptable values are 'startTime' and 'updated'.\", show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_FIND_EVENT_show_deleted', display_name='Show Deleted', info=\"Whether to include deleted events (with status equals 'cancelled') in the result.\", show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_FIND_EVENT_single_events', display_name='Single Events', info='Whether to expand recurring events into instances and only return single one-off events and instances of recurring events, but not the underlying recurring events themselves.', show=False, value=True, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_timeMax', display_name='Timemax', info=\"Upper bound (exclusive) for an event's start time to filter by. Accepts multiple formats:, 1. ISO format with timezone (e.g., 2024-12-06T13:00:00Z), 2. Comma-separated format (e.g., 2024,12,06,13,00,00), 3. Simple datetime format (e.g., 2024-12-06 13:00:00)\", show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_timeMin', display_name='Timemin', info=\"Lower bound (exclusive) for an event's end time to filter by. Accepts multiple formats:, 1. ISO format with timezone (e.g., 2024-12-06T13:00:00Z), 2. Comma-separated format (e.g., 2024,12,06,13,00,00), 3. Simple datetime format (e.g., 2024-12-06 13:00:00)\", show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_updated_min', display_name='Updated Min', info=\"Lower bound for an event's last modification time to filter by. Accepts multiple formats:, 1. ISO format with timezone (e.g., 2024-12-06T13:00:00Z), 2. Comma-separated format (e.g., 2024,12,06,13,00,00), 3. Simple datetime format (e.g., 2024-12-06 13:00:00)\", show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_event_types', display_name='Event Types', info='List of event types to return. Possible values are: default, outOfOffice, focusTime, workingLocation.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_EVENT_page_token', display_name='Page Token', info='Token specifying which result page to return. Optional.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_DUPLICATE_CALENDAR_summary', display_name='Summary/Title', info='Title of the calendar to be duplicated.', show=False, value=''), MessageTextInput(name='GOOGLECALENDAR_REMOVE_ATTENDEE_calendar_id', display_name='Calendar Id', info='ID of the Google Calendar', show=False, value='primary'), MessageTextInput(name='GOOGLECALENDAR_REMOVE_ATTENDEE_event_id', display_name='Event Id', info='ID of the event', show=False, required=True), MessageTextInput(name='GOOGLECALENDAR_REMOVE_ATTENDEE_attendee_email', display_name='Attendee Email', info='Email address of the attendee to be removed', show=False, required=True), MessageTextInput(name='GOOGLECALENDAR_GET_CALENDAR_calendar_id', display_name='Calendar Id', info=\"The ID of the Google Calendar that needs to be fetched. Default is 'primary'.\", show=False, value='primary'), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_description', display_name='Description', info='Description of the event. Can contain HTML. Optional.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_eventType', display_name='Event Type', info=\"Type of the event, immutable post-creation. Currently, only 'default'\", show=False, value='default', advanced=True), BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_create_meeting_room', display_name='Create Meeting Room', info='If true, a Google Meet link is created and added to the event.', show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_guestsCanSeeOtherGuests', display_name='Guests Can See Other Guests', info=\"Whether attendees other than the organizer can see who the event's attendees are.\", show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_guestsCanInviteOthers', display_name='Guests Can Invite Others', info='Whether attendees other than the organizer can invite others to the event.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_location', display_name='Location', info='Geographic location of the event as free-form text.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_summary', display_name='Summary/Title', info='Summary (title) of the event.', show=False), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_transparency', display_name='Event Transparency', info=\"'opaque' (busy) or 'transparent' (available).\", show=False, value='opaque', advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_visibility', display_name='Event Visibility', info=\"Event visibility: 'default', 'public', 'private', or 'confidential'.\", show=False, value='default', advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_timezone', display_name='Timezone', info=\"IANA timezone name (e.g., 'America/New_York'). Required if datetime is naive. If datetime includes timezone info (Z or offset), this field is optional and defaults to UTC.\", show=False), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_recurrence', display_name='Recurrence', info='List of RRULE, EXRULE, RDATE, EXDATE lines for recurring events.', show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_guests_can_modify', display_name='Guests Can Modify', info='If True, guests can modify the event.', show=False, value=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_attendees', display_name='Attendees', info='List of attendee emails (strings).', show=False), BoolInput(name='GOOGLECALENDAR_CREATE_EVENT_send_updates', display_name='Send Updates', info='Defaults to True. Whether to send updates to the attendees.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_start_datetime', display_name='Start Datetime', info=\"Naive date/time (YYYY-MM-DDTHH:MM:SS) with NO offsets or Z. e.g. '2025-01-16T13:00:00'\", show=False, required=True), IntInput(name='GOOGLECALENDAR_CREATE_EVENT_event_duration_hour', display_name='Event Duration Hour', info='Number of hours (0-24).', show=False, value=0, advanced=True), IntInput(name='GOOGLECALENDAR_CREATE_EVENT_event_duration_minutes', display_name='Event Duration Minutes', info='Number of minutes (0-59).', show=False, value=30, advanced=True), MessageTextInput(name='GOOGLECALENDAR_CREATE_EVENT_calendar_id', display_name='Calendar Id', info='The ID of the Google Calendar. `primary` for interacting with the primary calendar.', show=False, value='primary', advanced=True), MessageTextInput(name='GOOGLECALENDAR_DELETE_EVENT_calendar_id', display_name='Calendar Id', info='ID of the Google Calendar', show=False, value='primary'), MessageTextInput(name='GOOGLECALENDAR_DELETE_EVENT_event_id', display_name='Event Id', info='ID of the event to be deleted', show=False, required=True), MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_time_min', display_name='Time Min', info='The start datetime of the interval for the query. Supports multiple formats:, 1. ISO format with timezone (e.g., 2024-12-06T13:00:00Z), 2. Comma-separated format (e.g., 2024,12,06,13,00,00), 3. Simple datetime format (e.g., 2024-12-06 13:00:00)', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_time_max', display_name='Time Max', info='The end datetime of the interval for the query. Supports multiple formats:, 1. ISO format with timezone (e.g., 2024-12-06T13:00:00Z), 2. Comma-separated format (e.g., 2024,12,06,13,00,00), 3. Simple datetime format (e.g., 2024-12-06 13:00:00)', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_timezone', display_name='Timezone', info='Time zone used in the response. Optional. The default is UTC.', show=False, value='UTC', advanced=True), IntInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_group_expansion_max', display_name='Group Expansion Max', info='Maximal number of calendar identifiers to be provided for a single group. Optional. An error is returned for a group with more members than this value. Maximum value is 100.', show=False, value=100, advanced=True), IntInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_calendar_expansion_max', display_name='Calendar Expansion Max', info='Maximal number of calendars for which FreeBusy information is to be provided. Optional. Maximum value is 50.', show=False, value=50, advanced=True), MessageTextInput(name='GOOGLECALENDAR_FIND_FREE_SLOTS_items', display_name='Items', info='List of calendars ids for which to fetch', show=False), MessageTextInput(name='GOOGLECALENDAR_QUICK_ADD_calendar_id', display_name='Calendar Id', info=\"Calendar identifier. To list calendars to retrieve calendar IDs use relevant tools. To access the primary calendar of the currently logged in user, use the 'primary' keyword.\", show=False, value='primary'), MessageTextInput(name='GOOGLECALENDAR_QUICK_ADD_text', display_name='Text', info='The text describing the event to be created.', show=False, value=''), MessageTextInput(name='GOOGLECALENDAR_QUICK_ADD_send_updates', display_name='Send Updates', info=\"Guests who should receive notifications about the creation of the new event. Accepted fields include 'all', 'none', 'externalOnly'\", show=False, value='none', advanced=True), MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_calendar_id', display_name='Calendar Id', info='The ID of the Google Calendar that needs to be updated.', show=False, required=True), MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_description', display_name='Description', info='Description of the calendar. Optional.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_location', display_name='Location', info='Geographic location of the calendar as free-form text.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_summary', display_name='Title/Summary', info='Title of the calendar. This field is required and cannot be left blank as per the Google Calendar API requirements.', show=False, required=True), MessageTextInput(name='GOOGLECALENDAR_PATCH_CALENDAR_timezone', display_name='Timezone', info=\"The time zone of the calendar. (Formatted as an IANA Time Zone Database name, e.g. 'Europe/Zurich').\", show=False, advanced=True), IntInput(name='GOOGLECALENDAR_GET_CURRENT_DATE_TIME_timezone', display_name='Timezone', info='The timezone offset from UTC to retrieve current date and time, like for location of UTC+6, you give 6, for UTC -9, your give -9.', show=False, value=0, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_description', display_name='Description', info='Description of the event. Can contain HTML. Optional.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_eventType', display_name='EventType', info=\"Type of the event, immutable post-creation. Currently, only 'default' and 'workingLocation' can be created.\", show=False, value='default', advanced=True), BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_create_meeting_room', display_name='Create Meeting Room', info='If true, a Google Meet link is created and added to the event.', show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_guestsCanSeeOtherGuests', display_name='Guests Can See Other Guests', info=\"Whether attendees other than the organizer can see who the event's attendees are.\", show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_guestsCanInviteOthers', display_name='Guests Can Invite Others', info='Whether attendees other than the organizer can invite others to the event.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_location', display_name='Location', info='Geographic location of the event as free-form text.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_summary', display_name='Summary/Title', info='Summary (title) of the event.', show=False), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_transparency', display_name='Event Transparency', info=\"'opaque' (busy) or 'transparent' (available).\", show=False, value='opaque', advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_visibility', display_name='Event Visibility', info=\"Event visibility: 'default', 'public', 'private', or 'confidential'.\", show=False, value='default', advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_timezone', display_name='Timezone', info=\"IANA timezone name (e.g., 'America/New_York'). Required if datetime is naive. If datetime includes timezone info (Z or offset), this field is optional and defaults to UTC.\", show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_recurrence', display_name='Recurrence', info='List of RRULE, EXRULE, RDATE, EXDATE lines for recurring events.', show=False, advanced=True), BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_guests_can_modify', display_name='Guests Can Modify', info='If True, guests can modify the event.', show=False, value=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_attendees', display_name='Attendees', info='List of attendee emails (strings).', show=False), BoolInput(name='GOOGLECALENDAR_UPDATE_EVENT_send_updates', display_name='Send Updates', info='Defaults to True. Whether to send updates to the attendees.', show=False, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_start_datetime', display_name='Start Datetime', info=\"Naive date/time (YYYY-MM-DDTHH:MM:SS) with NO offsets or Z. e.g. '2025-01-16T13:00:00'\", show=False, required=True), IntInput(name='GOOGLECALENDAR_UPDATE_EVENT_event_duration_hour', display_name='Event Duration Hour', info='Number of hours (0-24).', show=False, value=0, advanced=True), IntInput(name='GOOGLECALENDAR_UPDATE_EVENT_event_duration_minutes', display_name='Event Duration Minutes', info='Number of minutes (0-59).', show=False, value=30, advanced=True), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_calendar_id', display_name='Calendar Id', info='ID of the Google Calendar', show=False, value='primary'), MessageTextInput(name='GOOGLECALENDAR_UPDATE_EVENT_event_id', display_name='Event Id', info='ID of the event to be updated', show=False, required=True)]", "outputs": "", "display_name": "Google Calendar", "name": "", "description": "Google Calendar API", "icon": "Googlecalendar"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/memory.py", "section": "class::MemoryComponent", "content": "from typing import cast\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import aget_messages\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\nclass MemoryComponent(Component):\n    display_name: str = \"Message History\"\n    description: str = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(name='memory',\n        display_name='External Memory',\n        input_types=['Memory'],\n        info='Retrieve messages from an external memory. If empty,\n        it will use the Langflow tables.'),\n        DropdownInput(name='sender',\n        display_name='Sender Type',\n        options=[MESSAGE_SENDER_AI,\n        MESSAGE_SENDER_USER,\n        'Machine and User'],\n        value='Machine and User',\n        info='Filter by sender type.',\n        advanced=True),\n        MessageTextInput(name='sender_name',\n        display_name='Sender Name',\n        info='Filter by sender name.',\n        advanced=True),\n        IntInput(name='n_messages',\n        display_name='Number of Messages',\n        value=100,\n        info='Number of messages to retrieve.',\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='The session ID of the chat. If empty,\n        the current session ID parameter will be used.',\n        advanced=True),\n        DropdownInput(name='order',\n        display_name='Order',\n        options=['Ascending',\n        'Descending'],\n        value='Ascending',\n        info='Order of the messages.',\n        advanced=True,\n        tool_mode=True),\n        MultilineInput(name='template',\n        display_name='Template',\n        info='The template to use for formatting the data. It can contain the keys {text},\n        {sender} or any other key in the message data.',\n        value='{sender_name}: {text}',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='messages',\n        method='retrieve_messages'),\n        Output(display_name='Message',\n        name='messages_text',\n        method='retrieve_messages_as_text'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "MemoryComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["from typing import cast", "from langflow.custom import Component", "from langflow.helpers.data import data_to_text", "from langflow.inputs import HandleInput", "from langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output", "from langflow.memory import aget_messages", "from langflow.schema import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.schema.message import Message", "from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER"], "inputs": "[HandleInput(name='memory', display_name='External Memory', input_types=['Memory'], info='Retrieve messages from an external memory. If empty, it will use the Langflow tables.'), DropdownInput(name='sender', display_name='Sender Type', options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, 'Machine and User'], value='Machine and User', info='Filter by sender type.', advanced=True), MessageTextInput(name='sender_name', display_name='Sender Name', info='Filter by sender name.', advanced=True), IntInput(name='n_messages', display_name='Number of Messages', value=100, info='Number of messages to retrieve.', advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='The session ID of the chat. If empty, the current session ID parameter will be used.', advanced=True), DropdownInput(name='order', display_name='Order', options=['Ascending', 'Descending'], value='Ascending', info='Order of the messages.', advanced=True, tool_mode=True), MultilineInput(name='template', display_name='Template', info='The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.', value='{sender_name}: {text}', advanced=True)]", "outputs": "[Output(display_name='Data', name='messages', method='retrieve_messages'), Output(display_name='Message', name='messages_text', method='retrieve_messages_as_text'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Message History", "name": "Memory", "description": "Retrieves stored chat messages from Langflow tables or an external memory.", "icon": "message-square-more"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/memories/zep.py", "section": "class::ZepChatMemory", "content": "from langflow.base.memory.model import LCChatMemoryComponent\nfrom langflow.field_typing.constants import Memory\nfrom langflow.inputs import DropdownInput, MessageTextInput, SecretStrInput\nimport zep_python.zep_client\nfrom zep_python import ZepClient\nfrom zep_python.langchain import ZepChatMessageHistory\n\nclass ZepChatMemory(LCChatMemoryComponent):\n    display_name: str = \"Zep Chat Memory\"\n    description: str = \"Retrieves and store chat messages from Zep.\"\n    icon = \"ZepMemory\"\n    name = \"ZepChatMemory\"\n\n    inputs = [\n        MessageTextInput(name='url',\n        display_name='Zep URL',\n        info='URL of the Zep instance.'),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        info='API Key for the Zep instance.'),\n        DropdownInput(name='api_base_path',\n        display_name='API Base Path',\n        options=['api/v1',\n        'api/v2'],\n        value='api/v1',\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='Session ID for the message.',\n        advanced=True)\n    ]\n\n    def build_message_history(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ZepChatMemory", "base_classes": ["LCChatMemoryComponent"], "public_methods": ["def build_message_history(self)"], "imports": ["from langflow.base.memory.model import LCChatMemoryComponent", "from langflow.field_typing.constants import Memory", "from langflow.inputs import DropdownInput, MessageTextInput, SecretStrInput", "import zep_python.zep_client", "from zep_python import ZepClient", "from zep_python.langchain import ZepChatMessageHistory"], "inputs": "[MessageTextInput(name='url', display_name='Zep URL', info='URL of the Zep instance.'), SecretStrInput(name='api_key', display_name='API Key', info='API Key for the Zep instance.'), DropdownInput(name='api_base_path', display_name='API Base Path', options=['api/v1', 'api/v2'], value='api/v1', advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='Session ID for the message.', advanced=True)]", "outputs": "", "display_name": "Zep Chat Memory", "name": "ZepChatMemory", "description": "Retrieves and store chat messages from Zep.", "icon": "ZepMemory"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/crewai/sequential_crew.py", "section": "class::SequentialCrewComponent", "content": "from crewai import Agent, Crew, Process, Task\nfrom langflow.base.agents.crewai.crew import BaseCrewComponent\nfrom langflow.io import HandleInput\nfrom langflow.schema.message import Message\n\nclass SequentialCrewComponent(BaseCrewComponent):\n    display_name: str = \"Sequential Crew\"\n    description: str = \"Represents a group of agents with tasks that are executed sequentially.\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        *BaseCrewComponent._base_inputs,\n        HandleInput(name='tasks',\n        display_name='Tasks',\n        input_types=['SequentialTask'],\n        is_list=True)\n    ]\n\n    def agents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_tasks_and_agents(self, agents_list):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_crew(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SequentialCrewComponent", "base_classes": ["BaseCrewComponent"], "public_methods": ["def agents(self)", "def get_tasks_and_agents(self, agents_list)", "def build_crew(self)"], "imports": ["from crewai import Agent, Crew, Process, Task", "from langflow.base.agents.crewai.crew import BaseCrewComponent", "from langflow.io import HandleInput", "from langflow.schema.message import Message"], "inputs": "[*BaseCrewComponent._base_inputs, HandleInput(name='tasks', display_name='Tasks', input_types=['SequentialTask'], is_list=True)]", "outputs": "", "display_name": "Sequential Crew", "name": "", "description": "Represents a group of agents with tasks that are executed sequentially.", "icon": "CrewAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/google_search_api.py", "section": "class::GoogleSearchAPIComponent", "content": "from langchain_core.tools import Tool\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import IntInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\nfrom langchain_google_community import GoogleSearchAPIWrapper\n\nclass GoogleSearchAPIComponent(LCToolComponent):\n    display_name: str = \"Google Search API [DEPRECATED]\"\n    description: str = \"Call Google Search API.\"\n    icon = \"Google\"\n    name = \"GoogleSearchAPI\"\n\n    inputs = [\n        SecretStrInput(name='google_api_key',\n        display_name='Google API Key',\n        required=True),\n        SecretStrInput(name='google_cse_id',\n        display_name='Google CSE ID',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input'),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleSearchAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["from langchain_core.tools import Tool", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.inputs import IntInput, MultilineInput, SecretStrInput", "from langflow.schema import Data", "from langchain_google_community import GoogleSearchAPIWrapper"], "inputs": "[SecretStrInput(name='google_api_key', display_name='Google API Key', required=True), SecretStrInput(name='google_cse_id', display_name='Google CSE ID', required=True), MultilineInput(name='input_value', display_name='Input'), IntInput(name='k', display_name='Number of results', value=4, required=True)]", "outputs": "", "display_name": "Google Search API [DEPRECATED]", "name": "GoogleSearchAPI", "description": "Call Google Search API.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/mistral.py", "section": "class::MistralAIEmbeddingsComponent", "content": "from langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\n\nclass MistralAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"MistralAI Embeddings\"\n    description: str = \"Generate embeddings using MistralAI models.\"\n    icon = \"MistralAI\"\n    name = \"MistalAIEmbeddings\"\n\n    inputs = [\n        DropdownInput(name='model',\n        display_name='Model',\n        advanced=False,\n        options=['mistral-embed'],\n        value='mistral-embed'),\n        SecretStrInput(name='mistral_api_key',\n        display_name='Mistral API Key',\n        required=True),\n        IntInput(name='max_concurrent_requests',\n        display_name='Max Concurrent Requests',\n        advanced=True,\n        value=64),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        advanced=True,\n        value=5),\n        IntInput(name='timeout',\n        display_name='Request Timeout',\n        advanced=True,\n        value=120),\n        MessageTextInput(name='endpoint',\n        display_name='API Endpoint',\n        advanced=True,\n        value='https://api.mistral.ai/v1/')\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MistralAIEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langchain_mistralai.embeddings import MistralAIEmbeddings", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput"], "inputs": "[DropdownInput(name='model', display_name='Model', advanced=False, options=['mistral-embed'], value='mistral-embed'), SecretStrInput(name='mistral_api_key', display_name='Mistral API Key', required=True), IntInput(name='max_concurrent_requests', display_name='Max Concurrent Requests', advanced=True, value=64), IntInput(name='max_retries', display_name='Max Retries', advanced=True, value=5), IntInput(name='timeout', display_name='Request Timeout', advanced=True, value=120), MessageTextInput(name='endpoint', display_name='API Endpoint', advanced=True, value='https://api.mistral.ai/v1/')]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "MistralAI Embeddings", "name": "MistalAIEmbeddings", "description": "Generate embeddings using MistralAI models.", "icon": "MistralAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/amazon/amazon_bedrock_model.py", "section": "class::AmazonBedrockComponent", "content": "from langflow.base.models.aws_constants import AWS_REGIONS, AWS_MODEL_IDs\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import MessageTextInput, SecretStrInput\nfrom langflow.io import DictInput, DropdownInput\nfrom langchain_aws import ChatBedrock\nimport boto3\n\nclass AmazonBedrockComponent(LCModelComponent):\n    display_name: str = \"Amazon Bedrock\"\n    description: str = \"Generate text using Amazon Bedrock LLMs.\"\n    icon = \"Amazon\"\n    name = \"AmazonBedrockModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(name='model_id',\n        display_name='Model ID',\n        options=AWS_MODEL_IDs,\n        value='anthropic.claude-3-haiku-20240307-v1:0',\n        info='List of available model IDs to choose from.'),\n        SecretStrInput(name='aws_access_key_id',\n        display_name='AWS Access Key ID',\n        info=\"The access key for your AWS account.Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.\",\n        value='AWS_ACCESS_KEY_ID',\n        required=True),\n        SecretStrInput(name='aws_secret_access_key',\n        display_name='AWS Secret Access Key',\n        info=\"The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.\",\n        value='AWS_SECRET_ACCESS_KEY',\n        required=True),\n        SecretStrInput(name='aws_session_token',\n        display_name='AWS Session Token',\n        advanced=False,\n        info=\"The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.\",\n        load_from_db=False),\n        SecretStrInput(name='credentials_profile_name',\n        display_name='Credentials Profile Name',\n        advanced=True,\n        info='The name of the profile to use from your ~/.aws/credentials file. If not provided,\n        the default profile will be used.',\n        load_from_db=False),\n        DropdownInput(name='region_name',\n        display_name='Region Name',\n        value='us-east-1',\n        options=AWS_REGIONS,\n        info='The AWS region where your Bedrock resources are located.'),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True,\n        is_list=True,\n        info='Additional keyword arguments to pass to the model.'),\n        MessageTextInput(name='endpoint_url',\n        display_name='Endpoint URL',\n        advanced=True,\n        info='The URL of the Bedrock endpoint to use.')\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AmazonBedrockComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langflow.base.models.aws_constants import AWS_REGIONS, AWS_MODEL_IDs", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.inputs import MessageTextInput, SecretStrInput", "from langflow.io import DictInput, DropdownInput", "from langchain_aws import ChatBedrock", "import boto3"], "inputs": "[*LCModelComponent._base_inputs, DropdownInput(name='model_id', display_name='Model ID', options=AWS_MODEL_IDs, value='anthropic.claude-3-haiku-20240307-v1:0', info='List of available model IDs to choose from.'), SecretStrInput(name='aws_access_key_id', display_name='AWS Access Key ID', info=\"The access key for your AWS account.Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.\", value='AWS_ACCESS_KEY_ID', required=True), SecretStrInput(name='aws_secret_access_key', display_name='AWS Secret Access Key', info=\"The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.\", value='AWS_SECRET_ACCESS_KEY', required=True), SecretStrInput(name='aws_session_token', display_name='AWS Session Token', advanced=False, info=\"The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.\", load_from_db=False), SecretStrInput(name='credentials_profile_name', display_name='Credentials Profile Name', advanced=True, info='The name of the profile to use from your ~/.aws/credentials file. If not provided, the default profile will be used.', load_from_db=False), DropdownInput(name='region_name', display_name='Region Name', value='us-east-1', options=AWS_REGIONS, info='The AWS region where your Bedrock resources are located.'), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True, is_list=True, info='Additional keyword arguments to pass to the model.'), MessageTextInput(name='endpoint_url', display_name='Endpoint URL', advanced=True, info='The URL of the Bedrock endpoint to use.')]", "outputs": "", "display_name": "Amazon Bedrock", "name": "AmazonBedrockModel", "description": "Generate text using Amazon Bedrock LLMs.", "icon": "Amazon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/openapi.py", "section": "class::OpenAPIAgentComponent", "content": "from pathlib import Path\nimport yaml\nfrom langchain.agents import AgentExecutor\nfrom langchain_community.agent_toolkits import create_openapi_agent\nfrom langchain_community.agent_toolkits.openapi.toolkit import OpenAPIToolkit\nfrom langchain_community.tools.json.tool import JsonSpec\nfrom langchain_community.utilities.requests import TextRequestsWrapper\nfrom langflow.base.agents.agent import LCAgentComponent\nfrom langflow.inputs import BoolInput, FileInput, HandleInput\n\nclass OpenAPIAgentComponent(LCAgentComponent):\n    display_name: str = \"OpenAPI Agent\"\n    description: str = \"Agent to interact with OpenAPI API.\"\n    icon = \"LangChain\"\n    name = \"OpenAPIAgent\"\n\n    inputs = [\n        *LCAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        FileInput(name='path',\n        display_name='File Path',\n        file_types=['json',\n        'yaml',\n        'yml'],\n        required=True),\n        BoolInput(name='allow_dangerous_requests',\n        display_name='Allow Dangerous Requests',\n        value=False,\n        required=True)\n    ]\n\n    def build_agent(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OpenAPIAgentComponent", "base_classes": ["LCAgentComponent"], "public_methods": ["def build_agent(self)"], "imports": ["from pathlib import Path", "import yaml", "from langchain.agents import AgentExecutor", "from langchain_community.agent_toolkits import create_openapi_agent", "from langchain_community.agent_toolkits.openapi.toolkit import OpenAPIToolkit", "from langchain_community.tools.json.tool import JsonSpec", "from langchain_community.utilities.requests import TextRequestsWrapper", "from langflow.base.agents.agent import LCAgentComponent", "from langflow.inputs import BoolInput, FileInput, HandleInput"], "inputs": "[*LCAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), FileInput(name='path', display_name='File Path', file_types=['json', 'yaml', 'yml'], required=True), BoolInput(name='allow_dangerous_requests', display_name='Allow Dangerous Requests', value=False, required=True)]", "outputs": "", "display_name": "OpenAPI Agent", "name": "OpenAPIAgent", "description": "Agent to interact with OpenAPI API.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/parse_dataframe.py", "section": "class::ParseDataFrameComponent", "content": "from langflow.custom import Component\nfrom langflow.io import DataFrameInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\nclass ParseDataFrameComponent(Component):\n    display_name: str = \"Parse DataFrame\"\n    description: str = \"Convert a DataFrame into plain text following a specified template. Each column in the DataFrame is treated as a possible template key, e.g. {col_name}.\"\n    icon = \"braces\"\n    name = \"ParseDataFrame\"\n\n    inputs = [\n        DataFrameInput(name='df',\n        display_name='DataFrame',\n        info='The DataFrame to convert to text rows.'),\n        MultilineInput(name='template',\n        display_name='Template',\n        info=\"The template for formatting each row. Use placeholders matching column names in the DataFrame,\n        for example '{col1}',\n        '{col2}'.\",\n        value='{text}'),\n        StrInput(name='sep',\n        display_name='Separator',\n        advanced=True,\n        value='\\n',\n        info='String that joins all row texts when building the single Text output.')\n    ]\n\n    outputs = [\n        Output(display_name='Text',\n        name='text',\n        info='All rows combined into a single text,\n        each row formatted by the template and separated by `sep`.',\n        method='parse_data')\n    ]\n\n    def parse_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ParseDataFrameComponent", "base_classes": ["Component"], "public_methods": ["def parse_data(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import DataFrameInput, MultilineInput, Output, StrInput", "from langflow.schema.message import Message"], "inputs": "[DataFrameInput(name='df', display_name='DataFrame', info='The DataFrame to convert to text rows.'), MultilineInput(name='template', display_name='Template', info=\"The template for formatting each row. Use placeholders matching column names in the DataFrame, for example '{col1}', '{col2}'.\", value='{text}'), StrInput(name='sep', display_name='Separator', advanced=True, value='\\n', info='String that joins all row texts when building the single Text output.')]", "outputs": "[Output(display_name='Text', name='text', info='All rows combined into a single text, each row formatted by the template and separated by `sep`.', method='parse_data')]", "display_name": "Parse DataFrame", "name": "ParseDataFrame", "description": "Convert a DataFrame into plain text following a specified template. Each column in the DataFrame is treated as a possible template key, e.g. {col_name}.", "icon": "braces"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/firecrawl/firecrawl_scrape_api.py", "section": "class::FirecrawlScrapeApi", "content": "from langflow.custom import Component\nfrom langflow.io import DataInput, IntInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom firecrawl import FirecrawlApp\n\nclass FirecrawlScrapeApi(Component):\n    display_name: str = \"FirecrawlScrapeApi\"\n    description: str = \"Firecrawl Scrape API.\"\n    name = \"FirecrawlScrapeApi\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True,\n        password=True,\n        info='The API key to use Firecrawl API.'),\n        MultilineInput(name='url',\n        display_name='URL',\n        required=True,\n        info='The URL to scrape.',\n        tool_mode=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='Timeout in milliseconds for the request.'),\n        DataInput(name='scrapeOptions',\n        display_name='Scrape Options',\n        info='The page options to send with the request.'),\n        DataInput(name='extractorOptions',\n        display_name='Extractor Options',\n        info='The extractor options to send with the request.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='scrape')\n    ]\n\n    def scrape(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FirecrawlScrapeApi", "base_classes": ["Component"], "public_methods": ["def scrape(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import DataInput, IntInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import Data", "from firecrawl import FirecrawlApp"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True, password=True, info='The API key to use Firecrawl API.'), MultilineInput(name='url', display_name='URL', required=True, info='The URL to scrape.', tool_mode=True), IntInput(name='timeout', display_name='Timeout', info='Timeout in milliseconds for the request.'), DataInput(name='scrapeOptions', display_name='Scrape Options', info='The page options to send with the request.'), DataInput(name='extractorOptions', display_name='Extractor Options', info='The extractor options to send with the request.')]", "outputs": "[Output(display_name='Data', name='data', method='scrape')]", "display_name": "FirecrawlScrapeApi", "name": "FirecrawlScrapeApi", "description": "Firecrawl Scrape API.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/notify.py", "section": "class::NotifyComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.schema import Data\n\nclass NotifyComponent(CustomComponent):\n    display_name: str = \"Notify\"\n    description: str = \"A component to generate a notification to Get Notified component.\"\n    icon = \"Notify\"\n    name = \"Notify\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotifyComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, name)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "Notify", "name": "Notify", "description": "A component to generate a notification to Get Notified component.", "icon": "Notify"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/google_oauth_token.py", "section": "class::GoogleOAuthToken", "content": "import json\nimport re\nfrom pathlib import Path\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom langflow.custom import Component\nfrom langflow.io import FileInput, MultilineInput, Output\nfrom langflow.schema import Data\n\nclass GoogleOAuthToken(Component):\n    display_name: str = \"Google OAuth Token\"\n    description: str = \"Generates a JSON string with your Google OAuth token.\"\n    icon = \"Google\"\n    name = \"GoogleOAuthToken\"\n\n    inputs = [\n        MultilineInput(name='scopes',\n        display_name='Scopes',\n        info='Input scopes for your application.',\n        required=True),\n        FileInput(name='oauth_credentials',\n        display_name='Credentials File',\n        info='Input OAuth Credentials file (e.g. credentials.json).',\n        file_types=['json'],\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Output',\n        name='output',\n        method='build_output')\n    ]\n\n    def validate_scopes(self, scopes):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleOAuthToken", "base_classes": ["Component"], "public_methods": ["def validate_scopes(self, scopes)", "def build_output(self)"], "imports": ["import json", "import re", "from pathlib import Path", "from google.auth.transport.requests import Request", "from google.oauth2.credentials import Credentials", "from google_auth_oauthlib.flow import InstalledAppFlow", "from langflow.custom import Component", "from langflow.io import FileInput, MultilineInput, Output", "from langflow.schema import Data"], "inputs": "[MultilineInput(name='scopes', display_name='Scopes', info='Input scopes for your application.', required=True), FileInput(name='oauth_credentials', display_name='Credentials File', info='Input OAuth Credentials file (e.g. credentials.json).', file_types=['json'], required=True)]", "outputs": "[Output(display_name='Output', name='output', method='build_output')]", "display_name": "Google OAuth Token", "name": "GoogleOAuthToken", "description": "Generates a JSON string with your Google OAuth token.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/nvidia.py", "section": "class::NVIDIAModelComponent", "content": "from typing import Any\nfrom loguru import logger\nfrom requests.exceptions import ConnectionError\nfrom urllib3.exceptions import MaxRetryError, NameResolutionError\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langflow.schema.dotdict import dotdict\nimport warnings\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nclass NVIDIAModelComponent(LCModelComponent):\n    display_name: str = \"NVIDIA\"\n    description: str = \"Generates text using NVIDIA LLMs.\"\n    icon = \"NVIDIA\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        info='The name of the NVIDIA model to use.',\n        advanced=False,\n        value=None,\n        options=[model.id for model in all_models],\n        combobox=True,\n        refresh_button=True),\n        BoolInput(name='detailed_thinking',\n        display_name='Detailed Thinking',\n        info='If true,\n        the model will return a detailed thought process. Only supported by reasoning models.',\n        value=False,\n        show=False),\n        BoolInput(name='tool_model_enabled',\n        display_name='Enable Tool Models',\n        info='If enabled,\n        only show models that support tool-calling.',\n        advanced=False,\n        value=False,\n        real_time_refresh=True),\n        MessageTextInput(name='base_url',\n        display_name='NVIDIA Base URL',\n        value='https://integrate.api.nvidia.com/v1',\n        info='The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.'),\n        SecretStrInput(name='api_key',\n        display_name='NVIDIA API Key',\n        info='The NVIDIA API Key.',\n        advanced=False,\n        value='NVIDIA_API_KEY'),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        info='Run inference with this temperature.',\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True),\n        IntInput(name='seed',\n        display_name='Seed',\n        info='The seed controls the reproducibility of the job.',\n        advanced=True,\n        value=1)\n    ]\n\n    def get_models(self, tool_model_enabled):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, _field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NVIDIAModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def get_models(self, tool_model_enabled)", "def update_build_config(self, build_config, _field_value, field_name)", "def build_model(self)"], "imports": ["from typing import Any", "from loguru import logger", "from requests.exceptions import ConnectionError", "from urllib3.exceptions import MaxRetryError, NameResolutionError", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput", "from langflow.schema.dotdict import dotdict", "import warnings", "from langchain_nvidia_ai_endpoints import ChatNVIDIA", "from langchain_nvidia_ai_endpoints import ChatNVIDIA", "from langchain_nvidia_ai_endpoints import ChatNVIDIA"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'), DropdownInput(name='model_name', display_name='Model Name', info='The name of the NVIDIA model to use.', advanced=False, value=None, options=[model.id for model in all_models], combobox=True, refresh_button=True), BoolInput(name='detailed_thinking', display_name='Detailed Thinking', info='If true, the model will return a detailed thought process. Only supported by reasoning models.', value=False, show=False), BoolInput(name='tool_model_enabled', display_name='Enable Tool Models', info='If enabled, only show models that support tool-calling.', advanced=False, value=False, real_time_refresh=True), MessageTextInput(name='base_url', display_name='NVIDIA Base URL', value='https://integrate.api.nvidia.com/v1', info='The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.'), SecretStrInput(name='api_key', display_name='NVIDIA API Key', info='The NVIDIA API Key.', advanced=False, value='NVIDIA_API_KEY'), SliderInput(name='temperature', display_name='Temperature', value=0.1, info='Run inference with this temperature.', range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True), IntInput(name='seed', display_name='Seed', info='The seed controls the reproducibility of the job.', advanced=True, value=1)]", "outputs": "", "display_name": "NVIDIA", "name": "", "description": "Generates text using NVIDIA LLMs.", "icon": "NVIDIA"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/playlist.py", "section": "class::YouTubePlaylistComponent", "content": "from pytube import Playlist\nfrom langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.schema import Data, DataFrame\nfrom langflow.template import Output\n\nclass YouTubePlaylistComponent(Component):\n    display_name: str = \"Youtube Playlist\"\n    description: str = \"Extracts all video URLs from a YouTube playlist.\"\n    icon = \"YouTube\"\n\n    inputs = [\n        MessageTextInput(name='playlist_url',\n        display_name='Playlist URL',\n        info='URL of the YouTube playlist.',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Video URLs',\n        name='video_urls',\n        method='extract_video_urls')\n    ]\n\n    def extract_video_urls(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubePlaylistComponent", "base_classes": ["Component"], "public_methods": ["def extract_video_urls(self)"], "imports": ["from pytube import Playlist", "from langflow.custom import Component", "from langflow.inputs import MessageTextInput", "from langflow.schema import Data, DataFrame", "from langflow.template import Output"], "inputs": "[MessageTextInput(name='playlist_url', display_name='Playlist URL', info='URL of the YouTube playlist.', required=True)]", "outputs": "[Output(display_name='Video URLs', name='video_urls', method='extract_video_urls')]", "display_name": "Youtube Playlist", "name": "", "description": "Extracts all video URLs from a YouTube playlist.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/dotenv.py", "section": "class::Dotenv", "content": "import io\nfrom dotenv import load_dotenv\nfrom langflow.custom import Component\nfrom langflow.inputs import MultilineSecretInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass Dotenv(Component):\n    display_name: str = \"Dotenv\"\n    description: str = \"Load .env file into env vars\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        MultilineSecretInput(name='dotenv_file_content',\n        display_name='Dotenv file content',\n        info=\"Paste the content of your .env file directly,\n        since contents are sensitive,\n        using a Global variable set as 'password' is recommended\")\n    ]\n\n    outputs = [\n        Output(display_name='env_set',\n        name='env_set',\n        method='process_inputs')\n    ]\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "Dotenv", "base_classes": ["Component"], "public_methods": ["def process_inputs(self)"], "imports": ["import io", "from dotenv import load_dotenv", "from langflow.custom import Component", "from langflow.inputs import MultilineSecretInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[MultilineSecretInput(name='dotenv_file_content', display_name='Dotenv file content', info=\"Paste the content of your .env file directly, since contents are sensitive, using a Global variable set as 'password' is recommended\")]", "outputs": "[Output(display_name='env_set', name='env_set', method='process_inputs')]", "display_name": "Dotenv", "name": "", "description": "Load .env file into env vars", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/milvus.py", "section": "class::MilvusVectorStoreComponent", "content": "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langchain_milvus.vectorstores import Milvus as LangchainMilvus\n\nclass MilvusVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    Milvus vector store with search capabilities.\n    \"\"\"\n\n    display_name: str = \"Milvus\"\n    description: str = \"Milvus vector store with search capabilities\"\n    icon = \"Milvus\"\n    name = \"Milvus\"\n\n    inputs = [\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        value='langflow'),\n        StrInput(name='collection_description',\n        display_name='Collection Description',\n        value=''),\n        StrInput(name='uri',\n        display_name='Connection URI',\n        value='http://localhost:19530'),\n        SecretStrInput(name='password',\n        display_name='Token',\n        value='',\n        info='Ignore this field if no token is required to make connection.'),\n        DictInput(name='connection_args',\n        display_name='Other Connection Arguments',\n        advanced=True),\n        StrInput(name='primary_field',\n        display_name='Primary Field Name',\n        value='pk'),\n        StrInput(name='text_field',\n        display_name='Text Field Name',\n        value='text'),\n        StrInput(name='vector_field',\n        display_name='Vector Field Name',\n        value='vector'),\n        DropdownInput(name='consistency_level',\n        display_name='Consistencey Level',\n        options=['Bounded',\n        'Session',\n        'Strong',\n        'Eventual'],\n        value='Session',\n        advanced=True),\n        DictInput(name='index_params',\n        display_name='Index Parameters',\n        advanced=True),\n        DictInput(name='search_params',\n        display_name='Search Parameters',\n        advanced=True),\n        BoolInput(name='drop_old',\n        display_name='Drop Old Collection',\n        value=False,\n        advanced=True),\n        FloatInput(name='timeout',\n        display_name='Timeout',\n        advanced=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MilvusVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langchain_milvus.vectorstores import Milvus as LangchainMilvus"], "inputs": "[StrInput(name='collection_name', display_name='Collection Name', value='langflow'), StrInput(name='collection_description', display_name='Collection Description', value=''), StrInput(name='uri', display_name='Connection URI', value='http://localhost:19530'), SecretStrInput(name='password', display_name='Token', value='', info='Ignore this field if no token is required to make connection.'), DictInput(name='connection_args', display_name='Other Connection Arguments', advanced=True), StrInput(name='primary_field', display_name='Primary Field Name', value='pk'), StrInput(name='text_field', display_name='Text Field Name', value='text'), StrInput(name='vector_field', display_name='Vector Field Name', value='vector'), DropdownInput(name='consistency_level', display_name='Consistencey Level', options=['Bounded', 'Session', 'Strong', 'Eventual'], value='Session', advanced=True), DictInput(name='index_params', display_name='Index Parameters', advanced=True), DictInput(name='search_params', display_name='Search Parameters', advanced=True), BoolInput(name='drop_old', display_name='Drop Old Collection', value=False, advanced=True), FloatInput(name='timeout', display_name='Timeout', advanced=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Milvus", "name": "Milvus", "description": "Milvus vector store with search capabilities", "icon": "Milvus"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/assemblyai/assemblyai_start_transcript.py", "section": "class::AssemblyAITranscriptionJobCreator", "content": "from pathlib import Path\nimport assemblyai as aai\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, FileInput, MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\n\nclass AssemblyAITranscriptionJobCreator(Component):\n    display_name: str = \"AssemblyAI Start Transcript\"\n    description: str = \"Create a transcription job for an audio file using AssemblyAI with advanced options\"\n    icon = \"AssemblyAI\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Assembly API Key',\n        info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/',\n        required=True),\n        FileInput(name='audio_file',\n        display_name='Audio File',\n        file_types=['3ga',\n        '8svx',\n        'aac',\n        'ac3',\n        'aif',\n        'aiff',\n        'alac',\n        'amr',\n        'ape',\n        'au',\n        'dss',\n        'flac',\n        'flv',\n        'm4a',\n        'm4b',\n        'm4p',\n        'm4r',\n        'mp3',\n        'mpga',\n        'ogg',\n        'oga',\n        'mogg',\n        'opus',\n        'qcp',\n        'tta',\n        'voc',\n        'wav',\n        'wma',\n        'wv',\n        'webm',\n        'mts',\n        'm2ts',\n        'ts',\n        'mov',\n        'mp2',\n        'mp4',\n        'm4p',\n        'm4v',\n        'mxf'],\n        info='The audio file to transcribe',\n        required=True),\n        MessageTextInput(name='audio_file_url',\n        display_name='Audio File URL',\n        info='The URL of the audio file to transcribe (Can be used instead of a File)',\n        advanced=True),\n        DropdownInput(name='speech_model',\n        display_name='Speech Model',\n        options=['best',\n        'nano'],\n        value='best',\n        info='The speech model to use for the transcription',\n        advanced=True),\n        BoolInput(name='language_detection',\n        display_name='Automatic Language Detection',\n        info='Enable automatic language detection',\n        advanced=True),\n        MessageTextInput(name='language_code',\n        display_name='Language',\n        info='\\n            The language of the audio file. Can be set manually if automatic language detection is disabled.\\n            See https://www.assemblyai.com/docs/getting-started/supported-languages for a list of supported language codes.',\n        advanced=True),\n        BoolInput(name='speaker_labels',\n        display_name='Enable Speaker Labels',\n        info='Enable speaker diarization'),\n        MessageTextInput(name='speakers_expected',\n        display_name='Expected Number of Speakers',\n        info='Set the expected number of speakers (optional,\n        enter a number)',\n        advanced=True),\n        BoolInput(name='punctuate',\n        display_name='Punctuate',\n        info='Enable automatic punctuation',\n        advanced=True,\n        value=True),\n        BoolInput(name='format_text',\n        display_name='Format Text',\n        info='Enable text formatting',\n        advanced=True,\n        value=True)\n    ]\n\n    outputs = [\n        Output(display_name='Transcript ID',\n        name='transcript_id',\n        method='create_transcription_job')\n    ]\n\n    def create_transcription_job(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssemblyAITranscriptionJobCreator", "base_classes": ["Component"], "public_methods": ["def create_transcription_job(self)"], "imports": ["from pathlib import Path", "import assemblyai as aai", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import BoolInput, DropdownInput, FileInput, MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='Assembly API Key', info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/', required=True), FileInput(name='audio_file', display_name='Audio File', file_types=['3ga', '8svx', 'aac', 'ac3', 'aif', 'aiff', 'alac', 'amr', 'ape', 'au', 'dss', 'flac', 'flv', 'm4a', 'm4b', 'm4p', 'm4r', 'mp3', 'mpga', 'ogg', 'oga', 'mogg', 'opus', 'qcp', 'tta', 'voc', 'wav', 'wma', 'wv', 'webm', 'mts', 'm2ts', 'ts', 'mov', 'mp2', 'mp4', 'm4p', 'm4v', 'mxf'], info='The audio file to transcribe', required=True), MessageTextInput(name='audio_file_url', display_name='Audio File URL', info='The URL of the audio file to transcribe (Can be used instead of a File)', advanced=True), DropdownInput(name='speech_model', display_name='Speech Model', options=['best', 'nano'], value='best', info='The speech model to use for the transcription', advanced=True), BoolInput(name='language_detection', display_name='Automatic Language Detection', info='Enable automatic language detection', advanced=True), MessageTextInput(name='language_code', display_name='Language', info='\\n            The language of the audio file. Can be set manually if automatic language detection is disabled.\\n            See https://www.assemblyai.com/docs/getting-started/supported-languages for a list of supported language codes.', advanced=True), BoolInput(name='speaker_labels', display_name='Enable Speaker Labels', info='Enable speaker diarization'), MessageTextInput(name='speakers_expected', display_name='Expected Number of Speakers', info='Set the expected number of speakers (optional, enter a number)', advanced=True), BoolInput(name='punctuate', display_name='Punctuate', info='Enable automatic punctuation', advanced=True, value=True), BoolInput(name='format_text', display_name='Format Text', info='Enable text formatting', advanced=True, value=True)]", "outputs": "[Output(display_name='Transcript ID', name='transcript_id', method='create_transcription_job')]", "display_name": "AssemblyAI Start Transcript", "name": "", "description": "Create a transcription job for an audio file using AssemblyAI with advanced options", "icon": "AssemblyAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/selective_passthrough.py", "section": "class::SelectivePassThroughComponent", "content": "from langflow.custom import Component\nfrom langflow.field_typing import Text\nfrom langflow.io import BoolInput, DropdownInput, MessageTextInput, Output\n\nclass SelectivePassThroughComponent(Component):\n    display_name: str = \"Selective Pass Through\"\n    description: str = \"Passes the specified value if a specified condition is met.\"\n    icon = \"filter\"\n    name = \"SelectivePassThrough\"\n\n    inputs = [\n        MessageTextInput(name='input_value',\n        display_name='Input Value',\n        info='The primary input value to evaluate.'),\n        MessageTextInput(name='comparison_value',\n        display_name='Comparison Value',\n        info='The value to compare against the input value.'),\n        DropdownInput(name='operator',\n        display_name='Operator',\n        options=['equals',\n        'not equals',\n        'contains',\n        'starts with',\n        'ends with'],\n        info='Condition to evaluate the input value.'),\n        MessageTextInput(name='value_to_pass',\n        display_name='Value to Pass',\n        info='The value to pass if the condition is met.'),\n        BoolInput(name='case_sensitive',\n        display_name='Case Sensitive',\n        info='If true,\n        the comparison will be case sensitive.',\n        value=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Passed Output',\n        name='passed_output',\n        method='pass_through')\n    ]\n\n    def evaluate_condition(self, input_value, comparison_value, operator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def pass_through(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SelectivePassThroughComponent", "base_classes": ["Component"], "public_methods": ["def evaluate_condition(self, input_value, comparison_value, operator)", "def pass_through(self)"], "imports": ["from langflow.custom import Component", "from langflow.field_typing import Text", "from langflow.io import BoolInput, DropdownInput, MessageTextInput, Output"], "inputs": "[MessageTextInput(name='input_value', display_name='Input Value', info='The primary input value to evaluate.'), MessageTextInput(name='comparison_value', display_name='Comparison Value', info='The value to compare against the input value.'), DropdownInput(name='operator', display_name='Operator', options=['equals', 'not equals', 'contains', 'starts with', 'ends with'], info='Condition to evaluate the input value.'), MessageTextInput(name='value_to_pass', display_name='Value to Pass', info='The value to pass if the condition is met.'), BoolInput(name='case_sensitive', display_name='Case Sensitive', info='If true, the comparison will be case sensitive.', value=False, advanced=True)]", "outputs": "[Output(display_name='Passed Output', name='passed_output', method='pass_through')]", "display_name": "Selective Pass Through", "name": "SelectivePassThrough", "description": "Passes the specified value if a specified condition is met.", "icon": "filter"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/composio/slack_composio.py", "section": "class::ComposioSlackAPIComponent", "content": "from typing import Any\nfrom composio import Action\nfrom langflow.base.composio.composio_base import ComposioBaseComponent\nfrom langflow.inputs import BoolInput, IntInput, MessageTextInput\nfrom langflow.logging import logger\n\nclass ComposioSlackAPIComponent(ComposioBaseComponent):\n    display_name: str = \"Slack\"\n    description: str = \"Slack API\"\n    icon = \"Slack\"\n\n    inputs = [\n        *ComposioBaseComponent._base_inputs,\n        IntInput(name='SLACK_LIST_ALL_SLACK_TEAM_USERS_WITH_PAGINATION_limit',\n        display_name='Limit',\n        info=\"The maximum number of items to return. Fewer than the requested number of items may be returned,\n        even if the end of the users list hasn't been reached. Providing no `limit` value will result in Slack attempting to deliver you the entire result set. If the collection is too large you may experience `limit_required` or HTTP 500 errors. \",\n        show=False,\n        value=1),\n        MessageTextInput(name='SLACK_LIST_ALL_SLACK_TEAM_USERS_WITH_PAGINATION_cursor',\n        display_name='Cursor',\n        info=\"Paginate through collections of data by setting the `cursor` parameter to a `next_cursor` attribute returned by a previous request's `response_metadata`. Default value fetches the first `page` of the collection\",\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_LIST_ALL_SLACK_TEAM_USERS_WITH_PAGINATION_include_locale',\n        display_name='Include Locale',\n        info='Set this to `true` to receive the locale for users. Defaults to `false`',\n        show=False),\n        BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_as_user',\n        display_name='As User',\n        info='Pass true to post the message as the authed user,\n        instead of as a bot. Defaults to false',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_attachments',\n        display_name='Attachments',\n        info='A JSON-based array of structured attachments,\n        presented as a URL-encoded string. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_blocks',\n        display_name='Blocks',\n        info='A JSON-based array of structured blocks,\n        presented as a URL-encoded string. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_channel',\n        display_name='Channel',\n        info='Channel,\n        private group,\n        or IM channel to send message to. Can be an encoded ID,\n        or a name ',\n        show=False,\n        required=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_icon_emoji',\n        display_name='Icon Emoji',\n        info='Emoji to use as the icon for this message. Overrides `icon_url`. Must be used in conjunction with `as_user` set to `false`,\n        otherwise ignored',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_icon_url',\n        display_name='Icon Url',\n        info='URL to an image to use as the icon for this message. Must be used in conjunction with `as_user` set to false,\n        otherwise ignored',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_link_names',\n        display_name='Link Names',\n        info='Find and link channel names and usernames.',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_mrkdwn',\n        display_name='Mrkdwn',\n        info='Disable Slack markup parsing by setting to `false`. Enabled by default.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_parse',\n        display_name='Parse',\n        info='Change how messages are treated. Defaults to `none` ',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_reply_broadcast',\n        display_name='Reply Broadcast',\n        info='Used in conjunction with `thread_ts` and indicates whether reply should be made visible to everyone in the channel or conversation. Defaults to `false`. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_text',\n        display_name='Text',\n        info='How this field works and whether it is required depends on other fields you use in your API call',\n        show=False),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_thread_ts',\n        display_name='Thread Ts',\n        info=\"Provide another message's `ts` value to make this message a reply. Avoid using a reply's `ts` value; use its parent instead. \",\n        show=False),\n        BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_unfurl_links',\n        display_name='Unfurl Links',\n        info='Pass true to enable unfurling of primarily text-based content.',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_unfurl_media',\n        display_name='Unfurl Media',\n        info='Pass false to disable unfurling of media content.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_username',\n        display_name='Username',\n        info=\"Set your bot's user name. Must be used in conjunction with `as_user` set to false,\n        otherwise ignored\",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_as_user',\n        display_name='As User',\n        info='Pass true to update the message as the authed user',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_attachments',\n        display_name='Attachments',\n        info=\"A JSON-based array of structured attachments,\n        presented as a URL-encoded string. This field is required when not presenting `text`. If you don't include this field,\n        the message's previous `attachments` will be retained. To remove previous `attachments`,\n        include an empty array for this field. \",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_blocks',\n        display_name='Blocks',\n        info=\"A JSON-based array of structured blocks,\n        presented as a URL-encoded string. If you don't include this field,\n        the message's previous `blocks` will be retained. To remove previous `blocks`,\n        include an empty array for this field. \",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_channel',\n        display_name='Channel ID',\n        info='Channel ID containing the message to be updated.',\n        show=False,\n        required=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_link_names',\n        display_name='Link Names',\n        info='Find and link channel names and usernames. Defaults to `none`. If you do not specify a value for this field,\n        the original value set for the message will be overwritten with the default,\n        `none`. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_parse',\n        display_name='Parse',\n        info='Change how messages are treated. Defaults to `client`,\n        unlike `chat.postMessage`. Accepts either `none` or `full`. If you do not specify a value for this field,\n        the original value set for the message will be overwritten with the default,\n        `client`. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_text',\n        display_name='Text',\n        info=\"New text for the message,\n        using the default formatting rules. It's not required when presenting `blocks` or `attachments`. \",\n        show=False),\n        MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_ts',\n        display_name='Ts',\n        info='Timestamp of the message to be updated.',\n        show=False,\n        required=True),\n        MessageTextInput(name='SLACK_FETCH_CONVERSATION_HISTORY_channel',\n        display_name='Channel ID',\n        info='Channel ID to fetch history for.',\n        show=False),\n        IntInput(name='SLACK_FETCH_CONVERSATION_HISTORY_latest',\n        display_name='Latest',\n        info='End of time range of messages to include in results.',\n        show=False,\n        advanced=True),\n        IntInput(name='SLACK_FETCH_CONVERSATION_HISTORY_oldest',\n        display_name='Oldest',\n        info='Start of time range of messages to include in results.',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_FETCH_CONVERSATION_HISTORY_inclusive',\n        display_name='Inclusive',\n        info='Include messages with latest or oldest timestamp in results only when either timestamp is specified. ',\n        show=False,\n        advanced=True),\n        IntInput(name='SLACK_FETCH_CONVERSATION_HISTORY_limit',\n        display_name='Limit',\n        info=\"The maximum number of items to return. Fewer than the requested number of items may be returned,\n        even if the end of the users list hasn't been reached. \",\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_FETCH_CONVERSATION_HISTORY_cursor',\n        display_name='Cursor',\n        info=\"Paginate through collections of data by setting the `cursor` parameter to a `next_cursor` attribute returned by a previous request's `response_metadata`. Default value fetches the first 'page' of the collection. \",\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_as_user',\n        display_name='As User',\n        info='Pass true to post the message as the authed user,\n        instead of as a bot. Defaults to false',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_attachments',\n        display_name='Attachments',\n        info='A JSON-based array of structured attachments,\n        presented as a URL-encoded string. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_blocks',\n        display_name='Blocks',\n        info='A JSON-based array of structured blocks,\n        presented as a URL-encoded string. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_channel',\n        display_name='Channel',\n        info='Channel,\n        private group,\n        or DM channel to send message to. Can be an encoded ID,\n        or a name',\n        show=False,\n        required=True),\n        BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_link_names',\n        display_name='Link Names',\n        info='Find and link channel names and usernames.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_parse',\n        display_name='Parse',\n        info='Change how messages are treated. Defaults to `none`',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_post_at',\n        display_name='Post At',\n        info='Unix EPOCH timestamp of time in future to send the message.',\n        show=False),\n        BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_reply_broadcast',\n        display_name='Reply Broadcast',\n        info='Used in conjunction with `thread_ts` and indicates whether reply should be made visible to everyone in the channel or conversation. Defaults to `false`. ',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_text',\n        display_name='Text',\n        info='How this field works and whether it is required depends on other fields you use in your API call',\n        show=False),\n        IntInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_thread_ts',\n        display_name='Thread Ts',\n        info=\"Provide another message's `ts` value to make this message a reply. Avoid using a reply's `ts` value; use its parent instead. \",\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_unfurl_links',\n        display_name='Unfurl Links',\n        info='Pass true to enable unfurling of primarily text-based content.',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_unfurl_media',\n        display_name='Unfurl Media',\n        info='Pass false to disable unfurling of media content.',\n        show=False,\n        advanced=True),\n        BoolInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_exclude_archived',\n        display_name='Exclude Archived',\n        info='Set to `true` to exclude archived channels from the list',\n        show=False),\n        MessageTextInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_types',\n        display_name='Types',\n        info='Mix and match channel types by providing a comma-separated list of any combination of `public_channel`,\n        `private_channel`,\n        `mpim`,\n        `im` ',\n        show=False),\n        IntInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_limit',\n        display_name='Limit',\n        info=\"The maximum number of items to return. Fewer than the requested number of items may be returned,\n        even if the end of the list hasn't been reached. Must be an integer no larger than 1000. \",\n        show=False,\n        value=1),\n        MessageTextInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_cursor',\n        display_name='Cursor',\n        info=\"Paginate through collections of data by setting the `cursor` parameter to a `next_cursor` attribute returned by a previous request's `response_metadata`. Default value fetches the first 'page' of the collection\",\n        show=False,\n        advanced=True),\n        IntInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_count',\n        display_name='Count',\n        info=\"Pass the number of results you want per 'page'. Maximum of `100`.\",\n        show=False,\n        value=1,\n        advanced=True),\n        BoolInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_highlight',\n        display_name='Highlight',\n        info='Pass a value of `true` to enable query highlight markers',\n        show=False,\n        advanced=True),\n        IntInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_page',\n        display_name='Page',\n        info='Page',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_query',\n        display_name='Query',\n        info='Search query.',\n        show=False,\n        required=True),\n        MessageTextInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_sort',\n        display_name='Sort',\n        info='Return matches sorted by either `score` or `timestamp`.',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_sort_dir',\n        display_name='Sort Dir',\n        info='Change sort direction to ascending (`asc`) or descending (`desc`).',\n        show=False,\n        advanced=True),\n        MessageTextInput(name='SLACK_CREATE_A_REMINDER_text',\n        display_name='Text',\n        info='The content of the reminder',\n        show=False,\n        required=True),\n        MessageTextInput(name='SLACK_CREATE_A_REMINDER_time',\n        display_name='Time',\n        info=\"When this reminder should happen: the Unix timestamp (up to five years from now),\n        the number of seconds until the reminder (if within 24 hours),\n        or a natural language description (Ex. 'in 15 minutes,' or 'every Thursday') \",\n        show=False,\n        required=True),\n        MessageTextInput(name='SLACK_CREATE_A_REMINDER_user',\n        display_name='User',\n        info='The user who will receive the reminder. If no user is specified,\n        the reminder will go to user who created it. ',\n        show=False)\n    ]\n\n    def execute_action(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def set_default_tools(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ComposioSlackAPIComponent", "base_classes": ["ComposioBaseComponent"], "public_methods": ["def execute_action(self)", "def update_build_config(self, build_config, field_value, field_name)", "def set_default_tools(self)"], "imports": ["from typing import Any", "from composio import Action", "from langflow.base.composio.composio_base import ComposioBaseComponent", "from langflow.inputs import BoolInput, IntInput, MessageTextInput", "from langflow.logging import logger"], "inputs": "[*ComposioBaseComponent._base_inputs, IntInput(name='SLACK_LIST_ALL_SLACK_TEAM_USERS_WITH_PAGINATION_limit', display_name='Limit', info=\"The maximum number of items to return. Fewer than the requested number of items may be returned, even if the end of the users list hasn't been reached. Providing no `limit` value will result in Slack attempting to deliver you the entire result set. If the collection is too large you may experience `limit_required` or HTTP 500 errors. \", show=False, value=1), MessageTextInput(name='SLACK_LIST_ALL_SLACK_TEAM_USERS_WITH_PAGINATION_cursor', display_name='Cursor', info=\"Paginate through collections of data by setting the `cursor` parameter to a `next_cursor` attribute returned by a previous request's `response_metadata`. Default value fetches the first `page` of the collection\", show=False, advanced=True), BoolInput(name='SLACK_LIST_ALL_SLACK_TEAM_USERS_WITH_PAGINATION_include_locale', display_name='Include Locale', info='Set this to `true` to receive the locale for users. Defaults to `false`', show=False), BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_as_user', display_name='As User', info='Pass true to post the message as the authed user, instead of as a bot. Defaults to false', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_attachments', display_name='Attachments', info='A JSON-based array of structured attachments, presented as a URL-encoded string. ', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_blocks', display_name='Blocks', info='A JSON-based array of structured blocks, presented as a URL-encoded string. ', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_channel', display_name='Channel', info='Channel, private group, or IM channel to send message to. Can be an encoded ID, or a name ', show=False, required=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_icon_emoji', display_name='Icon Emoji', info='Emoji to use as the icon for this message. Overrides `icon_url`. Must be used in conjunction with `as_user` set to `false`, otherwise ignored', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_icon_url', display_name='Icon Url', info='URL to an image to use as the icon for this message. Must be used in conjunction with `as_user` set to false, otherwise ignored', show=False, advanced=True), BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_link_names', display_name='Link Names', info='Find and link channel names and usernames.', show=False, advanced=True), BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_mrkdwn', display_name='Mrkdwn', info='Disable Slack markup parsing by setting to `false`. Enabled by default.', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_parse', display_name='Parse', info='Change how messages are treated. Defaults to `none` ', show=False, advanced=True), BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_reply_broadcast', display_name='Reply Broadcast', info='Used in conjunction with `thread_ts` and indicates whether reply should be made visible to everyone in the channel or conversation. Defaults to `false`. ', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_text', display_name='Text', info='How this field works and whether it is required depends on other fields you use in your API call', show=False), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_thread_ts', display_name='Thread Ts', info=\"Provide another message's `ts` value to make this message a reply. Avoid using a reply's `ts` value; use its parent instead. \", show=False), BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_unfurl_links', display_name='Unfurl Links', info='Pass true to enable unfurling of primarily text-based content.', show=False, advanced=True), BoolInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_unfurl_media', display_name='Unfurl Media', info='Pass false to disable unfurling of media content.', show=False, advanced=True), MessageTextInput(name='SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL_username', display_name='Username', info=\"Set your bot's user name. Must be used in conjunction with `as_user` set to false, otherwise ignored\", show=False, advanced=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_as_user', display_name='As User', info='Pass true to update the message as the authed user', show=False, advanced=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_attachments', display_name='Attachments', info=\"A JSON-based array of structured attachments, presented as a URL-encoded string. This field is required when not presenting `text`. If you don't include this field, the message's previous `attachments` will be retained. To remove previous `attachments`, include an empty array for this field. \", show=False, advanced=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_blocks', display_name='Blocks', info=\"A JSON-based array of structured blocks, presented as a URL-encoded string. If you don't include this field, the message's previous `blocks` will be retained. To remove previous `blocks`, include an empty array for this field. \", show=False, advanced=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_channel', display_name='Channel ID', info='Channel ID containing the message to be updated.', show=False, required=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_link_names', display_name='Link Names', info='Find and link channel names and usernames. Defaults to `none`. If you do not specify a value for this field, the original value set for the message will be overwritten with the default, `none`. ', show=False, advanced=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_parse', display_name='Parse', info='Change how messages are treated. Defaults to `client`, unlike `chat.postMessage`. Accepts either `none` or `full`. If you do not specify a value for this field, the original value set for the message will be overwritten with the default, `client`. ', show=False, advanced=True), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_text', display_name='Text', info=\"New text for the message, using the default formatting rules. It's not required when presenting `blocks` or `attachments`. \", show=False), MessageTextInput(name='SLACK_UPDATES_A_SLACK_MESSAGE_ts', display_name='Ts', info='Timestamp of the message to be updated.', show=False, required=True), MessageTextInput(name='SLACK_FETCH_CONVERSATION_HISTORY_channel', display_name='Channel ID', info='Channel ID to fetch history for.', show=False), IntInput(name='SLACK_FETCH_CONVERSATION_HISTORY_latest', display_name='Latest', info='End of time range of messages to include in results.', show=False, advanced=True), IntInput(name='SLACK_FETCH_CONVERSATION_HISTORY_oldest', display_name='Oldest', info='Start of time range of messages to include in results.', show=False, advanced=True), BoolInput(name='SLACK_FETCH_CONVERSATION_HISTORY_inclusive', display_name='Inclusive', info='Include messages with latest or oldest timestamp in results only when either timestamp is specified. ', show=False, advanced=True), IntInput(name='SLACK_FETCH_CONVERSATION_HISTORY_limit', display_name='Limit', info=\"The maximum number of items to return. Fewer than the requested number of items may be returned, even if the end of the users list hasn't been reached. \", show=False, advanced=True), MessageTextInput(name='SLACK_FETCH_CONVERSATION_HISTORY_cursor', display_name='Cursor', info=\"Paginate through collections of data by setting the `cursor` parameter to a `next_cursor` attribute returned by a previous request's `response_metadata`. Default value fetches the first 'page' of the collection. \", show=False, advanced=True), BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_as_user', display_name='As User', info='Pass true to post the message as the authed user, instead of as a bot. Defaults to false', show=False, advanced=True), MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_attachments', display_name='Attachments', info='A JSON-based array of structured attachments, presented as a URL-encoded string. ', show=False, advanced=True), MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_blocks', display_name='Blocks', info='A JSON-based array of structured blocks, presented as a URL-encoded string. ', show=False, advanced=True), MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_channel', display_name='Channel', info='Channel, private group, or DM channel to send message to. Can be an encoded ID, or a name', show=False, required=True), BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_link_names', display_name='Link Names', info='Find and link channel names and usernames.', show=False, advanced=True), MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_parse', display_name='Parse', info='Change how messages are treated. Defaults to `none`', show=False, advanced=True), MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_post_at', display_name='Post At', info='Unix EPOCH timestamp of time in future to send the message.', show=False), BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_reply_broadcast', display_name='Reply Broadcast', info='Used in conjunction with `thread_ts` and indicates whether reply should be made visible to everyone in the channel or conversation. Defaults to `false`. ', show=False, advanced=True), MessageTextInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_text', display_name='Text', info='How this field works and whether it is required depends on other fields you use in your API call', show=False), IntInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_thread_ts', display_name='Thread Ts', info=\"Provide another message's `ts` value to make this message a reply. Avoid using a reply's `ts` value; use its parent instead. \", show=False, advanced=True), BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_unfurl_links', display_name='Unfurl Links', info='Pass true to enable unfurling of primarily text-based content.', show=False, advanced=True), BoolInput(name='SLACK_SCHEDULES_A_MESSAGE_TO_A_CHANNEL_AT_A_SPECIFIED_TIME_unfurl_media', display_name='Unfurl Media', info='Pass false to disable unfurling of media content.', show=False, advanced=True), BoolInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_exclude_archived', display_name='Exclude Archived', info='Set to `true` to exclude archived channels from the list', show=False), MessageTextInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_types', display_name='Types', info='Mix and match channel types by providing a comma-separated list of any combination of `public_channel`, `private_channel`, `mpim`, `im` ', show=False), IntInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_limit', display_name='Limit', info=\"The maximum number of items to return. Fewer than the requested number of items may be returned, even if the end of the list hasn't been reached. Must be an integer no larger than 1000. \", show=False, value=1), MessageTextInput(name='SLACK_LIST_ALL_SLACK_TEAM_CHANNELS_WITH_VARIOUS_FILTERS_cursor', display_name='Cursor', info=\"Paginate through collections of data by setting the `cursor` parameter to a `next_cursor` attribute returned by a previous request's `response_metadata`. Default value fetches the first 'page' of the collection\", show=False, advanced=True), IntInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_count', display_name='Count', info=\"Pass the number of results you want per 'page'. Maximum of `100`.\", show=False, value=1, advanced=True), BoolInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_highlight', display_name='Highlight', info='Pass a value of `true` to enable query highlight markers', show=False, advanced=True), IntInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_page', display_name='Page', info='Page', show=False, advanced=True), MessageTextInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_query', display_name='Query', info='Search query.', show=False, required=True), MessageTextInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_sort', display_name='Sort', info='Return matches sorted by either `score` or `timestamp`.', show=False, advanced=True), MessageTextInput(name='SLACK_SEARCH_FOR_MESSAGES_WITH_QUERY_sort_dir', display_name='Sort Dir', info='Change sort direction to ascending (`asc`) or descending (`desc`).', show=False, advanced=True), MessageTextInput(name='SLACK_CREATE_A_REMINDER_text', display_name='Text', info='The content of the reminder', show=False, required=True), MessageTextInput(name='SLACK_CREATE_A_REMINDER_time', display_name='Time', info=\"When this reminder should happen: the Unix timestamp (up to five years from now), the number of seconds until the reminder (if within 24 hours), or a natural language description (Ex. 'in 15 minutes,' or 'every Thursday') \", show=False, required=True), MessageTextInput(name='SLACK_CREATE_A_REMINDER_user', display_name='User', info='The user who will receive the reminder. If no user is specified, the reminder will go to user who created it. ', show=False)]", "outputs": "", "display_name": "Slack", "name": "", "description": "Slack API", "icon": "Slack"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/store_message.py", "section": "class::MessageStoreComponent", "content": "from langflow.custom import Component\nfrom langflow.inputs import HandleInput\nfrom langflow.inputs.inputs import MessageTextInput\nfrom langflow.memory import aget_messages, astore_message\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI\n\nclass MessageStoreComponent(Component):\n    display_name: str = \"Message Store\"\n    description: str = \"Stores a chat message or text into Langflow tables or an external memory.\"\n    icon = \"message-square-text\"\n    name = \"StoreMessage\"\n\n    inputs = [\n        MessageTextInput(name='message',\n        display_name='Message',\n        info='The chat message to be stored.',\n        required=True,\n        tool_mode=True),\n        HandleInput(name='memory',\n        display_name='External Memory',\n        input_types=['Memory'],\n        info='The external memory to store the message. If empty,\n        it will use the Langflow tables.'),\n        MessageTextInput(name='sender',\n        display_name='Sender',\n        info='The sender of the message. Might be Machine or User. If empty,\n        the current sender parameter will be used.',\n        advanced=True),\n        MessageTextInput(name='sender_name',\n        display_name='Sender Name',\n        info='The name of the sender. Might be AI or User. If empty,\n        the current sender parameter will be used.',\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='The session ID of the chat. If empty,\n        the current session ID parameter will be used.',\n        value='',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Stored Messages',\n        name='stored_messages',\n        method='store_message',\n        hidden=True)\n    ]\n", "metadata": {"parser": "python_component", "class_name": "MessageStoreComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["from langflow.custom import Component", "from langflow.inputs import HandleInput", "from langflow.inputs.inputs import MessageTextInput", "from langflow.memory import aget_messages, astore_message", "from langflow.schema.message import Message", "from langflow.template import Output", "from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI"], "inputs": "[MessageTextInput(name='message', display_name='Message', info='The chat message to be stored.', required=True, tool_mode=True), HandleInput(name='memory', display_name='External Memory', input_types=['Memory'], info='The external memory to store the message. If empty, it will use the Langflow tables.'), MessageTextInput(name='sender', display_name='Sender', info='The sender of the message. Might be Machine or User. If empty, the current sender parameter will be used.', advanced=True), MessageTextInput(name='sender_name', display_name='Sender Name', info='The name of the sender. Might be AI or User. If empty, the current sender parameter will be used.', advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='The session ID of the chat. If empty, the current session ID parameter will be used.', value='', advanced=True)]", "outputs": "[Output(display_name='Stored Messages', name='stored_messages', method='store_message', hidden=True)]", "display_name": "Message Store", "name": "StoreMessage", "description": "Stores a chat message or text into Langflow tables or an external memory.", "icon": "message-square-text"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/memories/redis.py", "section": "class::RedisIndexChatMemory", "content": "from urllib import parse\nfrom langchain_community.chat_message_histories.redis import RedisChatMessageHistory\nfrom langflow.base.memory.model import LCChatMemoryComponent\nfrom langflow.field_typing.constants import Memory\nfrom langflow.inputs import IntInput, MessageTextInput, SecretStrInput, StrInput\n\nclass RedisIndexChatMemory(LCChatMemoryComponent):\n    display_name: str = \"Redis Chat Memory\"\n    description: str = \"Retrieves and store chat messages from Redis.\"\n    icon = \"Redis\"\n    name = \"RedisChatMemory\"\n\n    inputs = [\n        StrInput(name='host',\n        display_name='hostname',\n        required=True,\n        value='localhost',\n        info='IP address or hostname.'),\n        IntInput(name='port',\n        display_name='port',\n        required=True,\n        value=6379,\n        info='Redis Port Number.'),\n        StrInput(name='database',\n        display_name='database',\n        required=True,\n        value='0',\n        info='Redis database.'),\n        MessageTextInput(name='username',\n        display_name='Username',\n        value='',\n        info='The Redis user name.',\n        advanced=True),\n        SecretStrInput(name='password',\n        display_name='Password',\n        value='',\n        info='The password for username.',\n        advanced=True),\n        StrInput(name='key_prefix',\n        display_name='Key prefix',\n        info='Key prefix.',\n        advanced=True),\n        MessageTextInput(name='session_id',\n        display_name='Session ID',\n        info='Session ID for the message.',\n        advanced=True)\n    ]\n\n    def build_message_history(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RedisIndexChatMemory", "base_classes": ["LCChatMemoryComponent"], "public_methods": ["def build_message_history(self)"], "imports": ["from urllib import parse", "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory", "from langflow.base.memory.model import LCChatMemoryComponent", "from langflow.field_typing.constants import Memory", "from langflow.inputs import IntInput, MessageTextInput, SecretStrInput, StrInput"], "inputs": "[StrInput(name='host', display_name='hostname', required=True, value='localhost', info='IP address or hostname.'), IntInput(name='port', display_name='port', required=True, value=6379, info='Redis Port Number.'), StrInput(name='database', display_name='database', required=True, value='0', info='Redis database.'), MessageTextInput(name='username', display_name='Username', value='', info='The Redis user name.', advanced=True), SecretStrInput(name='password', display_name='Password', value='', info='The password for username.', advanced=True), StrInput(name='key_prefix', display_name='Key prefix', info='Key prefix.', advanced=True), MessageTextInput(name='session_id', display_name='Session ID', info='Session ID for the message.', advanced=True)]", "outputs": "", "display_name": "Redis Chat Memory", "name": "RedisChatMemory", "description": "Retrieves and store chat messages from Redis.", "icon": "Redis"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/crewai/hierarchical_task.py", "section": "class::HierarchicalTaskComponent", "content": "from langflow.base.agents.crewai.tasks import HierarchicalTask\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, MultilineInput, Output\n\nclass HierarchicalTaskComponent(Component):\n    display_name: str = \"Hierarchical Task\"\n    description: str = \"Each task must have a description, an expected output and an agent responsible for execution.\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        MultilineInput(name='task_description',\n        display_name='Description',\n        info=\"Descriptive text detailing task's purpose and execution.\"),\n        MultilineInput(name='expected_output',\n        display_name='Expected Output',\n        info='Clear definition of expected task outcome.'),\n        HandleInput(name='tools',\n        display_name='Tools',\n        input_types=['Tool'],\n        is_list=True,\n        info='List of tools/resources limited for task execution. Uses the Agent tools by default.',\n        required=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Task',\n        name='task_output',\n        method='build_task')\n    ]\n\n    def build_task(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HierarchicalTaskComponent", "base_classes": ["Component"], "public_methods": ["def build_task(self)"], "imports": ["from langflow.base.agents.crewai.tasks import HierarchicalTask", "from langflow.custom import Component", "from langflow.io import HandleInput, MultilineInput, Output"], "inputs": "[MultilineInput(name='task_description', display_name='Description', info=\"Descriptive text detailing task's purpose and execution.\"), MultilineInput(name='expected_output', display_name='Expected Output', info='Clear definition of expected task outcome.'), HandleInput(name='tools', display_name='Tools', input_types=['Tool'], is_list=True, info='List of tools/resources limited for task execution. Uses the Agent tools by default.', required=False, advanced=True)]", "outputs": "[Output(display_name='Task', name='task_output', method='build_task')]", "display_name": "Hierarchical Task", "name": "", "description": "Each task must have a description, an expected output and an agent responsible for execution.", "icon": "CrewAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/google_serper_api.py", "section": "class::QuerySchema", "content": "from typing import Any\nfrom langchain.tools import StructuredTool\nfrom langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DictInput, DropdownInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass QuerySchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "QuerySchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from typing import Any", "from langchain.tools import StructuredTool", "from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DictInput, DropdownInput, IntInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/google_serper_api.py", "section": "class::GoogleSerperAPIComponent", "content": "from typing import Any\nfrom langchain.tools import StructuredTool\nfrom langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DictInput, DropdownInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass GoogleSerperAPIComponent(LCToolComponent):\n    display_name: str = \"Google Serper API [DEPRECATED]\"\n    description: str = \"Call the Serper.dev Google Search API.\"\n    icon = \"Google\"\n    name = \"GoogleSerperAPI\"\n\n    inputs = [\n        SecretStrInput(name='serper_api_key',\n        display_name='Serper API Key',\n        required=True),\n        MultilineInput(name='query',\n        display_name='Query'),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True),\n        DropdownInput(name='query_type',\n        display_name='Query Type',\n        required=False,\n        options=['news',\n        'search'],\n        value='search'),\n        DictInput(name='query_params',\n        display_name='Query Params',\n        required=False,\n        value={'gl': 'us',\n        'hl': 'en'},\n        list=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleSerperAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["from typing import Any", "from langchain.tools import StructuredTool", "from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DictInput, DropdownInput, IntInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='serper_api_key', display_name='Serper API Key', required=True), MultilineInput(name='query', display_name='Query'), IntInput(name='k', display_name='Number of results', value=4, required=True), DropdownInput(name='query_type', display_name='Query Type', required=False, options=['news', 'search'], value='search'), DictInput(name='query_params', display_name='Query Params', required=False, value={'gl': 'us', 'hl': 'en'}, list=True)]", "outputs": "", "display_name": "Google Serper API [DEPRECATED]", "name": "GoogleSerperAPI", "description": "Call the Serper.dev Google Search API.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/similarity.py", "section": "class::EmbeddingSimilarityComponent", "content": "import numpy as np\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, Output\nfrom langflow.schema import Data\n\nclass EmbeddingSimilarityComponent(Component):\n    display_name: str = \"Embedding Similarity\"\n    description: str = \"Compute selected form of similarity between two embedding vectors.\"\n    icon = \"equal\"\n\n    inputs = [\n        DataInput(name='embedding_vectors',\n        display_name='Embedding Vectors',\n        info='A list containing exactly two data objects with embedding vectors to compare.',\n        is_list=True,\n        required=True),\n        DropdownInput(name='similarity_metric',\n        display_name='Similarity Metric',\n        info='Select the similarity metric to use.',\n        options=['Cosine Similarity',\n        'Euclidean Distance',\n        'Manhattan Distance'],\n        value='Cosine Similarity')\n    ]\n\n    outputs = [\n        Output(display_name='Similarity Data',\n        name='similarity_data',\n        method='compute_similarity')\n    ]\n\n    def compute_similarity(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "EmbeddingSimilarityComponent", "base_classes": ["Component"], "public_methods": ["def compute_similarity(self)"], "imports": ["import numpy as np", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='embedding_vectors', display_name='Embedding Vectors', info='A list containing exactly two data objects with embedding vectors to compare.', is_list=True, required=True), DropdownInput(name='similarity_metric', display_name='Similarity Metric', info='Select the similarity metric to use.', options=['Cosine Similarity', 'Euclidean Distance', 'Manhattan Distance'], value='Cosine Similarity')]", "outputs": "[Output(display_name='Similarity Data', name='similarity_data', method='compute_similarity')]", "display_name": "Embedding Similarity", "name": "", "description": "Compute selected form of similarity between two embedding vectors.", "icon": "equal"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/convert_astra_results.py", "section": "class::ConvertAstraToTwelveLabs", "content": "from typing import Any\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass ConvertAstraToTwelveLabs(Component):\n    \"\"\"\n    Convert AstraDB search results to TwelveLabs Pegasus inputs.\n    \"\"\"\n\n    display_name: str = \"Convert AstraDB to Pegasus Input\"\n    description: str = \"Converts AstraDB search results to inputs compatible with TwelveLabs Pegasus.\"\n    icon = \"TwelveLabs\"\n    name = \"ConvertAstraToTwelveLabs\"\n\n    inputs = [\n        HandleInput(name='astra_results',\n        display_name='AstraDB Results',\n        input_types=['Data'],\n        info='Search results from AstraDB component',\n        required=True,\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(name='index_id',\n        display_name='Index ID',\n        type_=Message,\n        method='get_index_id'),\n        Output(name='video_id',\n        display_name='Video ID',\n        type_=Message,\n        method='get_video_id')\n    ]\n\n    def build(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_video_id(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_index_id(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ConvertAstraToTwelveLabs", "base_classes": ["Component"], "public_methods": ["def build(self)", "def get_video_id(self)", "def get_index_id(self)"], "imports": ["from typing import Any", "from langflow.custom import Component", "from langflow.io import HandleInput, Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[HandleInput(name='astra_results', display_name='AstraDB Results', input_types=['Data'], info='Search results from AstraDB component', required=True, is_list=True)]", "outputs": "[Output(name='index_id', display_name='Index ID', type_=Message, method='get_index_id'), Output(name='video_id', display_name='Video ID', type_=Message, method='get_video_id')]", "display_name": "Convert AstraDB to Pegasus Input", "name": "ConvertAstraToTwelveLabs", "description": "Converts AstraDB search results to inputs compatible with TwelveLabs Pegasus.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/retrieval_qa.py", "section": "class::RetrievalQAComponent", "content": "from langchain.chains import RetrievalQA\nfrom langflow.base.chains.model import LCChainComponent\nfrom langflow.field_typing import Message\nfrom langflow.inputs import BoolInput, DropdownInput, HandleInput, MultilineInput\n\nclass RetrievalQAComponent(LCChainComponent):\n    display_name: str = \"Retrieval QA\"\n    description: str = \"Chain for question-answering querying sources from a retriever.\"\n    icon = \"LangChain\"\n    name = \"RetrievalQA\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input',\n        info='The input value to pass to the chain.',\n        required=True),\n        DropdownInput(name='chain_type',\n        display_name='Chain Type',\n        info='Chain type to use.',\n        options=['Stuff',\n        'Map Reduce',\n        'Refine',\n        'Map Rerank'],\n        value='Stuff',\n        advanced=True),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        HandleInput(name='retriever',\n        display_name='Retriever',\n        input_types=['Retriever'],\n        required=True),\n        HandleInput(name='memory',\n        display_name='Memory',\n        input_types=['BaseChatMemory']),\n        BoolInput(name='return_source_documents',\n        display_name='Return Source Documents',\n        value=False)\n    ]\n\n    def invoke_chain(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RetrievalQAComponent", "base_classes": ["LCChainComponent"], "public_methods": ["def invoke_chain(self)"], "imports": ["from langchain.chains import RetrievalQA", "from langflow.base.chains.model import LCChainComponent", "from langflow.field_typing import Message", "from langflow.inputs import BoolInput, DropdownInput, HandleInput, MultilineInput"], "inputs": "[MultilineInput(name='input_value', display_name='Input', info='The input value to pass to the chain.', required=True), DropdownInput(name='chain_type', display_name='Chain Type', info='Chain type to use.', options=['Stuff', 'Map Reduce', 'Refine', 'Map Rerank'], value='Stuff', advanced=True), HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), HandleInput(name='retriever', display_name='Retriever', input_types=['Retriever'], required=True), HandleInput(name='memory', display_name='Memory', input_types=['BaseChatMemory']), BoolInput(name='return_source_documents', display_name='Return Source Documents', value=False)]", "outputs": "", "display_name": "Retrieval QA", "name": "RetrievalQA", "description": "Chain for question-answering querying sources from a retriever.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/alter_metadata.py", "section": "class::AlterMetadataComponent", "content": "from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import HandleInput, NestedDictInput, Output, StrInput\nfrom langflow.schema import Data, DataFrame\n\nclass AlterMetadataComponent(Component):\n    display_name: str = \"Alter Metadata\"\n    description: str = \"Adds/Removes Metadata Dictionary on inputs\"\n    icon = \"merge\"\n    name = \"AlterMetadata\"\n\n    inputs = [\n        HandleInput(name='input_value',\n        display_name='Input',\n        info='Object(s) to which Metadata should be added',\n        required=False,\n        input_types=['Message',\n        'Data'],\n        is_list=True),\n        StrInput(name='text_in',\n        display_name='User Text',\n        info=\"Text input; value will be in 'text' attribute of Data object. Empty text entries are ignored.\",\n        required=False),\n        NestedDictInput(name='metadata',\n        display_name='Metadata',\n        info='Metadata to add to each object',\n        input_types=['Data'],\n        required=True),\n        MessageTextInput(name='remove_fields',\n        display_name='Fields to Remove',\n        info='Metadata Fields to Remove',\n        required=False,\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(name='data',\n        display_name='Data',\n        info='List of Input objects each with added Metadata',\n        method='process_output'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        info='Data objects as a DataFrame,\n        with metadata as columns',\n        method='as_dataframe')\n    ]\n\n    def process_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AlterMetadataComponent", "base_classes": ["Component"], "public_methods": ["def process_output(self)", "def as_dataframe(self)"], "imports": ["from langflow.custom import Component", "from langflow.inputs import MessageTextInput", "from langflow.io import HandleInput, NestedDictInput, Output, StrInput", "from langflow.schema import Data, DataFrame"], "inputs": "[HandleInput(name='input_value', display_name='Input', info='Object(s) to which Metadata should be added', required=False, input_types=['Message', 'Data'], is_list=True), StrInput(name='text_in', display_name='User Text', info=\"Text input; value will be in 'text' attribute of Data object. Empty text entries are ignored.\", required=False), NestedDictInput(name='metadata', display_name='Metadata', info='Metadata to add to each object', input_types=['Data'], required=True), MessageTextInput(name='remove_fields', display_name='Fields to Remove', info='Metadata Fields to Remove', required=False, is_list=True)]", "outputs": "[Output(name='data', display_name='Data', info='List of Input objects each with added Metadata', method='process_output'), Output(display_name='DataFrame', name='dataframe', info='Data objects as a DataFrame, with metadata as columns', method='as_dataframe')]", "display_name": "Alter Metadata", "name": "AlterMetadata", "description": "Adds/Removes Metadata Dictionary on inputs", "icon": "merge"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/conditional_router.py", "section": "class::ConditionalRouterComponent", "content": "import re\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\nclass ConditionalRouterComponent(Component):\n    display_name: str = \"If-Else\"\n    description: str = \"Routes an input message to a corresponding output based on text comparison.\"\n    icon = \"split\"\n    name = \"ConditionalRouter\"\n\n    inputs = [\n        MessageTextInput(name='input_text',\n        display_name='Text Input',\n        info='The primary text input for the operation.',\n        required=True),\n        MessageTextInput(name='match_text',\n        display_name='Match Text',\n        info='The text input to compare against.',\n        required=True),\n        DropdownInput(name='operator',\n        display_name='Operator',\n        options=['equals',\n        'not equals',\n        'contains',\n        'starts with',\n        'ends with',\n        'regex'],\n        info='The operator to apply for comparing the texts.',\n        value='equals',\n        real_time_refresh=True),\n        BoolInput(name='case_sensitive',\n        display_name='Case Sensitive',\n        info='If true,\n        the comparison will be case sensitive.',\n        value=False),\n        MessageInput(name='message',\n        display_name='Message',\n        info='The message to pass through either route.'),\n        IntInput(name='max_iterations',\n        display_name='Max Iterations',\n        info='The maximum number of iterations for the conditional router.',\n        value=10,\n        advanced=True),\n        DropdownInput(name='default_route',\n        display_name='Default Route',\n        options=['true_result',\n        'false_result'],\n        info='The default route to take when max iterations are reached.',\n        value='false_result',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='True',\n        name='true_result',\n        method='true_response'),\n        Output(display_name='False',\n        name='false_result',\n        method='false_response')\n    ]\n\n    def evaluate_condition(self, input_text, match_text, operator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def iterate_and_stop_once(self, route_to_stop):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def true_response(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def false_response(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ConditionalRouterComponent", "base_classes": ["Component"], "public_methods": ["def evaluate_condition(self, input_text, match_text, operator)", "def iterate_and_stop_once(self, route_to_stop)", "def true_response(self)", "def false_response(self)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["import re", "from langflow.custom import Component", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageInput, MessageTextInput, Output", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='input_text', display_name='Text Input', info='The primary text input for the operation.', required=True), MessageTextInput(name='match_text', display_name='Match Text', info='The text input to compare against.', required=True), DropdownInput(name='operator', display_name='Operator', options=['equals', 'not equals', 'contains', 'starts with', 'ends with', 'regex'], info='The operator to apply for comparing the texts.', value='equals', real_time_refresh=True), BoolInput(name='case_sensitive', display_name='Case Sensitive', info='If true, the comparison will be case sensitive.', value=False), MessageInput(name='message', display_name='Message', info='The message to pass through either route.'), IntInput(name='max_iterations', display_name='Max Iterations', info='The maximum number of iterations for the conditional router.', value=10, advanced=True), DropdownInput(name='default_route', display_name='Default Route', options=['true_result', 'false_result'], info='The default route to take when max iterations are reached.', value='false_result', advanced=True)]", "outputs": "[Output(display_name='True', name='true_result', method='true_response'), Output(display_name='False', name='false_result', method='false_response')]", "display_name": "If-Else", "name": "ConditionalRouter", "description": "Routes an input message to a corresponding output based on text comparison.", "icon": "split"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/google_drive.py", "section": "class::GoogleDriveComponent", "content": "import json\nfrom json.decoder import JSONDecodeError\nfrom google.auth.exceptions import RefreshError\nfrom google.oauth2.credentials import Credentials\nfrom langchain_google_community import GoogleDriveLoader\nfrom langflow.custom import Component\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass GoogleDriveComponent(Component):\n    display_name: str = \"Google Drive Loader\"\n    description: str = \"Loads documents from Google Drive using provided credentials.\"\n    icon = \"Google\"\n\n    inputs = [\n        SecretStrInput(name='json_string',\n        display_name='JSON String of the Service Account Token',\n        info='JSON string containing OAuth 2.0 access token information for service account access',\n        required=True),\n        MessageTextInput(name='document_id',\n        display_name='Document ID',\n        info='Single Google Drive document ID',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Loaded Documents',\n        name='docs',\n        method='load_documents')\n    ]\n\n    def load_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleDriveComponent", "base_classes": ["Component"], "public_methods": ["def load_documents(self)"], "imports": ["import json", "from json.decoder import JSONDecodeError", "from google.auth.exceptions import RefreshError", "from google.oauth2.credentials import Credentials", "from langchain_google_community import GoogleDriveLoader", "from langflow.custom import Component", "from langflow.helpers.data import docs_to_data", "from langflow.inputs import MessageTextInput", "from langflow.io import SecretStrInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "[SecretStrInput(name='json_string', display_name='JSON String of the Service Account Token', info='JSON string containing OAuth 2.0 access token information for service account access', required=True), MessageTextInput(name='document_id', display_name='Document ID', info='Single Google Drive document ID', required=True)]", "outputs": "[Output(display_name='Loaded Documents', name='docs', method='load_documents')]", "display_name": "Google Drive Loader", "name": "", "description": "Loads documents from Google Drive using provided credentials.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/google_drive.py", "section": "class::CustomGoogleDriveLoader", "content": "import json\nfrom json.decoder import JSONDecodeError\nfrom google.auth.exceptions import RefreshError\nfrom google.oauth2.credentials import Credentials\nfrom langchain_google_community import GoogleDriveLoader\nfrom langflow.custom import Component\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass CustomGoogleDriveLoader(GoogleDriveLoader):\n", "metadata": {"parser": "python_component", "class_name": "CustomGoogleDriveLoader", "base_classes": ["GoogleDriveLoader"], "public_methods": [], "imports": ["import json", "from json.decoder import JSONDecodeError", "from google.auth.exceptions import RefreshError", "from google.oauth2.credentials import Credentials", "from langchain_google_community import GoogleDriveLoader", "from langflow.custom import Component", "from langflow.helpers.data import docs_to_data", "from langflow.inputs import MessageTextInput", "from langflow.io import SecretStrInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/google/google_drive.py", "section": "class::Config", "content": "import json\nfrom json.decoder import JSONDecodeError\nfrom google.auth.exceptions import RefreshError\nfrom google.oauth2.credentials import Credentials\nfrom langchain_google_community import GoogleDriveLoader\nfrom langflow.custom import Component\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\nclass Config:\n", "metadata": {"parser": "python_component", "class_name": "Config", "base_classes": [], "public_methods": [], "imports": ["import json", "from json.decoder import JSONDecodeError", "from google.auth.exceptions import RefreshError", "from google.oauth2.credentials import Credentials", "from langchain_google_community import GoogleDriveLoader", "from langflow.custom import Component", "from langflow.helpers.data import docs_to_data", "from langflow.inputs import MessageTextInput", "from langflow.io import SecretStrInput", "from langflow.schema import Data", "from langflow.template import Output"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/mistral.py", "section": "class::MistralAIModelComponent", "content": "from langchain_mistralai import ChatMistralAI\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\nclass MistralAIModelComponent(LCModelComponent):\n    display_name: str = \"MistralAI\"\n    description: str = \"Generates text using MistralAI LLMs.\"\n    icon = \"MistralAI\"\n    name = \"MistralModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=['open-mixtral-8x7b',\n        'open-mixtral-8x22b',\n        'mistral-small-latest',\n        'mistral-medium-latest',\n        'mistral-large-latest',\n        'codestral-latest'],\n        value='codestral-latest'),\n        StrInput(name='mistral_api_base',\n        display_name='Mistral API Base',\n        advanced=True,\n        info='The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat,\n        LocalAI and Prem.'),\n        SecretStrInput(name='api_key',\n        display_name='Mistral API Key',\n        info='The Mistral API Key to use for the Mistral model.',\n        advanced=False,\n        required=True,\n        value='MISTRAL_API_KEY'),\n        FloatInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        advanced=True),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        advanced=True,\n        value=5),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        advanced=True,\n        value=60),\n        IntInput(name='max_concurrent_requests',\n        display_name='Max Concurrent Requests',\n        advanced=True,\n        value=3),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        advanced=True,\n        value=1),\n        IntInput(name='random_seed',\n        display_name='Random Seed',\n        value=1,\n        advanced=True),\n        BoolInput(name='safe_mode',\n        display_name='Safe Mode',\n        advanced=True,\n        value=False)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MistralAIModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_mistralai import ChatMistralAI", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=['open-mixtral-8x7b', 'open-mixtral-8x22b', 'mistral-small-latest', 'mistral-medium-latest', 'mistral-large-latest', 'codestral-latest'], value='codestral-latest'), StrInput(name='mistral_api_base', display_name='Mistral API Base', advanced=True, info='The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.'), SecretStrInput(name='api_key', display_name='Mistral API Key', info='The Mistral API Key to use for the Mistral model.', advanced=False, required=True, value='MISTRAL_API_KEY'), FloatInput(name='temperature', display_name='Temperature', value=0.1, advanced=True), IntInput(name='max_retries', display_name='Max Retries', advanced=True, value=5), IntInput(name='timeout', display_name='Timeout', advanced=True, value=60), IntInput(name='max_concurrent_requests', display_name='Max Concurrent Requests', advanced=True, value=3), FloatInput(name='top_p', display_name='Top P', advanced=True, value=1), IntInput(name='random_seed', display_name='Random Seed', value=1, advanced=True), BoolInput(name='safe_mode', display_name='Safe Mode', advanced=True, value=False)]", "outputs": "", "display_name": "MistralAI", "name": "MistralModel", "description": "Generates text using MistralAI LLMs.", "icon": "MistralAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/search.py", "section": "class::YouTubeSearchComponent", "content": "from contextlib import contextmanager\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import DataFrame\nfrom langflow.template import Output\n\nclass YouTubeSearchComponent(Component):\n    \"\"\"\n    A component that searches YouTube videos.\n    \"\"\"\n\n    display_name: str = \"YouTube Search\"\n    description: str = \"Searches YouTube videos based on query.\"\n    icon = \"YouTube\"\n\n    inputs = [\n        MessageTextInput(name='query',\n        display_name='Search Query',\n        info='The search query to look for on YouTube.',\n        tool_mode=True,\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='YouTube API Key',\n        info='Your YouTube Data API key.',\n        required=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=10,\n        info='The maximum number of results to return.'),\n        DropdownInput(name='order',\n        display_name='Sort Order',\n        options=['relevance',\n        'date',\n        'rating',\n        'title',\n        'viewCount'],\n        value='relevance',\n        info='Sort order for the search results.'),\n        BoolInput(name='include_metadata',\n        display_name='Include Metadata',\n        value=True,\n        info='Include video metadata like description and statistics.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='results',\n        display_name='Search Results',\n        method='search_videos')\n    ]\n\n    def youtube_client(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_videos(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubeSearchComponent", "base_classes": ["Component"], "public_methods": ["def youtube_client(self)", "def search_videos(self)"], "imports": ["from contextlib import contextmanager", "import pandas as pd", "from googleapiclient.discovery import build", "from googleapiclient.errors import HttpError", "from langflow.custom import Component", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import DataFrame", "from langflow.template import Output"], "inputs": "[MessageTextInput(name='query', display_name='Search Query', info='The search query to look for on YouTube.', tool_mode=True, required=True), SecretStrInput(name='api_key', display_name='YouTube API Key', info='Your YouTube Data API key.', required=True), IntInput(name='max_results', display_name='Max Results', value=10, info='The maximum number of results to return.'), DropdownInput(name='order', display_name='Sort Order', options=['relevance', 'date', 'rating', 'title', 'viewCount'], value='relevance', info='Sort order for the search results.'), BoolInput(name='include_metadata', display_name='Include Metadata', value=True, info='Include video metadata like description and statistics.', advanced=True)]", "outputs": "[Output(name='results', display_name='Search Results', method='search_videos')]", "display_name": "YouTube Search", "name": "", "description": "Searches YouTube videos based on query.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/create_page.py", "section": "class::NotionPageCreator", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionPageCreator(LCToolComponent):\n    display_name: str = \"Create Page \"\n    description: str = \"A component for creating Notion pages.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        StrInput(name='database_id',\n        display_name='Database ID',\n        info='The ID of the Notion database.'),\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True),\n        MultilineInput(name='properties_json',\n        display_name='Properties (JSON)',\n        info='The properties of the new page as a JSON string.')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionPageCreator", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='database_id', display_name='Database ID', info='The ID of the Notion database.'), SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True), MultilineInput(name='properties_json', display_name='Properties (JSON)', info='The properties of the new page as a JSON string.')]", "outputs": "", "display_name": "Create Page ", "name": "", "description": "A component for creating Notion pages.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/create_page.py", "section": "class::NotionPageCreatorSchema", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionPageCreatorSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionPageCreatorSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import json", "from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/list_assistants.py", "section": "class::AssistantsListAssistants", "content": "from langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.schema.message import Message\nfrom langflow.template.field.base import Output\n\nclass AssistantsListAssistants(ComponentWithCache):\n    display_name: str = \"List Assistants\"\n    description: str = \"Returns a list of assistant id's\"\n    icon = \"AstraDB\"\n\n    outputs = [\n        Output(display_name='Assistants',\n        name='assistants',\n        method='process_inputs')\n    ]\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssistantsListAssistants", "base_classes": ["ComponentWithCache"], "public_methods": ["def process_inputs(self)"], "imports": ["from langflow.base.astra_assistants.util import get_patched_openai_client", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.schema.message import Message", "from langflow.template.field.base import Output"], "inputs": "", "outputs": "[Output(display_name='Assistants', name='assistants', method='process_inputs')]", "display_name": "List Assistants", "name": "", "description": "Returns a list of assistant id's", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/clickhouse.py", "section": "class::ClickhouseVectorStoreComponent", "content": "from langchain_community.vectorstores import Clickhouse, ClickhouseSettings\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import BoolInput, FloatInput\nfrom langflow.io import DictInput, DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nimport clickhouse_connect\n\nclass ClickhouseVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Clickhouse\"\n    description: str = \"Clickhouse Vector Store with search capabilities\"\n    icon = \"Clickhouse\"\n    name = \"Clickhouse\"\n\n    inputs = [\n        StrInput(name='host',\n        display_name='hostname',\n        required=True,\n        value='localhost'),\n        IntInput(name='port',\n        display_name='port',\n        required=True,\n        value=8123),\n        StrInput(name='database',\n        display_name='database',\n        required=True),\n        StrInput(name='table',\n        display_name='Table name',\n        required=True),\n        StrInput(name='username',\n        display_name='The ClickHouse user name.',\n        required=True),\n        SecretStrInput(name='password',\n        display_name='The password for username.',\n        required=True),\n        DropdownInput(name='index_type',\n        display_name='index_type',\n        options=['annoy',\n        'vector_similarity'],\n        info='Type of the index.',\n        value='annoy',\n        advanced=True),\n        DropdownInput(name='metric',\n        display_name='metric',\n        options=['angular',\n        'euclidean',\n        'manhattan',\n        'hamming',\n        'dot'],\n        info='Metric to compute distance.',\n        value='angular',\n        advanced=True),\n        BoolInput(name='secure',\n        display_name='Use https/TLS. This overrides inferred values from the interface or port arguments.',\n        value=False,\n        advanced=True),\n        StrInput(name='index_param',\n        display_name='Param of the index',\n        value=\"100,'L2Distance'\",\n        advanced=True),\n        DictInput(name='index_query_params',\n        display_name='index query params',\n        advanced=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True),\n        FloatInput(name='score_threshold',\n        display_name='Score threshold',\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ClickhouseVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from langchain_community.vectorstores import Clickhouse, ClickhouseSettings", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.inputs import BoolInput, FloatInput", "from langflow.io import DictInput, DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "import clickhouse_connect"], "inputs": "[StrInput(name='host', display_name='hostname', required=True, value='localhost'), IntInput(name='port', display_name='port', required=True, value=8123), StrInput(name='database', display_name='database', required=True), StrInput(name='table', display_name='Table name', required=True), StrInput(name='username', display_name='The ClickHouse user name.', required=True), SecretStrInput(name='password', display_name='The password for username.', required=True), DropdownInput(name='index_type', display_name='index_type', options=['annoy', 'vector_similarity'], info='Type of the index.', value='annoy', advanced=True), DropdownInput(name='metric', display_name='metric', options=['angular', 'euclidean', 'manhattan', 'hamming', 'dot'], info='Metric to compute distance.', value='angular', advanced=True), BoolInput(name='secure', display_name='Use https/TLS. This overrides inferred values from the interface or port arguments.', value=False, advanced=True), StrInput(name='index_param', display_name='Param of the index', value=\"100,'L2Distance'\", advanced=True), DictInput(name='index_query_params', display_name='index query params', advanced=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True), FloatInput(name='score_threshold', display_name='Score threshold', advanced=True)]", "outputs": "", "display_name": "Clickhouse", "name": "Clickhouse", "description": "Clickhouse Vector Store with search capabilities", "icon": "Clickhouse"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/assemblyai/assemblyai_poll_transcript.py", "section": "class::AssemblyAITranscriptionJobPoller", "content": "import assemblyai as aai\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DataInput, FloatInput, Output, SecretStrInput\nfrom langflow.schema import Data\n\nclass AssemblyAITranscriptionJobPoller(Component):\n    display_name: str = \"AssemblyAI Poll Transcript\"\n    description: str = \"Poll for the status of a transcription job using AssemblyAI\"\n    icon = \"AssemblyAI\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Assembly API Key',\n        info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/',\n        required=True),\n        DataInput(name='transcript_id',\n        display_name='Transcript ID',\n        info='The ID of the transcription job to poll',\n        required=True),\n        FloatInput(name='polling_interval',\n        display_name='Polling Interval',\n        value=3.0,\n        info='The polling interval in seconds',\n        advanced=True,\n        range_spec=RangeSpec(min=3,\n        max=30))\n    ]\n\n    outputs = [\n        Output(display_name='Transcription Result',\n        name='transcription_result',\n        method='poll_transcription_job')\n    ]\n\n    def poll_transcription_job(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssemblyAITranscriptionJobPoller", "base_classes": ["Component"], "public_methods": ["def poll_transcription_job(self)"], "imports": ["import assemblyai as aai", "from loguru import logger", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import DataInput, FloatInput, Output, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='Assembly API Key', info='Your AssemblyAI API key. You can get one from https://www.assemblyai.com/', required=True), DataInput(name='transcript_id', display_name='Transcript ID', info='The ID of the transcription job to poll', required=True), FloatInput(name='polling_interval', display_name='Polling Interval', value=3.0, info='The polling interval in seconds', advanced=True, range_spec=RangeSpec(min=3, max=30))]", "outputs": "[Output(display_name='Transcription Result', name='transcription_result', method='poll_transcription_job')]", "display_name": "AssemblyAI Poll Transcript", "name": "", "description": "Poll for the status of a transcription job using AssemblyAI", "icon": "AssemblyAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/api_request.py", "section": "class::APIRequestComponent", "content": "import asyncio\nimport json\nimport re\nimport tempfile\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\nimport aiofiles\nimport aiofiles.os as aiofiles_os\nimport httpx\nimport validators\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DataInput, DropdownInput, FloatInput, IntInput, MessageTextInput, MultilineInput, Output, StrInput, TableInput\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.services.deps import get_settings_service\n\nclass APIRequestComponent(Component):\n    display_name: str = \"API Request\"\n    description: str = \"Make HTTP requests using URLs or cURL commands.\"\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(name='urls',\n        display_name='URLs',\n        list=True,\n        info='Enter one or more URLs,\n        separated by commas.',\n        advanced=False,\n        tool_mode=True),\n        MultilineInput(name='curl',\n        display_name='cURL',\n        info='Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.',\n        advanced=True,\n        real_time_refresh=True,\n        tool_mode=True),\n        DropdownInput(name='method',\n        display_name='Method',\n        options=['GET',\n        'POST',\n        'PATCH',\n        'PUT',\n        'DELETE'],\n        info='The HTTP method to use.',\n        real_time_refresh=True),\n        BoolInput(name='use_curl',\n        display_name='Use cURL',\n        value=False,\n        info='Enable cURL mode to populate fields from a cURL command.',\n        real_time_refresh=True),\n        DataInput(name='query_params',\n        display_name='Query Parameters',\n        info='The query parameters to append to the URL.',\n        advanced=True),\n        TableInput(name='body',\n        display_name='Body',\n        info='The body to send with the request as a dictionary (for POST,\n        PATCH,\n        PUT).',\n        table_schema=[{'name': 'key',\n        'display_name': 'Key',\n        'type': 'str',\n        'description': 'Parameter name'},\n        {'name': 'value',\n        'display_name': 'Value',\n        'description': 'Parameter value'}],\n        value=[],\n        input_types=['Data'],\n        advanced=True,\n        real_time_refresh=True),\n        TableInput(name='headers',\n        display_name='Headers',\n        info='The headers to send with the request',\n        table_schema=[{'name': 'key',\n        'display_name': 'Header',\n        'type': 'str',\n        'description': 'Header name'},\n        {'name': 'value',\n        'display_name': 'Value',\n        'type': 'str',\n        'description': 'Header value'}],\n        value=[{'key': 'User-Agent',\n        'value': get_settings_service().settings.user_agent}],\n        advanced=True,\n        input_types=['Data'],\n        real_time_refresh=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        value=30,\n        info='The timeout to use for the request.',\n        advanced=True),\n        BoolInput(name='follow_redirects',\n        display_name='Follow Redirects',\n        value=True,\n        info='Whether to follow http redirects.',\n        advanced=True),\n        BoolInput(name='save_to_file',\n        display_name='Save to File',\n        value=False,\n        info='Save the API response to a temporary file',\n        advanced=True),\n        BoolInput(name='include_httpx_metadata',\n        display_name='Include HTTPx Metadata',\n        value=False,\n        info='Include properties such as headers,\n        status_code,\n        response_headers,\n        and redirection_history in the output.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='make_requests'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def parse_curl(self, curl, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def add_query_params(self, url, params):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "APIRequestComponent", "base_classes": ["Component"], "public_methods": ["def parse_curl(self, curl, build_config)", "def update_build_config(self, build_config, field_value, field_name)", "def add_query_params(self, url, params)"], "imports": ["import asyncio", "import json", "import re", "import tempfile", "from datetime import datetime, timezone", "from pathlib import Path", "from typing import Any", "from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse", "import aiofiles", "import aiofiles.os as aiofiles_os", "import httpx", "import validators", "from langflow.base.curl.parse import parse_context", "from langflow.custom import Component", "from langflow.io import BoolInput, DataInput, DropdownInput, FloatInput, IntInput, MessageTextInput, MultilineInput, Output, StrInput, TableInput", "from langflow.schema import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.schema.dotdict import dotdict", "from langflow.services.deps import get_settings_service"], "inputs": "[MessageTextInput(name='urls', display_name='URLs', list=True, info='Enter one or more URLs, separated by commas.', advanced=False, tool_mode=True), MultilineInput(name='curl', display_name='cURL', info='Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.', advanced=True, real_time_refresh=True, tool_mode=True), DropdownInput(name='method', display_name='Method', options=['GET', 'POST', 'PATCH', 'PUT', 'DELETE'], info='The HTTP method to use.', real_time_refresh=True), BoolInput(name='use_curl', display_name='Use cURL', value=False, info='Enable cURL mode to populate fields from a cURL command.', real_time_refresh=True), DataInput(name='query_params', display_name='Query Parameters', info='The query parameters to append to the URL.', advanced=True), TableInput(name='body', display_name='Body', info='The body to send with the request as a dictionary (for POST, PATCH, PUT).', table_schema=[{'name': 'key', 'display_name': 'Key', 'type': 'str', 'description': 'Parameter name'}, {'name': 'value', 'display_name': 'Value', 'description': 'Parameter value'}], value=[], input_types=['Data'], advanced=True, real_time_refresh=True), TableInput(name='headers', display_name='Headers', info='The headers to send with the request', table_schema=[{'name': 'key', 'display_name': 'Header', 'type': 'str', 'description': 'Header name'}, {'name': 'value', 'display_name': 'Value', 'type': 'str', 'description': 'Header value'}], value=[{'key': 'User-Agent', 'value': get_settings_service().settings.user_agent}], advanced=True, input_types=['Data'], real_time_refresh=True), IntInput(name='timeout', display_name='Timeout', value=30, info='The timeout to use for the request.', advanced=True), BoolInput(name='follow_redirects', display_name='Follow Redirects', value=True, info='Whether to follow http redirects.', advanced=True), BoolInput(name='save_to_file', display_name='Save to File', value=False, info='Save the API response to a temporary file', advanced=True), BoolInput(name='include_httpx_metadata', display_name='Include HTTPx Metadata', value=False, info='Include properties such as headers, status_code, response_headers, and redirection_history in the output.', advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='make_requests'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "API Request", "name": "APIRequest", "description": "Make HTTP requests using URLs or cURL commands.", "icon": "Globe"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/store_message.py", "section": "class::StoreMessageComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.memory import aget_messages, astore_message\nfrom langflow.schema.message import Message\n\nclass StoreMessageComponent(CustomComponent):\n    display_name: str = \"Store Message\"\n    description: str = \"Stores a chat message.\"\n    name = \"StoreMessage\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "StoreMessageComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.memory import aget_messages, astore_message", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "Store Message", "name": "StoreMessage", "description": "Stores a chat message.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/memories/mem0_chat_memory.py", "section": "class::Mem0MemoryComponent", "content": "import os\nfrom loguru import logger\nfrom mem0 import Memory, MemoryClient\nfrom langflow.base.memory.model import LCChatMemoryComponent\nfrom langflow.inputs import DictInput, HandleInput, MessageTextInput, NestedDictInput, SecretStrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass Mem0MemoryComponent(LCChatMemoryComponent):\n    display_name: str = \"Mem0 Chat Memory\"\n    description: str = \"Retrieves and stores chat messages using Mem0 memory storage.\"\n    icon = \"Mem0\"\n    name = \"mem0_chat_memory\"\n\n    inputs = [\n        NestedDictInput(name='mem0_config',\n        display_name='Mem0 Configuration',\n        info='Configuration dictionary for initializing Mem0 memory instance.\\n                    Example:\\n                    {\\n                        \"graph_store\": {\\n                            \"provider\": \"neo4j\",\\n                            \"config\": {\\n                                \"url\": \"neo4j+s://your-neo4j-url\",\\n                                \"username\": \"neo4j\",\\n                                \"password\": \"your-password\"\\n                            }\\n                        },\\n                        \"version\": \"v1.1\"\\n                    }',\n        input_types=['Data']),\n        MessageTextInput(name='ingest_message',\n        display_name='Message to Ingest',\n        info='The message content to be ingested into Mem0 memory.'),\n        HandleInput(name='existing_memory',\n        display_name='Existing Memory Instance',\n        input_types=['Memory'],\n        info='Optional existing Mem0 memory instance. If not provided,\n        a new instance will be created.'),\n        MessageTextInput(name='user_id',\n        display_name='User ID',\n        info='Identifier for the user associated with the messages.'),\n        MessageTextInput(name='search_query',\n        display_name='Search Query',\n        info='Input text for searching related memories in Mem0.'),\n        SecretStrInput(name='mem0_api_key',\n        display_name='Mem0 API Key',\n        info='API key for Mem0 platform. Leave empty to use the local version.'),\n        DictInput(name='metadata',\n        display_name='Metadata',\n        info='Additional metadata to associate with the ingested message.',\n        advanced=True),\n        SecretStrInput(name='openai_api_key',\n        display_name='OpenAI API Key',\n        required=False,\n        info='API key for OpenAI. Required if using OpenAI Embeddings without a provided configuration.')\n    ]\n\n    outputs = [\n        Output(name='memory',\n        display_name='Mem0 Memory',\n        method='ingest_data'),\n        Output(name='search_results',\n        display_name='Search Results',\n        method='build_search_results')\n    ]\n\n    def build_mem0(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def ingest_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_search_results(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "Mem0MemoryComponent", "base_classes": ["LCChatMemoryComponent"], "public_methods": ["def build_mem0(self)", "def ingest_data(self)", "def build_search_results(self)"], "imports": ["import os", "from loguru import logger", "from mem0 import Memory, MemoryClient", "from langflow.base.memory.model import LCChatMemoryComponent", "from langflow.inputs import DictInput, HandleInput, MessageTextInput, NestedDictInput, SecretStrInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "[NestedDictInput(name='mem0_config', display_name='Mem0 Configuration', info='Configuration dictionary for initializing Mem0 memory instance.\\n                    Example:\\n                    {\\n                        \"graph_store\": {\\n                            \"provider\": \"neo4j\",\\n                            \"config\": {\\n                                \"url\": \"neo4j+s://your-neo4j-url\",\\n                                \"username\": \"neo4j\",\\n                                \"password\": \"your-password\"\\n                            }\\n                        },\\n                        \"version\": \"v1.1\"\\n                    }', input_types=['Data']), MessageTextInput(name='ingest_message', display_name='Message to Ingest', info='The message content to be ingested into Mem0 memory.'), HandleInput(name='existing_memory', display_name='Existing Memory Instance', input_types=['Memory'], info='Optional existing Mem0 memory instance. If not provided, a new instance will be created.'), MessageTextInput(name='user_id', display_name='User ID', info='Identifier for the user associated with the messages.'), MessageTextInput(name='search_query', display_name='Search Query', info='Input text for searching related memories in Mem0.'), SecretStrInput(name='mem0_api_key', display_name='Mem0 API Key', info='API key for Mem0 platform. Leave empty to use the local version.'), DictInput(name='metadata', display_name='Metadata', info='Additional metadata to associate with the ingested message.', advanced=True), SecretStrInput(name='openai_api_key', display_name='OpenAI API Key', required=False, info='API key for OpenAI. Required if using OpenAI Embeddings without a provided configuration.')]", "outputs": "[Output(name='memory', display_name='Mem0 Memory', method='ingest_data'), Output(name='search_results', display_name='Search Results', method='build_search_results')]", "display_name": "Mem0 Chat Memory", "name": "mem0_chat_memory", "description": "Retrieves and stores chat messages using Mem0 memory storage.", "icon": "Mem0"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/crewai/sequential_task_agent.py", "section": "class::SequentialTaskAgentComponent", "content": "from crewai import Agent, Task\nfrom langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\nclass SequentialTaskAgentComponent(Component):\n    display_name: str = \"Sequential Task Agent\"\n    description: str = \"Creates a CrewAI Task and its associated Agent.\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        MultilineInput(name='role',\n        display_name='Role',\n        info='The role of the agent.'),\n        MultilineInput(name='goal',\n        display_name='Goal',\n        info='The objective of the agent.'),\n        MultilineInput(name='backstory',\n        display_name='Backstory',\n        info='The backstory of the agent.'),\n        HandleInput(name='tools',\n        display_name='Tools',\n        input_types=['Tool'],\n        is_list=True,\n        info=\"Tools at agent's disposal\",\n        value=[]),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        info='Language model that will run the agent.',\n        input_types=['LanguageModel']),\n        BoolInput(name='memory',\n        display_name='Memory',\n        info='Whether the agent should have memory or not',\n        advanced=True,\n        value=True),\n        BoolInput(name='verbose',\n        display_name='Verbose',\n        advanced=True,\n        value=True),\n        BoolInput(name='allow_delegation',\n        display_name='Allow Delegation',\n        info='Whether the agent is allowed to delegate tasks to other agents.',\n        value=False,\n        advanced=True),\n        BoolInput(name='allow_code_execution',\n        display_name='Allow Code Execution',\n        info='Whether the agent is allowed to execute code.',\n        value=False,\n        advanced=True),\n        DictInput(name='agent_kwargs',\n        display_name='Agent kwargs',\n        info='Additional kwargs for the agent.',\n        is_list=True,\n        advanced=True),\n        MultilineInput(name='task_description',\n        display_name='Task Description',\n        info=\"Descriptive text detailing task's purpose and execution.\"),\n        MultilineInput(name='expected_output',\n        display_name='Expected Task Output',\n        info='Clear definition of expected task outcome.'),\n        BoolInput(name='async_execution',\n        display_name='Async Execution',\n        value=False,\n        advanced=True,\n        info='Boolean flag indicating asynchronous task execution.'),\n        HandleInput(name='previous_task',\n        display_name='Previous Task',\n        input_types=['SequentialTask'],\n        info='The previous task in the sequence (for chaining).',\n        required=False)\n    ]\n\n    outputs = [\n        Output(display_name='Sequential Task',\n        name='task_output',\n        method='build_agent_and_task')\n    ]\n\n    def build_agent_and_task(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SequentialTaskAgentComponent", "base_classes": ["Component"], "public_methods": ["def build_agent_and_task(self)"], "imports": ["from crewai import Agent, Task", "from langflow.base.agents.crewai.tasks import SequentialTask", "from langflow.custom import Component", "from langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output"], "inputs": "[MultilineInput(name='role', display_name='Role', info='The role of the agent.'), MultilineInput(name='goal', display_name='Goal', info='The objective of the agent.'), MultilineInput(name='backstory', display_name='Backstory', info='The backstory of the agent.'), HandleInput(name='tools', display_name='Tools', input_types=['Tool'], is_list=True, info=\"Tools at agent's disposal\", value=[]), HandleInput(name='llm', display_name='Language Model', info='Language model that will run the agent.', input_types=['LanguageModel']), BoolInput(name='memory', display_name='Memory', info='Whether the agent should have memory or not', advanced=True, value=True), BoolInput(name='verbose', display_name='Verbose', advanced=True, value=True), BoolInput(name='allow_delegation', display_name='Allow Delegation', info='Whether the agent is allowed to delegate tasks to other agents.', value=False, advanced=True), BoolInput(name='allow_code_execution', display_name='Allow Code Execution', info='Whether the agent is allowed to execute code.', value=False, advanced=True), DictInput(name='agent_kwargs', display_name='Agent kwargs', info='Additional kwargs for the agent.', is_list=True, advanced=True), MultilineInput(name='task_description', display_name='Task Description', info=\"Descriptive text detailing task's purpose and execution.\"), MultilineInput(name='expected_output', display_name='Expected Task Output', info='Clear definition of expected task outcome.'), BoolInput(name='async_execution', display_name='Async Execution', value=False, advanced=True, info='Boolean flag indicating asynchronous task execution.'), HandleInput(name='previous_task', display_name='Previous Task', input_types=['SequentialTask'], info='The previous task in the sequence (for chaining).', required=False)]", "outputs": "[Output(display_name='Sequential Task', name='task_output', method='build_agent_and_task')]", "display_name": "Sequential Task Agent", "name": "", "description": "Creates a CrewAI Task and its associated Agent.", "icon": "CrewAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/python_code_structured_tool.py", "section": "class::PythonCodeStructuredTool", "content": "import ast\nimport json\nfrom typing import Any\nfrom langchain.agents import Tool\nfrom langchain_core.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic.v1 import Field, create_model\nfrom pydantic.v1.fields import Undefined\nfrom typing_extensions import override\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, FieldTypes, HandleInput, MessageTextInput, MultilineInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\nclass PythonCodeStructuredTool(LCToolComponent):\n    display_name: str = \"Python Code Structured\"\n    description: str = \"structuredtool dataclass code to tool\"\n    icon = \"Python\"\n    name = \"PythonCodeStructuredTool\"\n\n    inputs = [\n        MultilineInput(name='tool_code',\n        display_name='Tool Code',\n        info='Enter the dataclass code.',\n        placeholder='def my_function(args):\\n    pass',\n        required=True,\n        real_time_refresh=True,\n        refresh_button=True),\n        MessageTextInput(name='tool_name',\n        display_name='Tool Name',\n        info='Enter the name of the tool.',\n        required=True),\n        MessageTextInput(name='tool_description',\n        display_name='Description',\n        info='Enter the description of the tool.',\n        required=True),\n        BoolInput(name='return_direct',\n        display_name='Return Directly',\n        info='Should the tool return the function output directly?'),\n        DropdownInput(name='tool_function',\n        display_name='Tool Function',\n        info='Select the function for additional expressions.',\n        options=[],\n        required=True,\n        real_time_refresh=True,\n        refresh_button=True),\n        HandleInput(name='global_variables',\n        display_name='Global Variables',\n        info='Enter the global variables or Create Data Component.',\n        input_types=['Data'],\n        field_type=FieldTypes.DICT,\n        is_list=True),\n        MessageTextInput(name='_classes',\n        display_name='Classes',\n        advanced=True),\n        MessageTextInput(name='_functions',\n        display_name='Functions',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Tool',\n        name='result_tool',\n        method='build_tool')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "PythonCodeStructuredTool", "base_classes": ["LCToolComponent"], "public_methods": [], "imports": ["import ast", "import json", "from typing import Any", "from langchain.agents import Tool", "from langchain_core.tools import StructuredTool", "from loguru import logger", "from pydantic.v1 import Field, create_model", "from pydantic.v1.fields import Undefined", "from typing_extensions import override", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.inputs.inputs import BoolInput, DropdownInput, FieldTypes, HandleInput, MessageTextInput, MultilineInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict"], "inputs": "[MultilineInput(name='tool_code', display_name='Tool Code', info='Enter the dataclass code.', placeholder='def my_function(args):\\n    pass', required=True, real_time_refresh=True, refresh_button=True), MessageTextInput(name='tool_name', display_name='Tool Name', info='Enter the name of the tool.', required=True), MessageTextInput(name='tool_description', display_name='Description', info='Enter the description of the tool.', required=True), BoolInput(name='return_direct', display_name='Return Directly', info='Should the tool return the function output directly?'), DropdownInput(name='tool_function', display_name='Tool Function', info='Select the function for additional expressions.', options=[], required=True, real_time_refresh=True, refresh_button=True), HandleInput(name='global_variables', display_name='Global Variables', info='Enter the global variables or Create Data Component.', input_types=['Data'], field_type=FieldTypes.DICT, is_list=True), MessageTextInput(name='_classes', display_name='Classes', advanced=True), MessageTextInput(name='_functions', display_name='Functions', advanced=True)]", "outputs": "[Output(display_name='Tool', name='result_tool', method='build_tool')]", "display_name": "Python Code Structured", "name": "PythonCodeStructuredTool", "description": "structuredtool dataclass code to tool", "icon": "Python"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/python_code_structured_tool.py", "section": "class::PythonCodeToolFunc", "content": "import ast\nimport json\nfrom typing import Any\nfrom langchain.agents import Tool\nfrom langchain_core.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic.v1 import Field, create_model\nfrom pydantic.v1.fields import Undefined\nfrom typing_extensions import override\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, FieldTypes, HandleInput, MessageTextInput, MultilineInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\nclass PythonCodeToolFunc:\n\n    def run():\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PythonCodeToolFunc", "base_classes": [], "public_methods": ["def run()"], "imports": ["import ast", "import json", "from typing import Any", "from langchain.agents import Tool", "from langchain_core.tools import StructuredTool", "from loguru import logger", "from pydantic.v1 import Field, create_model", "from pydantic.v1.fields import Undefined", "from typing_extensions import override", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.inputs.inputs import BoolInput, DropdownInput, FieldTypes, HandleInput, MessageTextInput, MultilineInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/aiml.py", "section": "class::AIMLEmbeddingsComponent", "content": "from langflow.base.embeddings.aiml_embeddings import AIMLEmbeddingsImpl\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput\nfrom langflow.io import SecretStrInput\n\nclass AIMLEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"AI/ML Embeddings\"\n    description: str = \"Generate embeddings using the AI/ML API.\"\n    icon = \"AI/ML\"\n    name = \"AIMLEmbeddings\"\n\n    inputs = [\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        options=['text-embedding-3-small',\n        'text-embedding-3-large',\n        'text-embedding-ada-002'],\n        required=True),\n        SecretStrInput(name='aiml_api_key',\n        display_name='AI/ML API Key',\n        value='AIML_API_KEY',\n        required=True)\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AIMLEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langflow.base.embeddings.aiml_embeddings import AIMLEmbeddingsImpl", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.inputs.inputs import DropdownInput", "from langflow.io import SecretStrInput"], "inputs": "[DropdownInput(name='model_name', display_name='Model Name', options=['text-embedding-3-small', 'text-embedding-3-large', 'text-embedding-ada-002'], required=True), SecretStrInput(name='aiml_api_key', display_name='AI/ML API Key', value='AIML_API_KEY', required=True)]", "outputs": "", "display_name": "AI/ML Embeddings", "name": "AIMLEmbeddings", "description": "Generate embeddings using the AI/ML API.", "icon": "AI/ML"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/video_file.py", "section": "class::VideoFileComponent", "content": "from pathlib import Path\nfrom langflow.base.data import BaseFileComponent\nfrom langflow.io import FileInput\nfrom langflow.schema import Data\n\nclass VideoFileComponent(BaseFileComponent):\n    \"\"\"\n    Handles loading and processing of video files.\n    \n    This component supports processing video files in common video formats.\n    \"\"\"\n\n    display_name: str = \"Video File\"\n    description: str = \"Load a video file in common video formats.\"\n    icon = \"TwelveLabs\"\n    name = \"VideoFile\"\n\n    inputs = [\n        FileInput(display_name='Video File',\n        name='file_path',\n        file_types=['mp4',\n        'avi',\n        'mov',\n        'mkv',\n        'webm',\n        'flv',\n        'wmv',\n        'mpg',\n        'mpeg',\n        'm4v',\n        '3gp',\n        '3g2',\n        'm2v',\n        'mxf',\n        'dv',\n        'vob',\n        'ogv',\n        'rm',\n        'rmvb',\n        'amv',\n        'divx',\n        'm2ts',\n        'mts',\n        'ts',\n        'qt',\n        'yuv',\n        'y4m'],\n        required=True,\n        info='Upload a video file in any common video format supported by ffmpeg')\n    ]\n\n    outputs = [\n        *BaseFileComponent._base_outputs\n    ]\n\n    def process_files(self, file_list):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def load_files(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VideoFileComponent", "base_classes": ["BaseFileComponent"], "public_methods": ["def process_files(self, file_list)", "def load_files(self)"], "imports": ["from pathlib import Path", "from langflow.base.data import BaseFileComponent", "from langflow.io import FileInput", "from langflow.schema import Data"], "inputs": "[FileInput(display_name='Video File', name='file_path', file_types=['mp4', 'avi', 'mov', 'mkv', 'webm', 'flv', 'wmv', 'mpg', 'mpeg', 'm4v', '3gp', '3g2', 'm2v', 'mxf', 'dv', 'vob', 'ogv', 'rm', 'rmvb', 'amv', 'divx', 'm2ts', 'mts', 'ts', 'qt', 'yuv', 'y4m'], required=True, info='Upload a video file in any common video format supported by ffmpeg')]", "outputs": "[*BaseFileComponent._base_outputs]", "display_name": "Video File", "name": "VideoFile", "description": "Load a video file in common video formats.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/natural_language.py", "section": "class::NaturalLanguageTextSplitterComponent", "content": "from typing import Any\nfrom langchain_text_splitters import NLTKTextSplitter, TextSplitter\nfrom langflow.base.textsplitters.model import LCTextSplitterComponent\nfrom langflow.inputs import DataInput, IntInput, MessageTextInput\nfrom langflow.utils.util import unescape_string\n\nclass NaturalLanguageTextSplitterComponent(LCTextSplitterComponent):\n    display_name: str = \"Natural Language Text Splitter\"\n    description: str = \"Split text based on natural language boundaries, optimized for a specified language.\"\n    icon = \"LangChain\"\n    name = \"NaturalLanguageTextSplitter\"\n\n    inputs = [\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        info='The maximum number of characters in each chunk after splitting.',\n        value=1000),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='The number of characters that overlap between consecutive chunks.',\n        value=200),\n        DataInput(name='data_input',\n        display_name='Input',\n        info='The text data to be split.',\n        input_types=['Document',\n        'Data'],\n        required=True),\n        MessageTextInput(name='separator',\n        display_name='Separator',\n        info='The character(s) to use as a delimiter when splitting text.\\nDefaults to \"\\\\n\\\\n\" if left empty.'),\n        MessageTextInput(name='language',\n        display_name='Language',\n        info='The language of the text. Default is \"English\". Supports multiple languages for better text boundary recognition.')\n    ]\n\n    def get_data_input(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_text_splitter(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NaturalLanguageTextSplitterComponent", "base_classes": ["LCTextSplitterComponent"], "public_methods": ["def get_data_input(self)", "def build_text_splitter(self)"], "imports": ["from typing import Any", "from langchain_text_splitters import NLTKTextSplitter, TextSplitter", "from langflow.base.textsplitters.model import LCTextSplitterComponent", "from langflow.inputs import DataInput, IntInput, MessageTextInput", "from langflow.utils.util import unescape_string"], "inputs": "[IntInput(name='chunk_size', display_name='Chunk Size', info='The maximum number of characters in each chunk after splitting.', value=1000), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='The number of characters that overlap between consecutive chunks.', value=200), DataInput(name='data_input', display_name='Input', info='The text data to be split.', input_types=['Document', 'Data'], required=True), MessageTextInput(name='separator', display_name='Separator', info='The character(s) to use as a delimiter when splitting text.\\nDefaults to \"\\\\n\\\\n\" if left empty.'), MessageTextInput(name='language', display_name='Language', info='The language of the text. Default is \"English\". Supports multiple languages for better text boundary recognition.')]", "outputs": "", "display_name": "Natural Language Text Splitter", "name": "NaturalLanguageTextSplitter", "description": "Split text based on natural language boundaries, optimized for a specified language.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/llm_router.py", "section": "class::LLMRouterComponent", "content": "import json\nimport requests\nfrom langflow.base.models.chat_result import get_chat_result\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, HandleInput, Output\nfrom langflow.schema.message import Message\n\nclass LLMRouterComponent(Component):\n    display_name: str = \"LLM Router\"\n    description: str = \"Routes the input to the most appropriate LLM based on OpenRouter model specifications\"\n    icon = \"git-branch\"\n\n    inputs = [\n        HandleInput(name='models',\n        display_name='Language Models',\n        input_types=['LanguageModel'],\n        required=True,\n        is_list=True,\n        info='List of LLMs to route between'),\n        HandleInput(name='input_value',\n        display_name='Input',\n        input_types=['Message'],\n        info='The input message to be routed'),\n        HandleInput(name='judge_llm',\n        display_name='Judge LLM',\n        input_types=['LanguageModel'],\n        info='LLM that will evaluate and select the most appropriate model'),\n        DropdownInput(name='optimization',\n        display_name='Optimization',\n        options=['quality',\n        'speed',\n        'cost',\n        'balanced'],\n        value='balanced',\n        info='Optimization preference for model selection')\n    ]\n\n    outputs = [\n        Output(display_name='Output',\n        name='output',\n        method='route_to_model'),\n        Output(display_name='Selected Model',\n        name='selected_model',\n        method='get_selected_model',\n        required_inputs=['output'])\n    ]\n\n    def get_selected_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LLMRouterComponent", "base_classes": ["Component"], "public_methods": ["def get_selected_model(self)"], "imports": ["import json", "import requests", "from langflow.base.models.chat_result import get_chat_result", "from langflow.base.models.model_utils import get_model_name", "from langflow.custom import Component", "from langflow.io import DropdownInput, HandleInput, Output", "from langflow.schema.message import Message"], "inputs": "[HandleInput(name='models', display_name='Language Models', input_types=['LanguageModel'], required=True, is_list=True, info='List of LLMs to route between'), HandleInput(name='input_value', display_name='Input', input_types=['Message'], info='The input message to be routed'), HandleInput(name='judge_llm', display_name='Judge LLM', input_types=['LanguageModel'], info='LLM that will evaluate and select the most appropriate model'), DropdownInput(name='optimization', display_name='Optimization', options=['quality', 'speed', 'cost', 'balanced'], value='balanced', info='Optimization preference for model selection')]", "outputs": "[Output(display_name='Output', name='output', method='route_to_model'), Output(display_name='Selected Model', name='selected_model', method='get_selected_model', required_inputs=['output'])]", "display_name": "LLM Router", "name": "", "description": "Routes the input to the most appropriate LLM based on OpenRouter model specifications", "icon": "git-branch"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/run_flow.py", "section": "class::RunFlowComponent", "content": "from typing import Any\nfrom loguru import logger\nfrom langflow.base.tools.run_flow import RunFlowBaseComponent\nfrom langflow.helpers.flow import run_flow\nfrom langflow.schema import dotdict\n\nclass RunFlowComponent(RunFlowBaseComponent):\n    display_name: str = \"Run Flow\"\n    description: str = \"Creates a tool component from a Flow that takes all its inputs and runs it.  \n **Select a Flow to use the tool mode**\"\n    icon = \"Workflow\"\n    name = \"RunFlow\"\n\n    inputs = [\n        RunFlowBaseComponent._base_inputs\n    ]\n\n    outputs = [\n        RunFlowBaseComponent._base_outputs\n    ]\n", "metadata": {"parser": "python_component", "class_name": "RunFlowComponent", "base_classes": ["RunFlowBaseComponent"], "public_methods": [], "imports": ["from typing import Any", "from loguru import logger", "from langflow.base.tools.run_flow import RunFlowBaseComponent", "from langflow.helpers.flow import run_flow", "from langflow.schema import dotdict"], "inputs": "RunFlowBaseComponent._base_inputs", "outputs": "RunFlowBaseComponent._base_outputs", "display_name": "Run Flow", "name": "RunFlow", "description": "Creates a tool component from a Flow that takes all its inputs and runs it.  \n **Select a Flow to use the tool mode**", "icon": "Workflow"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/sambanova.py", "section": "class::SambaNovaComponent", "content": "from langchain_sambanova import ChatSambaNovaCloud\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.sambanova_constants import SAMBANOVA_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\n\nclass SambaNovaComponent(LCModelComponent):\n    display_name: str = \"SambaNova\"\n    description: str = \"Generate text using Sambanova LLMs.\"\n    icon = \"SambaNova\"\n    name = \"SambaNovaModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(name='base_url',\n        display_name='SambaNova Cloud Base Url',\n        advanced=True,\n        info='The base URL of the Sambanova Cloud API. Defaults to https://api.sambanova.ai/v1/chat/completions. You can change this to use other urls like Sambastudio'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=SAMBANOVA_MODEL_NAMES,\n        value=SAMBANOVA_MODEL_NAMES[0]),\n        SecretStrInput(name='api_key',\n        display_name='Sambanova API Key',\n        info='The Sambanova API Key to use for the Sambanova model.',\n        advanced=False,\n        value='SAMBANOVA_API_KEY',\n        required=True),\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        value=2048,\n        info='The maximum number of tokens to generate.'),\n        SliderInput(name='top_p',\n        display_name='top_p',\n        advanced=True,\n        value=1.0,\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        info='Model top_p'),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SambaNovaComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_sambanova import ChatSambaNovaCloud", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.sambanova_constants import SAMBANOVA_MODEL_NAMES", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput"], "inputs": "[*LCModelComponent._base_inputs, StrInput(name='base_url', display_name='SambaNova Cloud Base Url', advanced=True, info='The base URL of the Sambanova Cloud API. Defaults to https://api.sambanova.ai/v1/chat/completions. You can change this to use other urls like Sambastudio'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=SAMBANOVA_MODEL_NAMES, value=SAMBANOVA_MODEL_NAMES[0]), SecretStrInput(name='api_key', display_name='Sambanova API Key', info='The Sambanova API Key to use for the Sambanova model.', advanced=False, value='SAMBANOVA_API_KEY', required=True), IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, value=2048, info='The maximum number of tokens to generate.'), SliderInput(name='top_p', display_name='top_p', advanced=True, value=1.0, range_spec=RangeSpec(min=0, max=1, step=0.01), info='Model top_p'), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01), advanced=True)]", "outputs": "", "display_name": "SambaNova", "name": "SambaNovaModel", "description": "Generate text using Sambanova LLMs.", "icon": "SambaNova"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/video_details.py", "section": "class::YouTubeVideoDetailsComponent", "content": "from contextlib import contextmanager\nimport googleapiclient\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import DataFrame\nfrom langflow.template import Output\nimport re\nimport re\n\nclass YouTubeVideoDetailsComponent(Component):\n    \"\"\"\n    A component that retrieves detailed information about YouTube videos.\n    \"\"\"\n\n    display_name: str = \"YouTube Video Details\"\n    description: str = \"Retrieves detailed information and statistics about YouTube videos.\"\n    icon = \"YouTube\"\n\n    inputs = [\n        MessageTextInput(name='video_url',\n        display_name='Video URL',\n        info='The URL of the YouTube video.',\n        tool_mode=True,\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='YouTube API Key',\n        info='Your YouTube Data API key.',\n        required=True),\n        BoolInput(name='include_statistics',\n        display_name='Include Statistics',\n        value=True,\n        info='Include video statistics (views,\n        likes,\n        comments).'),\n        BoolInput(name='include_content_details',\n        display_name='Include Content Details',\n        value=True,\n        info='Include video duration,\n        quality,\n        and age restriction info.',\n        advanced=True),\n        BoolInput(name='include_tags',\n        display_name='Include Tags',\n        value=True,\n        info='Include video tags and keywords.',\n        advanced=True),\n        BoolInput(name='include_thumbnails',\n        display_name='Include Thumbnails',\n        value=True,\n        info='Include video thumbnail URLs in different resolutions.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='video_data',\n        display_name='Video Data',\n        method='get_video_details')\n    ]\n\n    def youtube_client(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_video_details(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubeVideoDetailsComponent", "base_classes": ["Component"], "public_methods": ["def youtube_client(self)", "def get_video_details(self)"], "imports": ["from contextlib import contextmanager", "import googleapiclient", "import pandas as pd", "from googleapiclient.discovery import build", "from googleapiclient.errors import HttpError", "from langflow.custom import Component", "from langflow.inputs import BoolInput, MessageTextInput, SecretStrInput", "from langflow.schema import DataFrame", "from langflow.template import Output", "import re", "import re"], "inputs": "[MessageTextInput(name='video_url', display_name='Video URL', info='The URL of the YouTube video.', tool_mode=True, required=True), SecretStrInput(name='api_key', display_name='YouTube API Key', info='Your YouTube Data API key.', required=True), BoolInput(name='include_statistics', display_name='Include Statistics', value=True, info='Include video statistics (views, likes, comments).'), BoolInput(name='include_content_details', display_name='Include Content Details', value=True, info='Include video duration, quality, and age restriction info.', advanced=True), BoolInput(name='include_tags', display_name='Include Tags', value=True, info='Include video tags and keywords.', advanced=True), BoolInput(name='include_thumbnails', display_name='Include Thumbnails', value=True, info='Include video thumbnail URLs in different resolutions.', advanced=True)]", "outputs": "[Output(name='video_data', display_name='Video Data', method='get_video_details')]", "display_name": "YouTube Video Details", "name": "", "description": "Retrieves detailed information and statistics about YouTube videos.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/list_database_properties.py", "section": "class::NotionDatabaseProperties", "content": "import requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionDatabaseProperties(LCToolComponent):\n    display_name: str = \"List Database Properties \"\n    description: str = \"Retrieve properties of a Notion database.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        StrInput(name='database_id',\n        display_name='Database ID',\n        info='The ID of the Notion database.'),\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionDatabaseProperties", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='database_id', display_name='Database ID', info='The ID of the Notion database.'), SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True)]", "outputs": "", "display_name": "List Database Properties ", "name": "", "description": "Retrieve properties of a Notion database.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/list_database_properties.py", "section": "class::NotionDatabasePropertiesSchema", "content": "import requests\nfrom langchain.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionDatabasePropertiesSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionDatabasePropertiesSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import requests", "from langchain.tools import StructuredTool", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/get_assistant.py", "section": "class::AssistantsGetAssistantName", "content": "from langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput, StrInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass AssistantsGetAssistantName(ComponentWithCache):\n    display_name: str = \"Get Assistant name\"\n    description: str = \"Assistant by id\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        StrInput(name='assistant_id',\n        display_name='Assistant ID',\n        info='ID of the assistant'),\n        MultilineInput(name='env_set',\n        display_name='Environment Set',\n        info='Dummy input to allow chaining with Dotenv Component.')\n    ]\n\n    outputs = [\n        Output(display_name='Assistant Name',\n        name='assistant_name',\n        method='process_inputs')\n    ]\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssistantsGetAssistantName", "base_classes": ["ComponentWithCache"], "public_methods": ["def process_inputs(self)"], "imports": ["from langflow.base.astra_assistants.util import get_patched_openai_client", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.inputs import MultilineInput, StrInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[StrInput(name='assistant_id', display_name='Assistant ID', info='ID of the assistant'), MultilineInput(name='env_set', display_name='Environment Set', info='Dummy input to allow chaining with Dotenv Component.')]", "outputs": "[Output(display_name='Assistant Name', name='assistant_name', method='process_inputs')]", "display_name": "Get Assistant name", "name": "", "description": "Assistant by id", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/pinecone.py", "section": "class::PineconeVectorStoreComponent", "content": "import numpy as np\nfrom langchain_core.vectorstores import VectorStore\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_pinecone._utilities import DistanceStrategy\n\nclass PineconeVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Pinecone\"\n    description: str = \"Pinecone Vector Store with search capabilities\"\n    icon = \"Pinecone\"\n    name = \"Pinecone\"\n\n    inputs = [\n        StrInput(name='index_name',\n        display_name='Index Name',\n        required=True),\n        StrInput(name='namespace',\n        display_name='Namespace',\n        info='Namespace for the index.'),\n        DropdownInput(name='distance_strategy',\n        display_name='Distance Strategy',\n        options=['Cosine',\n        'Euclidean',\n        'Dot Product'],\n        value='Cosine',\n        advanced=True),\n        SecretStrInput(name='pinecone_api_key',\n        display_name='Pinecone API Key',\n        required=True),\n        StrInput(name='text_key',\n        display_name='Text Key',\n        info='Key in the record to use as text.',\n        value='text',\n        advanced=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PineconeVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["import numpy as np", "from langchain_core.vectorstores import VectorStore", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langchain_pinecone import PineconeVectorStore", "from langchain_pinecone._utilities import DistanceStrategy"], "inputs": "[StrInput(name='index_name', display_name='Index Name', required=True), StrInput(name='namespace', display_name='Namespace', info='Namespace for the index.'), DropdownInput(name='distance_strategy', display_name='Distance Strategy', options=['Cosine', 'Euclidean', 'Dot Product'], value='Cosine', advanced=True), SecretStrInput(name='pinecone_api_key', display_name='Pinecone API Key', required=True), StrInput(name='text_key', display_name='Text Key', info='Key in the record to use as text.', value='text', advanced=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Pinecone", "name": "Pinecone", "description": "Pinecone Vector Store with search capabilities", "icon": "Pinecone"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/pinecone.py", "section": "class::Float32Embeddings", "content": "import numpy as np\nfrom langchain_core.vectorstores import VectorStore\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_pinecone._utilities import DistanceStrategy\n\nclass Float32Embeddings:\n    \"\"\"\n    Wrapper class to ensure float32 embeddings.\n    \"\"\"\n\n\n    def embed_documents(self, texts):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def embed_query(self, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "Float32Embeddings", "base_classes": [], "public_methods": ["def embed_documents(self, texts)", "def embed_query(self, text)"], "imports": ["import numpy as np", "from langchain_core.vectorstores import VectorStore", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langchain_pinecone import PineconeVectorStore", "from langchain_pinecone._utilities import DistanceStrategy"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/url.py", "section": "class::URLComponent", "content": "import re\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom langchain_community.document_loaders import RecursiveUrlLoader\nfrom loguru import logger\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs.inputs import TableInput\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.services.deps import get_settings_service\n\nclass URLComponent(Component):\n    \"\"\"\n    A component that loads and parses child links from a root URL recursively.\n    \"\"\"\n\n    display_name: str = \"URL\"\n    description: str = \"Load and parse child links from a root URL recursively\"\n    icon = \"layout-template\"\n    name = \"URLComponent\"\n\n    inputs = [\n        MessageTextInput(name='urls',\n        display_name='URLs',\n        info=\"Enter one or more URLs to crawl recursively,\n        by clicking the '+' button.\",\n        is_list=True,\n        tool_mode=True,\n        placeholder='Enter a URL...',\n        list_add_label='Add URL'),\n        IntInput(name='max_depth',\n        display_name='Max Depth',\n        info=\"Controls how many 'clicks' away from the initial page the crawler will go:\\n- depth 1: only the initial page\\n- depth 2: initial page + all pages linked directly from it\\n- depth 3: initial page + direct links + links found on those direct link pages\\nNote: This is about link traversal,\n        not URL path depth.\",\n        value=1,\n        required=False),\n        BoolInput(name='prevent_outside',\n        display_name='Prevent Outside',\n        info='If enabled,\n        only crawls URLs within the same domain as the root URL. This helps prevent the crawler from going to external websites.',\n        value=True,\n        required=False,\n        advanced=True),\n        BoolInput(name='use_async',\n        display_name='Use Async',\n        info='If enabled,\n        uses asynchronous loading which can be significantly faster but might use more system resources.',\n        value=True,\n        required=False,\n        advanced=True),\n        DropdownInput(name='format',\n        display_name='Output Format',\n        info=\"Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.\",\n        options=['Text',\n        'HTML'],\n        value='Text',\n        advanced=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='Timeout for the request in seconds.',\n        value=30,\n        required=False,\n        advanced=True),\n        TableInput(name='headers',\n        display_name='Headers',\n        info='The headers to send with the request',\n        table_schema=[{'name': 'key',\n        'display_name': 'Header',\n        'type': 'str',\n        'description': 'Header name'},\n        {'name': 'value',\n        'display_name': 'Value',\n        'type': 'str',\n        'description': 'Header value'}],\n        value=[{'key': 'User-Agent',\n        'value': get_settings_service().settings.user_agent}],\n        advanced=True,\n        input_types=['DataFrame'])\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Message',\n        name='text',\n        method='fetch_content_text'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def validate_url(self, string):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def ensure_url(self, url):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "URLComponent", "base_classes": ["Component"], "public_methods": ["def validate_url(self, string)", "def ensure_url(self, url)", "def fetch_content(self)", "def fetch_content_text(self)", "def as_dataframe(self)"], "imports": ["import re", "import httpx", "from bs4 import BeautifulSoup", "from langchain_community.document_loaders import RecursiveUrlLoader", "from loguru import logger", "from langflow.custom.custom_component.component import Component", "from langflow.helpers.data import data_to_text", "from langflow.inputs.inputs import TableInput", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.schema.message import Message", "from langflow.services.deps import get_settings_service"], "inputs": "[MessageTextInput(name='urls', display_name='URLs', info=\"Enter one or more URLs to crawl recursively, by clicking the '+' button.\", is_list=True, tool_mode=True, placeholder='Enter a URL...', list_add_label='Add URL'), IntInput(name='max_depth', display_name='Max Depth', info=\"Controls how many 'clicks' away from the initial page the crawler will go:\\n- depth 1: only the initial page\\n- depth 2: initial page + all pages linked directly from it\\n- depth 3: initial page + direct links + links found on those direct link pages\\nNote: This is about link traversal, not URL path depth.\", value=1, required=False), BoolInput(name='prevent_outside', display_name='Prevent Outside', info='If enabled, only crawls URLs within the same domain as the root URL. This helps prevent the crawler from going to external websites.', value=True, required=False, advanced=True), BoolInput(name='use_async', display_name='Use Async', info='If enabled, uses asynchronous loading which can be significantly faster but might use more system resources.', value=True, required=False, advanced=True), DropdownInput(name='format', display_name='Output Format', info=\"Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.\", options=['Text', 'HTML'], value='Text', advanced=True), IntInput(name='timeout', display_name='Timeout', info='Timeout for the request in seconds.', value=30, required=False, advanced=True), TableInput(name='headers', display_name='Headers', info='The headers to send with the request', table_schema=[{'name': 'key', 'display_name': 'Header', 'type': 'str', 'description': 'Header name'}, {'name': 'value', 'display_name': 'Value', 'type': 'str', 'description': 'Header value'}], value=[{'key': 'User-Agent', 'value': get_settings_service().settings.user_agent}], advanced=True, input_types=['DataFrame'])]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Message', name='text', method='fetch_content_text'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "URL", "name": "URLComponent", "description": "Load and parse child links from a root URL recursively", "icon": "layout-template"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/should_run_next.py", "section": "class::ShouldRunNextComponent", "content": "from langchain_core.messages import BaseMessage\nfrom langchain_core.prompts import PromptTemplate\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import LanguageModel, Text\n\nclass ShouldRunNextComponent(CustomComponent):\n    display_name: str = \"Should Run Next\"\n    description: str = \"Determines if a vertex is runnable.\"\n    name = \"ShouldRunNext\"\n\n    def build(self, llm, question, context, retries):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ShouldRunNextComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build(self, llm, question, context, retries)"], "imports": ["from langchain_core.messages import BaseMessage", "from langchain_core.prompts import PromptTemplate", "from langflow.custom import CustomComponent", "from langflow.field_typing import LanguageModel, Text"], "inputs": "", "outputs": "", "display_name": "Should Run Next", "name": "ShouldRunNext", "description": "Determines if a vertex is runnable.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/output_parser.py", "section": "class::OutputParserComponent", "content": "from langchain_core.output_parsers import CommaSeparatedListOutputParser\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.field_typing.constants import OutputParser\nfrom langflow.io import DropdownInput, Output\nfrom langflow.schema.message import Message\n\nclass OutputParserComponent(Component):\n    display_name: str = \"Output Parser\"\n    description: str = \"Transforms the output of an LLM into a specified format.\"\n    icon = \"type\"\n    name = \"OutputParser\"\n\n    inputs = [\n        DropdownInput(name='parser_type',\n        display_name='Parser',\n        options=['CSV'],\n        value='CSV')\n    ]\n\n    outputs = [\n        Output(display_name='Format Instructions',\n        name='format_instructions',\n        info='Pass to a prompt template to include formatting instructions for LLM responses.',\n        method='format_instructions'),\n        Output(display_name='Output Parser',\n        name='output_parser',\n        method='build_parser')\n    ]\n\n    def build_parser(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def format_instructions(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OutputParserComponent", "base_classes": ["Component"], "public_methods": ["def build_parser(self)", "def format_instructions(self)"], "imports": ["from langchain_core.output_parsers import CommaSeparatedListOutputParser", "from langflow.custom.custom_component.component import Component", "from langflow.field_typing.constants import OutputParser", "from langflow.io import DropdownInput, Output", "from langflow.schema.message import Message"], "inputs": "[DropdownInput(name='parser_type', display_name='Parser', options=['CSV'], value='CSV')]", "outputs": "[Output(display_name='Format Instructions', name='format_instructions', info='Pass to a prompt template to include formatting instructions for LLM responses.', method='format_instructions'), Output(display_name='Output Parser', name='output_parser', method='build_parser')]", "display_name": "Output Parser", "name": "OutputParser", "description": "Transforms the output of an LLM into a specified format.", "icon": "type"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_extract.py", "section": "class::TavilyExtractComponent", "content": "import httpx\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import BoolInput, DropdownInput, MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass TavilyExtractComponent(Component):\n    \"\"\"\n    Separate component specifically for Tavily Extract functionality.\n    \"\"\"\n\n    display_name: str = \"Tavily Extract API\"\n    description: str = \"**Tavily Extract** extract raw content from URLs.\"\n    icon = \"TavilyIcon\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Tavily API Key',\n        required=True,\n        info='Your Tavily API Key.'),\n        MessageTextInput(name='urls',\n        display_name='URLs',\n        info='Comma-separated list of URLs to extract content from.',\n        required=True),\n        DropdownInput(name='extract_depth',\n        display_name='Extract Depth',\n        info='The depth of the extraction process.',\n        options=['basic',\n        'advanced'],\n        value='basic',\n        advanced=True),\n        BoolInput(name='include_images',\n        display_name='Include Images',\n        info='Include a list of images extracted from the URLs.',\n        value=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Text',\n        name='text',\n        method='fetch_content_text')\n    ]\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TavilyExtractComponent", "base_classes": ["Component"], "public_methods": ["def fetch_content(self)", "def fetch_content_text(self)"], "imports": ["import httpx", "from loguru import logger", "from langflow.custom import Component", "from langflow.helpers.data import data_to_text", "from langflow.io import BoolInput, DropdownInput, MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[SecretStrInput(name='api_key', display_name='Tavily API Key', required=True, info='Your Tavily API Key.'), MessageTextInput(name='urls', display_name='URLs', info='Comma-separated list of URLs to extract content from.', required=True), DropdownInput(name='extract_depth', display_name='Extract Depth', info='The depth of the extraction process.', options=['basic', 'advanced'], value='basic', advanced=True), BoolInput(name='include_images', display_name='Include Images', info='Include a list of images extracted from the URLs.', value=False, advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Text', name='text', method='fetch_content_text')]", "display_name": "Tavily Extract API", "name": "", "description": "**Tavily Extract** extract raw content from URLs.", "icon": "TavilyIcon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/pegasus_index.py", "section": "class::TwelveLabsError", "content": "import time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass TwelveLabsError(Exception):\n    \"\"\"\n    Base exception for Twelve Labs errors.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "TwelveLabsError", "base_classes": ["Exception"], "public_methods": [], "imports": ["import time", "from concurrent.futures import ThreadPoolExecutor", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/pegasus_index.py", "section": "class::IndexCreationError", "content": "import time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass IndexCreationError(TwelveLabsError):\n    \"\"\"\n    Error raised when there's an issue with an index.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "IndexCreationError", "base_classes": ["TwelveLabsError"], "public_methods": [], "imports": ["import time", "from concurrent.futures import ThreadPoolExecutor", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/pegasus_index.py", "section": "class::TaskError", "content": "import time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass TaskError(TwelveLabsError):\n    \"\"\"\n    Error raised when a task fails.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "TaskError", "base_classes": ["TwelveLabsError"], "public_methods": [], "imports": ["import time", "from concurrent.futures import ThreadPoolExecutor", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/pegasus_index.py", "section": "class::TaskTimeoutError", "content": "import time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass TaskTimeoutError(TwelveLabsError):\n    \"\"\"\n    Error raised when a task times out.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "TaskTimeoutError", "base_classes": ["TwelveLabsError"], "public_methods": [], "imports": ["import time", "from concurrent.futures import ThreadPoolExecutor", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/twelvelabs/pegasus_index.py", "section": "class::PegasusIndexVideo", "content": "import time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\nfrom typing import Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom twelvelabs import TwelveLabs\nfrom langflow.custom import Component\nfrom langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass PegasusIndexVideo(Component):\n    \"\"\"\n    Indexes videos using Twelve Labs Pegasus API and adds the video ID to metadata.\n    \"\"\"\n\n    display_name: str = \"Twelve Labs Pegasus Index Video\"\n    description: str = \"Index videos using Twelve Labs and add the video_id to metadata.\"\n    icon = \"TwelveLabs\"\n    name = \"TwelveLabsPegasusIndexVideo\"\n\n    inputs = [\n        DataInput(name='videodata',\n        display_name='Video Data',\n        info='Video Data objects (from VideoFile or SplitVideo)',\n        is_list=True,\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='Twelve Labs API Key',\n        info='Enter your Twelve Labs API Key.',\n        required=True),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        info='Pegasus model to use for indexing',\n        options=['pegasus1.2'],\n        value='pegasus1.2',\n        advanced=False),\n        StrInput(name='index_name',\n        display_name='Index Name',\n        info=\"Name of the index to use. If the index doesn't exist,\n        it will be created.\",\n        required=False),\n        StrInput(name='index_id',\n        display_name='Index ID',\n        info='ID of an existing index to use. If provided,\n        index_name will be ignored.',\n        required=False)\n    ]\n\n    outputs = [\n        Output(display_name='Indexed Data',\n        name='indexed_data',\n        method='index_videos',\n        output_types=['Data'],\n        is_list=True)\n    ]\n\n    def on_task_update(self, task, video_path):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def index_videos(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PegasusIndexVideo", "base_classes": ["Component"], "public_methods": ["def on_task_update(self, task, video_path)", "def index_videos(self)"], "imports": ["import time", "from concurrent.futures import ThreadPoolExecutor", "from pathlib import Path", "from typing import Any", "from tenacity import retry, stop_after_attempt, wait_exponential", "from twelvelabs import TwelveLabs", "from langflow.custom import Component", "from langflow.inputs import DataInput, DropdownInput, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='videodata', display_name='Video Data', info='Video Data objects (from VideoFile or SplitVideo)', is_list=True, required=True), SecretStrInput(name='api_key', display_name='Twelve Labs API Key', info='Enter your Twelve Labs API Key.', required=True), DropdownInput(name='model_name', display_name='Model', info='Pegasus model to use for indexing', options=['pegasus1.2'], value='pegasus1.2', advanced=False), StrInput(name='index_name', display_name='Index Name', info=\"Name of the index to use. If the index doesn't exist, it will be created.\", required=False), StrInput(name='index_id', display_name='Index ID', info='ID of an existing index to use. If provided, index_name will be ignored.', required=False)]", "outputs": "[Output(display_name='Indexed Data', name='indexed_data', method='index_videos', output_types=['Data'], is_list=True)]", "display_name": "Twelve Labs Pegasus Index Video", "name": "TwelveLabsPegasusIndexVideo", "description": "Index videos using Twelve Labs and add the video_id to metadata.", "icon": "TwelveLabs"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/retriever.py", "section": "class::RetrieverToolComponent", "content": "from langchain_core.tools import create_retriever_tool\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import BaseRetriever, Tool\n\nclass RetrieverToolComponent(CustomComponent):\n    display_name: str = \"RetrieverTool\"\n    description: str = \"Tool for interacting with retriever\"\n    icon = \"LangChain\"\n    name = \"RetrieverTool\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, retriever, name, description):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RetrieverToolComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, retriever, name, description)"], "imports": ["from langchain_core.tools import create_retriever_tool", "from langflow.custom import CustomComponent", "from langflow.field_typing import BaseRetriever, Tool"], "inputs": "", "outputs": "", "display_name": "RetrieverTool", "name": "RetrieverTool", "description": "Tool for interacting with retriever", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/parse_json_data.py", "section": "class::ParseJSONDataComponent", "content": "import json\nfrom json import JSONDecodeError\nimport jq\nfrom json_repair import repair_json\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.inputs import HandleInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass ParseJSONDataComponent(Component):\n    display_name: str = \"Parse JSON\"\n    description: str = \"Convert and extract JSON fields.\"\n    icon = \"braces\"\n    name = \"ParseJSONData\"\n\n    inputs = [\n        HandleInput(name='input_value',\n        display_name='Input',\n        info='Data object to filter.',\n        required=True,\n        input_types=['Message',\n        'Data']),\n        MessageTextInput(name='query',\n        display_name='JQ Query',\n        info='JQ Query to filter the data. The input is always a JSON list.',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Filtered Data',\n        name='filtered_data',\n        method='filter_data')\n    ]\n\n    def filter_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ParseJSONDataComponent", "base_classes": ["Component"], "public_methods": ["def filter_data(self)"], "imports": ["import json", "from json import JSONDecodeError", "import jq", "from json_repair import repair_json", "from loguru import logger", "from langflow.custom import Component", "from langflow.inputs import HandleInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[HandleInput(name='input_value', display_name='Input', info='Data object to filter.', required=True, input_types=['Message', 'Data']), MessageTextInput(name='query', display_name='JQ Query', info='JQ Query to filter the data. The input is always a JSON list.', required=True)]", "outputs": "[Output(display_name='Filtered Data', name='filtered_data', method='filter_data')]", "display_name": "Parse JSON", "name": "ParseJSONData", "description": "Convert and extract JSON fields.", "icon": "braces"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/loop.py", "section": "class::LoopComponent", "content": "from langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data\n\nclass LoopComponent(Component):\n    display_name: str = \"Loop\"\n    description: str = \"Iterates over a list of Data objects, outputting one item at a time and aggregating results from loop inputs.\"\n    icon = \"infinity\"\n\n    inputs = [\n        DataInput(name='data',\n        display_name='Data',\n        info='The initial list of Data objects to iterate over.')\n    ]\n\n    outputs = [\n        Output(display_name='Item',\n        name='item',\n        method='item_output',\n        allows_loop=True),\n        Output(display_name='Done',\n        name='done',\n        method='done_output')\n    ]\n\n    def initialize_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def evaluate_stop_loop(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def item_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def done_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def loop_variables(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def aggregated_output(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LoopComponent", "base_classes": ["Component"], "public_methods": ["def initialize_data(self)", "def evaluate_stop_loop(self)", "def item_output(self)", "def done_output(self)", "def loop_variables(self)", "def aggregated_output(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import DataInput, Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='data', display_name='Data', info='The initial list of Data objects to iterate over.')]", "outputs": "[Output(display_name='Item', name='item', method='item_output', allows_loop=True), Output(display_name='Done', name='done', method='done_output')]", "display_name": "Loop", "name": "", "description": "Iterates over a list of Data objects, outputting one item at a time and aggregating results from loop inputs.", "icon": "infinity"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/openrouter.py", "section": "class::OpenRouterComponent", "content": "from collections import defaultdict\nfrom typing import Any\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom openai import BadRequestError\n\nclass OpenRouterComponent(LCModelComponent):\n    \"\"\"\n    OpenRouter API component for language models.\n    \"\"\"\n\n    display_name: str = \"OpenRouter\"\n    description: str = \"OpenRouter provides unified access to multiple AI models from different providers through a single API.\"\n    icon = \"OpenRouter\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name='api_key',\n        display_name='OpenRouter API Key',\n        required=True,\n        info='Your OpenRouter API key'),\n        StrInput(name='site_url',\n        display_name='Site URL',\n        info='Your site URL for OpenRouter rankings',\n        advanced=True),\n        StrInput(name='app_name',\n        display_name='App Name',\n        info='Your app name for OpenRouter rankings',\n        advanced=True),\n        DropdownInput(name='provider',\n        display_name='Provider',\n        info='The AI model provider',\n        options=['Loading providers...'],\n        value='Loading providers...',\n        real_time_refresh=True,\n        required=True),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        info='The model to use for chat completion',\n        options=['Select a provider first'],\n        value='Select a provider first',\n        real_time_refresh=True,\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.7,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        info='Controls randomness. Lower values are more deterministic,\n        higher values are more creative.',\n        advanced=True),\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        info='Maximum number of tokens to generate',\n        advanced=True)\n    ]\n\n    def fetch_models(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OpenRouterComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def fetch_models(self)", "def build_model(self)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from collections import defaultdict", "from typing import Any", "import httpx", "from langchain_openai import ChatOpenAI", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput", "from openai import BadRequestError"], "inputs": "[*LCModelComponent._base_inputs, SecretStrInput(name='api_key', display_name='OpenRouter API Key', required=True, info='Your OpenRouter API key'), StrInput(name='site_url', display_name='Site URL', info='Your site URL for OpenRouter rankings', advanced=True), StrInput(name='app_name', display_name='App Name', info='Your app name for OpenRouter rankings', advanced=True), DropdownInput(name='provider', display_name='Provider', info='The AI model provider', options=['Loading providers...'], value='Loading providers...', real_time_refresh=True, required=True), DropdownInput(name='model_name', display_name='Model', info='The model to use for chat completion', options=['Select a provider first'], value='Select a provider first', real_time_refresh=True, required=True), SliderInput(name='temperature', display_name='Temperature', value=0.7, range_spec=RangeSpec(min=0, max=2, step=0.01), info='Controls randomness. Lower values are more deterministic, higher values are more creative.', advanced=True), IntInput(name='max_tokens', display_name='Max Tokens', info='Maximum number of tokens to generate', advanced=True)]", "outputs": "", "display_name": "OpenRouter", "name": "", "description": "OpenRouter provides unified access to multiple AI models from different providers through a single API.", "icon": "OpenRouter"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/youtube/comments.py", "section": "class::YouTubeCommentsComponent", "content": "from contextlib import contextmanager\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import DataFrame\nfrom langflow.template import Output\nimport re\n\nclass YouTubeCommentsComponent(Component):\n    \"\"\"\n    A component that retrieves comments from YouTube videos.\n    \"\"\"\n\n    display_name: str = \"YouTube Comments\"\n    description: str = \"Retrieves and analyzes comments from YouTube videos.\"\n    icon = \"YouTube\"\n\n    inputs = [\n        MessageTextInput(name='video_url',\n        display_name='Video URL',\n        info='The URL of the YouTube video to get comments from.',\n        tool_mode=True,\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='YouTube API Key',\n        info='Your YouTube Data API key.',\n        required=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=20,\n        info='The maximum number of comments to return.'),\n        DropdownInput(name='sort_by',\n        display_name='Sort By',\n        options=['time',\n        'relevance'],\n        value='relevance',\n        info='Sort comments by time or relevance.'),\n        BoolInput(name='include_replies',\n        display_name='Include Replies',\n        value=False,\n        info='Whether to include replies to comments.',\n        advanced=True),\n        BoolInput(name='include_metrics',\n        display_name='Include Metrics',\n        value=True,\n        info='Include metrics like like count and reply count.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(name='comments',\n        display_name='Comments',\n        method='get_video_comments')\n    ]\n\n    def youtube_client(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_video_comments(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YouTubeCommentsComponent", "base_classes": ["Component"], "public_methods": ["def youtube_client(self)", "def get_video_comments(self)"], "imports": ["from contextlib import contextmanager", "import pandas as pd", "from googleapiclient.discovery import build", "from googleapiclient.errors import HttpError", "from langflow.custom import Component", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import DataFrame", "from langflow.template import Output", "import re"], "inputs": "[MessageTextInput(name='video_url', display_name='Video URL', info='The URL of the YouTube video to get comments from.', tool_mode=True, required=True), SecretStrInput(name='api_key', display_name='YouTube API Key', info='Your YouTube Data API key.', required=True), IntInput(name='max_results', display_name='Max Results', value=20, info='The maximum number of comments to return.'), DropdownInput(name='sort_by', display_name='Sort By', options=['time', 'relevance'], value='relevance', info='Sort comments by time or relevance.'), BoolInput(name='include_replies', display_name='Include Replies', value=False, info='Whether to include replies to comments.', advanced=True), BoolInput(name='include_metrics', display_name='Include Metrics', value=True, info='Include metrics like like count and reply count.', advanced=True)]", "outputs": "[Output(name='comments', display_name='Comments', method='get_video_comments')]", "display_name": "YouTube Comments", "name": "", "description": "Retrieves and analyzes comments from YouTube videos.", "icon": "YouTube"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/search.py", "section": "class::NotionSearch", "content": "from typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DropdownInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionSearch(LCToolComponent):\n    display_name: str = \"Search \"\n    description: str = \"Searches all pages and databases that have been shared with an integration.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True),\n        StrInput(name='query',\n        display_name='Search Query',\n        info='The text that the API compares page and database titles against.'),\n        DropdownInput(name='filter_value',\n        display_name='Filter Type',\n        info='Limits the results to either only pages or only databases.',\n        options=['page',\n        'database'],\n        value='page'),\n        DropdownInput(name='sort_direction',\n        display_name='Sort Direction',\n        info='The direction to sort the results.',\n        options=['ascending',\n        'descending'],\n        value='descending')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionSearch", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DropdownInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True), StrInput(name='query', display_name='Search Query', info='The text that the API compares page and database titles against.'), DropdownInput(name='filter_value', display_name='Filter Type', info='Limits the results to either only pages or only databases.', options=['page', 'database'], value='page'), DropdownInput(name='sort_direction', display_name='Sort Direction', info='The direction to sort the results.', options=['ascending', 'descending'], value='descending')]", "outputs": "", "display_name": "Search ", "name": "", "description": "Searches all pages and databases that have been shared with an integration.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/search.py", "section": "class::NotionSearchSchema", "content": "from typing import Any\nimport requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DropdownInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass NotionSearchSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionSearchSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from typing import Any", "import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DropdownInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/getenvvar.py", "section": "class::GetEnvVar", "content": "import os\nfrom langflow.custom import Component\nfrom langflow.inputs import StrInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass GetEnvVar(Component):\n    display_name: str = \"Get env var\"\n    description: str = \"Get env var\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        StrInput(name='env_var_name',\n        display_name='Env var name',\n        info='Name of the environment variable to get')\n    ]\n\n    outputs = [\n        Output(display_name='Env var value',\n        name='env_var_value',\n        method='process_inputs')\n    ]\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GetEnvVar", "base_classes": ["Component"], "public_methods": ["def process_inputs(self)"], "imports": ["import os", "from langflow.custom import Component", "from langflow.inputs import StrInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[StrInput(name='env_var_name', display_name='Env var name', info='Name of the environment variable to get')]", "outputs": "[Output(display_name='Env var value', name='env_var_value', method='process_inputs')]", "display_name": "Get env var", "name": "", "description": "Get env var", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/opensearch.py", "section": "class::OpenSearchVectorStoreComponent", "content": "import json\nfrom typing import Any\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, HandleInput, IntInput, MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\n\nclass OpenSearchVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    OpenSearch Vector Store with advanced, customizable search capabilities.\n    \"\"\"\n\n    display_name: str = \"OpenSearch\"\n    description: str = \"OpenSearch Vector Store with advanced, customizable search capabilities.\"\n    icon = \"OpenSearch\"\n    name = \"OpenSearch\"\n\n    inputs = [\n        StrInput(name='opensearch_url',\n        display_name='OpenSearch URL',\n        value='http://localhost:9200',\n        info='URL for OpenSearch cluster (e.g. https://192.168.1.1:9200).'),\n        StrInput(name='index_name',\n        display_name='Index Name',\n        value='langflow',\n        info='The index name where the vectors will be stored in OpenSearch cluster.'),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        options=['similarity',\n        'similarity_score_threshold',\n        'mmr'],\n        value='similarity',\n        advanced=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=4),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info='Minimum similarity score threshold for search results.',\n        value=0.0,\n        advanced=True),\n        StrInput(name='username',\n        display_name='Username',\n        value='admin',\n        advanced=True),\n        SecretStrInput(name='password',\n        display_name='Password',\n        value='admin',\n        advanced=True),\n        BoolInput(name='use_ssl',\n        display_name='Use SSL',\n        value=True,\n        advanced=True),\n        BoolInput(name='verify_certs',\n        display_name='Verify Certificates',\n        value=False,\n        advanced=True),\n        MultilineInput(name='hybrid_search_query',\n        display_name='Hybrid Search Query',\n        value='',\n        advanced=True,\n        info='Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching.')\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search(self, query):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OpenSearchVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search(self, query)", "def search_documents(self)"], "imports": ["import json", "from typing import Any", "from langchain_community.vectorstores import OpenSearchVectorSearch", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection", "from langflow.io import BoolInput, DropdownInput, FloatInput, HandleInput, IntInput, MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langchain_community.vectorstores import OpenSearchVectorSearch"], "inputs": "[StrInput(name='opensearch_url', display_name='OpenSearch URL', value='http://localhost:9200', info='URL for OpenSearch cluster (e.g. https://192.168.1.1:9200).'), StrInput(name='index_name', display_name='Index Name', value='langflow', info='The index name where the vectors will be stored in OpenSearch cluster.'), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), DropdownInput(name='search_type', display_name='Search Type', options=['similarity', 'similarity_score_threshold', 'mmr'], value='similarity', advanced=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=4), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info='Minimum similarity score threshold for search results.', value=0.0, advanced=True), StrInput(name='username', display_name='Username', value='admin', advanced=True), SecretStrInput(name='password', display_name='Password', value='admin', advanced=True), BoolInput(name='use_ssl', display_name='Use SSL', value=True, advanced=True), BoolInput(name='verify_certs', display_name='Verify Certificates', value=False, advanced=True), MultilineInput(name='hybrid_search_query', display_name='Hybrid Search Query', value='', advanced=True, info='Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching.')]", "outputs": "", "display_name": "OpenSearch", "name": "OpenSearch", "description": "OpenSearch Vector Store with advanced, customizable search capabilities.", "icon": "OpenSearch"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/file.py", "section": "class::FileComponent", "content": "from langflow.base.data import BaseFileComponent\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.io import BoolInput, IntInput\nfrom langflow.schema import Data\n\nclass FileComponent(BaseFileComponent):\n    \"\"\"\n    Handles loading and processing of individual or zipped text files.\n    \n    This component supports processing multiple valid files within a zip archive,\n    resolving paths, validating file types, and optionally using multithreading for processing.\n    \"\"\"\n\n    display_name: str = \"File\"\n    description: str = \"Load a file to be used in your project.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        *BaseFileComponent._base_inputs,\n        BoolInput(name='use_multithreading',\n        display_name='[Deprecated] Use Multithreading',\n        advanced=True,\n        value=True,\n        info=\"Set 'Processing Concurrency' greater than 1 to enable multithreading.\"),\n        IntInput(name='concurrency_multithreading',\n        display_name='Processing Concurrency',\n        advanced=True,\n        info='When multiple files are being processed,\n        the number of files to process concurrently.',\n        value=1)\n    ]\n\n    outputs = [\n        *BaseFileComponent._base_outputs\n    ]\n\n    def process_files(self, file_list):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FileComponent", "base_classes": ["BaseFileComponent"], "public_methods": ["def process_files(self, file_list)"], "imports": ["from langflow.base.data import BaseFileComponent", "from langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data", "from langflow.io import BoolInput, IntInput", "from langflow.schema import Data"], "inputs": "[*BaseFileComponent._base_inputs, BoolInput(name='use_multithreading', display_name='[Deprecated] Use Multithreading', advanced=True, value=True, info=\"Set 'Processing Concurrency' greater than 1 to enable multithreading.\"), IntInput(name='concurrency_multithreading', display_name='Processing Concurrency', advanced=True, info='When multiple files are being processed, the number of files to process concurrently.', value=1)]", "outputs": "[*BaseFileComponent._base_outputs]", "display_name": "File", "name": "File", "description": "Load a file to be used in your project.", "icon": "file-text"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/arxiv.py", "section": "class::ArXivComponent", "content": "import urllib.request\nfrom urllib.parse import urlparse\nfrom xml.etree.ElementTree import Element\nfrom defusedxml.ElementTree import fromstring\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data, DataFrame\n\nclass ArXivComponent(Component):\n    display_name: str = \"arXiv\"\n    description: str = \"Search and retrieve papers from arXiv.org\"\n    icon = \"arXiv\"\n\n    inputs = [\n        MessageTextInput(name='search_query',\n        display_name='Search Query',\n        info=\"The search query for arXiv papers (e.g.,\n        'quantum computing')\",\n        tool_mode=True),\n        DropdownInput(name='search_type',\n        display_name='Search Field',\n        info='The field to search in',\n        options=['all',\n        'title',\n        'abstract',\n        'author',\n        'cat'],\n        value='all'),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        info='Maximum number of results to return',\n        value=10)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='search_papers'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def build_query_url(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_atom_response(self, response_text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_papers(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ArXivComponent", "base_classes": ["Component"], "public_methods": ["def build_query_url(self)", "def parse_atom_response(self, response_text)", "def search_papers(self)", "def as_dataframe(self)"], "imports": ["import urllib.request", "from urllib.parse import urlparse", "from xml.etree.ElementTree import Element", "from defusedxml.ElementTree import fromstring", "from langflow.custom import Component", "from langflow.io import DropdownInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data, DataFrame"], "inputs": "[MessageTextInput(name='search_query', display_name='Search Query', info=\"The search query for arXiv papers (e.g., 'quantum computing')\", tool_mode=True), DropdownInput(name='search_type', display_name='Search Field', info='The field to search in', options=['all', 'title', 'abstract', 'author', 'cat'], value='all'), IntInput(name='max_results', display_name='Max Results', info='Maximum number of results to return', value=10)]", "outputs": "[Output(display_name='Data', name='data', method='search_papers'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "arXiv", "name": "", "description": "Search and retrieve papers from arXiv.org", "icon": "arXiv"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/arxiv.py", "section": "class::RestrictedHTTPHandler", "content": "import urllib.request\nfrom urllib.parse import urlparse\nfrom xml.etree.ElementTree import Element\nfrom defusedxml.ElementTree import fromstring\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data, DataFrame\n\nclass RestrictedHTTPHandler(urllib.request.HTTPHandler):\n\n    def http_open(self, req):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RestrictedHTTPHandler", "base_classes": ["urllib.request.HTTPHandler"], "public_methods": ["def http_open(self, req)"], "imports": ["import urllib.request", "from urllib.parse import urlparse", "from xml.etree.ElementTree import Element", "from defusedxml.ElementTree import fromstring", "from langflow.custom import Component", "from langflow.io import DropdownInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data, DataFrame"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/arxiv.py", "section": "class::RestrictedHTTPSHandler", "content": "import urllib.request\nfrom urllib.parse import urlparse\nfrom xml.etree.ElementTree import Element\nfrom defusedxml.ElementTree import fromstring\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data, DataFrame\n\nclass RestrictedHTTPSHandler(urllib.request.HTTPSHandler):\n\n    def https_open(self, req):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RestrictedHTTPSHandler", "base_classes": ["urllib.request.HTTPSHandler"], "public_methods": ["def https_open(self, req)"], "imports": ["import urllib.request", "from urllib.parse import urlparse", "from xml.etree.ElementTree import Element", "from defusedxml.ElementTree import fromstring", "from langflow.custom import Component", "from langflow.io import DropdownInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data, DataFrame"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/cloudflare.py", "section": "class::CloudflareWorkersAIEmbeddingsComponent", "content": "from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, IntInput, MessageTextInput, Output, SecretStrInput\n\nclass CloudflareWorkersAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Cloudflare Workers AI Embeddings\"\n    description: str = \"Generate embeddings using Cloudflare Workers AI models.\"\n    icon = \"Cloudflare\"\n    name = \"CloudflareWorkersAIEmbeddings\"\n\n    inputs = [\n        MessageTextInput(name='account_id',\n        display_name='Cloudflare account ID',\n        info='Find your account ID https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages',\n        required=True),\n        SecretStrInput(name='api_token',\n        display_name='Cloudflare API token',\n        info='Create an API token https://developers.cloudflare.com/fundamentals/api/get-started/create-token/',\n        required=True),\n        MessageTextInput(name='model_name',\n        display_name='Model Name',\n        info='List of supported models https://developers.cloudflare.com/workers-ai/models/#text-embeddings',\n        required=True,\n        value='@cf/baai/bge-base-en-v1.5'),\n        BoolInput(name='strip_new_lines',\n        display_name='Strip New Lines',\n        advanced=True,\n        value=True),\n        IntInput(name='batch_size',\n        display_name='Batch Size',\n        advanced=True,\n        value=50),\n        MessageTextInput(name='api_base_url',\n        display_name='Cloudflare API base URL',\n        advanced=True,\n        value='https://api.cloudflare.com/client/v4/accounts'),\n        DictInput(name='headers',\n        display_name='Headers',\n        info='Additional request headers',\n        is_list=True,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CloudflareWorkersAIEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import Embeddings", "from langflow.io import BoolInput, DictInput, IntInput, MessageTextInput, Output, SecretStrInput"], "inputs": "[MessageTextInput(name='account_id', display_name='Cloudflare account ID', info='Find your account ID https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages', required=True), SecretStrInput(name='api_token', display_name='Cloudflare API token', info='Create an API token https://developers.cloudflare.com/fundamentals/api/get-started/create-token/', required=True), MessageTextInput(name='model_name', display_name='Model Name', info='List of supported models https://developers.cloudflare.com/workers-ai/models/#text-embeddings', required=True, value='@cf/baai/bge-base-en-v1.5'), BoolInput(name='strip_new_lines', display_name='Strip New Lines', advanced=True, value=True), IntInput(name='batch_size', display_name='Batch Size', advanced=True, value=50), MessageTextInput(name='api_base_url', display_name='Cloudflare API base URL', advanced=True, value='https://api.cloudflare.com/client/v4/accounts'), DictInput(name='headers', display_name='Headers', info='Additional request headers', is_list=True, advanced=True)]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "Cloudflare Workers AI Embeddings", "name": "CloudflareWorkersAIEmbeddings", "description": "Generate embeddings using Cloudflare Workers AI models.", "icon": "Cloudflare"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/openai_tools.py", "section": "class::OpenAIToolsAgentComponent", "content": "from langchain.agents import create_openai_tools_agent\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MultilineInput\nfrom langflow.inputs.inputs import DataInput, HandleInput\nfrom langflow.schema import Data\n\nclass OpenAIToolsAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"OpenAI Tools Agent\"\n    description: str = \"Agent that uses tools via openai-tools.\"\n    icon = \"LangChain\"\n    name = \"OpenAIToolsAgent\"\n\n    inputs = [\n        *LCToolsAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel',\n        'ToolEnabledLanguageModel'],\n        required=True),\n        MultilineInput(name='system_prompt',\n        display_name='System Prompt',\n        info='System prompt for the agent.',\n        value='You are a helpful assistant'),\n        MultilineInput(name='user_prompt',\n        display_name='Prompt',\n        info=\"This prompt must contain 'input' key.\",\n        value='{input}'),\n        DataInput(name='chat_history',\n        display_name='Chat History',\n        is_list=True,\n        advanced=True)\n    ]\n\n    def get_chat_history_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_agent_runnable(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OpenAIToolsAgentComponent", "base_classes": ["LCToolsAgentComponent"], "public_methods": ["def get_chat_history_data(self)", "def create_agent_runnable(self)"], "imports": ["from langchain.agents import create_openai_tools_agent", "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate", "from langflow.base.agents.agent import LCToolsAgentComponent", "from langflow.inputs import MultilineInput", "from langflow.inputs.inputs import DataInput, HandleInput", "from langflow.schema import Data"], "inputs": "[*LCToolsAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel', 'ToolEnabledLanguageModel'], required=True), MultilineInput(name='system_prompt', display_name='System Prompt', info='System prompt for the agent.', value='You are a helpful assistant'), MultilineInput(name='user_prompt', display_name='Prompt', info=\"This prompt must contain 'input' key.\", value='{input}'), DataInput(name='chat_history', display_name='Chat History', is_list=True, advanced=True)]", "outputs": "", "display_name": "OpenAI Tools Agent", "name": "OpenAIToolsAgent", "description": "Agent that uses tools via openai-tools.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/json_cleaner.py", "section": "class::JSONCleaner", "content": "import json\nimport unicodedata\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\nfrom json_repair import repair_json\n\nclass JSONCleaner(Component):\n    display_name: str = \"JSON Cleaner\"\n    description: str = \"Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.\"\n    icon = \"braces\"\n\n    inputs = [\n        MessageTextInput(name='json_str',\n        display_name='JSON String',\n        info='The JSON string to be cleaned.',\n        required=True),\n        BoolInput(name='remove_control_chars',\n        display_name='Remove Control Characters',\n        info='Remove control characters from the JSON string.',\n        required=False),\n        BoolInput(name='normalize_unicode',\n        display_name='Normalize Unicode',\n        info='Normalize Unicode characters in the JSON string.',\n        required=False),\n        BoolInput(name='validate_json',\n        display_name='Validate JSON',\n        info='Validate the JSON string to ensure it is well-formed.',\n        required=False)\n    ]\n\n    outputs = [\n        Output(display_name='Cleaned JSON String',\n        name='output',\n        method='clean_json')\n    ]\n\n    def clean_json(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "JSONCleaner", "base_classes": ["Component"], "public_methods": ["def clean_json(self)"], "imports": ["import json", "import unicodedata", "from langflow.custom import Component", "from langflow.inputs import BoolInput, MessageTextInput", "from langflow.schema.message import Message", "from langflow.template import Output", "from json_repair import repair_json"], "inputs": "[MessageTextInput(name='json_str', display_name='JSON String', info='The JSON string to be cleaned.', required=True), BoolInput(name='remove_control_chars', display_name='Remove Control Characters', info='Remove control characters from the JSON string.', required=False), BoolInput(name='normalize_unicode', display_name='Normalize Unicode', info='Normalize Unicode characters in the JSON string.', required=False), BoolInput(name='validate_json', display_name='Validate JSON', info='Validate the JSON string to ensure it is well-formed.', required=False)]", "outputs": "[Output(display_name='Cleaned JSON String', name='output', method='clean_json')]", "display_name": "JSON Cleaner", "name": "", "description": "Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.", "icon": "braces"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/listen.py", "section": "class::ListenComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.schema import Data\n\nclass ListenComponent(CustomComponent):\n    display_name: str = \"Listen\"\n    description: str = \"A component to listen for a notification.\"\n    icon = \"Radio\"\n    name = \"Listen\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ListenComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, name)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "Listen", "name": "Listen", "description": "A component to listen for a notification.", "icon": "Radio"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/aiml.py", "section": "class::AIMLModelComponent", "content": "from langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\nfrom langflow.base.models.aiml_constants import AimlModels\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom openai.error import BadRequestError\n\nclass AIMLModelComponent(LCModelComponent):\n    display_name: str = \"AIML\"\n    description: str = \"Generates text using AIML LLMs.\"\n    icon = \"AIML\"\n    name = \"AIMLModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.',\n        range_spec=RangeSpec(min=0,\n        max=128000)),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=[],\n        refresh_button=True),\n        StrInput(name='aiml_api_base',\n        display_name='AIML API Base',\n        advanced=True,\n        info='The base URL of the OpenAI API. Defaults to https://api.aimlapi.com . You can change this to use other APIs like JinaChat,\n        LocalAI and Prem.'),\n        SecretStrInput(name='api_key',\n        display_name='AIML API Key',\n        info='The AIML API Key to use for the OpenAI model.',\n        advanced=False,\n        value='AIML_API_KEY',\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01))\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AIMLModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def build_model(self)"], "imports": ["from langchain_openai import ChatOpenAI", "from pydantic.v1 import SecretStr", "from typing_extensions import override", "from langflow.base.models.aiml_constants import AimlModels", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput", "from openai.error import BadRequestError"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.', range_spec=RangeSpec(min=0, max=128000)), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=[], refresh_button=True), StrInput(name='aiml_api_base', display_name='AIML API Base', advanced=True, info='The base URL of the OpenAI API. Defaults to https://api.aimlapi.com . You can change this to use other APIs like JinaChat, LocalAI and Prem.'), SecretStrInput(name='api_key', display_name='AIML API Key', info='The AIML API Key to use for the OpenAI model.', advanced=False, value='AIML_API_KEY', required=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01))]", "outputs": "", "display_name": "AIML", "name": "AIMLModel", "description": "Generates text using AIML LLMs.", "icon": "AIML"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/list_users.py", "section": "class::NotionUserList", "content": "import requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput\nfrom langflow.schema import Data\n\nclass NotionUserList(LCToolComponent):\n    display_name: str = \"List Users \"\n    description: str = \"Retrieve users from Notion.\"\n    icon = \"NotionDirectoryLoader\"\n\n    inputs = [\n        SecretStrInput(name='notion_secret',\n        display_name='Notion Secret',\n        info='The Notion integration token.',\n        required=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NotionUserList", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='notion_secret', display_name='Notion Secret', info='The Notion integration token.', required=True)]", "outputs": "", "display_name": "List Users ", "name": "", "description": "Retrieve users from Notion.", "icon": "NotionDirectoryLoader"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/Notion/list_users.py", "section": "class::NotionUserListSchema", "content": "import requests\nfrom langchain.tools import StructuredTool\nfrom pydantic import BaseModel\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import SecretStrInput\nfrom langflow.schema import Data\n\nclass NotionUserListSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "NotionUserListSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import requests", "from langchain.tools import StructuredTool", "from pydantic import BaseModel", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/astra_assistants/create_thread.py", "section": "class::AssistantsCreateThread", "content": "from langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass AssistantsCreateThread(ComponentWithCache):\n    display_name: str = \"Create Assistant Thread\"\n    description: str = \"Creates a thread and returns the thread id\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        MultilineInput(name='env_set',\n        display_name='Environment Set',\n        info='Dummy input to allow chaining with Dotenv Component.')\n    ]\n\n    outputs = [\n        Output(display_name='Thread ID',\n        name='thread_id',\n        method='process_inputs')\n    ]\n\n    def process_inputs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AssistantsCreateThread", "base_classes": ["ComponentWithCache"], "public_methods": ["def process_inputs(self)"], "imports": ["from langflow.base.astra_assistants.util import get_patched_openai_client", "from langflow.custom.custom_component.component_with_cache import ComponentWithCache", "from langflow.inputs import MultilineInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[MultilineInput(name='env_set', display_name='Environment Set', info='Dummy input to allow chaining with Dotenv Component.')]", "outputs": "[Output(display_name='Thread ID', name='thread_id', method='process_inputs')]", "display_name": "Create Assistant Thread", "name": "", "description": "Creates a thread and returns the thread id", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/supabase.py", "section": "class::SupabaseVectorStoreComponent", "content": "from langchain_community.vectorstores import SupabaseVectorStore\nfrom supabase.client import Client, create_client\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass SupabaseVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Supabase\"\n    description: str = \"Supabase Vector Store with search capabilities\"\n    icon = \"Supabase\"\n    name = \"SupabaseVectorStore\"\n\n    inputs = [\n        StrInput(name='supabase_url',\n        display_name='Supabase URL',\n        required=True),\n        SecretStrInput(name='supabase_service_key',\n        display_name='Supabase Service Key',\n        required=True),\n        StrInput(name='table_name',\n        display_name='Table Name',\n        advanced=True),\n        StrInput(name='query_name',\n        display_name='Query Name'),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SupabaseVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from langchain_community.vectorstores import SupabaseVectorStore", "from supabase.client import Client, create_client", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='supabase_url', display_name='Supabase URL', required=True), SecretStrInput(name='supabase_service_key', display_name='Supabase Service Key', required=True), StrInput(name='table_name', display_name='Table Name', advanced=True), StrInput(name='query_name', display_name='Query Name'), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Supabase", "name": "SupabaseVectorStore", "description": "Supabase Vector Store with search capabilities", "icon": "Supabase"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/directory.py", "section": "class::DirectoryComponent", "content": "from langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data, retrieve_file_paths\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, IntInput, MessageTextInput, MultiselectInput\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.template import Output\n\nclass DirectoryComponent(Component):\n    display_name: str = \"Directory\"\n    description: str = \"Recursively load files from a directory.\"\n    icon = \"folder\"\n    name = \"Directory\"\n\n    inputs = [\n        MessageTextInput(name='path',\n        display_name='Path',\n        info=\"Path to the directory to load files from. Defaults to current directory ('.')\",\n        value='.',\n        tool_mode=True),\n        MultiselectInput(name='types',\n        display_name='File Types',\n        info='File types to load. Select one or more types or leave empty to load all supported types.',\n        options=TEXT_FILE_TYPES,\n        value=[]),\n        IntInput(name='depth',\n        display_name='Depth',\n        info='Depth to search for files.',\n        value=0),\n        IntInput(name='max_concurrency',\n        display_name='Max Concurrency',\n        advanced=True,\n        info='Maximum concurrency for loading files.',\n        value=2),\n        BoolInput(name='load_hidden',\n        display_name='Load Hidden',\n        advanced=True,\n        info='If true,\n        hidden files will be loaded.'),\n        BoolInput(name='recursive',\n        display_name='Recursive',\n        advanced=True,\n        info='If true,\n        the search will be recursive.'),\n        BoolInput(name='silent_errors',\n        display_name='Silent Errors',\n        advanced=True,\n        info='If true,\n        errors will not raise an exception.'),\n        BoolInput(name='use_multithreading',\n        display_name='Use Multithreading',\n        advanced=True,\n        info='If true,\n        multithreading will be used.')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='load_directory'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def load_directory(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DirectoryComponent", "base_classes": ["Component"], "public_methods": ["def load_directory(self)", "def as_dataframe(self)"], "imports": ["from langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data, retrieve_file_paths", "from langflow.custom import Component", "from langflow.io import BoolInput, IntInput, MessageTextInput, MultiselectInput", "from langflow.schema import Data", "from langflow.schema.dataframe import DataFrame", "from langflow.template import Output"], "inputs": "[MessageTextInput(name='path', display_name='Path', info=\"Path to the directory to load files from. Defaults to current directory ('.')\", value='.', tool_mode=True), MultiselectInput(name='types', display_name='File Types', info='File types to load. Select one or more types or leave empty to load all supported types.', options=TEXT_FILE_TYPES, value=[]), IntInput(name='depth', display_name='Depth', info='Depth to search for files.', value=0), IntInput(name='max_concurrency', display_name='Max Concurrency', advanced=True, info='Maximum concurrency for loading files.', value=2), BoolInput(name='load_hidden', display_name='Load Hidden', advanced=True, info='If true, hidden files will be loaded.'), BoolInput(name='recursive', display_name='Recursive', advanced=True, info='If true, the search will be recursive.'), BoolInput(name='silent_errors', display_name='Silent Errors', advanced=True, info='If true, errors will not raise an exception.'), BoolInput(name='use_multithreading', display_name='Use Multithreading', advanced=True, info='If true, multithreading will be used.')]", "outputs": "[Output(display_name='Data', name='data', method='load_directory'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Directory", "name": "Directory", "description": "Recursively load files from a directory.", "icon": "folder"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/message.py", "section": "class::MessageComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\nclass MessageComponent(CustomComponent):\n    display_name: str = \"Message\"\n    description: str = \"Creates a Message object given a Session ID.\"\n    name = \"Message\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self, sender, sender_name, session_id, text):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MessageComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)", "def build(self, sender, sender_name, session_id, text)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.schema.message import Message", "from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER"], "inputs": "", "outputs": "", "display_name": "Message", "name": "Message", "description": "Creates a Message object given a Session ID.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/batch_run.py", "section": "class::BatchRunComponent", "content": "from __future__ import annotations\nfrom typing import TYPE_CHECKING, Any, cast\nimport toml\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DataFrameInput, HandleInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema import DataFrame\nfrom langchain_core.runnables import Runnable\n\nclass BatchRunComponent(Component):\n    display_name: str = \"Batch Run\"\n    description: str = \"Runs an LLM over each row of a DataFrame's column. If no column is set, the entire row is passed.\"\n    icon = \"List\"\n\n    inputs = [\n        HandleInput(name='model',\n        display_name='Language Model',\n        info=\"Connect the 'Language Model' output from your LLM component here.\",\n        input_types=['LanguageModel'],\n        required=True),\n        MultilineInput(name='system_message',\n        display_name='Instructions',\n        info='Multi-line system instruction for all rows in the DataFrame.',\n        required=False),\n        DataFrameInput(name='df',\n        display_name='DataFrame',\n        info=\"The DataFrame whose column (specified by 'column_name') we'll treat as text messages.\",\n        required=True),\n        MessageTextInput(name='column_name',\n        display_name='Column Name',\n        info='The name of the DataFrame column to treat as text messages. If empty,\n        all columns will be formatted in TOML.',\n        required=False,\n        advanced=False),\n        MessageTextInput(name='output_column_name',\n        display_name='Output Column Name',\n        info=\"Name of the column where the model's response will be stored.\",\n        value='model_response',\n        required=False,\n        advanced=True),\n        BoolInput(name='enable_metadata',\n        display_name='Enable Metadata',\n        info='If True,\n        add metadata to the output DataFrame.',\n        value=False,\n        required=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='DataFrame',\n        name='batch_results',\n        method='run_batch',\n        info=\"A DataFrame with all original columns plus the model's response column.\")\n    ]\n", "metadata": {"parser": "python_component", "class_name": "BatchRunComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["from __future__ import annotations", "from typing import TYPE_CHECKING, Any, cast", "import toml", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import BoolInput, DataFrameInput, HandleInput, MessageTextInput, MultilineInput, Output", "from langflow.schema import DataFrame", "from langchain_core.runnables import Runnable"], "inputs": "[HandleInput(name='model', display_name='Language Model', info=\"Connect the 'Language Model' output from your LLM component here.\", input_types=['LanguageModel'], required=True), MultilineInput(name='system_message', display_name='Instructions', info='Multi-line system instruction for all rows in the DataFrame.', required=False), DataFrameInput(name='df', display_name='DataFrame', info=\"The DataFrame whose column (specified by 'column_name') we'll treat as text messages.\", required=True), MessageTextInput(name='column_name', display_name='Column Name', info='The name of the DataFrame column to treat as text messages. If empty, all columns will be formatted in TOML.', required=False, advanced=False), MessageTextInput(name='output_column_name', display_name='Output Column Name', info=\"Name of the column where the model's response will be stored.\", value='model_response', required=False, advanced=True), BoolInput(name='enable_metadata', display_name='Enable Metadata', info='If True, add metadata to the output DataFrame.', value=False, required=False, advanced=True)]", "outputs": "[Output(display_name='DataFrame', name='batch_results', method='run_batch', info=\"A DataFrame with all original columns plus the model's response column.\")]", "display_name": "Batch Run", "name": "", "description": "Runs an LLM over each row of a DataFrame's column. If no column is set, the entire row is passed.", "icon": "List"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wikipedia_api.py", "section": "class::WikipediaAPIComponent", "content": "from typing import cast\nfrom langchain_community.tools import WikipediaQueryRun\nfrom langchain_community.utilities.wikipedia import WikipediaAPIWrapper\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, IntInput, MessageTextInput, MultilineInput\nfrom langflow.schema import Data\n\nclass WikipediaAPIComponent(LCToolComponent):\n    display_name: str = \"Wikipedia API [Deprecated]\"\n    description: str = \"Call Wikipedia API.\"\n    icon = \"Wikipedia\"\n    name = \"WikipediaAPI\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input'),\n        MessageTextInput(name='lang',\n        display_name='Language',\n        value='en'),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True),\n        BoolInput(name='load_all_available_meta',\n        display_name='Load all available meta',\n        value=False,\n        advanced=True),\n        IntInput(name='doc_content_chars_max',\n        display_name='Document content characters max',\n        value=4000,\n        advanced=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WikipediaAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["from typing import cast", "from langchain_community.tools import WikipediaQueryRun", "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import BoolInput, IntInput, MessageTextInput, MultilineInput", "from langflow.schema import Data"], "inputs": "[MultilineInput(name='input_value', display_name='Input'), MessageTextInput(name='lang', display_name='Language', value='en'), IntInput(name='k', display_name='Number of results', value=4, required=True), BoolInput(name='load_all_available_meta', display_name='Load all available meta', value=False, advanced=True), IntInput(name='doc_content_chars_max', display_name='Document content characters max', value=4000, advanced=True)]", "outputs": "", "display_name": "Wikipedia API [Deprecated]", "name": "WikipediaAPI", "description": "Call Wikipedia API.", "icon": "Wikipedia"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/astra_vectorize.py", "section": "class::AstraVectorizeComponent", "content": "from typing import Any\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DictInput, DropdownInput, MessageTextInput, SecretStrInput\nfrom langflow.template.field.base import Output\n\nclass AstraVectorizeComponent(Component):\n    display_name: str = \"Astra Vectorize [DEPRECATED]\"\n    description: str = \"Configuration options for Astra Vectorize server-side embeddings. This component is deprecated. Please use the Astra DB Component directly.\"\n    icon = \"AstraDB\"\n    name = \"AstraVectorize\"\n\n    inputs = [\n        DropdownInput(name='provider',\n        display_name='Provider',\n        options=VECTORIZE_PROVIDERS_MAPPING.keys(),\n        value='',\n        required=True),\n        MessageTextInput(name='model_name',\n        display_name='Model Name',\n        info=f'The embedding model to use for the selected provider. Each provider has a different set of models available (full list at https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html):\\n\\n{VECTORIZE_MODELS_STR}',\n        required=True),\n        MessageTextInput(name='api_key_name',\n        display_name='API Key name',\n        info=\"The name of the embeddings provider API key stored on Astra. If set,\n        it will override the 'ProviderKey' in the authentication parameters.\"),\n        DictInput(name='authentication',\n        display_name='Authentication parameters',\n        is_list=True,\n        advanced=True),\n        SecretStrInput(name='provider_api_key',\n        display_name='Provider API Key',\n        info=\"An alternative to the Astra Authentication that passes an API key for the provider with each request to Astra DB. This may be used when Vectorize is configured for the collection,\n        but no corresponding provider secret is stored within Astra's key management system.\",\n        advanced=True),\n        DictInput(name='authentication',\n        display_name='Authentication Parameters',\n        is_list=True,\n        advanced=True),\n        DictInput(name='model_parameters',\n        display_name='Model Parameters',\n        advanced=True,\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(display_name='Vectorize',\n        name='config',\n        method='build_options',\n        types=['dict'])\n    ]\n\n    def build_options(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AstraVectorizeComponent", "base_classes": ["Component"], "public_methods": ["def build_options(self)"], "imports": ["from typing import Any", "from langflow.custom import Component", "from langflow.inputs.inputs import DictInput, DropdownInput, MessageTextInput, SecretStrInput", "from langflow.template.field.base import Output"], "inputs": "[DropdownInput(name='provider', display_name='Provider', options=VECTORIZE_PROVIDERS_MAPPING.keys(), value='', required=True), MessageTextInput(name='model_name', display_name='Model Name', info=f'The embedding model to use for the selected provider. Each provider has a different set of models available (full list at https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html):\\n\\n{VECTORIZE_MODELS_STR}', required=True), MessageTextInput(name='api_key_name', display_name='API Key name', info=\"The name of the embeddings provider API key stored on Astra. If set, it will override the 'ProviderKey' in the authentication parameters.\"), DictInput(name='authentication', display_name='Authentication parameters', is_list=True, advanced=True), SecretStrInput(name='provider_api_key', display_name='Provider API Key', info=\"An alternative to the Astra Authentication that passes an API key for the provider with each request to Astra DB. This may be used when Vectorize is configured for the collection, but no corresponding provider secret is stored within Astra's key management system.\", advanced=True), DictInput(name='authentication', display_name='Authentication Parameters', is_list=True, advanced=True), DictInput(name='model_parameters', display_name='Model Parameters', advanced=True, is_list=True)]", "outputs": "[Output(display_name='Vectorize', name='config', method='build_options', types=['dict'])]", "display_name": "Astra Vectorize [DEPRECATED]", "name": "AstraVectorize", "description": "Configuration options for Astra Vectorize server-side embeddings. This component is deprecated. Please use the Astra DB Component directly.", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/sql_generator.py", "section": "class::SQLGeneratorComponent", "content": "from typing import TYPE_CHECKING\nfrom langchain.chains import create_sql_query_chain\nfrom langchain_core.prompts import PromptTemplate\nfrom langflow.base.chains.model import LCChainComponent\nfrom langflow.field_typing import Message\nfrom langflow.inputs import HandleInput, IntInput, MultilineInput\nfrom langflow.template import Output\nfrom langchain_core.runnables import Runnable\n\nclass SQLGeneratorComponent(LCChainComponent):\n    display_name: str = \"Natural Language to SQL\"\n    description: str = \"Generate SQL from natural language.\"\n    icon = \"LangChain\"\n    name = \"SQLGenerator\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input',\n        info='The input value to pass to the chain.',\n        required=True),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        HandleInput(name='db',\n        display_name='SQLDatabase',\n        input_types=['SQLDatabase'],\n        required=True),\n        IntInput(name='top_k',\n        display_name='Top K',\n        info='The number of results per select statement to return.',\n        value=5),\n        MultilineInput(name='prompt',\n        display_name='Prompt',\n        info='The prompt must contain `{question}`.')\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='text',\n        method='invoke_chain')\n    ]\n\n    def invoke_chain(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SQLGeneratorComponent", "base_classes": ["LCChainComponent"], "public_methods": ["def invoke_chain(self)"], "imports": ["from typing import TYPE_CHECKING", "from langchain.chains import create_sql_query_chain", "from langchain_core.prompts import PromptTemplate", "from langflow.base.chains.model import LCChainComponent", "from langflow.field_typing import Message", "from langflow.inputs import HandleInput, IntInput, MultilineInput", "from langflow.template import Output", "from langchain_core.runnables import Runnable"], "inputs": "[MultilineInput(name='input_value', display_name='Input', info='The input value to pass to the chain.', required=True), HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), HandleInput(name='db', display_name='SQLDatabase', input_types=['SQLDatabase'], required=True), IntInput(name='top_k', display_name='Top K', info='The number of results per select statement to return.', value=5), MultilineInput(name='prompt', display_name='Prompt', info='The prompt must contain `{question}`.')]", "outputs": "[Output(display_name='Message', name='text', method='invoke_chain')]", "display_name": "Natural Language to SQL", "name": "SQLGenerator", "description": "Generate SQL from natural language.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/logic/pass_message.py", "section": "class::PassMessageComponent", "content": "from langflow.custom import Component\nfrom langflow.io import MessageInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass PassMessageComponent(Component):\n    display_name: str = \"Pass\"\n    description: str = \"Forwards the input message, unchanged.\"\n    icon = \"arrow-right\"\n    name = \"Pass\"\n\n    inputs = [\n        MessageInput(name='input_message',\n        display_name='Input Message',\n        info='The message to be passed forward.',\n        required=True),\n        MessageInput(name='ignored_message',\n        display_name='Ignored Message',\n        info='A second message to be ignored. Used as a workaround for continuity.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Output Message',\n        name='output_message',\n        method='pass_message')\n    ]\n\n    def pass_message(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PassMessageComponent", "base_classes": ["Component"], "public_methods": ["def pass_message(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import MessageInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[MessageInput(name='input_message', display_name='Input Message', info='The message to be passed forward.', required=True), MessageInput(name='ignored_message', display_name='Ignored Message', info='A second message to be ignored. Used as a workaround for continuity.', advanced=True)]", "outputs": "[Output(display_name='Output Message', name='output_message', method='pass_message')]", "display_name": "Pass", "name": "Pass", "description": "Forwards the input message, unchanged.", "icon": "arrow-right"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/novita.py", "section": "class::NovitaModelComponent", "content": "import requests\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.novita_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import HandleInput\n\nclass NovitaModelComponent(LCModelComponent):\n    display_name: str = \"Novita AI\"\n    description: str = \"Generates text using Novita AI LLMs (OpenAI compatible).\"\n    icon = \"Novita\"\n    name = \"NovitaModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.',\n        range_spec=RangeSpec(min=0,\n        max=128000)),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True,\n        info='Additional keyword arguments to pass to the model.'),\n        BoolInput(name='json_mode',\n        display_name='JSON Mode',\n        advanced=True,\n        info='If True,\n        it will output JSON regardless of passing a schema.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=MODEL_NAMES,\n        value=MODEL_NAMES[0],\n        refresh_button=True),\n        SecretStrInput(name='api_key',\n        display_name='Novita API Key',\n        info='The Novita API Key to use for Novita AI models.',\n        advanced=False,\n        value='NOVITA_API_KEY',\n        real_time_refresh=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=1)),\n        IntInput(name='seed',\n        display_name='Seed',\n        info='The seed controls the reproducibility of the job.',\n        advanced=True,\n        value=1),\n        HandleInput(name='output_parser',\n        display_name='Output Parser',\n        info='The parser to use to parse the output of the model',\n        advanced=True,\n        input_types=['OutputParser'])\n    ]\n\n    def get_models(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "NovitaModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def get_models(self)", "def update_build_config(self, build_config, field_value, field_name)", "def build_model(self)"], "imports": ["import requests", "from langchain_openai import ChatOpenAI", "from pydantic.v1 import SecretStr", "from typing_extensions import override", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.novita_constants import MODEL_NAMES", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput", "from langflow.inputs.inputs import HandleInput"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.', range_spec=RangeSpec(min=0, max=128000)), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True, info='Additional keyword arguments to pass to the model.'), BoolInput(name='json_mode', display_name='JSON Mode', advanced=True, info='If True, it will output JSON regardless of passing a schema.'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0], refresh_button=True), SecretStrInput(name='api_key', display_name='Novita API Key', info='The Novita API Key to use for Novita AI models.', advanced=False, value='NOVITA_API_KEY', real_time_refresh=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=1)), IntInput(name='seed', display_name='Seed', info='The seed controls the reproducibility of the job.', advanced=True, value=1), HandleInput(name='output_parser', display_name='Output Parser', info='The parser to use to parse the output of the model', advanced=True, input_types=['OutputParser'])]", "outputs": "", "display_name": "Novita AI", "name": "NovitaModel", "description": "Generates text using Novita AI LLMs (OpenAI compatible).", "icon": "Novita"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/qdrant.py", "section": "class::QdrantVectorStoreComponent", "content": "from langchain.embeddings.base import Embeddings\nfrom langchain_community.vectorstores import Qdrant\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom qdrant_client import QdrantClient\n\nclass QdrantVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Qdrant\"\n    description: str = \"Qdrant Vector Store with search capabilities\"\n    icon = \"Qdrant\"\n\n    inputs = [\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        required=True),\n        StrInput(name='host',\n        display_name='Host',\n        value='localhost',\n        advanced=True),\n        IntInput(name='port',\n        display_name='Port',\n        value=6333,\n        advanced=True),\n        IntInput(name='grpc_port',\n        display_name='gRPC Port',\n        value=6334,\n        advanced=True),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        advanced=True),\n        StrInput(name='prefix',\n        display_name='Prefix',\n        advanced=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        advanced=True),\n        StrInput(name='path',\n        display_name='Path',\n        advanced=True),\n        StrInput(name='url',\n        display_name='URL',\n        advanced=True),\n        DropdownInput(name='distance_func',\n        display_name='Distance Function',\n        options=['Cosine',\n        'Euclidean',\n        'Dot Product'],\n        value='Cosine',\n        advanced=True),\n        StrInput(name='content_payload_key',\n        display_name='Content Payload Key',\n        value='page_content',\n        advanced=True),\n        StrInput(name='metadata_payload_key',\n        display_name='Metadata Payload Key',\n        value='metadata',\n        advanced=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "QdrantVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from langchain.embeddings.base import Embeddings", "from langchain_community.vectorstores import Qdrant", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from qdrant_client import QdrantClient"], "inputs": "[StrInput(name='collection_name', display_name='Collection Name', required=True), StrInput(name='host', display_name='Host', value='localhost', advanced=True), IntInput(name='port', display_name='Port', value=6333, advanced=True), IntInput(name='grpc_port', display_name='gRPC Port', value=6334, advanced=True), SecretStrInput(name='api_key', display_name='API Key', advanced=True), StrInput(name='prefix', display_name='Prefix', advanced=True), IntInput(name='timeout', display_name='Timeout', advanced=True), StrInput(name='path', display_name='Path', advanced=True), StrInput(name='url', display_name='URL', advanced=True), DropdownInput(name='distance_func', display_name='Distance Function', options=['Cosine', 'Euclidean', 'Dot Product'], value='Cosine', advanced=True), StrInput(name='content_payload_key', display_name='Content Payload Key', value='page_content', advanced=True), StrInput(name='metadata_payload_key', display_name='Metadata Payload Key', value='metadata', advanced=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Qdrant", "name": "", "description": "Qdrant Vector Store with search capabilities", "icon": "Qdrant"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/data/csv_to_data.py", "section": "class::CSVToDataComponent", "content": "import csv\nimport io\nfrom pathlib import Path\nfrom langflow.custom import Component\nfrom langflow.io import FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema import Data\n\nclass CSVToDataComponent(Component):\n    display_name: str = \"Load CSV\"\n    description: str = \"Load a CSV file, CSV from a file path, or a valid CSV string and convert it to a list of Data\"\n    icon = \"file-spreadsheet\"\n    name = \"CSVtoData\"\n\n    inputs = [\n        FileInput(name='csv_file',\n        display_name='CSV File',\n        file_types=['csv'],\n        info='Upload a CSV file to convert to a list of Data objects'),\n        MessageTextInput(name='csv_path',\n        display_name='CSV File Path',\n        info='Provide the path to the CSV file as pure text'),\n        MultilineInput(name='csv_string',\n        display_name='CSV String',\n        info='Paste a CSV string directly to convert to a list of Data objects'),\n        MessageTextInput(name='text_key',\n        display_name='Text Key',\n        info=\"The key to use for the text column. Defaults to 'text'.\",\n        value='text')\n    ]\n\n    outputs = [\n        Output(name='data_list',\n        display_name='Data List',\n        method='load_csv_to_data')\n    ]\n\n    def load_csv_to_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CSVToDataComponent", "base_classes": ["Component"], "public_methods": ["def load_csv_to_data(self)"], "imports": ["import csv", "import io", "from pathlib import Path", "from langflow.custom import Component", "from langflow.io import FileInput, MessageTextInput, MultilineInput, Output", "from langflow.schema import Data"], "inputs": "[FileInput(name='csv_file', display_name='CSV File', file_types=['csv'], info='Upload a CSV file to convert to a list of Data objects'), MessageTextInput(name='csv_path', display_name='CSV File Path', info='Provide the path to the CSV file as pure text'), MultilineInput(name='csv_string', display_name='CSV String', info='Paste a CSV string directly to convert to a list of Data objects'), MessageTextInput(name='text_key', display_name='Text Key', info=\"The key to use for the text column. Defaults to 'text'.\", value='text')]", "outputs": "[Output(name='data_list', display_name='Data List', method='load_csv_to_data')]", "display_name": "Load CSV", "name": "CSVtoData", "description": "Load a CSV file, CSV from a file path, or a valid CSV string and convert it to a list of Data", "icon": "file-spreadsheet"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/code_block_extractor.py", "section": "class::CodeBlockExtractor", "content": "import re\nfrom langflow.custom import Component\nfrom langflow.field_typing import Input, Output, Text\n\nclass CodeBlockExtractor(Component):\n    display_name: str = \"Code Block Extractor\"\n    description: str = \"Extracts code block from text.\"\n    name = \"CodeBlockExtractor\"\n\n    inputs = [\n        Input(name='text',\n        field_type=Text,\n        description='Text to extract code blocks from.')\n    ]\n\n    outputs = [\n        Output(name='code_block',\n        display_name='Code Block',\n        method='get_code_block')\n    ]\n\n    def get_code_block(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CodeBlockExtractor", "base_classes": ["Component"], "public_methods": ["def get_code_block(self)"], "imports": ["import re", "from langflow.custom import Component", "from langflow.field_typing import Input, Output, Text"], "inputs": "[Input(name='text', field_type=Text, description='Text to extract code blocks from.')]", "outputs": "[Output(name='code_block', display_name='Code Block', method='get_code_block')]", "display_name": "Code Block Extractor", "name": "CodeBlockExtractor", "description": "Extracts code block from text.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/helpers/current_date.py", "section": "class::CurrentDateComponent", "content": "from datetime import datetime\nfrom zoneinfo import ZoneInfo, available_timezones\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, Output\nfrom langflow.schema.message import Message\n\nclass CurrentDateComponent(Component):\n    display_name: str = \"Current Date\"\n    description: str = \"Returns the current date and time in the selected timezone.\"\n    icon = \"clock\"\n    name = \"CurrentDate\"\n\n    inputs = [\n        DropdownInput(name='timezone',\n        display_name='Timezone',\n        options=list(available_timezones()),\n        value='UTC',\n        info='Select the timezone for the current date and time.',\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Current Date',\n        name='current_date',\n        method='get_current_date')\n    ]\n\n    def get_current_date(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CurrentDateComponent", "base_classes": ["Component"], "public_methods": ["def get_current_date(self)"], "imports": ["from datetime import datetime", "from zoneinfo import ZoneInfo, available_timezones", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import DropdownInput, Output", "from langflow.schema.message import Message"], "inputs": "[DropdownInput(name='timezone', display_name='Timezone', options=list(available_timezones()), value='UTC', info='Select the timezone for the current date and time.', tool_mode=True)]", "outputs": "[Output(display_name='Current Date', name='current_date', method='get_current_date')]", "display_name": "Current Date", "name": "CurrentDate", "description": "Returns the current date and time in the selected timezone.", "icon": "clock"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/astradb.py", "section": "class::AstraDBToolComponent", "content": "import os\nfrom datetime import datetime, timezone\nfrom typing import Any\nfrom astrapy import Collection, DataAPIClient, Database\nfrom astrapy.admin import parse_api_endpoint\nfrom langchain.pydantic_v1 import BaseModel, Field, create_model\nfrom langchain_core.tools import StructuredTool, Tool\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.io import BoolInput, DictInput, HandleInput, IntInput, SecretStrInput, StrInput, TableInput\nfrom langflow.logging import logger\nfrom langflow.schema import Data\nfrom langflow.schema.table import EditMode\n\nclass AstraDBToolComponent(LCToolComponent):\n    display_name: str = \"Astra DB Tool\"\n    description: str = \"Tool to run hybrid vector and metadata search on DataStax Astra DB Collection\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        StrInput(name='tool_name',\n        display_name='Tool Name',\n        info='The name of the tool to be passed to the LLM.',\n        required=True),\n        StrInput(name='tool_description',\n        display_name='Tool Description',\n        info='Describe the tool to LLM. Add any information that can help the LLM to use the tool.',\n        required=True),\n        StrInput(name='keyspace',\n        display_name='Keyspace Name',\n        info='The name of the keyspace within Astra where the collection is stored.',\n        value='default_keyspace',\n        advanced=True),\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        info='The name of the collection within Astra DB where the vectors will be stored.',\n        required=True),\n        SecretStrInput(name='token',\n        display_name='Astra DB Application Token',\n        info='Authentication token for accessing Astra DB.',\n        value='ASTRA_DB_APPLICATION_TOKEN',\n        required=True),\n        SecretStrInput(name='api_endpoint',\n        display_name='Database' if os.getenv('ASTRA_ENHANCED',\n        'false').lower() == 'true' else 'API Endpoint',\n        info='API endpoint URL for the Astra DB service.',\n        value='ASTRA_DB_API_ENDPOINT',\n        required=True),\n        StrInput(name='projection_attributes',\n        display_name='Projection Attributes',\n        info='Attributes to be returned by the tool separated by comma.',\n        required=True,\n        value='*',\n        advanced=True),\n        TableInput(name='tools_params_v2',\n        display_name='Tools Parameters',\n        info='Define the structure for the tool parameters. Describe the parameters in a way the LLM can understand how to use them.',\n        required=False,\n        table_schema=[{'name': 'name',\n        'display_name': 'Name',\n        'type': 'str',\n        'description': 'Specify the name of the output field/parameter for the model.',\n        'default': 'field',\n        'edit_mode': EditMode.INLINE},\n        {'name': 'attribute_name',\n        'display_name': 'Attribute Name',\n        'type': 'str',\n        'description': 'Specify the attribute name to be filtered on the collection. Leave empty if the attribute name is the same as the name of the field.',\n        'default': '',\n        'edit_mode': EditMode.INLINE},\n        {'name': 'description',\n        'display_name': 'Description',\n        'type': 'str',\n        'description': 'Describe the purpose of the output field.',\n        'default': 'description of field',\n        'edit_mode': EditMode.POPOVER},\n        {'name': 'metadata',\n        'display_name': 'Is Metadata',\n        'type': 'boolean',\n        'edit_mode': EditMode.INLINE,\n        'description': 'Indicate if the field is included in the metadata field.',\n        'options': ['True',\n        'False'],\n        'default': 'False'},\n        {'name': 'mandatory',\n        'display_name': 'Is Mandatory',\n        'type': 'boolean',\n        'edit_mode': EditMode.INLINE,\n        'description': 'Indicate if the field is mandatory.',\n        'options': ['True',\n        'False'],\n        'default': 'False'},\n        {'name': 'is_timestamp',\n        'display_name': 'Is Timestamp',\n        'type': 'boolean',\n        'edit_mode': EditMode.INLINE,\n        'description': 'Indicate if the field is a timestamp.',\n        'options': ['True',\n        'False'],\n        'default': 'False'},\n        {'name': 'operator',\n        'display_name': 'Operator',\n        'type': 'str',\n        'description': 'Set the operator for the field. https://docs.datastax.com/en/astra-db-serverless/api-reference/documents.html#operators',\n        'default': '$eq',\n        'options': ['$gt',\n        '$gte',\n        '$lt',\n        '$lte',\n        '$eq',\n        '$ne',\n        '$in',\n        '$nin',\n        '$exists',\n        '$all',\n        '$size'],\n        'edit_mode': EditMode.INLINE}],\n        value=[]),\n        DictInput(name='tool_params',\n        info='DEPRECATED: Attributes to filter and description to the model. Add ! for mandatory (e.g: !customerId)',\n        display_name='Tool params',\n        is_list=True,\n        advanced=True),\n        DictInput(name='static_filters',\n        info='Attributes to filter and correspoding value',\n        display_name='Static filters',\n        advanced=True,\n        is_list=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=5),\n        BoolInput(name='use_search_query',\n        display_name='Semantic Search',\n        info='When this parameter is activated,\n        the search query parameter will be used to search the collection.',\n        advanced=False,\n        value=False),\n        BoolInput(name='use_vectorize',\n        display_name='Use Astra DB Vectorize',\n        info='When this parameter is activated,\n        Astra DB Vectorize method will be used to generate the embeddings.',\n        advanced=False,\n        value=False),\n        HandleInput(name='embedding',\n        display_name='Embedding Model',\n        input_types=['Embeddings']),\n        StrInput(name='semantic_search_instruction',\n        display_name='Semantic Search Instruction',\n        info='The instruction to use for the semantic search.',\n        required=True,\n        value='Search query to find relevant documents.',\n        advanced=True)\n    ]\n\n    def create_args_schema(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_args_schema_v2(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def projection_args(self, input_str):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_timestamp(self, timestamp_str):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_filter(self, args, filter_settings):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AstraDBToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def create_args_schema(self)", "def create_args_schema_v2(self)", "def build_tool(self)", "def projection_args(self, input_str)", "def parse_timestamp(self, timestamp_str)", "def build_filter(self, args, filter_settings)", "def run_model(self)"], "imports": ["import os", "from datetime import datetime, timezone", "from typing import Any", "from astrapy import Collection, DataAPIClient, Database", "from astrapy.admin import parse_api_endpoint", "from langchain.pydantic_v1 import BaseModel, Field, create_model", "from langchain_core.tools import StructuredTool, Tool", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.io import BoolInput, DictInput, HandleInput, IntInput, SecretStrInput, StrInput, TableInput", "from langflow.logging import logger", "from langflow.schema import Data", "from langflow.schema.table import EditMode"], "inputs": "[StrInput(name='tool_name', display_name='Tool Name', info='The name of the tool to be passed to the LLM.', required=True), StrInput(name='tool_description', display_name='Tool Description', info='Describe the tool to LLM. Add any information that can help the LLM to use the tool.', required=True), StrInput(name='keyspace', display_name='Keyspace Name', info='The name of the keyspace within Astra where the collection is stored.', value='default_keyspace', advanced=True), StrInput(name='collection_name', display_name='Collection Name', info='The name of the collection within Astra DB where the vectors will be stored.', required=True), SecretStrInput(name='token', display_name='Astra DB Application Token', info='Authentication token for accessing Astra DB.', value='ASTRA_DB_APPLICATION_TOKEN', required=True), SecretStrInput(name='api_endpoint', display_name='Database' if os.getenv('ASTRA_ENHANCED', 'false').lower() == 'true' else 'API Endpoint', info='API endpoint URL for the Astra DB service.', value='ASTRA_DB_API_ENDPOINT', required=True), StrInput(name='projection_attributes', display_name='Projection Attributes', info='Attributes to be returned by the tool separated by comma.', required=True, value='*', advanced=True), TableInput(name='tools_params_v2', display_name='Tools Parameters', info='Define the structure for the tool parameters. Describe the parameters in a way the LLM can understand how to use them.', required=False, table_schema=[{'name': 'name', 'display_name': 'Name', 'type': 'str', 'description': 'Specify the name of the output field/parameter for the model.', 'default': 'field', 'edit_mode': EditMode.INLINE}, {'name': 'attribute_name', 'display_name': 'Attribute Name', 'type': 'str', 'description': 'Specify the attribute name to be filtered on the collection. Leave empty if the attribute name is the same as the name of the field.', 'default': '', 'edit_mode': EditMode.INLINE}, {'name': 'description', 'display_name': 'Description', 'type': 'str', 'description': 'Describe the purpose of the output field.', 'default': 'description of field', 'edit_mode': EditMode.POPOVER}, {'name': 'metadata', 'display_name': 'Is Metadata', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is included in the metadata field.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'mandatory', 'display_name': 'Is Mandatory', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is mandatory.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'is_timestamp', 'display_name': 'Is Timestamp', 'type': 'boolean', 'edit_mode': EditMode.INLINE, 'description': 'Indicate if the field is a timestamp.', 'options': ['True', 'False'], 'default': 'False'}, {'name': 'operator', 'display_name': 'Operator', 'type': 'str', 'description': 'Set the operator for the field. https://docs.datastax.com/en/astra-db-serverless/api-reference/documents.html#operators', 'default': '$eq', 'options': ['$gt', '$gte', '$lt', '$lte', '$eq', '$ne', '$in', '$nin', '$exists', '$all', '$size'], 'edit_mode': EditMode.INLINE}], value=[]), DictInput(name='tool_params', info='DEPRECATED: Attributes to filter and description to the model. Add ! for mandatory (e.g: !customerId)', display_name='Tool params', is_list=True, advanced=True), DictInput(name='static_filters', info='Attributes to filter and correspoding value', display_name='Static filters', advanced=True, is_list=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=5), BoolInput(name='use_search_query', display_name='Semantic Search', info='When this parameter is activated, the search query parameter will be used to search the collection.', advanced=False, value=False), BoolInput(name='use_vectorize', display_name='Use Astra DB Vectorize', info='When this parameter is activated, Astra DB Vectorize method will be used to generate the embeddings.', advanced=False, value=False), HandleInput(name='embedding', display_name='Embedding Model', input_types=['Embeddings']), StrInput(name='semantic_search_instruction', display_name='Semantic Search Instruction', info='The instruction to use for the semantic search.', required=True, value='Search query to find relevant documents.', advanced=True)]", "outputs": "", "display_name": "Astra DB Tool", "name": "", "description": "Tool to run hybrid vector and metadata search on DataStax Astra DB Collection", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/google_generative_ai.py", "section": "class::GoogleGenerativeAIEmbeddingsComponent", "content": "from google.ai.generativelanguage_v1beta.types import BatchEmbedContentsRequest\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_google_genai._common import GoogleGenerativeAIError\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\nclass GoogleGenerativeAIEmbeddingsComponent(Component):\n    display_name: str = \"Google Generative AI Embeddings\"\n    description: str = \"Connect to Google's generative AI embeddings service using the GoogleGenerativeAIEmbeddings class, found in the langchain-google-genai package.\"\n    icon = \"Google\"\n    name = \"Google Generative AI Embeddings\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True),\n        MessageTextInput(name='model_name',\n        display_name='Model Name',\n        value='models/text-embedding-004')\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleGenerativeAIEmbeddingsComponent", "base_classes": ["Component"], "public_methods": ["def build_embeddings(self)"], "imports": ["from google.ai.generativelanguage_v1beta.types import BatchEmbedContentsRequest", "from langchain_core.embeddings import Embeddings", "from langchain_google_genai import GoogleGenerativeAIEmbeddings", "from langchain_google_genai._common import GoogleGenerativeAIError", "from langflow.custom import Component", "from langflow.io import MessageTextInput, Output, SecretStrInput"], "inputs": "[SecretStrInput(name='api_key', display_name='API Key', required=True), MessageTextInput(name='model_name', display_name='Model Name', value='models/text-embedding-004')]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "Google Generative AI Embeddings", "name": "Google Generative AI Embeddings", "description": "Connect to Google's generative AI embeddings service using the GoogleGenerativeAIEmbeddings class, found in the langchain-google-genai package.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/google_generative_ai.py", "section": "class::HotaGoogleGenerativeAIEmbeddings", "content": "from google.ai.generativelanguage_v1beta.types import BatchEmbedContentsRequest\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_google_genai._common import GoogleGenerativeAIError\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\nclass HotaGoogleGenerativeAIEmbeddings(GoogleGenerativeAIEmbeddings):\n\n    def embed_documents(self, texts):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def embed_query(self, text, task_type, title, output_dimensionality):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HotaGoogleGenerativeAIEmbeddings", "base_classes": ["GoogleGenerativeAIEmbeddings"], "public_methods": ["def embed_documents(self, texts)", "def embed_query(self, text, task_type, title, output_dimensionality)"], "imports": ["from google.ai.generativelanguage_v1beta.types import BatchEmbedContentsRequest", "from langchain_core.embeddings import Embeddings", "from langchain_google_genai import GoogleGenerativeAIEmbeddings", "from langchain_google_genai._common import GoogleGenerativeAIError", "from langflow.custom import Component", "from langflow.io import MessageTextInput, Output, SecretStrInput"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/llm_checker.py", "section": "class::LLMCheckerChainComponent", "content": "from langchain.chains import LLMCheckerChain\nfrom langflow.base.chains.model import LCChainComponent\nfrom langflow.field_typing import Message\nfrom langflow.inputs import HandleInput, MultilineInput\n\nclass LLMCheckerChainComponent(LCChainComponent):\n    display_name: str = \"LLMCheckerChain\"\n    description: str = \"Chain for question-answering with self-verification.\"\n    icon = \"LangChain\"\n    name = \"LLMCheckerChain\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input',\n        info='The input value to pass to the chain.',\n        required=True),\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True)\n    ]\n\n    def invoke_chain(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LLMCheckerChainComponent", "base_classes": ["LCChainComponent"], "public_methods": ["def invoke_chain(self)"], "imports": ["from langchain.chains import LLMCheckerChain", "from langflow.base.chains.model import LCChainComponent", "from langflow.field_typing import Message", "from langflow.inputs import HandleInput, MultilineInput"], "inputs": "[MultilineInput(name='input_value', display_name='Input', info='The input value to pass to the chain.', required=True), HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True)]", "outputs": "", "display_name": "LLMCheckerChain", "name": "LLMCheckerChain", "description": "Chain for question-answering with self-verification.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/split_text.py", "section": "class::SplitTextComponent", "content": "from langchain_text_splitters import CharacterTextSplitter\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.utils.util import unescape_string\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(name='data_inputs',\n        display_name='Data or DataFrame',\n        info='The data with texts to split in chunks.',\n        input_types=['Data',\n        'DataFrame'],\n        required=True),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='Number of characters to overlap between chunks.',\n        value=200),\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        info=\"The maximum length of each chunk. Text is first split by separator,\n        then chunks are merged up to this size. Individual splits larger than this won't be further divided.\",\n        value=1000),\n        MessageTextInput(name='separator',\n        display_name='Separator',\n        info='The character to split on. Use \\\\n for newline. Examples: \\\\n\\\\n for paragraphs,\n        \\\\n for lines,\n        . for sentences',\n        value='\\n'),\n        MessageTextInput(name='text_key',\n        display_name='Text Key',\n        info='The key to use for the text column.',\n        value='text',\n        advanced=True),\n        DropdownInput(name='keep_separator',\n        display_name='Keep Separator',\n        info='Whether to keep the separator in the output chunks and where to place it.',\n        options=['False',\n        'True',\n        'Start',\n        'End'],\n        value='False',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Chunks',\n        name='chunks',\n        method='split_text'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def split_text_base(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def split_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SplitTextComponent", "base_classes": ["Component"], "public_methods": ["def split_text_base(self)", "def split_text(self)", "def as_dataframe(self)"], "imports": ["from langchain_text_splitters import CharacterTextSplitter", "from langflow.custom import Component", "from langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data, DataFrame", "from langflow.utils.util import unescape_string"], "inputs": "[HandleInput(name='data_inputs', display_name='Data or DataFrame', info='The data with texts to split in chunks.', input_types=['Data', 'DataFrame'], required=True), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='Number of characters to overlap between chunks.', value=200), IntInput(name='chunk_size', display_name='Chunk Size', info=\"The maximum length of each chunk. Text is first split by separator, then chunks are merged up to this size. Individual splits larger than this won't be further divided.\", value=1000), MessageTextInput(name='separator', display_name='Separator', info='The character to split on. Use \\\\n for newline. Examples: \\\\n\\\\n for paragraphs, \\\\n for lines, . for sentences', value='\\n'), MessageTextInput(name='text_key', display_name='Text Key', info='The key to use for the text column.', value='text', advanced=True), DropdownInput(name='keep_separator', display_name='Keep Separator', info='Whether to keep the separator in the output chunks and where to place it.', options=['False', 'True', 'Start', 'End'], value='False', advanced=True)]", "outputs": "[Output(display_name='Chunks', name='chunks', method='split_text'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Split Text", "name": "SplitText", "description": "Split text into chunks based on specified criteria.", "icon": "scissors-line-dashed"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/perplexity.py", "section": "class::PerplexityComponent", "content": "from langchain_community.chat_models import ChatPerplexity\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\n\nclass PerplexityComponent(LCModelComponent):\n    display_name: str = \"Perplexity\"\n    description: str = \"Generate text using Perplexity LLMs.\"\n    icon = \"Perplexity\"\n    name = \"PerplexityModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=['llama-3.1-sonar-small-128k-online',\n        'llama-3.1-sonar-large-128k-online',\n        'llama-3.1-sonar-huge-128k-online',\n        'llama-3.1-sonar-small-128k-chat',\n        'llama-3.1-sonar-large-128k-chat',\n        'llama-3.1-8b-instruct',\n        'llama-3.1-70b-instruct'],\n        value='llama-3.1-sonar-small-128k-online'),\n        IntInput(name='max_output_tokens',\n        display_name='Max Output Tokens',\n        info='The maximum number of tokens to generate.'),\n        SecretStrInput(name='api_key',\n        display_name='Perplexity API Key',\n        info='The Perplexity API Key to use for the Perplexity model.',\n        advanced=False,\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.75,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.05)),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        info='The maximum cumulative probability of tokens to consider when sampling.',\n        advanced=True),\n        IntInput(name='n',\n        display_name='N',\n        info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.',\n        advanced=True),\n        IntInput(name='top_k',\n        display_name='Top K',\n        info='Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.',\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PerplexityComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_community.chat_models import ChatPerplexity", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput"], "inputs": "[*LCModelComponent._base_inputs, DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=['llama-3.1-sonar-small-128k-online', 'llama-3.1-sonar-large-128k-online', 'llama-3.1-sonar-huge-128k-online', 'llama-3.1-sonar-small-128k-chat', 'llama-3.1-sonar-large-128k-chat', 'llama-3.1-8b-instruct', 'llama-3.1-70b-instruct'], value='llama-3.1-sonar-small-128k-online'), IntInput(name='max_output_tokens', display_name='Max Output Tokens', info='The maximum number of tokens to generate.'), SecretStrInput(name='api_key', display_name='Perplexity API Key', info='The Perplexity API Key to use for the Perplexity model.', advanced=False, required=True), SliderInput(name='temperature', display_name='Temperature', value=0.75, range_spec=RangeSpec(min=0, max=2, step=0.05)), FloatInput(name='top_p', display_name='Top P', info='The maximum cumulative probability of tokens to consider when sampling.', advanced=True), IntInput(name='n', display_name='N', info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.', advanced=True), IntInput(name='top_k', display_name='Top K', info='Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.', advanced=True)]", "outputs": "", "display_name": "Perplexity", "name": "PerplexityModel", "description": "Generate text using Perplexity LLMs.", "icon": "Perplexity"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/astradb.py", "section": "class::AstraDBVectorStoreComponent", "content": "import re\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass, field\nfrom astrapy import DataAPIClient, Database\nfrom astrapy.data.info.reranking import RerankServiceOptions\nfrom astrapy.info import CollectionDescriptor, CollectionLexicalOptions, CollectionRerankOptions\nfrom langchain_astradb import AstraDBVectorStore, VectorServiceOptions\nfrom langchain_astradb.utils.astradb import HybridSearchMode, _AstraDBCollectionEnvironment\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import FloatInput, NestedDictInput\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, QueryInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langflow.utils.version import get_version_info\nfrom langchain_astradb import AstraDBVectorStore\n\nclass AstraDBVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Astra DB\"\n    description: str = \"Ingest and search documents in Astra DB\"\n    icon = \"AstraDB\"\n    name = \"AstraDB\"\n\n    inputs = [\n        SecretStrInput(name='token',\n        display_name='Astra DB Application Token',\n        info='Authentication token for accessing Astra DB.',\n        value='ASTRA_DB_APPLICATION_TOKEN',\n        required=True,\n        real_time_refresh=True,\n        input_types=[]),\n        DropdownInput(name='environment',\n        display_name='Environment',\n        info='The environment for the Astra DB API Endpoint.',\n        options=['prod',\n        'test',\n        'dev'],\n        value='prod',\n        advanced=True,\n        real_time_refresh=True,\n        combobox=True),\n        DropdownInput(name='database_name',\n        display_name='Database',\n        info='The Database name for the Astra DB instance.',\n        required=True,\n        refresh_button=True,\n        real_time_refresh=True,\n        dialog_inputs=asdict(NewDatabaseInput()),\n        combobox=True),\n        StrInput(name='api_endpoint',\n        display_name='Astra DB API Endpoint',\n        info='The API Endpoint for the Astra DB instance. Supercedes database selection.',\n        show=False),\n        DropdownInput(name='keyspace',\n        display_name='Keyspace',\n        info='Optional keyspace within Astra DB to use for the collection.',\n        advanced=True,\n        options=[],\n        real_time_refresh=True),\n        DropdownInput(name='collection_name',\n        display_name='Collection',\n        info='The name of the collection within Astra DB where the vectors will be stored.',\n        required=True,\n        refresh_button=True,\n        real_time_refresh=True,\n        dialog_inputs=asdict(NewCollectionInput()),\n        combobox=True,\n        show=False),\n        HandleInput(name='embedding_model',\n        display_name='Embedding Model',\n        input_types=['Embeddings'],\n        info='Specify the Embedding Model. Not required for Astra Vectorize collections.',\n        required=False,\n        show=False),\n        *LCVectorStoreComponent.inputs,\n        DropdownInput(name='search_method',\n        display_name='Search Method',\n        info='Determine how your content is matched: Vector finds semantic similarity,\n        and Hybrid Search (suggested) combines both approaches with a reranker.',\n        options=['Hybrid Search',\n        'Vector Search'],\n        options_metadata=[{'icon': 'SearchHybrid'},\n        {'icon': 'SearchVector'}],\n        value='Vector Search',\n        advanced=True,\n        real_time_refresh=True),\n        DropdownInput(name='reranker',\n        display_name='Reranker',\n        info='Post-retrieval model that re-scores results for optimal relevance ranking.',\n        show=False,\n        toggle=True),\n        QueryInput(name='lexical_terms',\n        display_name='Lexical Terms',\n        info='Add additional terms/keywords to augment search precision.',\n        placeholder='Enter terms to search...',\n        separator=' ',\n        show=False,\n        value='',\n        advanced=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Search Results',\n        info='Number of search results to return.',\n        advanced=True,\n        value=4),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        info='Search type to use',\n        options=['Similarity',\n        'Similarity with score threshold',\n        'MMR (Max Marginal Relevance)'],\n        value='Similarity',\n        advanced=True),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n        value=0,\n        advanced=True),\n        NestedDictInput(name='advanced_search_filter',\n        display_name='Search Metadata Filter',\n        info='Optional dictionary of filters to apply to the search query.',\n        advanced=True),\n        BoolInput(name='autodetect_collection',\n        display_name='Autodetect Collection',\n        info='Boolean flag to determine whether to autodetect the collection.',\n        advanced=True,\n        value=True),\n        StrInput(name='content_field',\n        display_name='Content Field',\n        info='Field to use as the text content field for the vector store.',\n        advanced=True),\n        StrInput(name='deletion_field',\n        display_name='Deletion Based On Field',\n        info='When this parameter is provided,\n        documents in the target collection with metadata field values matching the input metadata field value will be deleted before new data is loaded.',\n        advanced=True),\n        BoolInput(name='ignore_invalid_documents',\n        display_name='Ignore Invalid Documents',\n        info='Boolean flag to determine whether to ignore invalid documents at runtime.',\n        advanced=True),\n        NestedDictInput(name='astradb_vectorstore_kwargs',\n        display_name='AstraDBVectorStore Parameters',\n        info='Optional dictionary of additional parameters for the AstraDBVectorStore.',\n        advanced=True)\n    ]\n\n    def map_cloud_providers(cls):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_vectorize_providers(cls, token, environment, api_endpoint):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_database_list_static(cls, token, environment):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_database_list(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_api_endpoint_static(cls, token, environment, api_endpoint, database_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_api_endpoint(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_database_id_static(cls, api_endpoint):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_database_id(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_keyspace(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_database_object(self, api_endpoint):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def collection_data(self, collection_name, database):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_provider_icon(cls, collection, provider_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def reset_provider_options(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def reset_dimension_field(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def reset_collection_list(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def reset_database_list(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def reset_build_config(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self, vector_store):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_retriever_kwargs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AstraDBVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def map_cloud_providers(cls)", "def get_vectorize_providers(cls, token, environment, api_endpoint)", "def get_database_list_static(cls, token, environment)", "def get_database_list(self)", "def get_api_endpoint_static(cls, token, environment, api_endpoint, database_name)", "def get_api_endpoint(self)", "def get_database_id_static(cls, api_endpoint)", "def get_database_id(self)", "def get_keyspace(self)", "def get_database_object(self, api_endpoint)", "def collection_data(self, collection_name, database)", "def get_provider_icon(cls, collection, provider_name)", "def reset_provider_options(self, build_config)", "def reset_dimension_field(self, build_config)", "def reset_collection_list(self, build_config)", "def reset_database_list(self, build_config)", "def reset_build_config(self, build_config)", "def build_vector_store(self)", "def search_documents(self, vector_store)", "def get_retriever_kwargs(self)"], "imports": ["import re", "from collections import defaultdict", "from dataclasses import asdict, dataclass, field", "from astrapy import DataAPIClient, Database", "from astrapy.data.info.reranking import RerankServiceOptions", "from astrapy.info import CollectionDescriptor, CollectionLexicalOptions, CollectionRerankOptions", "from langchain_astradb import AstraDBVectorStore, VectorServiceOptions", "from langchain_astradb.utils.astradb import HybridSearchMode, _AstraDBCollectionEnvironment", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection", "from langflow.helpers import docs_to_data", "from langflow.inputs import FloatInput, NestedDictInput", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, QueryInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langflow.utils.version import get_version_info", "from langchain_astradb import AstraDBVectorStore"], "inputs": "[SecretStrInput(name='token', display_name='Astra DB Application Token', info='Authentication token for accessing Astra DB.', value='ASTRA_DB_APPLICATION_TOKEN', required=True, real_time_refresh=True, input_types=[]), DropdownInput(name='environment', display_name='Environment', info='The environment for the Astra DB API Endpoint.', options=['prod', 'test', 'dev'], value='prod', advanced=True, real_time_refresh=True, combobox=True), DropdownInput(name='database_name', display_name='Database', info='The Database name for the Astra DB instance.', required=True, refresh_button=True, real_time_refresh=True, dialog_inputs=asdict(NewDatabaseInput()), combobox=True), StrInput(name='api_endpoint', display_name='Astra DB API Endpoint', info='The API Endpoint for the Astra DB instance. Supercedes database selection.', show=False), DropdownInput(name='keyspace', display_name='Keyspace', info='Optional keyspace within Astra DB to use for the collection.', advanced=True, options=[], real_time_refresh=True), DropdownInput(name='collection_name', display_name='Collection', info='The name of the collection within Astra DB where the vectors will be stored.', required=True, refresh_button=True, real_time_refresh=True, dialog_inputs=asdict(NewCollectionInput()), combobox=True, show=False), HandleInput(name='embedding_model', display_name='Embedding Model', input_types=['Embeddings'], info='Specify the Embedding Model. Not required for Astra Vectorize collections.', required=False, show=False), *LCVectorStoreComponent.inputs, DropdownInput(name='search_method', display_name='Search Method', info='Determine how your content is matched: Vector finds semantic similarity, and Hybrid Search (suggested) combines both approaches with a reranker.', options=['Hybrid Search', 'Vector Search'], options_metadata=[{'icon': 'SearchHybrid'}, {'icon': 'SearchVector'}], value='Vector Search', advanced=True, real_time_refresh=True), DropdownInput(name='reranker', display_name='Reranker', info='Post-retrieval model that re-scores results for optimal relevance ranking.', show=False, toggle=True), QueryInput(name='lexical_terms', display_name='Lexical Terms', info='Add additional terms/keywords to augment search precision.', placeholder='Enter terms to search...', separator=' ', show=False, value='', advanced=True), IntInput(name='number_of_results', display_name='Number of Search Results', info='Number of search results to return.', advanced=True, value=4), DropdownInput(name='search_type', display_name='Search Type', info='Search type to use', options=['Similarity', 'Similarity with score threshold', 'MMR (Max Marginal Relevance)'], value='Similarity', advanced=True), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\", value=0, advanced=True), NestedDictInput(name='advanced_search_filter', display_name='Search Metadata Filter', info='Optional dictionary of filters to apply to the search query.', advanced=True), BoolInput(name='autodetect_collection', display_name='Autodetect Collection', info='Boolean flag to determine whether to autodetect the collection.', advanced=True, value=True), StrInput(name='content_field', display_name='Content Field', info='Field to use as the text content field for the vector store.', advanced=True), StrInput(name='deletion_field', display_name='Deletion Based On Field', info='When this parameter is provided, documents in the target collection with metadata field values matching the input metadata field value will be deleted before new data is loaded.', advanced=True), BoolInput(name='ignore_invalid_documents', display_name='Ignore Invalid Documents', info='Boolean flag to determine whether to ignore invalid documents at runtime.', advanced=True), NestedDictInput(name='astradb_vectorstore_kwargs', display_name='AstraDBVectorStore Parameters', info='Optional dictionary of additional parameters for the AstraDBVectorStore.', advanced=True)]", "outputs": "", "display_name": "Astra DB", "name": "AstraDB", "description": "Ingest and search documents in Astra DB", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/astradb.py", "section": "class::NewDatabaseInput", "content": "import re\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass, field\nfrom astrapy import DataAPIClient, Database\nfrom astrapy.data.info.reranking import RerankServiceOptions\nfrom astrapy.info import CollectionDescriptor, CollectionLexicalOptions, CollectionRerankOptions\nfrom langchain_astradb import AstraDBVectorStore, VectorServiceOptions\nfrom langchain_astradb.utils.astradb import HybridSearchMode, _AstraDBCollectionEnvironment\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import FloatInput, NestedDictInput\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, QueryInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langflow.utils.version import get_version_info\nfrom langchain_astradb import AstraDBVectorStore\n\nclass NewDatabaseInput:\n", "metadata": {"parser": "python_component", "class_name": "NewDatabaseInput", "base_classes": [], "public_methods": [], "imports": ["import re", "from collections import defaultdict", "from dataclasses import asdict, dataclass, field", "from astrapy import DataAPIClient, Database", "from astrapy.data.info.reranking import RerankServiceOptions", "from astrapy.info import CollectionDescriptor, CollectionLexicalOptions, CollectionRerankOptions", "from langchain_astradb import AstraDBVectorStore, VectorServiceOptions", "from langchain_astradb.utils.astradb import HybridSearchMode, _AstraDBCollectionEnvironment", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection", "from langflow.helpers import docs_to_data", "from langflow.inputs import FloatInput, NestedDictInput", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, QueryInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langflow.utils.version import get_version_info", "from langchain_astradb import AstraDBVectorStore"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/astradb.py", "section": "class::NewCollectionInput", "content": "import re\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass, field\nfrom astrapy import DataAPIClient, Database\nfrom astrapy.data.info.reranking import RerankServiceOptions\nfrom astrapy.info import CollectionDescriptor, CollectionLexicalOptions, CollectionRerankOptions\nfrom langchain_astradb import AstraDBVectorStore, VectorServiceOptions\nfrom langchain_astradb.utils.astradb import HybridSearchMode, _AstraDBCollectionEnvironment\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import FloatInput, NestedDictInput\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, QueryInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langflow.utils.version import get_version_info\nfrom langchain_astradb import AstraDBVectorStore\n\nclass NewCollectionInput:\n", "metadata": {"parser": "python_component", "class_name": "NewCollectionInput", "base_classes": [], "public_methods": [], "imports": ["import re", "from collections import defaultdict", "from dataclasses import asdict, dataclass, field", "from astrapy import DataAPIClient, Database", "from astrapy.data.info.reranking import RerankServiceOptions", "from astrapy.info import CollectionDescriptor, CollectionLexicalOptions, CollectionRerankOptions", "from langchain_astradb import AstraDBVectorStore, VectorServiceOptions", "from langchain_astradb.utils.astradb import HybridSearchMode, _AstraDBCollectionEnvironment", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection", "from langflow.helpers import docs_to_data", "from langflow.inputs import FloatInput, NestedDictInput", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, QueryInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langflow.utils.version import get_version_info", "from langchain_astradb import AstraDBVectorStore"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/mcp_stdio.py", "section": "class::MCPStdio", "content": "from langchain_core.tools import StructuredTool\nfrom mcp import types\nfrom langflow.base.mcp.util import MCPStdioClient, create_input_schema_from_json_schema, create_tool_coroutine, create_tool_func\nfrom langflow.custom import Component\nfrom langflow.field_typing import Tool\nfrom langflow.io import MessageTextInput, Output\n\nclass MCPStdio(Component):\n    display_name: str = \"MCP Tools (stdio) [DEPRECATED]\"\n    description: str = \"Connects to an MCP server over stdio and exposes it's tools as langflow tools to be used by an Agent.\"\n    icon = \"code\"\n    name = \"MCPStdio\"\n\n    inputs = [\n        MessageTextInput(name='command',\n        display_name='mcp command',\n        info='mcp command',\n        value='uvx mcp-sse-shim@latest',\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Tools',\n        name='tools',\n        method='build_output')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "MCPStdio", "base_classes": ["Component"], "public_methods": [], "imports": ["from langchain_core.tools import StructuredTool", "from mcp import types", "from langflow.base.mcp.util import MCPStdioClient, create_input_schema_from_json_schema, create_tool_coroutine, create_tool_func", "from langflow.custom import Component", "from langflow.field_typing import Tool", "from langflow.io import MessageTextInput, Output"], "inputs": "[MessageTextInput(name='command', display_name='mcp command', info='mcp command', value='uvx mcp-sse-shim@latest', tool_mode=True)]", "outputs": "[Output(display_name='Tools', name='tools', method='build_output')]", "display_name": "MCP Tools (stdio) [DEPRECATED]", "name": "MCPStdio", "description": "Connects to an MCP server over stdio and exposes it's tools as langflow tools to be used by an Agent.", "icon": "code"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/yahoo.py", "section": "class::YahooFinanceMethod", "content": "import ast\nimport pprint\nfrom enum import Enum\nimport yfinance as yf\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\nclass YahooFinanceMethod(Enum):\n", "metadata": {"parser": "python_component", "class_name": "YahooFinanceMethod", "base_classes": ["Enum"], "public_methods": [], "imports": ["import ast", "import pprint", "from enum import Enum", "import yfinance as yf", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/yahoo.py", "section": "class::YahooFinanceSchema", "content": "import ast\nimport pprint\nfrom enum import Enum\nimport yfinance as yf\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\nclass YahooFinanceSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "YahooFinanceSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import ast", "import pprint", "from enum import Enum", "import yfinance as yf", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame", "from langflow.schema.message import Message"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/yahoo.py", "section": "class::YfinanceComponent", "content": "import ast\nimport pprint\nfrom enum import Enum\nimport yfinance as yf\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\nclass YfinanceComponent(Component):\n    display_name: str = \"Yahoo Finance\"\n    description: str = \"Uses [yfinance](https://pypi.org/project/yfinance/) (unofficial package) to access financial data and market information from Yahoo Finance.\"\n    icon = \"trending-up\"\n\n    inputs = [\n        MessageTextInput(name='symbol',\n        display_name='Stock Symbol',\n        info='The stock symbol to retrieve data for (e.g.,\n        AAPL,\n        GOOG).',\n        tool_mode=True),\n        DropdownInput(name='method',\n        display_name='Data Method',\n        info='The type of data to retrieve.',\n        options=list(YahooFinanceMethod),\n        value='get_news'),\n        IntInput(name='num_news',\n        display_name='Number of News',\n        info='The number of news articles to retrieve (only applicable for get_news).',\n        value=5)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Text',\n        name='text',\n        method='fetch_content_text'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YfinanceComponent", "base_classes": ["Component"], "public_methods": ["def run_model(self)", "def fetch_content_text(self)", "def fetch_content(self)", "def as_dataframe(self)"], "imports": ["import ast", "import pprint", "from enum import Enum", "import yfinance as yf", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='symbol', display_name='Stock Symbol', info='The stock symbol to retrieve data for (e.g., AAPL, GOOG).', tool_mode=True), DropdownInput(name='method', display_name='Data Method', info='The type of data to retrieve.', options=list(YahooFinanceMethod), value='get_news'), IntInput(name='num_news', display_name='Number of News', info='The number of news articles to retrieve (only applicable for get_news).', value=5)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Text', name='text', method='fetch_content_text'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Yahoo Finance", "name": "", "description": "Uses [yfinance](https://pypi.org/project/yfinance/) (unofficial package) to access financial data and market information from Yahoo Finance.", "icon": "trending-up"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/openai.py", "section": "class::OpenAIEmbeddingsComponent", "content": "from langchain_openai import OpenAIEmbeddings\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"OpenAI Embeddings\"\n    description: str = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIEmbeddings\"\n\n    inputs = [\n        DictInput(name='default_headers',\n        display_name='Default Headers',\n        advanced=True,\n        info='Default headers to use for the API request.'),\n        DictInput(name='default_query',\n        display_name='Default Query',\n        advanced=True,\n        info='Default query parameters to use for the API request.'),\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        advanced=True,\n        value=1000),\n        MessageTextInput(name='client',\n        display_name='Client',\n        advanced=True),\n        MessageTextInput(name='deployment',\n        display_name='Deployment',\n        advanced=True),\n        IntInput(name='embedding_ctx_length',\n        display_name='Embedding Context Length',\n        advanced=True,\n        value=1536),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        value=3,\n        advanced=True),\n        DropdownInput(name='model',\n        display_name='Model',\n        advanced=False,\n        options=OPENAI_EMBEDDING_MODEL_NAMES,\n        value='text-embedding-3-small'),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True),\n        SecretStrInput(name='openai_api_key',\n        display_name='OpenAI API Key',\n        value='OPENAI_API_KEY',\n        required=True),\n        MessageTextInput(name='openai_api_base',\n        display_name='OpenAI API Base',\n        advanced=True),\n        MessageTextInput(name='openai_api_type',\n        display_name='OpenAI API Type',\n        advanced=True),\n        MessageTextInput(name='openai_api_version',\n        display_name='OpenAI API Version',\n        advanced=True),\n        MessageTextInput(name='openai_organization',\n        display_name='OpenAI Organization',\n        advanced=True),\n        MessageTextInput(name='openai_proxy',\n        display_name='OpenAI Proxy',\n        advanced=True),\n        FloatInput(name='request_timeout',\n        display_name='Request Timeout',\n        advanced=True),\n        BoolInput(name='show_progress_bar',\n        display_name='Show Progress Bar',\n        advanced=True),\n        BoolInput(name='skip_empty',\n        display_name='Skip Empty',\n        advanced=True),\n        MessageTextInput(name='tiktoken_model_name',\n        display_name='TikToken Model Name',\n        advanced=True),\n        BoolInput(name='tiktoken_enable',\n        display_name='TikToken Enable',\n        advanced=True,\n        value=True,\n        info='If False,\n        you must have transformers installed.'),\n        IntInput(name='dimensions',\n        display_name='Dimensions',\n        info='The number of dimensions the resulting output embeddings should have. Only supported by certain models.',\n        advanced=True)\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OpenAIEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langchain_openai import OpenAIEmbeddings", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES", "from langflow.field_typing import Embeddings", "from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput"], "inputs": "[DictInput(name='default_headers', display_name='Default Headers', advanced=True, info='Default headers to use for the API request.'), DictInput(name='default_query', display_name='Default Query', advanced=True, info='Default query parameters to use for the API request.'), IntInput(name='chunk_size', display_name='Chunk Size', advanced=True, value=1000), MessageTextInput(name='client', display_name='Client', advanced=True), MessageTextInput(name='deployment', display_name='Deployment', advanced=True), IntInput(name='embedding_ctx_length', display_name='Embedding Context Length', advanced=True, value=1536), IntInput(name='max_retries', display_name='Max Retries', value=3, advanced=True), DropdownInput(name='model', display_name='Model', advanced=False, options=OPENAI_EMBEDDING_MODEL_NAMES, value='text-embedding-3-small'), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True), SecretStrInput(name='openai_api_key', display_name='OpenAI API Key', value='OPENAI_API_KEY', required=True), MessageTextInput(name='openai_api_base', display_name='OpenAI API Base', advanced=True), MessageTextInput(name='openai_api_type', display_name='OpenAI API Type', advanced=True), MessageTextInput(name='openai_api_version', display_name='OpenAI API Version', advanced=True), MessageTextInput(name='openai_organization', display_name='OpenAI Organization', advanced=True), MessageTextInput(name='openai_proxy', display_name='OpenAI Proxy', advanced=True), FloatInput(name='request_timeout', display_name='Request Timeout', advanced=True), BoolInput(name='show_progress_bar', display_name='Show Progress Bar', advanced=True), BoolInput(name='skip_empty', display_name='Skip Empty', advanced=True), MessageTextInput(name='tiktoken_model_name', display_name='TikToken Model Name', advanced=True), BoolInput(name='tiktoken_enable', display_name='TikToken Enable', advanced=True, value=True, info='If False, you must have transformers installed.'), IntInput(name='dimensions', display_name='Dimensions', info='The number of dimensions the resulting output embeddings should have. Only supported by certain models.', advanced=True)]", "outputs": "", "display_name": "OpenAI Embeddings", "name": "OpenAIEmbeddings", "description": "Generate embeddings using OpenAI models.", "icon": "OpenAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/parser.py", "section": "class::ParserComponent", "content": "import json\nfrom typing import Any\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, HandleInput, MessageTextInput, MultilineInput, Output, TabInput\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\nclass ParserComponent(Component):\n    display_name: str = \"Parser\"\n    description: str = \"Format a DataFrame or Data object into text using a template. Enable 'Stringify' to convert input into a readable string instead.\"\n    icon = \"braces\"\n\n    inputs = [\n        TabInput(name='mode',\n        display_name='Mode',\n        options=['Parser',\n        'Stringify'],\n        value='Parser',\n        info='Convert into raw string instead of using a template.',\n        real_time_refresh=True),\n        MultilineInput(name='pattern',\n        display_name='Template',\n        info='Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name},\n        Age: {Age},\n        Country: {Country}`',\n        value='Text: {text}',\n        dynamic=True,\n        show=True,\n        required=True),\n        HandleInput(name='input_data',\n        display_name='Data or DataFrame',\n        input_types=['DataFrame',\n        'Data'],\n        info='Accepts either a DataFrame or a Data object.',\n        required=True),\n        MessageTextInput(name='sep',\n        display_name='Separator',\n        advanced=True,\n        value='\\n',\n        info='String used to separate rows/items.')\n    ]\n\n    outputs = [\n        Output(display_name='Parsed Text',\n        name='parsed_text',\n        info='Formatted text output.',\n        method='parse_combined_text')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_combined_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def convert_to_string(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ParserComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def parse_combined_text(self)", "def convert_to_string(self)"], "imports": ["import json", "from typing import Any", "from langflow.custom import Component", "from langflow.io import BoolInput, HandleInput, MessageTextInput, MultilineInput, Output, TabInput", "from langflow.schema import Data, DataFrame", "from langflow.schema.message import Message"], "inputs": "[TabInput(name='mode', display_name='Mode', options=['Parser', 'Stringify'], value='Parser', info='Convert into raw string instead of using a template.', real_time_refresh=True), MultilineInput(name='pattern', display_name='Template', info='Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`', value='Text: {text}', dynamic=True, show=True, required=True), HandleInput(name='input_data', display_name='Data or DataFrame', input_types=['DataFrame', 'Data'], info='Accepts either a DataFrame or a Data object.', required=True), MessageTextInput(name='sep', display_name='Separator', advanced=True, value='\\n', info='String used to separate rows/items.')]", "outputs": "[Output(display_name='Parsed Text', name='parsed_text', info='Formatted text output.', method='parse_combined_text')]", "display_name": "Parser", "name": "", "description": "Format a DataFrame or Data object into text using a template. Enable 'Stringify' to convert input into a readable string instead.", "icon": "braces"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/mongodb_atlas.py", "section": "class::MongoVectorStoreComponent", "content": "import tempfile\nimport time\nimport certifi\nfrom langchain_community.vectorstores import MongoDBAtlasVectorSearch\nfrom pymongo.collection import Collection\nfrom pymongo.operations import SearchIndexModel\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom bson.objectid import ObjectId\nfrom pymongo import MongoClient\n\nclass MongoVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"MongoDB Atlas\"\n    description: str = \"MongoDB Atlas Vector Store with search capabilities\"\n    icon = \"MongoDB\"\n    name = \"MongoDBAtlasVector\"\n\n    inputs = [\n        SecretStrInput(name='mongodb_atlas_cluster_uri',\n        display_name='MongoDB Atlas Cluster URI',\n        required=True),\n        BoolInput(name='enable_mtls',\n        display_name='Enable mTLS',\n        value=False,\n        advanced=True,\n        required=True),\n        SecretStrInput(name='mongodb_atlas_client_cert',\n        display_name='MongoDB Atlas Combined Client Certificate',\n        required=False,\n        info='Client Certificate combined with the private key in the following format:\\n -----BEGIN PRIVATE KEY-----\\n...\\n -----END PRIVATE KEY-----\\n-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----\\n'),\n        StrInput(name='db_name',\n        display_name='Database Name',\n        required=True),\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        required=True),\n        StrInput(name='index_name',\n        display_name='Index Name',\n        required=True,\n        info='The name of Atlas Search index,\n        it should be a Vector Search.'),\n        *LCVectorStoreComponent.inputs,\n        DropdownInput(name='insert_mode',\n        display_name='Insert Mode',\n        options=INSERT_MODES,\n        value=INSERT_MODES[0],\n        info='How to insert new documents into the collection.',\n        advanced=True),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True),\n        StrInput(name='index_field',\n        display_name='Index Field',\n        advanced=True,\n        required=True,\n        info='The field to index.',\n        value='embedding'),\n        StrInput(name='filter_field',\n        display_name='Filter Field',\n        advanced=True,\n        info='The field to filter the index.'),\n        IntInput(name='number_dimensions',\n        display_name='Number of Dimensions',\n        info='Embedding Context Length.',\n        value=1536,\n        advanced=True,\n        required=True),\n        DropdownInput(name='similarity',\n        display_name='Similarity',\n        options=SIMILARITY_OPTIONS,\n        value=SIMILARITY_OPTIONS[0],\n        info='The method used to measure the similarity between vectors.',\n        advanced=True),\n        DropdownInput(name='quantization',\n        display_name='Quantization',\n        options=QUANTIZATION_OPTIONS,\n        value=None,\n        info='Quantization reduces memory costs converting 32-bit floats to smaller data types',\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def verify_search_index(self, collection):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MongoVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)", "def verify_search_index(self, collection)"], "imports": ["import tempfile", "import time", "import certifi", "from langchain_community.vectorstores import MongoDBAtlasVectorSearch", "from pymongo.collection import Collection", "from pymongo.operations import SearchIndexModel", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from bson.objectid import ObjectId", "from pymongo import MongoClient"], "inputs": "[SecretStrInput(name='mongodb_atlas_cluster_uri', display_name='MongoDB Atlas Cluster URI', required=True), BoolInput(name='enable_mtls', display_name='Enable mTLS', value=False, advanced=True, required=True), SecretStrInput(name='mongodb_atlas_client_cert', display_name='MongoDB Atlas Combined Client Certificate', required=False, info='Client Certificate combined with the private key in the following format:\\n -----BEGIN PRIVATE KEY-----\\n...\\n -----END PRIVATE KEY-----\\n-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----\\n'), StrInput(name='db_name', display_name='Database Name', required=True), StrInput(name='collection_name', display_name='Collection Name', required=True), StrInput(name='index_name', display_name='Index Name', required=True, info='The name of Atlas Search index, it should be a Vector Search.'), *LCVectorStoreComponent.inputs, DropdownInput(name='insert_mode', display_name='Insert Mode', options=INSERT_MODES, value=INSERT_MODES[0], info='How to insert new documents into the collection.', advanced=True), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True), StrInput(name='index_field', display_name='Index Field', advanced=True, required=True, info='The field to index.', value='embedding'), StrInput(name='filter_field', display_name='Filter Field', advanced=True, info='The field to filter the index.'), IntInput(name='number_dimensions', display_name='Number of Dimensions', info='Embedding Context Length.', value=1536, advanced=True, required=True), DropdownInput(name='similarity', display_name='Similarity', options=SIMILARITY_OPTIONS, value=SIMILARITY_OPTIONS[0], info='The method used to measure the similarity between vectors.', advanced=True), DropdownInput(name='quantization', display_name='Quantization', options=QUANTIZATION_OPTIONS, value=None, info='Quantization reduces memory costs converting 32-bit floats to smaller data types', advanced=True)]", "outputs": "", "display_name": "MongoDB Atlas", "name": "MongoDBAtlasVector", "description": "MongoDB Atlas Vector Store with search capabilities", "icon": "MongoDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/list_flows.py", "section": "class::ListFlowsComponent", "content": "from langflow.custom import CustomComponent\nfrom langflow.schema import Data\n\nclass ListFlowsComponent(CustomComponent):\n    display_name: str = \"List Flows\"\n    description: str = \"A component to list all available flows.\"\n    icon = \"ListFlows\"\n    name = \"ListFlows\"\n\n    def build_config(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ListFlowsComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build_config(self)"], "imports": ["from langflow.custom import CustomComponent", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "List Flows", "name": "ListFlows", "description": "A component to list all available flows.", "icon": "ListFlows"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/mcp_component.py", "section": "class::MCPToolsComponent", "content": "import re\nimport shutil\nfrom typing import Any\nfrom langchain_core.tools import StructuredTool\nfrom langflow.base.mcp.util import MCPSseClient, MCPStdioClient, create_input_schema_from_json_schema, create_tool_coroutine, create_tool_func\nfrom langflow.custom import Component\nfrom langflow.inputs import DropdownInput, TableInput\nfrom langflow.inputs.inputs import InputTypes\nfrom langflow.io import MessageTextInput, MultilineInput, Output, TabInput\nfrom langflow.io.schema import flatten_schema, schema_to_langflow_inputs\nfrom langflow.logging import logger\nfrom langflow.schema import Message\n\nclass MCPToolsComponent(Component):\n    display_name: str = \"MCP Connection\"\n    description: str = \"Connect to an MCP server to use its tools.\"\n    icon = \"Mcp\"\n    name = \"MCPTools\"\n\n    inputs = [\n        TabInput(name='mode',\n        display_name='Mode',\n        options=['Stdio',\n        'SSE'],\n        value='Stdio',\n        info='Select the connection mode',\n        real_time_refresh=True),\n        MessageTextInput(name='command',\n        display_name='MCP Command',\n        info='Command for MCP stdio connection',\n        value='uvx mcp-server-fetch',\n        show=True,\n        refresh_button=True),\n        MessageTextInput(name='env',\n        display_name='Env',\n        info='Env vars to include in mcp stdio connection (i.e. DEBUG=true)',\n        value='',\n        is_list=True,\n        show=True,\n        tool_mode=False,\n        advanced=True),\n        MultilineInput(name='sse_url',\n        display_name='MCP SSE URL',\n        info='URL for MCP SSE connection',\n        show=False,\n        refresh_button=True,\n        value='MCP_SSE',\n        real_time_refresh=True),\n        TableInput(name='headers_input',\n        display_name='Headers',\n        info='Headers to include in the tool',\n        show=False,\n        real_time_refresh=True,\n        table_schema=[{'name': 'key',\n        'display_name': 'Header',\n        'type': 'str',\n        'description': 'Header name'},\n        {'name': 'value',\n        'display_name': 'Value',\n        'type': 'str',\n        'description': 'Header value'}],\n        value=[],\n        advanced=True),\n        DropdownInput(name='tool',\n        display_name='Tool',\n        options=[],\n        value='',\n        info='Select the tool to execute',\n        show=True,\n        required=True,\n        real_time_refresh=True),\n        MessageTextInput(name='tool_placeholder',\n        display_name='Tool Placeholder',\n        info='Placeholder for the tool',\n        value='',\n        show=False,\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Response',\n        name='response',\n        method='build_output')\n    ]\n\n    def get_inputs_for_all_tools(self, tools):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def remove_input_schema_from_build_config(self, build_config, tool_name, input_schema):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def remove_non_default_keys(self, build_config):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MCPToolsComponent", "base_classes": ["Component"], "public_methods": ["def get_inputs_for_all_tools(self, tools)", "def remove_input_schema_from_build_config(self, build_config, tool_name, input_schema)", "def remove_non_default_keys(self, build_config)"], "imports": ["import re", "import shutil", "from typing import Any", "from langchain_core.tools import StructuredTool", "from langflow.base.mcp.util import MCPSseClient, MCPStdioClient, create_input_schema_from_json_schema, create_tool_coroutine, create_tool_func", "from langflow.custom import Component", "from langflow.inputs import DropdownInput, TableInput", "from langflow.inputs.inputs import InputTypes", "from langflow.io import MessageTextInput, MultilineInput, Output, TabInput", "from langflow.io.schema import flatten_schema, schema_to_langflow_inputs", "from langflow.logging import logger", "from langflow.schema import Message"], "inputs": "[TabInput(name='mode', display_name='Mode', options=['Stdio', 'SSE'], value='Stdio', info='Select the connection mode', real_time_refresh=True), MessageTextInput(name='command', display_name='MCP Command', info='Command for MCP stdio connection', value='uvx mcp-server-fetch', show=True, refresh_button=True), MessageTextInput(name='env', display_name='Env', info='Env vars to include in mcp stdio connection (i.e. DEBUG=true)', value='', is_list=True, show=True, tool_mode=False, advanced=True), MultilineInput(name='sse_url', display_name='MCP SSE URL', info='URL for MCP SSE connection', show=False, refresh_button=True, value='MCP_SSE', real_time_refresh=True), TableInput(name='headers_input', display_name='Headers', info='Headers to include in the tool', show=False, real_time_refresh=True, table_schema=[{'name': 'key', 'display_name': 'Header', 'type': 'str', 'description': 'Header name'}, {'name': 'value', 'display_name': 'Value', 'type': 'str', 'description': 'Header value'}], value=[], advanced=True), DropdownInput(name='tool', display_name='Tool', options=[], value='', info='Select the tool to execute', show=True, required=True, real_time_refresh=True), MessageTextInput(name='tool_placeholder', display_name='Tool Placeholder', info='Placeholder for the tool', value='', show=False, tool_mode=True)]", "outputs": "[Output(display_name='Response', name='response', method='build_output')]", "display_name": "MCP Connection", "name": "MCPTools", "description": "Connect to an MCP server to use its tools.", "icon": "Mcp"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/lmstudioembeddings.py", "section": "class::LMStudioEmbeddingsComponent", "content": "from typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom typing_extensions import override\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\nclass LMStudioEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"LM Studio Embeddings\"\n    description: str = \"Generate embeddings using LM Studio.\"\n    icon = \"LMStudio\"\n\n    inputs = [\n        DropdownInput(name='model',\n        display_name='Model',\n        advanced=False,\n        refresh_button=True,\n        required=True),\n        MessageTextInput(name='base_url',\n        display_name='LM Studio Base URL',\n        refresh_button=True,\n        value='http://localhost:1234/v1',\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='LM Studio API Key',\n        advanced=True,\n        value='LMSTUDIO_API_KEY'),\n        FloatInput(name='temperature',\n        display_name='Model Temperature',\n        value=0.1,\n        advanced=True)\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LMStudioEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)"], "imports": ["from typing import Any", "from urllib.parse import urljoin", "import httpx", "from typing_extensions import override", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.inputs.inputs import DropdownInput, SecretStrInput", "from langflow.io import FloatInput, MessageTextInput", "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings"], "inputs": "[DropdownInput(name='model', display_name='Model', advanced=False, refresh_button=True, required=True), MessageTextInput(name='base_url', display_name='LM Studio Base URL', refresh_button=True, value='http://localhost:1234/v1', required=True), SecretStrInput(name='api_key', display_name='LM Studio API Key', advanced=True, value='LMSTUDIO_API_KEY'), FloatInput(name='temperature', display_name='Model Temperature', value=0.1, advanced=True)]", "outputs": "", "display_name": "LM Studio Embeddings", "name": "", "description": "Generate embeddings using LM Studio.", "icon": "LMStudio"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/csv_agent.py", "section": "class::CSVAgentComponent", "content": "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\nfrom langflow.base.agents.agent import LCAgentComponent\nfrom langflow.field_typing import AgentExecutor\nfrom langflow.inputs import DropdownInput, FileInput, HandleInput\nfrom langflow.inputs.inputs import DictInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template.field.base import Output\n\nclass CSVAgentComponent(LCAgentComponent):\n    display_name: str = \"CSVAgent\"\n    description: str = \"Construct a CSV agent from a CSV and tools.\"\n    icon = \"LangChain\"\n    name = \"CSVAgent\"\n\n    inputs = [\n        *LCAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True,\n        info='An LLM Model Object (It can be found in any LLM Component).'),\n        FileInput(name='path',\n        display_name='File Path',\n        file_types=['csv'],\n        input_types=['str',\n        'Message'],\n        required=True,\n        info='A CSV File or File Path.'),\n        DropdownInput(name='agent_type',\n        display_name='Agent Type',\n        advanced=True,\n        options=['zero-shot-react-description',\n        'openai-functions',\n        'openai-tools'],\n        value='openai-tools'),\n        MessageTextInput(name='input_value',\n        display_name='Text',\n        info='Text to be passed as input and extract info from the CSV File.',\n        required=True),\n        DictInput(name='pandas_kwargs',\n        display_name='Pandas Kwargs',\n        info='Pandas Kwargs to be passed to the agent.',\n        advanced=True,\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(display_name='Response',\n        name='response',\n        method='build_agent_response'),\n        Output(display_name='Agent',\n        name='agent',\n        method='build_agent',\n        hidden=True,\n        tool_mode=False)\n    ]\n\n    def build_agent_response(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_agent(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CSVAgentComponent", "base_classes": ["LCAgentComponent"], "public_methods": ["def build_agent_response(self)", "def build_agent(self)"], "imports": ["from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent", "from langflow.base.agents.agent import LCAgentComponent", "from langflow.field_typing import AgentExecutor", "from langflow.inputs import DropdownInput, FileInput, HandleInput", "from langflow.inputs.inputs import DictInput, MessageTextInput", "from langflow.schema.message import Message", "from langflow.template.field.base import Output"], "inputs": "[*LCAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True, info='An LLM Model Object (It can be found in any LLM Component).'), FileInput(name='path', display_name='File Path', file_types=['csv'], input_types=['str', 'Message'], required=True, info='A CSV File or File Path.'), DropdownInput(name='agent_type', display_name='Agent Type', advanced=True, options=['zero-shot-react-description', 'openai-functions', 'openai-tools'], value='openai-tools'), MessageTextInput(name='input_value', display_name='Text', info='Text to be passed as input and extract info from the CSV File.', required=True), DictInput(name='pandas_kwargs', display_name='Pandas Kwargs', info='Pandas Kwargs to be passed to the agent.', advanced=True, is_list=True)]", "outputs": "[Output(display_name='Response', name='response', method='build_agent_response'), Output(display_name='Agent', name='agent', method='build_agent', hidden=True, tool_mode=False)]", "display_name": "CSVAgent", "name": "CSVAgent", "description": "Construct a CSV agent from a CSV and tools.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/merge_data.py", "section": "class::DataOperation", "content": "from enum import Enum\nfrom typing import cast\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, Output\nfrom langflow.schema import DataFrame\n\nclass DataOperation(str, Enum):\n", "metadata": {"parser": "python_component", "class_name": "DataOperation", "base_classes": ["str", "Enum"], "public_methods": [], "imports": ["from enum import Enum", "from typing import cast", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, Output", "from langflow.schema import DataFrame"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/merge_data.py", "section": "class::MergeDataComponent", "content": "from enum import Enum\nfrom typing import cast\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, Output\nfrom langflow.schema import DataFrame\n\nclass MergeDataComponent(Component):\n    display_name: str = \"Combine Data\"\n    description: str = \"Combines data using different operations\"\n    icon = \"merge\"\n\n    inputs = [\n        DataInput(name='data_inputs',\n        display_name='Data Inputs',\n        info='Data to combine',\n        is_list=True,\n        required=True),\n        DropdownInput(name='operation',\n        display_name='Operation Type',\n        options=[op.value for op in DataOperation],\n        value=DataOperation.CONCATENATE.value)\n    ]\n\n    outputs = [\n        Output(display_name='DataFrame',\n        name='combined_data',\n        method='combine_data')\n    ]\n\n    def combine_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MergeDataComponent", "base_classes": ["Component"], "public_methods": ["def combine_data(self)"], "imports": ["from enum import Enum", "from typing import cast", "from loguru import logger", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, Output", "from langflow.schema import DataFrame"], "inputs": "[DataInput(name='data_inputs', display_name='Data Inputs', info='Data to combine', is_list=True, required=True), DropdownInput(name='operation', display_name='Operation Type', options=[op.value for op in DataOperation], value=DataOperation.CONCATENATE.value)]", "outputs": "[Output(display_name='DataFrame', name='combined_data', method='combine_data')]", "display_name": "Combine Data", "name": "", "description": "Combines data using different operations", "icon": "merge"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/split_text.py", "section": "class::SplitTextComponent", "content": "from langchain_text_splitters import CharacterTextSplitter\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(name='data_inputs',\n        display_name='Data Inputs',\n        info='The data to split.',\n        input_types=['Data'],\n        is_list=True),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='Number of characters to overlap between chunks.',\n        value=200),\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        info='The maximum number of characters in each chunk.',\n        value=1000),\n        MessageTextInput(name='separator',\n        display_name='Separator',\n        info='The character to split on. Defaults to newline.',\n        value='\\n')\n    ]\n\n    outputs = [\n        Output(display_name='Chunks',\n        name='chunks',\n        method='split_text')\n    ]\n\n    def split_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SplitTextComponent", "base_classes": ["Component"], "public_methods": ["def split_text(self)"], "imports": ["from langchain_text_splitters import CharacterTextSplitter", "from langflow.custom import Component", "from langflow.io import HandleInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data", "from langflow.utils.util import unescape_string"], "inputs": "[HandleInput(name='data_inputs', display_name='Data Inputs', info='The data to split.', input_types=['Data'], is_list=True), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='Number of characters to overlap between chunks.', value=200), IntInput(name='chunk_size', display_name='Chunk Size', info='The maximum number of characters in each chunk.', value=1000), MessageTextInput(name='separator', display_name='Separator', info='The character to split on. Defaults to newline.', value='\\n')]", "outputs": "[Output(display_name='Chunks', name='chunks', method='split_text')]", "display_name": "Split Text", "name": "SplitText", "description": "Split text into chunks based on specified criteria.", "icon": "scissors-line-dashed"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/searxng.py", "section": "class::SearXNGToolComponent", "content": "import json\nfrom collections.abc import Sequence\nfrom typing import Any\nimport requests\nfrom langchain.agents import Tool\nfrom langchain_core.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic.v1 import Field, create_model\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput, MultiselectInput\nfrom langflow.io import Output\nfrom langflow.schema.dotdict import dotdict\n\nclass SearXNGToolComponent(LCToolComponent):\n    display_name: str = \"SearXNG Search\"\n    description: str = \"A component that searches for tools using SearXNG.\"\n    name = \"SearXNGTool\"\n\n    inputs = [\n        MessageTextInput(name='url',\n        display_name='URL',\n        value='http://localhost',\n        required=True,\n        refresh_button=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=10,\n        required=True),\n        MultiselectInput(name='categories',\n        display_name='Categories',\n        options=[],\n        value=[]),\n        DropdownInput(name='language',\n        display_name='Language',\n        options=[])\n    ]\n\n    outputs = [\n        Output(display_name='Tool',\n        name='result_tool',\n        method='build_tool')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SearXNGToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def build_tool(self)"], "imports": ["import json", "from collections.abc import Sequence", "from typing import Any", "import requests", "from langchain.agents import Tool", "from langchain_core.tools import StructuredTool", "from loguru import logger", "from pydantic.v1 import Field, create_model", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput, MultiselectInput", "from langflow.io import Output", "from langflow.schema.dotdict import dotdict"], "inputs": "[MessageTextInput(name='url', display_name='URL', value='http://localhost', required=True, refresh_button=True), IntInput(name='max_results', display_name='Max Results', value=10, required=True), MultiselectInput(name='categories', display_name='Categories', options=[], value=[]), DropdownInput(name='language', display_name='Language', options=[])]", "outputs": "[Output(display_name='Tool', name='result_tool', method='build_tool')]", "display_name": "SearXNG Search", "name": "SearXNGTool", "description": "A component that searches for tools using SearXNG.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/searxng.py", "section": "class::SearxSearch", "content": "import json\nfrom collections.abc import Sequence\nfrom typing import Any\nimport requests\nfrom langchain.agents import Tool\nfrom langchain_core.tools import StructuredTool\nfrom loguru import logger\nfrom pydantic.v1 import Field, create_model\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput, MultiselectInput\nfrom langflow.io import Output\nfrom langflow.schema.dotdict import dotdict\n\nclass SearxSearch:\n\n    def search(query, categories):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SearxSearch", "base_classes": [], "public_methods": ["def search(query, categories)"], "imports": ["import json", "from collections.abc import Sequence", "from typing import Any", "import requests", "from langchain.agents import Tool", "from langchain_core.tools import StructuredTool", "from loguru import logger", "from pydantic.v1 import Field, create_model", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput, MultiselectInput", "from langflow.io import Output", "from langflow.schema.dotdict import dotdict"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/azure_openai.py", "section": "class::AzureOpenAIEmbeddingsComponent", "content": "from langchain_openai import AzureOpenAIEmbeddings\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\n\nclass AzureOpenAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI Embeddings\"\n    description: str = \"Generate embeddings using Azure OpenAI models.\"\n    icon = \"Azure\"\n    name = \"AzureOpenAIEmbeddings\"\n\n    inputs = [\n        DropdownInput(name='model',\n        display_name='Model',\n        advanced=False,\n        options=OPENAI_EMBEDDING_MODEL_NAMES,\n        value=OPENAI_EMBEDDING_MODEL_NAMES[0]),\n        MessageTextInput(name='azure_endpoint',\n        display_name='Azure Endpoint',\n        required=True,\n        info='Your Azure endpoint,\n        including the resource. Example: `https://example-resource.azure.openai.com/`'),\n        MessageTextInput(name='azure_deployment',\n        display_name='Deployment Name',\n        required=True),\n        DropdownInput(name='api_version',\n        display_name='API Version',\n        options=API_VERSION_OPTIONS,\n        value=API_VERSION_OPTIONS[-1],\n        advanced=True),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True),\n        IntInput(name='dimensions',\n        display_name='Dimensions',\n        info='The number of dimensions the resulting output embeddings should have. Only supported by certain models.',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AzureOpenAIEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langchain_openai import AzureOpenAIEmbeddings", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput"], "inputs": "[DropdownInput(name='model', display_name='Model', advanced=False, options=OPENAI_EMBEDDING_MODEL_NAMES, value=OPENAI_EMBEDDING_MODEL_NAMES[0]), MessageTextInput(name='azure_endpoint', display_name='Azure Endpoint', required=True, info='Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`'), MessageTextInput(name='azure_deployment', display_name='Deployment Name', required=True), DropdownInput(name='api_version', display_name='API Version', options=API_VERSION_OPTIONS, value=API_VERSION_OPTIONS[-1], advanced=True), SecretStrInput(name='api_key', display_name='API Key', required=True), IntInput(name='dimensions', display_name='Dimensions', info='The number of dimensions the resulting output embeddings should have. Only supported by certain models.', advanced=True)]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "Azure OpenAI Embeddings", "name": "AzureOpenAIEmbeddings", "description": "Generate embeddings using Azure OpenAI models.", "icon": "Azure"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/self_query.py", "section": "class::SelfQueryRetrieverComponent", "content": "from langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langflow.custom import Component\nfrom langflow.inputs import HandleInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass SelfQueryRetrieverComponent(Component):\n    display_name: str = \"Self Query Retriever\"\n    description: str = \"Retriever that uses a vector store and an LLM to generate the vector store queries.\"\n    icon = \"LangChain\"\n    name = \"SelfQueryRetriever\"\n\n    inputs = [\n        HandleInput(name='query',\n        display_name='Query',\n        info='Query to be passed as input.',\n        input_types=['Message']),\n        HandleInput(name='vectorstore',\n        display_name='Vector Store',\n        info='Vector Store to be passed as input.',\n        input_types=['VectorStore']),\n        HandleInput(name='attribute_infos',\n        display_name='Metadata Field Info',\n        info='Metadata Field Info to be passed as input.',\n        input_types=['Data'],\n        is_list=True),\n        MessageTextInput(name='document_content_description',\n        display_name='Document Content Description',\n        info='Document Content Description to be passed as input.'),\n        HandleInput(name='llm',\n        display_name='LLM',\n        info='LLM to be passed as input.',\n        input_types=['LanguageModel'])\n    ]\n\n    outputs = [\n        Output(display_name='Retrieved Documents',\n        name='documents',\n        method='retrieve_documents')\n    ]\n\n    def retrieve_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SelfQueryRetrieverComponent", "base_classes": ["Component"], "public_methods": ["def retrieve_documents(self)"], "imports": ["from langchain.chains.query_constructor.base import AttributeInfo", "from langchain.retrievers.self_query.base import SelfQueryRetriever", "from langflow.custom import Component", "from langflow.inputs import HandleInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[HandleInput(name='query', display_name='Query', info='Query to be passed as input.', input_types=['Message']), HandleInput(name='vectorstore', display_name='Vector Store', info='Vector Store to be passed as input.', input_types=['VectorStore']), HandleInput(name='attribute_infos', display_name='Metadata Field Info', info='Metadata Field Info to be passed as input.', input_types=['Data'], is_list=True), MessageTextInput(name='document_content_description', display_name='Document Content Description', info='Document Content Description to be passed as input.'), HandleInput(name='llm', display_name='LLM', info='LLM to be passed as input.', input_types=['LanguageModel'])]", "outputs": "[Output(display_name='Retrieved Documents', name='documents', method='retrieve_documents')]", "display_name": "Self Query Retriever", "name": "SelfQueryRetriever", "description": "Retriever that uses a vector store and an LLM to generate the vector store queries.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/extract_key.py", "section": "class::ExtractDataKeyComponent", "content": "from langflow.custom import Component\nfrom langflow.io import DataInput, Output, StrInput\nfrom langflow.schema import Data\n\nclass ExtractDataKeyComponent(Component):\n    display_name: str = \"Extract Key\"\n    description: str = \"Extract a specific key from a Data object or a list of Data objects and return the extracted value(s) as Data object(s).\"\n    icon = \"key\"\n    name = \"ExtractaKey\"\n\n    inputs = [\n        DataInput(name='data_input',\n        display_name='Data Input',\n        info='The Data object or list of Data objects to extract the key from.'),\n        StrInput(name='key',\n        display_name='Key to Extract',\n        info='The key in the Data object(s) to extract.')\n    ]\n\n    outputs = [\n        Output(display_name='Extracted Data',\n        name='extracted_data',\n        method='extract_key')\n    ]\n\n    def extract_key(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ExtractDataKeyComponent", "base_classes": ["Component"], "public_methods": ["def extract_key(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import DataInput, Output, StrInput", "from langflow.schema import Data"], "inputs": "[DataInput(name='data_input', display_name='Data Input', info='The Data object or list of Data objects to extract the key from.'), StrInput(name='key', display_name='Key to Extract', info='The key in the Data object(s) to extract.')]", "outputs": "[Output(display_name='Extracted Data', name='extracted_data', method='extract_key')]", "display_name": "Extract Key", "name": "ExtractaKey", "description": "Extract a specific key from a Data object or a list of Data objects and return the extracted value(s) as Data object(s).", "icon": "key"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/google_generative_ai.py", "section": "class::GoogleGenerativeAIComponent", "content": "from typing import Any\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nimport google.generativeai as genai\nfrom langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name: str = \"Google Generative AI\"\n    description: str = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_output_tokens',\n        display_name='Max Output Tokens',\n        info='The maximum number of tokens to generate.'),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        info='The name of the model to use.',\n        options=GOOGLE_GENERATIVE_AI_MODELS,\n        value='gemini-1.5-pro',\n        refresh_button=True,\n        combobox=True),\n        SecretStrInput(name='api_key',\n        display_name='Google API Key',\n        info='The Google API Key to use for the Google Generative AI.',\n        required=True,\n        real_time_refresh=True),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        info='The maximum cumulative probability of tokens to consider when sampling.',\n        advanced=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        info='Controls randomness. Lower values are more deterministic,\n        higher values are more creative.'),\n        IntInput(name='n',\n        display_name='N',\n        info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.',\n        advanced=True),\n        IntInput(name='top_k',\n        display_name='Top K',\n        info='Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.',\n        advanced=True),\n        BoolInput(name='tool_model_enabled',\n        display_name='Tool Model Enabled',\n        info='Whether to use the tool model.',\n        value=False)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_models(self, tool_model_enabled):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleGenerativeAIComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)", "def get_models(self, tool_model_enabled)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any", "import requests", "from loguru import logger", "from pydantic.v1 import SecretStr", "from langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput", "from langflow.inputs.inputs import BoolInput", "from langflow.schema import dotdict", "from langchain_google_genai import ChatGoogleGenerativeAI", "import google.generativeai as genai", "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_output_tokens', display_name='Max Output Tokens', info='The maximum number of tokens to generate.'), DropdownInput(name='model_name', display_name='Model', info='The name of the model to use.', options=GOOGLE_GENERATIVE_AI_MODELS, value='gemini-1.5-pro', refresh_button=True, combobox=True), SecretStrInput(name='api_key', display_name='Google API Key', info='The Google API Key to use for the Google Generative AI.', required=True, real_time_refresh=True), FloatInput(name='top_p', display_name='Top P', info='The maximum cumulative probability of tokens to consider when sampling.', advanced=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01), info='Controls randomness. Lower values are more deterministic, higher values are more creative.'), IntInput(name='n', display_name='N', info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.', advanced=True), IntInput(name='top_k', display_name='Top K', info='Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.', advanced=True), BoolInput(name='tool_model_enabled', display_name='Tool Model Enabled', info='Whether to use the tool model.', value=False)]", "outputs": "", "display_name": "Google Generative AI", "name": "GoogleGenerativeAIModel", "description": "Generate text using Google Generative AI.", "icon": "GoogleGenerativeAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/faiss.py", "section": "class::FaissVectorStoreComponent", "content": "from pathlib import Path\nfrom langchain_community.vectorstores import FAISS\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, HandleInput, IntInput, StrInput\nfrom langflow.schema import Data\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    FAISS Vector Store with search capabilities.\n    \"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    icon = \"FAISS\"\n    name = \"FAISS\"\n\n    inputs = [\n        StrInput(name='index_name',\n        display_name='Index Name',\n        value='langflow_index'),\n        StrInput(name='persist_directory',\n        display_name='Persist Directory',\n        info='Path to save the FAISS index. It will be relative to where Langflow is running.'),\n        *LCVectorStoreComponent.inputs,\n        BoolInput(name='allow_dangerous_deserialization',\n        display_name='Allow Dangerous Deserialization',\n        info='Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.',\n        advanced=True,\n        value=True),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=4)\n    ]\n\n    def resolve_path(path):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_persist_directory(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FaissVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def resolve_path(path)", "def get_persist_directory(self)", "def build_vector_store(self)", "def search_documents(self)"], "imports": ["from pathlib import Path", "from langchain_community.vectorstores import FAISS", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import BoolInput, HandleInput, IntInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='index_name', display_name='Index Name', value='langflow_index'), StrInput(name='persist_directory', display_name='Persist Directory', info='Path to save the FAISS index. It will be relative to where Langflow is running.'), *LCVectorStoreComponent.inputs, BoolInput(name='allow_dangerous_deserialization', display_name='Allow Dangerous Deserialization', info='Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.', advanced=True, value=True), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=4)]", "outputs": "", "display_name": "FAISS", "name": "FAISS", "description": "FAISS Vector Store with search capabilities", "icon": "FAISS"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/merge_data.py", "section": "class::MergeDataComponent", "content": "from loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data\n\nclass MergeDataComponent(Component):\n    \"\"\"\n    MergeDataComponent is responsible for combining multiple Data objects into a unified list of Data objects.\n    \n    It ensures that all keys across the input Data objects are present in each merged Data object.\n    Missing keys are filled with empty strings to maintain consistency.\n    \"\"\"\n\n    display_name: str = \"Merge Data\"\n    description: str = \"Combines multiple Data objects into a unified list, ensuring all keys are present in each Data object.\"\n    icon = \"merge\"\n\n    inputs = [\n        DataInput(name='data_inputs',\n        display_name='Data Inputs',\n        is_list=True,\n        info='A list of Data inputs objects to be merged.')\n    ]\n\n    outputs = [\n        Output(display_name='Merged Data',\n        name='merged_data',\n        method='merge_data')\n    ]\n\n    def merge_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MergeDataComponent", "base_classes": ["Component"], "public_methods": ["def merge_data(self)"], "imports": ["from loguru import logger", "from langflow.custom import Component", "from langflow.io import DataInput, Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='data_inputs', display_name='Data Inputs', is_list=True, info='A list of Data inputs objects to be merged.')]", "outputs": "[Output(display_name='Merged Data', name='merged_data', method='merge_data')]", "display_name": "Merge Data", "name": "", "description": "Combines multiple Data objects into a unified list, ensuring all keys are present in each Data object.", "icon": "merge"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/text_embedder.py", "section": "class::TextEmbedderComponent", "content": "import logging\nfrom typing import TYPE_CHECKING\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, MessageInput, Output\nfrom langflow.schema import Data\nfrom langflow.field_typing import Embeddings\nfrom langflow.schema.message import Message\n\nclass TextEmbedderComponent(Component):\n    display_name: str = \"Text Embedder\"\n    description: str = \"Generate embeddings for a given message using the specified embedding model.\"\n    icon = \"binary\"\n\n    inputs = [\n        HandleInput(name='embedding_model',\n        display_name='Embedding Model',\n        info='The embedding model to use for generating embeddings.',\n        input_types=['Embeddings'],\n        required=True),\n        MessageInput(name='message',\n        display_name='Message',\n        info='The message to generate embeddings for.',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embedding Data',\n        name='embeddings',\n        method='generate_embeddings')\n    ]\n\n    def generate_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TextEmbedderComponent", "base_classes": ["Component"], "public_methods": ["def generate_embeddings(self)"], "imports": ["import logging", "from typing import TYPE_CHECKING", "from langflow.custom import Component", "from langflow.io import HandleInput, MessageInput, Output", "from langflow.schema import Data", "from langflow.field_typing import Embeddings", "from langflow.schema.message import Message"], "inputs": "[HandleInput(name='embedding_model', display_name='Embedding Model', info='The embedding model to use for generating embeddings.', input_types=['Embeddings'], required=True), MessageInput(name='message', display_name='Message', info='The message to generate embeddings for.', required=True)]", "outputs": "[Output(display_name='Embedding Data', name='embeddings', method='generate_embeddings')]", "display_name": "Text Embedder", "name": "", "description": "Generate embeddings for a given message using the specified embedding model.", "icon": "binary"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/filter_data.py", "section": "class::FilterDataComponent", "content": "from langflow.custom import Component\nfrom langflow.io import DataInput, MessageTextInput, Output\nfrom langflow.schema import Data\n\nclass FilterDataComponent(Component):\n    display_name: str = \"Filter Data\"\n    description: str = \"Filters a Data object based on a list of keys.\"\n    icon = \"filter\"\n    name = \"FilterData\"\n\n    inputs = [\n        DataInput(name='data',\n        display_name='Data',\n        info='Data object to filter.'),\n        MessageTextInput(name='filter_criteria',\n        display_name='Filter Criteria',\n        info='List of keys to filter by.',\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(display_name='Filtered Data',\n        name='filtered_data',\n        method='filter_data')\n    ]\n\n    def filter_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FilterDataComponent", "base_classes": ["Component"], "public_methods": ["def filter_data(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import DataInput, MessageTextInput, Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='data', display_name='Data', info='Data object to filter.'), MessageTextInput(name='filter_criteria', display_name='Filter Criteria', info='List of keys to filter by.', is_list=True)]", "outputs": "[Output(display_name='Filtered Data', name='filtered_data', method='filter_data')]", "display_name": "Filter Data", "name": "FilterData", "description": "Filters a Data object based on a list of keys.", "icon": "filter"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/groq.py", "section": "class::GroqModel", "content": "import requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.groq_constants import GROQ_MODELS, TOOL_CALLING_UNSUPPORTED_GROQ_MODELS, UNSUPPORTED_GROQ_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langchain_groq import ChatGroq\nfrom langchain_groq import ChatGroq\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name='api_key',\n        display_name='Groq API Key',\n        info='API key for the Groq API.',\n        real_time_refresh=True),\n        MessageTextInput(name='base_url',\n        display_name='Groq API Base',\n        info='Base URL path for API requests,\n        leave blank if not using a proxy or service emulator.',\n        advanced=True,\n        value='https://api.groq.com',\n        real_time_refresh=True),\n        IntInput(name='max_tokens',\n        display_name='Max Output Tokens',\n        info='The maximum number of tokens to generate.',\n        advanced=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        info='Run inference with this temperature. Must by in the closed interval [0.0,\n        1.0].',\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True),\n        IntInput(name='n',\n        display_name='N',\n        info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.',\n        advanced=True),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        info='The name of the model to use.',\n        options=GROQ_MODELS,\n        value=GROQ_MODELS[0],\n        refresh_button=True,\n        combobox=True),\n        BoolInput(name='tool_model_enabled',\n        display_name='Enable Tool Models',\n        info='Select if you want to use models that can work with tools. If yes,\n        only those models will be shown.',\n        advanced=False,\n        value=False,\n        real_time_refresh=True)\n    ]\n\n    def get_models(self, tool_model_enabled):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GroqModel", "base_classes": ["LCModelComponent"], "public_methods": ["def get_models(self, tool_model_enabled)", "def update_build_config(self, build_config, field_value, field_name)", "def build_model(self)"], "imports": ["import requests", "from loguru import logger", "from pydantic.v1 import SecretStr", "from langflow.base.models.groq_constants import GROQ_MODELS, TOOL_CALLING_UNSUPPORTED_GROQ_MODELS, UNSUPPORTED_GROQ_MODELS", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput", "from langchain_groq import ChatGroq", "from langchain_groq import ChatGroq"], "inputs": "[*LCModelComponent._base_inputs, SecretStrInput(name='api_key', display_name='Groq API Key', info='API key for the Groq API.', real_time_refresh=True), MessageTextInput(name='base_url', display_name='Groq API Base', info='Base URL path for API requests, leave blank if not using a proxy or service emulator.', advanced=True, value='https://api.groq.com', real_time_refresh=True), IntInput(name='max_tokens', display_name='Max Output Tokens', info='The maximum number of tokens to generate.', advanced=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, info='Run inference with this temperature. Must by in the closed interval [0.0, 1.0].', range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True), IntInput(name='n', display_name='N', info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.', advanced=True), DropdownInput(name='model_name', display_name='Model', info='The name of the model to use.', options=GROQ_MODELS, value=GROQ_MODELS[0], refresh_button=True, combobox=True), BoolInput(name='tool_model_enabled', display_name='Enable Tool Models', info='Select if you want to use models that can work with tools. If yes, only those models will be shown.', advanced=False, value=False, real_time_refresh=True)]", "outputs": "", "display_name": "Groq", "name": "GroqModel", "description": "Generate text using Groq.", "icon": "Groq"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/local_db.py", "section": "class::LocalDBComponent", "content": "from copy import deepcopy\nfrom pathlib import Path\nfrom langchain_chroma import Chroma\nfrom loguru import logger\nfrom typing_extensions import override\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.inputs.inputs import MultilineInput\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, MessageTextInput, TabInput\nfrom langflow.schema import Data, DataFrame\nfrom langflow.template.field.base import Output\nfrom langflow.services.cache.utils import CACHE_DIR\nfrom langflow.services.cache.utils import CACHE_DIR\nfrom langchain_chroma import Chroma\n\nclass LocalDBComponent(LCVectorStoreComponent):\n    \"\"\"\n    Chroma Vector Store with search capabilities.\n    \"\"\"\n\n    display_name: str = \"Local DB\"\n    description: str = \"Local Vector Store with search capabilities\"\n    icon = \"database\"\n    name = \"LocalDB\"\n\n    inputs = [\n        TabInput(name='mode',\n        display_name='Mode',\n        options=['Ingest',\n        'Retrieve'],\n        info='Select the operation mode',\n        value='Ingest',\n        real_time_refresh=True,\n        show=True),\n        MessageTextInput(name='collection_name',\n        display_name='Collection Name',\n        value='langflow'),\n        MessageTextInput(name='persist_directory',\n        display_name='Persist Directory',\n        info=\"Custom base directory to save the vector store. Collections will be stored under '{directory}/vector_stores/{collection_name}'. If not specified,\n        it will use your system's cache folder.\",\n        advanced=True),\n        DropdownInput(name='existing_collections',\n        display_name='Existing Collections',\n        options=[],\n        info='Select a previously created collection to search through its stored data.',\n        show=False,\n        combobox=True),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        BoolInput(name='allow_duplicates',\n        display_name='Allow Duplicates',\n        advanced=True,\n        info='If false,\n        will not add documents that are already in the Vector Store.'),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        options=['Similarity',\n        'MMR'],\n        value='Similarity',\n        advanced=True),\n        HandleInput(name='ingest_data',\n        display_name='Ingest Data',\n        input_types=['Data',\n        'DataFrame'],\n        is_list=True,\n        info='Data to store. It will be embedded and indexed for semantic search.',\n        show=True),\n        MultilineInput(name='search_query',\n        display_name='Search Query',\n        tool_mode=True,\n        info='Enter text to search for similar content in the selected collection.',\n        show=False),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=10),\n        IntInput(name='limit',\n        display_name='Limit',\n        advanced=True,\n        info='Limit the number of records to compare when Allow Duplicates is False.')\n    ]\n\n    outputs = [\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def get_vector_store_directory(self, base_dir):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_default_persist_dir(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def list_existing_collections(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LocalDBComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def get_vector_store_directory(self, base_dir)", "def get_default_persist_dir(self)", "def list_existing_collections(self)", "def update_build_config(self, build_config, field_value, field_name)", "def build_vector_store(self)"], "imports": ["from copy import deepcopy", "from pathlib import Path", "from langchain_chroma import Chroma", "from loguru import logger", "from typing_extensions import override", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.base.vectorstores.utils import chroma_collection_to_data", "from langflow.inputs.inputs import MultilineInput", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, MessageTextInput, TabInput", "from langflow.schema import Data, DataFrame", "from langflow.template.field.base import Output", "from langflow.services.cache.utils import CACHE_DIR", "from langflow.services.cache.utils import CACHE_DIR", "from langchain_chroma import Chroma"], "inputs": "[TabInput(name='mode', display_name='Mode', options=['Ingest', 'Retrieve'], info='Select the operation mode', value='Ingest', real_time_refresh=True, show=True), MessageTextInput(name='collection_name', display_name='Collection Name', value='langflow'), MessageTextInput(name='persist_directory', display_name='Persist Directory', info=\"Custom base directory to save the vector store. Collections will be stored under '{directory}/vector_stores/{collection_name}'. If not specified, it will use your system's cache folder.\", advanced=True), DropdownInput(name='existing_collections', display_name='Existing Collections', options=[], info='Select a previously created collection to search through its stored data.', show=False, combobox=True), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), BoolInput(name='allow_duplicates', display_name='Allow Duplicates', advanced=True, info='If false, will not add documents that are already in the Vector Store.'), DropdownInput(name='search_type', display_name='Search Type', options=['Similarity', 'MMR'], value='Similarity', advanced=True), HandleInput(name='ingest_data', display_name='Ingest Data', input_types=['Data', 'DataFrame'], is_list=True, info='Data to store. It will be embedded and indexed for semantic search.', show=True), MultilineInput(name='search_query', display_name='Search Query', tool_mode=True, info='Enter text to search for similar content in the selected collection.', show=False), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=10), IntInput(name='limit', display_name='Limit', advanced=True, info='Limit the number of records to compare when Allow Duplicates is False.')]", "outputs": "[Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Local DB", "name": "LocalDB", "description": "Local Vector Store with search capabilities", "icon": "database"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/chat_litellm_model.py", "section": "class::ChatLiteLLMModelComponent", "content": "from langchain_community.chat_models.litellm import ChatLiteLLM, ChatLiteLLMException\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageInput, SecretStrInput, StrInput\nimport litellm\n\nclass ChatLiteLLMModelComponent(LCModelComponent):\n    display_name: str = \"LiteLLM\"\n    description: str = \"`LiteLLM` collection of large language models.\"\n    icon = \"🚄\"\n\n    inputs = [\n        MessageInput(name='input_value',\n        display_name='Input'),\n        StrInput(name='model',\n        display_name='Model name',\n        advanced=False,\n        required=True,\n        info='The name of the model to use. For example,\n        `gpt-3.5-turbo`.'),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        advanced=False,\n        required=False),\n        DropdownInput(name='provider',\n        display_name='Provider',\n        info='The provider of the API key.',\n        options=['OpenAI',\n        'Azure',\n        'Anthropic',\n        'Replicate',\n        'Cohere',\n        'OpenRouter']),\n        FloatInput(name='temperature',\n        display_name='Temperature',\n        advanced=False,\n        required=False,\n        value=0.7),\n        DictInput(name='kwargs',\n        display_name='Kwargs',\n        advanced=True,\n        required=False,\n        is_list=True,\n        value={}),\n        DictInput(name='model_kwargs',\n        display_name='Model kwargs',\n        advanced=True,\n        required=False,\n        is_list=True,\n        value={}),\n        FloatInput(name='top_p',\n        display_name='Top p',\n        advanced=True,\n        required=False,\n        value=0.5),\n        IntInput(name='top_k',\n        display_name='Top k',\n        advanced=True,\n        required=False,\n        value=35),\n        IntInput(name='n',\n        display_name='N',\n        advanced=True,\n        required=False,\n        info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.',\n        value=1),\n        IntInput(name='max_tokens',\n        display_name='Max tokens',\n        advanced=False,\n        value=256,\n        info='The maximum number of tokens to generate for each chat completion.'),\n        IntInput(name='max_retries',\n        display_name='Max retries',\n        advanced=True,\n        required=False,\n        value=6),\n        BoolInput(name='verbose',\n        display_name='Verbose',\n        advanced=True,\n        required=False,\n        value=False),\n        BoolInput(name='stream',\n        display_name='Stream',\n        info=STREAM_INFO_TEXT,\n        advanced=True),\n        StrInput(name='system_message',\n        display_name='System Message',\n        info='System message to pass to the model.',\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ChatLiteLLMModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_community.chat_models.litellm import ChatLiteLLM, ChatLiteLLMException", "from langflow.base.constants import STREAM_INFO_TEXT", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageInput, SecretStrInput, StrInput", "import litellm"], "inputs": "[MessageInput(name='input_value', display_name='Input'), StrInput(name='model', display_name='Model name', advanced=False, required=True, info='The name of the model to use. For example, `gpt-3.5-turbo`.'), SecretStrInput(name='api_key', display_name='API Key', advanced=False, required=False), DropdownInput(name='provider', display_name='Provider', info='The provider of the API key.', options=['OpenAI', 'Azure', 'Anthropic', 'Replicate', 'Cohere', 'OpenRouter']), FloatInput(name='temperature', display_name='Temperature', advanced=False, required=False, value=0.7), DictInput(name='kwargs', display_name='Kwargs', advanced=True, required=False, is_list=True, value={}), DictInput(name='model_kwargs', display_name='Model kwargs', advanced=True, required=False, is_list=True, value={}), FloatInput(name='top_p', display_name='Top p', advanced=True, required=False, value=0.5), IntInput(name='top_k', display_name='Top k', advanced=True, required=False, value=35), IntInput(name='n', display_name='N', advanced=True, required=False, info='Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.', value=1), IntInput(name='max_tokens', display_name='Max tokens', advanced=False, value=256, info='The maximum number of tokens to generate for each chat completion.'), IntInput(name='max_retries', display_name='Max retries', advanced=True, required=False, value=6), BoolInput(name='verbose', display_name='Verbose', advanced=True, required=False, value=False), BoolInput(name='stream', display_name='Stream', info=STREAM_INFO_TEXT, advanced=True), StrInput(name='system_message', display_name='System Message', info='System message to pass to the model.', advanced=True)]", "outputs": "", "display_name": "LiteLLM", "name": "", "description": "`LiteLLM` collection of large language models.", "icon": "🚄"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/exa_search.py", "section": "class::ExaSearchToolkit", "content": "from langchain_core.tools import tool\nfrom metaphor_python import Metaphor\nfrom langflow.custom import Component\nfrom langflow.field_typing import Tool\nfrom langflow.io import BoolInput, IntInput, Output, SecretStrInput\n\nclass ExaSearchToolkit(Component):\n    display_name: str = \"Exa Search\"\n    description: str = \"Exa Search toolkit for search and content retrieval\"\n    icon = \"ExaSearch\"\n    name = \"ExaSearch\"\n\n    inputs = [\n        SecretStrInput(name='metaphor_api_key',\n        display_name='Exa Search API Key',\n        password=True),\n        BoolInput(name='use_autoprompt',\n        display_name='Use Autoprompt',\n        value=True),\n        IntInput(name='search_num_results',\n        display_name='Search Number of Results',\n        value=5),\n        IntInput(name='similar_num_results',\n        display_name='Similar Number of Results',\n        value=5)\n    ]\n\n    outputs = [\n        Output(name='tools',\n        display_name='Tools',\n        method='build_toolkit')\n    ]\n\n    def build_toolkit(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ExaSearchToolkit", "base_classes": ["Component"], "public_methods": ["def build_toolkit(self)"], "imports": ["from langchain_core.tools import tool", "from metaphor_python import Metaphor", "from langflow.custom import Component", "from langflow.field_typing import Tool", "from langflow.io import BoolInput, IntInput, Output, SecretStrInput"], "inputs": "[SecretStrInput(name='metaphor_api_key', display_name='Exa Search API Key', password=True), BoolInput(name='use_autoprompt', display_name='Use Autoprompt', value=True), IntInput(name='search_num_results', display_name='Search Number of Results', value=5), IntInput(name='similar_num_results', display_name='Similar Number of Results', value=5)]", "outputs": "[Output(name='tools', display_name='Tools', method='build_toolkit')]", "display_name": "Exa Search", "name": "ExaSearch", "description": "Exa Search toolkit for search and content retrieval", "icon": "ExaSearch"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/watsonx.py", "section": "class::WatsonxEmbeddingsComponent", "content": "from typing import Any\nimport requests\nfrom ibm_watsonx_ai import APIClient, Credentials\nfrom ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\nfrom langchain_ibm import WatsonxEmbeddings\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DropdownInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema.dotdict import dotdict\n\nclass WatsonxEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"IBM watsonx.ai Embeddings\"\n    description: str = \"Generate embeddings using IBM watsonx.ai models.\"\n    icon = \"WatsonxAI\"\n    name = \"WatsonxEmbeddingsComponent\"\n\n    inputs = [\n        DropdownInput(name='url',\n        display_name='watsonx API Endpoint',\n        info='The base URL of the API.',\n        value=None,\n        options=['https://us-south.ml.cloud.ibm.com',\n        'https://eu-de.ml.cloud.ibm.com',\n        'https://eu-gb.ml.cloud.ibm.com',\n        'https://au-syd.ml.cloud.ibm.com',\n        'https://jp-tok.ml.cloud.ibm.com',\n        'https://ca-tor.ml.cloud.ibm.com'],\n        real_time_refresh=True),\n        StrInput(name='project_id',\n        display_name='watsonx project id',\n        info='The project ID or deployment space ID that is associated with the foundation model.',\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        info='The API Key to use for the model.',\n        required=True),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        options=[],\n        value=None,\n        dynamic=True,\n        required=True),\n        IntInput(name='truncate_input_tokens',\n        display_name='Truncate Input Tokens',\n        advanced=True,\n        value=200),\n        BoolInput(name='input_text',\n        display_name='Include the original text in the output',\n        value=True,\n        advanced=True)\n    ]\n\n    def fetch_models(base_url):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WatsonxEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def fetch_models(base_url)", "def update_build_config(self, build_config, field_value, field_name)", "def build_embeddings(self)"], "imports": ["from typing import Any", "import requests", "from ibm_watsonx_ai import APIClient, Credentials", "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames", "from langchain_ibm import WatsonxEmbeddings", "from loguru import logger", "from pydantic.v1 import SecretStr", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import BoolInput, DropdownInput, IntInput, SecretStrInput, StrInput", "from langflow.schema.dotdict import dotdict"], "inputs": "[DropdownInput(name='url', display_name='watsonx API Endpoint', info='The base URL of the API.', value=None, options=['https://us-south.ml.cloud.ibm.com', 'https://eu-de.ml.cloud.ibm.com', 'https://eu-gb.ml.cloud.ibm.com', 'https://au-syd.ml.cloud.ibm.com', 'https://jp-tok.ml.cloud.ibm.com', 'https://ca-tor.ml.cloud.ibm.com'], real_time_refresh=True), StrInput(name='project_id', display_name='watsonx project id', info='The project ID or deployment space ID that is associated with the foundation model.', required=True), SecretStrInput(name='api_key', display_name='API Key', info='The API Key to use for the model.', required=True), DropdownInput(name='model_name', display_name='Model Name', options=[], value=None, dynamic=True, required=True), IntInput(name='truncate_input_tokens', display_name='Truncate Input Tokens', advanced=True, value=200), BoolInput(name='input_text', display_name='Include the original text in the output', value=True, advanced=True)]", "outputs": "", "display_name": "IBM watsonx.ai Embeddings", "name": "WatsonxEmbeddingsComponent", "description": "Generate embeddings using IBM watsonx.ai models.", "icon": "WatsonxAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/html_link_extractor.py", "section": "class::HtmlLinkExtractorComponent", "content": "from typing import Any\nfrom langchain_community.graph_vectorstores.extractors import HtmlLinkExtractor, LinkExtractorTransformer\nfrom langchain_core.documents import BaseDocumentTransformer\nfrom langflow.base.document_transformers.model import LCDocumentTransformerComponent\nfrom langflow.inputs import BoolInput, DataInput, StrInput\n\nclass HtmlLinkExtractorComponent(LCDocumentTransformerComponent):\n    display_name: str = \"HTML Link Extractor\"\n    description: str = \"Extract hyperlinks from HTML content.\"\n    icon = \"LangChain\"\n    name = \"HtmlLinkExtractor\"\n\n    inputs = [\n        StrInput(name='kind',\n        display_name='Kind of edge',\n        value='hyperlink',\n        required=False),\n        BoolInput(name='drop_fragments',\n        display_name='Drop URL fragments',\n        value=True,\n        required=False),\n        DataInput(name='data_input',\n        display_name='Input',\n        info='The texts from which to extract links.',\n        input_types=['Document',\n        'Data'],\n        required=True)\n    ]\n\n    def get_data_input(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_document_transformer(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HtmlLinkExtractorComponent", "base_classes": ["LCDocumentTransformerComponent"], "public_methods": ["def get_data_input(self)", "def build_document_transformer(self)"], "imports": ["from typing import Any", "from langchain_community.graph_vectorstores.extractors import HtmlLinkExtractor, LinkExtractorTransformer", "from langchain_core.documents import BaseDocumentTransformer", "from langflow.base.document_transformers.model import LCDocumentTransformerComponent", "from langflow.inputs import BoolInput, DataInput, StrInput"], "inputs": "[StrInput(name='kind', display_name='Kind of edge', value='hyperlink', required=False), BoolInput(name='drop_fragments', display_name='Drop URL fragments', value=True, required=False), DataInput(name='data_input', display_name='Input', info='The texts from which to extract links.', input_types=['Document', 'Data'], required=True)]", "outputs": "", "display_name": "HTML Link Extractor", "name": "HtmlLinkExtractor", "description": "Extract hyperlinks from HTML content.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/select_data.py", "section": "class::SelectDataComponent", "content": "from langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DataInput, IntInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass SelectDataComponent(Component):\n    display_name: str = \"Select Data\"\n    description: str = \"Select a single data from a list of data.\"\n    icon = \"prototypes\"\n    name = \"SelectData\"\n\n    inputs = [\n        DataInput(name='data_list',\n        display_name='Data List',\n        info='List of data to select from.',\n        is_list=True),\n        IntInput(name='data_index',\n        display_name='Data Index',\n        info='Index of the data to select.',\n        value=0,\n        range_spec=RangeSpec(min=0,\n        max=15,\n        step=1,\n        step_type='int'))\n    ]\n\n    outputs = [\n        Output(display_name='Selected Data',\n        name='selected_data',\n        method='select_data')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "SelectDataComponent", "base_classes": ["Component"], "public_methods": [], "imports": ["from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs.inputs import DataInput, IntInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='data_list', display_name='Data List', info='List of data to select from.', is_list=True), IntInput(name='data_index', display_name='Data Index', info='Index of the data to select.', value=0, range_spec=RangeSpec(min=0, max=15, step=1, step_type='int'))]", "outputs": "[Output(display_name='Selected Data', name='selected_data', method='select_data')]", "display_name": "Select Data", "name": "SelectData", "description": "Select a single data from a list of data.", "icon": "prototypes"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/azure_openai.py", "section": "class::AzureChatOpenAIComponent", "content": "from langchain_openai import AzureChatOpenAI\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput\n\nclass AzureChatOpenAIComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI\"\n    description: str = \"Generate text using Azure OpenAI LLMs.\"\n    icon = \"Azure\"\n    name = \"AzureOpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        MessageTextInput(name='azure_endpoint',\n        display_name='Azure Endpoint',\n        info='Your Azure endpoint,\n        including the resource. Example: `https://example-resource.azure.openai.com/`',\n        required=True),\n        MessageTextInput(name='azure_deployment',\n        display_name='Deployment Name',\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=True),\n        DropdownInput(name='api_version',\n        display_name='API Version',\n        options=sorted(AZURE_OPENAI_API_VERSIONS,\n        reverse=True),\n        value=next((version for version in sorted(AZURE_OPENAI_API_VERSIONS,\n        reverse=True) if not version.endswith('-preview')),\n        AZURE_OPENAI_API_VERSIONS[0])),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.7,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        info='Controls randomness. Lower values are more deterministic,\n        higher values are more creative.',\n        advanced=True),\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.')\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AzureChatOpenAIComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_openai import AzureChatOpenAI", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import MessageTextInput", "from langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput"], "inputs": "[*LCModelComponent._base_inputs, MessageTextInput(name='azure_endpoint', display_name='Azure Endpoint', info='Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`', required=True), MessageTextInput(name='azure_deployment', display_name='Deployment Name', required=True), SecretStrInput(name='api_key', display_name='API Key', required=True), DropdownInput(name='api_version', display_name='API Version', options=sorted(AZURE_OPENAI_API_VERSIONS, reverse=True), value=next((version for version in sorted(AZURE_OPENAI_API_VERSIONS, reverse=True) if not version.endswith('-preview')), AZURE_OPENAI_API_VERSIONS[0])), SliderInput(name='temperature', display_name='Temperature', value=0.7, range_spec=RangeSpec(min=0, max=2, step=0.01), info='Controls randomness. Lower values are more deterministic, higher values are more creative.', advanced=True), IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.')]", "outputs": "", "display_name": "Azure OpenAI", "name": "AzureOpenAIModel", "description": "Generate text using Azure OpenAI LLMs.", "icon": "Azure"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/pgvector.py", "section": "class::PGVectorStoreComponent", "content": "from langchain_community.vectorstores import PGVector\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langflow.utils.connection_string_parser import transform_connection_string\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"PGVector\"\n    description: str = \"PGVector Vector Store with search capabilities\"\n    icon = \"cpu\"\n    name = \"pgvector\"\n\n    inputs = [\n        SecretStrInput(name='pg_server_url',\n        display_name='PostgreSQL Server Connection String',\n        required=True),\n        StrInput(name='collection_name',\n        display_name='Table',\n        required=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings'],\n        required=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PGVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from langchain_community.vectorstores import PGVector", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langflow.utils.connection_string_parser import transform_connection_string"], "inputs": "[SecretStrInput(name='pg_server_url', display_name='PostgreSQL Server Connection String', required=True), StrInput(name='collection_name', display_name='Table', required=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings'], required=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "PGVector", "name": "pgvector", "description": "PGVector Vector Store with search capabilities", "icon": "cpu"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/deactivated/mcp_sse.py", "section": "class::MCPSse", "content": "from langchain_core.tools import StructuredTool\nfrom mcp import types\nfrom langflow.base.mcp.util import MCPSseClient, create_input_schema_from_json_schema, create_tool_coroutine, create_tool_func\nfrom langflow.custom import Component\nfrom langflow.field_typing import Tool\nfrom langflow.io import MessageTextInput, Output\n\nclass MCPSse(Component):\n    display_name: str = \"MCP Tools (SSE) [DEPRECATED]\"\n    description: str = \"Connects to an MCP server over SSE and exposes it's tools as langflow tools to be used by an Agent.\"\n    icon = \"code\"\n    name = \"MCPSse\"\n\n    inputs = [\n        MessageTextInput(name='url',\n        display_name='mcp sse url',\n        info='sse url',\n        value='http://localhost:7860/api/v1/mcp/sse',\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Tools',\n        name='tools',\n        method='build_output')\n    ]\n", "metadata": {"parser": "python_component", "class_name": "MCPSse", "base_classes": ["Component"], "public_methods": [], "imports": ["from langchain_core.tools import StructuredTool", "from mcp import types", "from langflow.base.mcp.util import MCPSseClient, create_input_schema_from_json_schema, create_tool_coroutine, create_tool_func", "from langflow.custom import Component", "from langflow.field_typing import Tool", "from langflow.io import MessageTextInput, Output"], "inputs": "[MessageTextInput(name='url', display_name='mcp sse url', info='sse url', value='http://localhost:7860/api/v1/mcp/sse', tool_mode=True)]", "outputs": "[Output(display_name='Tools', name='tools', method='build_output')]", "display_name": "MCP Tools (SSE) [DEPRECATED]", "name": "MCPSse", "description": "Connects to an MCP server over SSE and exposes it's tools as langflow tools to be used by an Agent.", "icon": "code"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wolfram_alpha_api.py", "section": "class::WolframAlphaAPIComponent", "content": "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput, SecretStrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\n\nclass WolframAlphaAPIComponent(LCToolComponent):\n    display_name: str = \"WolframAlpha API\"\n    description: str = \"Enables queries to Wolfram Alpha for computational data, facts, and calculations across various topics, delivering structured responses.\"\n    icon = \"WolframAlphaAPI\"\n    name = \"WolframAlphaAPI\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input Query',\n        info=\"Example query: 'What is the population of France?'\"),\n        SecretStrInput(name='app_id',\n        display_name='App ID',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='run_model'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WolframAlphaAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)", "def as_dataframe(self)"], "imports": ["from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput, SecretStrInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame"], "inputs": "[MultilineInput(name='input_value', display_name='Input Query', info=\"Example query: 'What is the population of France?'\"), SecretStrInput(name='app_id', display_name='App ID', required=True)]", "outputs": "[Output(display_name='Data', name='data', method='run_model'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "WolframAlpha API", "name": "WolframAlphaAPI", "description": "Enables queries to Wolfram Alpha for computational data, facts, and calculations across various topics, delivering structured responses.", "icon": "WolframAlphaAPI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/cohere.py", "section": "class::CohereEmbeddingsComponent", "content": "from typing import Any\nimport cohere\nfrom langchain_cohere import CohereEmbeddings\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput\n\nclass CohereEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Cohere Embeddings\"\n    description: str = \"Generate embeddings using Cohere models.\"\n    icon = \"Cohere\"\n    name = \"CohereEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Cohere API Key',\n        required=True,\n        real_time_refresh=True),\n        DropdownInput(name='model_name',\n        display_name='Model',\n        advanced=False,\n        options=['embed-english-v2.0',\n        'embed-multilingual-v2.0',\n        'embed-english-light-v2.0',\n        'embed-multilingual-light-v2.0'],\n        value='embed-english-v2.0',\n        refresh_button=True,\n        combobox=True),\n        MessageTextInput(name='truncate',\n        display_name='Truncate',\n        advanced=True),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        value=3,\n        advanced=True),\n        MessageTextInput(name='user_agent',\n        display_name='User Agent',\n        advanced=True,\n        value='langchain'),\n        FloatInput(name='request_timeout',\n        display_name='Request Timeout',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CohereEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)", "def get_model(self)"], "imports": ["from typing import Any", "import cohere", "from langchain_cohere import CohereEmbeddings", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput"], "inputs": "[SecretStrInput(name='api_key', display_name='Cohere API Key', required=True, real_time_refresh=True), DropdownInput(name='model_name', display_name='Model', advanced=False, options=['embed-english-v2.0', 'embed-multilingual-v2.0', 'embed-english-light-v2.0', 'embed-multilingual-light-v2.0'], value='embed-english-v2.0', refresh_button=True, combobox=True), MessageTextInput(name='truncate', display_name='Truncate', advanced=True), IntInput(name='max_retries', display_name='Max Retries', value=3, advanced=True), MessageTextInput(name='user_agent', display_name='User Agent', advanced=True, value='langchain'), FloatInput(name='request_timeout', display_name='Request Timeout', advanced=True)]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "Cohere Embeddings", "name": "CohereEmbeddings", "description": "Generate embeddings using Cohere models.", "icon": "Cohere"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/sql_database.py", "section": "class::SQLDatabaseComponent", "content": "from langchain_community.utilities.sql_database import SQLDatabase\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.pool import StaticPool\nfrom langflow.custom import Component\nfrom langflow.io import Output, StrInput\n\nclass SQLDatabaseComponent(Component):\n    display_name: str = \"SQLDatabase\"\n    description: str = \"SQL Database\"\n    icon = \"LangChain\"\n    name = \"SQLDatabase\"\n\n    inputs = [\n        StrInput(name='uri',\n        display_name='URI',\n        info='URI to the database.',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='SQLDatabase',\n        name='SQLDatabase',\n        method='build_sqldatabase')\n    ]\n\n    def clean_up_uri(self, uri):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_sqldatabase(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SQLDatabaseComponent", "base_classes": ["Component"], "public_methods": ["def clean_up_uri(self, uri)", "def build_sqldatabase(self)"], "imports": ["from langchain_community.utilities.sql_database import SQLDatabase", "from sqlalchemy import create_engine", "from sqlalchemy.pool import StaticPool", "from langflow.custom import Component", "from langflow.io import Output, StrInput"], "inputs": "[StrInput(name='uri', display_name='URI', info='URI to the database.', required=True)]", "outputs": "[Output(display_name='SQLDatabase', name='SQLDatabase', method='build_sqldatabase')]", "display_name": "SQLDatabase", "name": "SQLDatabase", "description": "SQL Database", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/data_to_dataframe.py", "section": "class::DataToDataFrameComponent", "content": "from langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data, DataFrame\n\nclass DataToDataFrameComponent(Component):\n    display_name: str = \"Data → DataFrame\"\n    description: str = \"Converts one or multiple Data objects into a DataFrame. Each Data object corresponds to one row. Fields from `.data` become columns, and the `.text` (if present) is placed in a 'text' column.\"\n    icon = \"table\"\n    name = \"DataToDataFrame\"\n\n    inputs = [\n        DataInput(name='data_list',\n        display_name='Data or Data List',\n        info='One or multiple Data objects to transform into a DataFrame.',\n        is_list=True)\n    ]\n\n    outputs = [\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='build_dataframe',\n        info=\"A DataFrame built from each Data object's fields plus a 'text' column.\")\n    ]\n\n    def build_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DataToDataFrameComponent", "base_classes": ["Component"], "public_methods": ["def build_dataframe(self)"], "imports": ["from langflow.custom import Component", "from langflow.io import DataInput, Output", "from langflow.schema import Data, DataFrame"], "inputs": "[DataInput(name='data_list', display_name='Data or Data List', info='One or multiple Data objects to transform into a DataFrame.', is_list=True)]", "outputs": "[Output(display_name='DataFrame', name='dataframe', method='build_dataframe', info=\"A DataFrame built from each Data object's fields plus a 'text' column.\")]", "display_name": "Data → DataFrame", "name": "DataToDataFrame", "description": "Converts one or multiple Data objects into a DataFrame. Each Data object corresponds to one row. Fields from `.data` become columns, and the `.text` (if present) is placed in a 'text' column.", "icon": "table"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/lmstudiomodel.py", "section": "class::LMStudioModelComponent", "content": "from typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom openai import BadRequestError\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name: str = \"LM Studio\"\n    description: str = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.',\n        range_spec=RangeSpec(min=0,\n        max=128000)),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        refresh_button=True),\n        StrInput(name='base_url',\n        display_name='Base URL',\n        advanced=False,\n        info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n        value='http://localhost:1234/v1'),\n        SecretStrInput(name='api_key',\n        display_name='LM Studio API Key',\n        info='The LM Studio API Key to use for LM Studio.',\n        advanced=True,\n        value='LMSTUDIO_API_KEY'),\n        FloatInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        advanced=True),\n        IntInput(name='seed',\n        display_name='Seed',\n        info='The seed controls the reproducibility of the job.',\n        advanced=True,\n        value=1)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LMStudioModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from typing import Any", "from urllib.parse import urljoin", "import httpx", "from langchain_openai import ChatOpenAI", "from typing_extensions import override", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput", "from openai import BadRequestError"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.', range_spec=RangeSpec(min=0, max=128000)), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True), DropdownInput(name='model_name', display_name='Model Name', advanced=False, refresh_button=True), StrInput(name='base_url', display_name='Base URL', advanced=False, info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\", value='http://localhost:1234/v1'), SecretStrInput(name='api_key', display_name='LM Studio API Key', info='The LM Studio API Key to use for LM Studio.', advanced=True, value='LMSTUDIO_API_KEY'), FloatInput(name='temperature', display_name='Temperature', value=0.1, advanced=True), IntInput(name='seed', display_name='Seed', info='The seed controls the reproducibility of the job.', advanced=True, value=1)]", "outputs": "", "display_name": "LM Studio", "name": "LMStudioModel", "description": "Generate text using LM Studio Local LLMs.", "icon": "LMStudio"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/redis.py", "section": "class::RedisVectorStoreComponent", "content": "from pathlib import Path\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.vectorstores.redis import Redis\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass RedisVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    A custom component for implementing a Vector Store using Redis.\n    \"\"\"\n\n    display_name: str = \"Redis\"\n    description: str = \"Implementation of Vector Store using Redis\"\n    icon = \"Redis\"\n    name = \"Redis\"\n\n    inputs = [\n        SecretStrInput(name='redis_server_url',\n        display_name='Redis Server Connection String',\n        required=True),\n        StrInput(name='redis_index_name',\n        display_name='Redis Index'),\n        StrInput(name='code',\n        display_name='Code',\n        advanced=True),\n        StrInput(name='schema',\n        display_name='Schema'),\n        *LCVectorStoreComponent.inputs,\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings'])\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RedisVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from pathlib import Path", "from langchain.text_splitter import CharacterTextSplitter", "from langchain_community.vectorstores.redis import Redis", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='redis_server_url', display_name='Redis Server Connection String', required=True), StrInput(name='redis_index_name', display_name='Redis Index'), StrInput(name='code', display_name='Code', advanced=True), StrInput(name='schema', display_name='Schema'), *LCVectorStoreComponent.inputs, IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings'])]", "outputs": "", "display_name": "Redis", "name": "Redis", "description": "Implementation of Vector Store using Redis", "icon": "Redis"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/google_search_api_core.py", "section": "class::GoogleSearchAPICore", "content": "from langchain_google_community import GoogleSearchAPIWrapper\nfrom langflow.custom import Component\nfrom langflow.io import IntInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import DataFrame\n\nclass GoogleSearchAPICore(Component):\n    display_name: str = \"Google Search API\"\n    description: str = \"Call Google Search API and return results as a DataFrame.\"\n    icon = \"Google\"\n\n    inputs = [\n        SecretStrInput(name='google_api_key',\n        display_name='Google API Key',\n        required=True),\n        SecretStrInput(name='google_cse_id',\n        display_name='Google CSE ID',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input',\n        tool_mode=True),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Results',\n        name='results',\n        type_=DataFrame,\n        method='search_google')\n    ]\n\n    def search_google(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleSearchAPICore", "base_classes": ["Component"], "public_methods": ["def search_google(self)", "def build(self)"], "imports": ["from langchain_google_community import GoogleSearchAPIWrapper", "from langflow.custom import Component", "from langflow.io import IntInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import DataFrame"], "inputs": "[SecretStrInput(name='google_api_key', display_name='Google API Key', required=True), SecretStrInput(name='google_cse_id', display_name='Google CSE ID', required=True), MultilineInput(name='input_value', display_name='Input', tool_mode=True), IntInput(name='k', display_name='Number of results', value=4, required=True)]", "outputs": "[Output(display_name='Results', name='results', type_=DataFrame, method='search_google')]", "display_name": "Google Search API", "name": "", "description": "Call Google Search API and return results as a DataFrame.", "icon": "Google"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/embeddings/ollama.py", "section": "class::OllamaEmbeddingsComponent", "content": "from typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom langchain_ollama import OllamaEmbeddings\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.ollama_constants import OLLAMA_EMBEDDING_MODELS, URL_LIST\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, MessageTextInput, Output\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        DropdownInput(name='model_name',\n        display_name='Ollama Model',\n        value='',\n        options=[],\n        real_time_refresh=True,\n        refresh_button=True,\n        combobox=True,\n        required=True),\n        MessageTextInput(name='base_url',\n        display_name='Ollama Base URL',\n        value='',\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Embeddings',\n        name='embeddings',\n        method='build_embeddings')\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OllamaEmbeddingsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_embeddings(self)"], "imports": ["from typing import Any", "from urllib.parse import urljoin", "import httpx", "from langchain_ollama import OllamaEmbeddings", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.ollama_constants import OLLAMA_EMBEDDING_MODELS, URL_LIST", "from langflow.field_typing import Embeddings", "from langflow.io import DropdownInput, MessageTextInput, Output"], "inputs": "[DropdownInput(name='model_name', display_name='Ollama Model', value='', options=[], real_time_refresh=True, refresh_button=True, combobox=True, required=True), MessageTextInput(name='base_url', display_name='Ollama Base URL', value='', required=True)]", "outputs": "[Output(display_name='Embeddings', name='embeddings', method='build_embeddings')]", "display_name": "Ollama Embeddings", "name": "OllamaEmbeddings", "description": "Generate embeddings using Ollama models.", "icon": "Ollama"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/vector_store_info.py", "section": "class::VectorStoreInfoComponent", "content": "from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreInfo\nfrom langflow.custom import Component\nfrom langflow.inputs import HandleInput, MessageTextInput, MultilineInput\nfrom langflow.template import Output\n\nclass VectorStoreInfoComponent(Component):\n    display_name: str = \"VectorStoreInfo\"\n    description: str = \"Information about a VectorStore\"\n    icon = \"LangChain\"\n    name = \"VectorStoreInfo\"\n\n    inputs = [\n        MessageTextInput(name='vectorstore_name',\n        display_name='Name',\n        info='Name of the VectorStore',\n        required=True),\n        MultilineInput(name='vectorstore_description',\n        display_name='Description',\n        info='Description of the VectorStore',\n        required=True),\n        HandleInput(name='input_vectorstore',\n        display_name='Vector Store',\n        input_types=['VectorStore'],\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Vector Store Info',\n        name='info',\n        method='build_info')\n    ]\n\n    def build_info(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VectorStoreInfoComponent", "base_classes": ["Component"], "public_methods": ["def build_info(self)"], "imports": ["from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreInfo", "from langflow.custom import Component", "from langflow.inputs import HandleInput, MessageTextInput, MultilineInput", "from langflow.template import Output"], "inputs": "[MessageTextInput(name='vectorstore_name', display_name='Name', info='Name of the VectorStore', required=True), MultilineInput(name='vectorstore_description', display_name='Description', info='Description of the VectorStore', required=True), HandleInput(name='input_vectorstore', display_name='Vector Store', input_types=['VectorStore'], required=True)]", "outputs": "[Output(display_name='Vector Store Info', name='info', method='build_info')]", "display_name": "VectorStoreInfo", "name": "VectorStoreInfo", "description": "Information about a VectorStore", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/dataframe_operations.py", "section": "class::DataFrameOperationsComponent", "content": "from langflow.custom import Component\nfrom langflow.io import BoolInput, DataFrameInput, DropdownInput, IntInput, MessageTextInput, Output, StrInput\nfrom langflow.schema import DataFrame\n\nclass DataFrameOperationsComponent(Component):\n    display_name: str = \"DataFrame Operations\"\n    description: str = \"Perform various operations on a DataFrame.\"\n    icon = \"table\"\n\n    inputs = [\n        DataFrameInput(name='df',\n        display_name='DataFrame',\n        info='The input DataFrame to operate on.'),\n        DropdownInput(name='operation',\n        display_name='Operation',\n        options=OPERATION_CHOICES,\n        info='Select the DataFrame operation to perform.',\n        real_time_refresh=True),\n        StrInput(name='column_name',\n        display_name='Column Name',\n        info='The column name to use for the operation.',\n        dynamic=True,\n        show=False),\n        MessageTextInput(name='filter_value',\n        display_name='Filter Value',\n        info='The value to filter rows by.',\n        dynamic=True,\n        show=False),\n        BoolInput(name='ascending',\n        display_name='Sort Ascending',\n        info='Whether to sort in ascending order.',\n        dynamic=True,\n        show=False,\n        value=True),\n        StrInput(name='new_column_name',\n        display_name='New Column Name',\n        info='The new column name when renaming or adding a column.',\n        dynamic=True,\n        show=False),\n        MessageTextInput(name='new_column_value',\n        display_name='New Column Value',\n        info='The value to populate the new column with.',\n        dynamic=True,\n        show=False),\n        StrInput(name='columns_to_select',\n        display_name='Columns to Select',\n        dynamic=True,\n        is_list=True,\n        show=False),\n        IntInput(name='num_rows',\n        display_name='Number of Rows',\n        info='Number of rows to return (for head/tail).',\n        dynamic=True,\n        show=False,\n        value=5),\n        MessageTextInput(name='replace_value',\n        display_name='Value to Replace',\n        info='The value to replace in the column.',\n        dynamic=True,\n        show=False),\n        MessageTextInput(name='replacement_value',\n        display_name='Replacement Value',\n        info='The value to replace with.',\n        dynamic=True,\n        show=False)\n    ]\n\n    outputs = [\n        Output(display_name='DataFrame',\n        name='output',\n        method='perform_operation',\n        info='The resulting DataFrame after the operation.')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def perform_operation(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def filter_rows_by_value(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def sort_by_column(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def drop_column(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def rename_column(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def add_column(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def select_columns(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def head(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def tail(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def replace_values(self, df):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DataFrameOperationsComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def perform_operation(self)", "def filter_rows_by_value(self, df)", "def sort_by_column(self, df)", "def drop_column(self, df)", "def rename_column(self, df)", "def add_column(self, df)", "def select_columns(self, df)", "def head(self, df)", "def tail(self, df)", "def replace_values(self, df)"], "imports": ["from langflow.custom import Component", "from langflow.io import BoolInput, DataFrameInput, DropdownInput, IntInput, MessageTextInput, Output, StrInput", "from langflow.schema import DataFrame"], "inputs": "[DataFrameInput(name='df', display_name='DataFrame', info='The input DataFrame to operate on.'), DropdownInput(name='operation', display_name='Operation', options=OPERATION_CHOICES, info='Select the DataFrame operation to perform.', real_time_refresh=True), StrInput(name='column_name', display_name='Column Name', info='The column name to use for the operation.', dynamic=True, show=False), MessageTextInput(name='filter_value', display_name='Filter Value', info='The value to filter rows by.', dynamic=True, show=False), BoolInput(name='ascending', display_name='Sort Ascending', info='Whether to sort in ascending order.', dynamic=True, show=False, value=True), StrInput(name='new_column_name', display_name='New Column Name', info='The new column name when renaming or adding a column.', dynamic=True, show=False), MessageTextInput(name='new_column_value', display_name='New Column Value', info='The value to populate the new column with.', dynamic=True, show=False), StrInput(name='columns_to_select', display_name='Columns to Select', dynamic=True, is_list=True, show=False), IntInput(name='num_rows', display_name='Number of Rows', info='Number of rows to return (for head/tail).', dynamic=True, show=False, value=5), MessageTextInput(name='replace_value', display_name='Value to Replace', info='The value to replace in the column.', dynamic=True, show=False), MessageTextInput(name='replacement_value', display_name='Replacement Value', info='The value to replace with.', dynamic=True, show=False)]", "outputs": "[Output(display_name='DataFrame', name='output', method='perform_operation', info='The resulting DataFrame after the operation.')]", "display_name": "DataFrame Operations", "name": "", "description": "Perform various operations on a DataFrame.", "icon": "table"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/openai_chat_model.py", "section": "class::OpenAIModelComponent", "content": "from typing import Any\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES, OPENAI_REASONING_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.logging import logger\nfrom openai import BadRequestError\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name: str = \"OpenAI\"\n    description: str = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.',\n        range_spec=RangeSpec(min=0,\n        max=128000)),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True,\n        info='Additional keyword arguments to pass to the model.'),\n        BoolInput(name='json_mode',\n        display_name='JSON Mode',\n        advanced=True,\n        info='If True,\n        it will output JSON regardless of passing a schema.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=OPENAI_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n        value=OPENAI_MODEL_NAMES[1],\n        combobox=True,\n        real_time_refresh=True),\n        StrInput(name='openai_api_base',\n        display_name='OpenAI API Base',\n        advanced=True,\n        info='The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat,\n        LocalAI and Prem.'),\n        SecretStrInput(name='api_key',\n        display_name='OpenAI API Key',\n        info='The OpenAI API Key to use for the OpenAI model.',\n        advanced=False,\n        value='OPENAI_API_KEY',\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        show=True),\n        IntInput(name='seed',\n        display_name='Seed',\n        info='The seed controls the reproducibility of the job.',\n        advanced=True,\n        value=1),\n        IntInput(name='max_retries',\n        display_name='Max Retries',\n        info='The maximum number of retries to make when generating.',\n        advanced=True,\n        value=5),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='The timeout for requests to OpenAI completion API.',\n        advanced=True,\n        value=700)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "OpenAIModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any", "from langchain_openai import ChatOpenAI", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.openai_constants import OPENAI_MODEL_NAMES, OPENAI_REASONING_MODEL_NAMES", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput", "from langflow.logging import logger", "from openai import BadRequestError"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.', range_spec=RangeSpec(min=0, max=128000)), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True, info='Additional keyword arguments to pass to the model.'), BoolInput(name='json_mode', display_name='JSON Mode', advanced=True, info='If True, it will output JSON regardless of passing a schema.'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=OPENAI_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES, value=OPENAI_MODEL_NAMES[1], combobox=True, real_time_refresh=True), StrInput(name='openai_api_base', display_name='OpenAI API Base', advanced=True, info='The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.'), SecretStrInput(name='api_key', display_name='OpenAI API Key', info='The OpenAI API Key to use for the OpenAI model.', advanced=False, value='OPENAI_API_KEY', required=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=1, step=0.01), show=True), IntInput(name='seed', display_name='Seed', info='The seed controls the reproducibility of the job.', advanced=True, value=1), IntInput(name='max_retries', display_name='Max Retries', info='The maximum number of retries to make when generating.', advanced=True, value=5), IntInput(name='timeout', display_name='Timeout', info='The timeout for requests to OpenAI completion API.', advanced=True, value=700)]", "outputs": "", "display_name": "OpenAI", "name": "OpenAIModel", "description": "Generates text using OpenAI LLMs.", "icon": "OpenAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/vectara.py", "section": "class::VectaraVectorStoreComponent", "content": "from typing import TYPE_CHECKING\nfrom langchain_community.vectorstores import Vectara\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data, DataFrame\nfrom langchain_community.vectorstores import Vectara\nfrom langchain_community.vectorstores import Vectara\n\nclass VectaraVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    Vectara Vector Store with search capabilities.\n    \"\"\"\n\n    display_name: str = \"Vectara\"\n    description: str = \"Vectara Vector Store with search capabilities\"\n    icon = \"Vectara\"\n    name = \"Vectara\"\n\n    inputs = [\n        StrInput(name='vectara_customer_id',\n        display_name='Vectara Customer ID',\n        required=True),\n        StrInput(name='vectara_corpus_id',\n        display_name='Vectara Corpus ID',\n        required=True),\n        SecretStrInput(name='vectara_api_key',\n        display_name='Vectara API Key',\n        required=True),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        *LCVectorStoreComponent.inputs,\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VectaraVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from typing import TYPE_CHECKING", "from langchain_community.vectorstores import Vectara", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data, DataFrame", "from langchain_community.vectorstores import Vectara", "from langchain_community.vectorstores import Vectara"], "inputs": "[StrInput(name='vectara_customer_id', display_name='Vectara Customer ID', required=True), StrInput(name='vectara_corpus_id', display_name='Vectara Corpus ID', required=True), SecretStrInput(name='vectara_api_key', display_name='Vectara API Key', required=True), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), *LCVectorStoreComponent.inputs, IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Vectara", "name": "Vectara", "description": "Vectara Vector Store with search capabilities", "icon": "Vectara"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_search.py", "section": "class::TavilySearchComponent", "content": "import httpx\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass TavilySearchComponent(Component):\n    display_name: str = \"Tavily Search API\"\n    description: str = \"**Tavily Search** is a search engine optimized for LLMs and RAG,         aimed at efficient, quick, and persistent search results.\"\n    icon = \"TavilyIcon\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Tavily API Key',\n        required=True,\n        info='Your Tavily API Key.'),\n        MessageTextInput(name='query',\n        display_name='Search Query',\n        info='The search query you want to execute with Tavily.',\n        tool_mode=True),\n        DropdownInput(name='search_depth',\n        display_name='Search Depth',\n        info='The depth of the search.',\n        options=['basic',\n        'advanced'],\n        value='advanced',\n        advanced=True),\n        IntInput(name='chunks_per_source',\n        display_name='Chunks Per Source',\n        info='The number of content chunks to retrieve from each source (1-3). Only works with advanced search.',\n        value=3,\n        advanced=True),\n        DropdownInput(name='topic',\n        display_name='Search Topic',\n        info='The category of the search.',\n        options=['general',\n        'news'],\n        value='general',\n        advanced=True),\n        IntInput(name='days',\n        display_name='Days',\n        info='Number of days back from current date to include. Only available with news topic.',\n        value=7,\n        advanced=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        info='The maximum number of search results to return.',\n        value=5,\n        advanced=True),\n        BoolInput(name='include_answer',\n        display_name='Include Answer',\n        info='Include a short answer to original query.',\n        value=True,\n        advanced=True),\n        DropdownInput(name='time_range',\n        display_name='Time Range',\n        info='The time range back from the current date to filter results.',\n        options=['day',\n        'week',\n        'month',\n        'year'],\n        value=None,\n        advanced=True),\n        BoolInput(name='include_images',\n        display_name='Include Images',\n        info='Include a list of query-related images in the response.',\n        value=True,\n        advanced=True),\n        MessageTextInput(name='include_domains',\n        display_name='Include Domains',\n        info='Comma-separated list of domains to include in the search results.',\n        advanced=True),\n        MessageTextInput(name='exclude_domains',\n        display_name='Exclude Domains',\n        info='Comma-separated list of domains to exclude from the search results.',\n        advanced=True),\n        BoolInput(name='include_raw_content',\n        display_name='Include Raw Content',\n        info='Include the cleaned and parsed HTML content of each search result.',\n        value=False,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Text',\n        name='text',\n        method='fetch_content_text')\n    ]\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TavilySearchComponent", "base_classes": ["Component"], "public_methods": ["def fetch_content(self)", "def fetch_content_text(self)"], "imports": ["import httpx", "from loguru import logger", "from langflow.custom import Component", "from langflow.helpers.data import data_to_text", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[SecretStrInput(name='api_key', display_name='Tavily API Key', required=True, info='Your Tavily API Key.'), MessageTextInput(name='query', display_name='Search Query', info='The search query you want to execute with Tavily.', tool_mode=True), DropdownInput(name='search_depth', display_name='Search Depth', info='The depth of the search.', options=['basic', 'advanced'], value='advanced', advanced=True), IntInput(name='chunks_per_source', display_name='Chunks Per Source', info='The number of content chunks to retrieve from each source (1-3). Only works with advanced search.', value=3, advanced=True), DropdownInput(name='topic', display_name='Search Topic', info='The category of the search.', options=['general', 'news'], value='general', advanced=True), IntInput(name='days', display_name='Days', info='Number of days back from current date to include. Only available with news topic.', value=7, advanced=True), IntInput(name='max_results', display_name='Max Results', info='The maximum number of search results to return.', value=5, advanced=True), BoolInput(name='include_answer', display_name='Include Answer', info='Include a short answer to original query.', value=True, advanced=True), DropdownInput(name='time_range', display_name='Time Range', info='The time range back from the current date to filter results.', options=['day', 'week', 'month', 'year'], value=None, advanced=True), BoolInput(name='include_images', display_name='Include Images', info='Include a list of query-related images in the response.', value=True, advanced=True), MessageTextInput(name='include_domains', display_name='Include Domains', info='Comma-separated list of domains to include in the search results.', advanced=True), MessageTextInput(name='exclude_domains', display_name='Exclude Domains', info='Comma-separated list of domains to exclude from the search results.', advanced=True), BoolInput(name='include_raw_content', display_name='Include Raw Content', info='Include the cleaned and parsed HTML content of each search result.', value=False, advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Text', name='text', method='fetch_content_text')]", "display_name": "Tavily Search API", "name": "", "description": "**Tavily Search** is a search engine optimized for LLMs and RAG,         aimed at efficient, quick, and persistent search results.", "icon": "TavilyIcon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/xml_agent.py", "section": "class::XMLAgentComponent", "content": "from langchain.agents import create_xml_agent\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MultilineInput\nfrom langflow.inputs.inputs import DataInput, HandleInput\nfrom langflow.schema import Data\n\nclass XMLAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"XML Agent\"\n    description: str = \"Agent that uses tools formatting instructions as xml to the Language Model.\"\n    icon = \"LangChain\"\n    name = \"XMLAgent\"\n\n    inputs = [\n        *LCToolsAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        DataInput(name='chat_history',\n        display_name='Chat History',\n        is_list=True,\n        advanced=True),\n        MultilineInput(name='system_prompt',\n        display_name='System Prompt',\n        info='System prompt for the agent.',\n        value=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool,\n        you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\n\\nFor example,\n        if you have a tool called 'search' that could run a google search,\n        in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n\\n<observation>64 degrees</observation>\\n\\nWhen you are done,\n        respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nQuestion: {input}\\n\\n{agent_scratchpad}\\n            \"),\n        MultilineInput(name='user_prompt',\n        display_name='Prompt',\n        info=\"This prompt must contain 'input' key.\",\n        value='{input}')\n    ]\n\n    def get_chat_history_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_agent_runnable(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "XMLAgentComponent", "base_classes": ["LCToolsAgentComponent"], "public_methods": ["def get_chat_history_data(self)", "def create_agent_runnable(self)"], "imports": ["from langchain.agents import create_xml_agent", "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate", "from langflow.base.agents.agent import LCToolsAgentComponent", "from langflow.inputs import MultilineInput", "from langflow.inputs.inputs import DataInput, HandleInput", "from langflow.schema import Data"], "inputs": "[*LCToolsAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), DataInput(name='chat_history', display_name='Chat History', is_list=True, advanced=True), MultilineInput(name='system_prompt', display_name='System Prompt', info='System prompt for the agent.', value=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\n\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nQuestion: {input}\\n\\n{agent_scratchpad}\\n            \"), MultilineInput(name='user_prompt', display_name='Prompt', info=\"This prompt must contain 'input' key.\", value='{input}')]", "outputs": "", "display_name": "XML Agent", "name": "XMLAgent", "description": "Agent that uses tools formatting instructions as xml to the Language Model.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/parse_data.py", "section": "class::ParseDataComponent", "content": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text, data_to_text_list\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass ParseDataComponent(Component):\n    display_name: str = \"Data to Message\"\n    description: str = \"Convert Data objects into Messages using any {field_name} from input data.\"\n    icon = \"message-square\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name='data',\n        display_name='Data',\n        info='The data to convert to text.',\n        is_list=True,\n        required=True),\n        MultilineInput(name='template',\n        display_name='Template',\n        info='The template to use for formatting the data. It can contain the keys {text},\n        {data} or any other key in the Data.',\n        value='{text}',\n        required=True),\n        StrInput(name='sep',\n        display_name='Separator',\n        advanced=True,\n        value='\\n')\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='text',\n        info='Data as a single Message,\n        with each input Data separated by Separator',\n        method='parse_data'),\n        Output(display_name='Data List',\n        name='data_list',\n        info='Data as a list of new Data,\n        each having `text` formatted by Template',\n        method='parse_data_as_list')\n    ]\n\n    def parse_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def parse_data_as_list(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ParseDataComponent", "base_classes": ["Component"], "public_methods": ["def parse_data(self)", "def parse_data_as_list(self)"], "imports": ["from langflow.custom import Component", "from langflow.helpers.data import data_to_text, data_to_text_list", "from langflow.io import DataInput, MultilineInput, Output, StrInput", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[DataInput(name='data', display_name='Data', info='The data to convert to text.', is_list=True, required=True), MultilineInput(name='template', display_name='Template', info='The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.', value='{text}', required=True), StrInput(name='sep', display_name='Separator', advanced=True, value='\\n')]", "outputs": "[Output(display_name='Message', name='text', info='Data as a single Message, with each input Data separated by Separator', method='parse_data'), Output(display_name='Data List', name='data_list', info='Data as a list of new Data, each having `text` formatted by Template', method='parse_data_as_list')]", "display_name": "Data to Message", "name": "ParseData", "description": "Convert Data objects into Messages using any {field_name} from input data.", "icon": "message-square"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/huggingface.py", "section": "class::HuggingFaceEndpointsComponent", "content": "from typing import Any\nfrom langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput, StrInput\n\nclass HuggingFaceEndpointsComponent(LCModelComponent):\n    display_name: str = \"HuggingFace\"\n    description: str = \"Generate text using Hugging Face Inference APIs.\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(name='model_id',\n        display_name='Model ID',\n        info='Select a model from HuggingFace Hub',\n        options=[DEFAULT_MODEL,\n        'mistralai/Mixtral-8x7B-Instruct-v0.1',\n        'mistralai/Mistral-7B-Instruct-v0.3',\n        'meta-llama/Llama-3.1-8B-Instruct',\n        'Qwen/Qwen2.5-Coder-32B-Instruct',\n        'Qwen/QwQ-32B-Preview',\n        'openai-community/gpt2',\n        'custom'],\n        value=DEFAULT_MODEL,\n        required=True,\n        real_time_refresh=True),\n        StrInput(name='custom_model',\n        display_name='Custom Model ID',\n        info='Enter a custom model ID from HuggingFace Hub',\n        value='',\n        show=False,\n        required=True),\n        IntInput(name='max_new_tokens',\n        display_name='Max New Tokens',\n        value=512,\n        info='Maximum number of generated tokens'),\n        IntInput(name='top_k',\n        display_name='Top K',\n        advanced=True,\n        info='The number of highest probability vocabulary tokens to keep for top-k-filtering'),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        value=0.95,\n        advanced=True,\n        info='If set to < 1,\n        only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation'),\n        FloatInput(name='typical_p',\n        display_name='Typical P',\n        value=0.95,\n        advanced=True,\n        info='Typical Decoding mass.'),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.8,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        info='The value used to module the logits distribution',\n        advanced=True),\n        FloatInput(name='repetition_penalty',\n        display_name='Repetition Penalty',\n        info='The parameter for repetition penalty. 1.0 means no penalty.',\n        advanced=True),\n        StrInput(name='inference_endpoint',\n        display_name='Inference Endpoint',\n        value='https://api-inference.huggingface.co/models/',\n        info='Custom inference endpoint URL.',\n        required=True),\n        DropdownInput(name='task',\n        display_name='Task',\n        options=['text2text-generation',\n        'text-generation',\n        'summarization',\n        'translation'],\n        value='text-generation',\n        advanced=True,\n        info='The task to call the model with. Should be a task that returns `generated_text` or `summary_text`.'),\n        SecretStrInput(name='huggingfacehub_api_token',\n        display_name='API Token',\n        password=True,\n        required=True),\n        DictInput(name='model_kwargs',\n        display_name='Model Keyword Arguments',\n        advanced=True),\n        IntInput(name='retry_attempts',\n        display_name='Retry Attempts',\n        value=1,\n        advanced=True)\n    ]\n\n    def get_api_url(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_huggingface_endpoint(self, task, huggingfacehub_api_token, model_kwargs, max_new_tokens, top_k, top_p, typical_p, temperature, repetition_penalty):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HuggingFaceEndpointsComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def get_api_url(self)", "def create_huggingface_endpoint(self, task, huggingfacehub_api_token, model_kwargs, max_new_tokens, top_k, top_p, typical_p, temperature, repetition_penalty)", "def build_model(self)"], "imports": ["from typing import Any", "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint", "from tenacity import retry, stop_after_attempt, wait_fixed", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput, StrInput"], "inputs": "[*LCModelComponent._base_inputs, DropdownInput(name='model_id', display_name='Model ID', info='Select a model from HuggingFace Hub', options=[DEFAULT_MODEL, 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'mistralai/Mistral-7B-Instruct-v0.3', 'meta-llama/Llama-3.1-8B-Instruct', 'Qwen/Qwen2.5-Coder-32B-Instruct', 'Qwen/QwQ-32B-Preview', 'openai-community/gpt2', 'custom'], value=DEFAULT_MODEL, required=True, real_time_refresh=True), StrInput(name='custom_model', display_name='Custom Model ID', info='Enter a custom model ID from HuggingFace Hub', value='', show=False, required=True), IntInput(name='max_new_tokens', display_name='Max New Tokens', value=512, info='Maximum number of generated tokens'), IntInput(name='top_k', display_name='Top K', advanced=True, info='The number of highest probability vocabulary tokens to keep for top-k-filtering'), FloatInput(name='top_p', display_name='Top P', value=0.95, advanced=True, info='If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation'), FloatInput(name='typical_p', display_name='Typical P', value=0.95, advanced=True, info='Typical Decoding mass.'), SliderInput(name='temperature', display_name='Temperature', value=0.8, range_spec=RangeSpec(min=0, max=2, step=0.01), info='The value used to module the logits distribution', advanced=True), FloatInput(name='repetition_penalty', display_name='Repetition Penalty', info='The parameter for repetition penalty. 1.0 means no penalty.', advanced=True), StrInput(name='inference_endpoint', display_name='Inference Endpoint', value='https://api-inference.huggingface.co/models/', info='Custom inference endpoint URL.', required=True), DropdownInput(name='task', display_name='Task', options=['text2text-generation', 'text-generation', 'summarization', 'translation'], value='text-generation', advanced=True, info='The task to call the model with. Should be a task that returns `generated_text` or `summary_text`.'), SecretStrInput(name='huggingfacehub_api_token', display_name='API Token', password=True, required=True), DictInput(name='model_kwargs', display_name='Model Keyword Arguments', advanced=True), IntInput(name='retry_attempts', display_name='Retry Attempts', value=1, advanced=True)]", "outputs": "", "display_name": "HuggingFace", "name": "HuggingFaceModel", "description": "Generate text using Hugging Face Inference APIs.", "icon": "HuggingFace"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/graph_rag.py", "section": "class::GraphRAGComponent", "content": "import inspect\nfrom abc import ABC\nimport graph_retriever.strategies as strategies_module\nfrom langchain_graph_retriever import GraphRetriever\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import DropdownInput, HandleInput, MultilineInput, NestedDictInput, StrInput\nfrom langflow.schema import Data\nfrom graph_retriever.edges.metadata import Id\n\nclass GraphRAGComponent(LCVectorStoreComponent):\n    \"\"\"\n    GraphRAGComponent is a component for performing Graph RAG traversal in a vector store.\n    \n    Attributes:\n        display_name (str): The display name of the component.\n        description (str): A brief description of the component.\n        name (str): The name of the component.\n        icon (str): The icon representing the component.\n        inputs (list): A list of input configurations for the component.\n    \n    Methods:\n        _build_search_args():\n            Builds the arguments required for the search operation.\n        search_documents() -> list[Data]:\n            Searches for documents using the specified strategy, edge definition, and query.\n        _edge_definition_from_input() -> tuple:\n            Processes the edge definition input and returns it as a tuple.\n    \"\"\"\n\n    display_name: str = \"Graph RAG\"\n    description: str = \"Graph RAG traversal for vector store.\"\n    icon = \"AstraDB\"\n    name = \"Graph RAG\"\n\n    inputs = [\n        HandleInput(name='embedding_model',\n        display_name='Embedding Model',\n        input_types=['Embeddings'],\n        info='Specify the Embedding Model. Not required for Astra Vectorize collections.',\n        required=False),\n        HandleInput(name='vector_store',\n        display_name='Vector Store Connection',\n        input_types=['VectorStore'],\n        info='Connection to Vector Store.'),\n        StrInput(name='edge_definition',\n        display_name='Edge Definition',\n        info='Edge definition for the graph traversal.'),\n        DropdownInput(name='strategy',\n        display_name='Traversal Strategies',\n        options=traversal_strategies()),\n        MultilineInput(name='search_query',\n        display_name='Search Query',\n        tool_mode=True),\n        NestedDictInput(name='graphrag_strategy_kwargs',\n        display_name='Strategy Parameters',\n        info='Optional dictionary of additional parameters for the retrieval strategy. Please see https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/ for details.',\n        advanced=True)\n    ]\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GraphRAGComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def search_documents(self)"], "imports": ["import inspect", "from abc import ABC", "import graph_retriever.strategies as strategies_module", "from langchain_graph_retriever import GraphRetriever", "from langflow.base.vectorstores.model import LCVectorStoreComponent", "from langflow.helpers import docs_to_data", "from langflow.inputs import DropdownInput, HandleInput, MultilineInput, NestedDictInput, StrInput", "from langflow.schema import Data", "from graph_retriever.edges.metadata import Id"], "inputs": "[HandleInput(name='embedding_model', display_name='Embedding Model', input_types=['Embeddings'], info='Specify the Embedding Model. Not required for Astra Vectorize collections.', required=False), HandleInput(name='vector_store', display_name='Vector Store Connection', input_types=['VectorStore'], info='Connection to Vector Store.'), StrInput(name='edge_definition', display_name='Edge Definition', info='Edge definition for the graph traversal.'), DropdownInput(name='strategy', display_name='Traversal Strategies', options=traversal_strategies()), MultilineInput(name='search_query', display_name='Search Query', tool_mode=True), NestedDictInput(name='graphrag_strategy_kwargs', display_name='Strategy Parameters', info='Optional dictionary of additional parameters for the retrieval strategy. Please see https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/ for details.', advanced=True)]", "outputs": "", "display_name": "Graph RAG", "name": "Graph RAG", "description": "Graph RAG traversal for vector store.", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/calculator.py", "section": "class::CalculatorToolComponent", "content": "import ast\nimport operator\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MessageTextInput\nfrom langflow.schema import Data\n\nclass CalculatorToolComponent(LCToolComponent):\n    display_name: str = \"Calculator [DEPRECATED]\"\n    description: str = \"Perform basic arithmetic operations on a given expression.\"\n    icon = \"calculator\"\n    name = \"CalculatorTool\"\n\n    inputs = [\n        MessageTextInput(name='expression',\n        display_name='Expression',\n        info=\"The arithmetic expression to evaluate (e.g.,\n        '4*4*(33/22)+12-20').\")\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CalculatorToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import ast", "import operator", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MessageTextInput", "from langflow.schema import Data"], "inputs": "[MessageTextInput(name='expression', display_name='Expression', info=\"The arithmetic expression to evaluate (e.g., '4*4*(33/22)+12-20').\")]", "outputs": "", "display_name": "Calculator [DEPRECATED]", "name": "CalculatorTool", "description": "Perform basic arithmetic operations on a given expression.", "icon": "calculator"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/calculator.py", "section": "class::CalculatorToolSchema", "content": "import ast\nimport operator\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MessageTextInput\nfrom langflow.schema import Data\n\nclass CalculatorToolSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "CalculatorToolSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import ast", "import operator", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MessageTextInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/vector_store_router.py", "section": "class::VectorStoreRouterAgentComponent", "content": "from langchain.agents import AgentExecutor, create_vectorstore_router_agent\nfrom langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreRouterToolkit\nfrom langflow.base.agents.agent import LCAgentComponent\nfrom langflow.inputs import HandleInput\n\nclass VectorStoreRouterAgentComponent(LCAgentComponent):\n    display_name: str = \"VectorStoreRouterAgent\"\n    description: str = \"Construct an agent from a Vector Store Router.\"\n    name = \"VectorStoreRouterAgent\"\n\n    inputs = [\n        *LCAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        HandleInput(name='vectorstores',\n        display_name='Vector Stores',\n        input_types=['VectorStoreInfo'],\n        is_list=True,\n        required=True)\n    ]\n\n    def build_agent(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VectorStoreRouterAgentComponent", "base_classes": ["LCAgentComponent"], "public_methods": ["def build_agent(self)"], "imports": ["from langchain.agents import AgentExecutor, create_vectorstore_router_agent", "from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreRouterToolkit", "from langflow.base.agents.agent import LCAgentComponent", "from langflow.inputs import HandleInput"], "inputs": "[*LCAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), HandleInput(name='vectorstores', display_name='Vector Stores', input_types=['VectorStoreInfo'], is_list=True, required=True)]", "outputs": "", "display_name": "VectorStoreRouterAgent", "name": "VectorStoreRouterAgent", "description": "Construct an agent from a Vector Store Router.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/create_data.py", "section": "class::CreateDataComponent", "content": "from typing import Any\nfrom langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput, DictInput, IntInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\nclass CreateDataComponent(Component):\n    display_name: str = \"Create Data\"\n    description: str = \"Dynamically create a Data with a specified number of fields.\"\n    icon = \"ListFilter\"\n    name = \"CreateData\"\n\n    inputs = [\n        IntInput(name='number_of_fields',\n        display_name='Number of Fields',\n        info='Number of fields to be added to the record.',\n        real_time_refresh=True,\n        value=1,\n        range_spec=RangeSpec(min=1,\n        max=MAX_FIELDS,\n        step=1,\n        step_type='int')),\n        MessageTextInput(name='text_key',\n        display_name='Text Key',\n        info='Key that identifies the field to be used as the text content.',\n        advanced=True),\n        BoolInput(name='text_key_validator',\n        display_name='Text Key Validator',\n        advanced=True,\n        info=\"If enabled,\n        checks if the given 'Text Key' is present in the given 'Data'.\")\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='build_data')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def validate_text_key(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CreateDataComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)", "def get_data(self)", "def validate_text_key(self)"], "imports": ["from typing import Any", "from langflow.custom import Component", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs.inputs import BoolInput, DictInput, IntInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict"], "inputs": "[IntInput(name='number_of_fields', display_name='Number of Fields', info='Number of fields to be added to the record.', real_time_refresh=True, value=1, range_spec=RangeSpec(min=1, max=MAX_FIELDS, step=1, step_type='int')), MessageTextInput(name='text_key', display_name='Text Key', info='Key that identifies the field to be used as the text content.', advanced=True), BoolInput(name='text_key_validator', display_name='Text Key Validator', advanced=True, info=\"If enabled, checks if the given 'Text Key' is present in the given 'Data'.\")]", "outputs": "[Output(display_name='Data', name='data', method='build_data')]", "display_name": "Create Data", "name": "CreateData", "description": "Dynamically create a Data with a specified number of fields.", "icon": "ListFilter"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/deepseek.py", "section": "class::DeepSeekModelComponent", "content": "import requests\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langchain_openai import ChatOpenAI\nfrom openai import BadRequestError\n\nclass DeepSeekModelComponent(LCModelComponent):\n    display_name: str = \"DeepSeek\"\n    description: str = \"Generate text using DeepSeek LLMs.\"\n    icon = \"DeepSeek\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='Maximum number of tokens to generate. Set to 0 for unlimited.',\n        range_spec=RangeSpec(min=0,\n        max=128000)),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True,\n        info='Additional keyword arguments to pass to the model.'),\n        BoolInput(name='json_mode',\n        display_name='JSON Mode',\n        advanced=True,\n        info='If True,\n        it will output JSON regardless of passing a schema.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        info='DeepSeek model to use',\n        options=DEEPSEEK_MODELS,\n        value='deepseek-chat',\n        refresh_button=True),\n        StrInput(name='api_base',\n        display_name='DeepSeek API Base',\n        advanced=True,\n        info='Base URL for API requests. Defaults to https://api.deepseek.com',\n        value='https://api.deepseek.com'),\n        SecretStrInput(name='api_key',\n        display_name='DeepSeek API Key',\n        info='The DeepSeek API Key',\n        advanced=False,\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        info='Controls randomness in responses',\n        value=1.0,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        advanced=True),\n        IntInput(name='seed',\n        display_name='Seed',\n        info='The seed controls the reproducibility of the job.',\n        advanced=True,\n        value=1)\n    ]\n\n    def get_models(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DeepSeekModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def get_models(self)", "def update_build_config(self, build_config, field_value, field_name)", "def build_model(self)"], "imports": ["import requests", "from pydantic.v1 import SecretStr", "from typing_extensions import override", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput", "from langchain_openai import ChatOpenAI", "from openai import BadRequestError"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='Maximum number of tokens to generate. Set to 0 for unlimited.', range_spec=RangeSpec(min=0, max=128000)), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True, info='Additional keyword arguments to pass to the model.'), BoolInput(name='json_mode', display_name='JSON Mode', advanced=True, info='If True, it will output JSON regardless of passing a schema.'), DropdownInput(name='model_name', display_name='Model Name', info='DeepSeek model to use', options=DEEPSEEK_MODELS, value='deepseek-chat', refresh_button=True), StrInput(name='api_base', display_name='DeepSeek API Base', advanced=True, info='Base URL for API requests. Defaults to https://api.deepseek.com', value='https://api.deepseek.com'), SecretStrInput(name='api_key', display_name='DeepSeek API Key', info='The DeepSeek API Key', advanced=False, required=True), SliderInput(name='temperature', display_name='Temperature', info='Controls randomness in responses', value=1.0, range_spec=RangeSpec(min=0, max=2, step=0.01), advanced=True), IntInput(name='seed', display_name='Seed', info='The seed controls the reproducibility of the job.', advanced=True, value=1)]", "outputs": "", "display_name": "DeepSeek", "name": "", "description": "Generate text using DeepSeek LLMs.", "icon": "DeepSeek"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/weaviate.py", "section": "class::WeaviateVectorStoreComponent", "content": "import weaviate\nfrom langchain_community.vectorstores import Weaviate\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass WeaviateVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Weaviate\"\n    description: str = \"Weaviate Vector Store with search capabilities\"\n    icon = \"Weaviate\"\n    name = \"Weaviate\"\n\n    inputs = [\n        StrInput(name='url',\n        display_name='Weaviate URL',\n        value='http://localhost:8080',\n        required=True),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        required=False),\n        StrInput(name='index_name',\n        display_name='Index Name',\n        required=True,\n        info='Requires capitalized index name.'),\n        StrInput(name='text_key',\n        display_name='Text Key',\n        value='text',\n        advanced=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True),\n        BoolInput(name='search_by_text',\n        display_name='Search By Text',\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WeaviateVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["import weaviate", "from langchain_community.vectorstores import Weaviate", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import BoolInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='url', display_name='Weaviate URL', value='http://localhost:8080', required=True), SecretStrInput(name='api_key', display_name='API Key', required=False), StrInput(name='index_name', display_name='Index Name', required=True, info='Requires capitalized index name.'), StrInput(name='text_key', display_name='Text Key', value='text', advanced=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True), BoolInput(name='search_by_text', display_name='Search By Text', advanced=True)]", "outputs": "", "display_name": "Weaviate", "name": "Weaviate", "description": "Weaviate Vector Store with search capabilities", "icon": "Weaviate"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/python_repl_core.py", "section": "class::PythonREPLComponent", "content": "import importlib\nfrom langchain_experimental.utilities import PythonREPL\nfrom langflow.custom import Component\nfrom langflow.io import CodeInput, Output, StrInput\nfrom langflow.schema import Data\n\nclass PythonREPLComponent(Component):\n    display_name: str = \"Python REPL\"\n    description: str = \"A Python code executor that lets you run Python code with specific imported modules. Remember to always use print() to see your results. Example: print(df.head())\"\n    icon = \"Python\"\n\n    inputs = [\n        StrInput(name='global_imports',\n        display_name='Global Imports',\n        info=\"A comma-separated list of modules to import globally,\n        e.g. 'math,numpy,pandas'.\",\n        value='math,pandas',\n        required=True),\n        CodeInput(name='python_code',\n        display_name='Python Code',\n        info='The Python code to execute. Only modules specified in Global Imports can be used.',\n        value=\"print('Hello,\n        World!')\",\n        input_types=['Message'],\n        tool_mode=True,\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Results',\n        name='results',\n        type_=Data,\n        method='run_python_repl')\n    ]\n\n    def get_globals(self, global_imports):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_python_repl(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PythonREPLComponent", "base_classes": ["Component"], "public_methods": ["def get_globals(self, global_imports)", "def run_python_repl(self)", "def build(self)"], "imports": ["import importlib", "from langchain_experimental.utilities import PythonREPL", "from langflow.custom import Component", "from langflow.io import CodeInput, Output, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='global_imports', display_name='Global Imports', info=\"A comma-separated list of modules to import globally, e.g. 'math,numpy,pandas'.\", value='math,pandas', required=True), CodeInput(name='python_code', display_name='Python Code', info='The Python code to execute. Only modules specified in Global Imports can be used.', value=\"print('Hello, World!')\", input_types=['Message'], tool_mode=True, required=True)]", "outputs": "[Output(display_name='Results', name='results', type_=Data, method='run_python_repl')]", "display_name": "Python REPL", "name": "", "description": "A Python code executor that lets you run Python code with specific imported modules. Remember to always use print() to see your results. Example: print(df.head())", "icon": "Python"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/runnable_executor.py", "section": "class::RunnableExecComponent", "content": "from langchain.agents import AgentExecutor\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, HandleInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\nclass RunnableExecComponent(Component):\n    display_name: str = \"Runnable Executor\"\n    description: str = \"Execute a runnable. It will try to guess the input and output keys.\"\n    icon = \"LangChain\"\n    name = \"RunnableExecutor\"\n\n    inputs = [\n        MessageTextInput(name='input_value',\n        display_name='Input',\n        required=True),\n        HandleInput(name='runnable',\n        display_name='Agent Executor',\n        input_types=['Chain',\n        'AgentExecutor',\n        'Agent',\n        'Runnable'],\n        required=True),\n        MessageTextInput(name='input_key',\n        display_name='Input Key',\n        value='input',\n        advanced=True),\n        MessageTextInput(name='output_key',\n        display_name='Output Key',\n        value='output',\n        advanced=True),\n        BoolInput(name='use_stream',\n        display_name='Stream',\n        value=False)\n    ]\n\n    outputs = [\n        Output(display_name='Message',\n        name='text',\n        method='build_executor')\n    ]\n\n    def get_output(self, result, input_key, output_key):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_input_dict(self, runnable, input_key, input_value):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RunnableExecComponent", "base_classes": ["Component"], "public_methods": ["def get_output(self, result, input_key, output_key)", "def get_input_dict(self, runnable, input_key, input_value)"], "imports": ["from langchain.agents import AgentExecutor", "from langflow.custom import Component", "from langflow.inputs import BoolInput, HandleInput, MessageTextInput", "from langflow.schema.message import Message", "from langflow.template import Output"], "inputs": "[MessageTextInput(name='input_value', display_name='Input', required=True), HandleInput(name='runnable', display_name='Agent Executor', input_types=['Chain', 'AgentExecutor', 'Agent', 'Runnable'], required=True), MessageTextInput(name='input_key', display_name='Input Key', value='input', advanced=True), MessageTextInput(name='output_key', display_name='Output Key', value='output', advanced=True), BoolInput(name='use_stream', display_name='Stream', value=False)]", "outputs": "[Output(display_name='Message', name='text', method='build_executor')]", "display_name": "Runnable Executor", "name": "RunnableExecutor", "description": "Execute a runnable. It will try to guess the input and output keys.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/regex.py", "section": "class::RegexExtractorComponent", "content": "import re\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass RegexExtractorComponent(Component):\n    display_name: str = \"Regex Extractor\"\n    description: str = \"Extract patterns from text using regular expressions.\"\n    icon = \"regex\"\n\n    inputs = [\n        MessageTextInput(name='input_text',\n        display_name='Input Text',\n        info='The text to analyze',\n        required=True),\n        MessageTextInput(name='pattern',\n        display_name='Regex Pattern',\n        info='The regular expression pattern to match',\n        value='',\n        required=True,\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='extract_matches'),\n        Output(display_name='Message',\n        name='text',\n        method='get_matches_text')\n    ]\n\n    def extract_matches(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_matches_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RegexExtractorComponent", "base_classes": ["Component"], "public_methods": ["def extract_matches(self)", "def get_matches_text(self)"], "imports": ["import re", "from langflow.custom import Component", "from langflow.io import MessageTextInput, Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='input_text', display_name='Input Text', info='The text to analyze', required=True), MessageTextInput(name='pattern', display_name='Regex Pattern', info='The regular expression pattern to match', value='', required=True, tool_mode=True)]", "outputs": "[Output(display_name='Data', name='data', method='extract_matches'), Output(display_name='Message', name='text', method='get_matches_text')]", "display_name": "Regex Extractor", "name": "", "description": "Extract patterns from text using regular expressions.", "icon": "regex"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/anthropic.py", "section": "class::AnthropicModelComponent", "content": "from typing import Any, cast\nimport requests\nfrom loguru import logger\nfrom langflow.base.models.anthropic_constants import ANTHROPIC_MODELS, DEFAULT_ANTHROPIC_API_URL, TOOL_CALLING_SUPPORTED_ANTHROPIC_MODELS, TOOL_CALLING_UNSUPPORTED_ANTHROPIC_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langflow.schema.dotdict import dotdict\nfrom langchain_anthropic.chat_models import ChatAnthropic\nimport anthropic\nfrom anthropic import BadRequestError\nfrom langchain_anthropic.chat_models import ChatAnthropic\n\nclass AnthropicModelComponent(LCModelComponent):\n    display_name: str = \"Anthropic\"\n    description: str = \"Generate text using Anthropic Chat&Completion LLMs with prefill support.\"\n    icon = \"Anthropic\"\n    name = \"AnthropicModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        value=4096,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        options=ANTHROPIC_MODELS,\n        refresh_button=True,\n        value=ANTHROPIC_MODELS[0],\n        combobox=True),\n        SecretStrInput(name='api_key',\n        display_name='Anthropic API Key',\n        info='Your Anthropic API key.',\n        value=None,\n        required=True,\n        real_time_refresh=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        info='Run inference with this temperature. Must by in the closed interval [0.0,\n        1.0].',\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True),\n        MessageTextInput(name='base_url',\n        display_name='Anthropic API URL',\n        info=\"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.\",\n        value=DEFAULT_ANTHROPIC_API_URL,\n        real_time_refresh=True,\n        advanced=True),\n        BoolInput(name='tool_model_enabled',\n        display_name='Enable Tool Models',\n        info='Select if you want to use models that can work with tools. If yes,\n        only those models will be shown.',\n        advanced=False,\n        value=False,\n        real_time_refresh=True),\n        MessageTextInput(name='prefill',\n        display_name='Prefill',\n        info=\"Prefill text to guide the model's response.\",\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_models(self, tool_model_enabled):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AnthropicModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)", "def get_models(self, tool_model_enabled)", "def update_build_config(self, build_config, field_value, field_name)"], "imports": ["from typing import Any, cast", "import requests", "from loguru import logger", "from langflow.base.models.anthropic_constants import ANTHROPIC_MODELS, DEFAULT_ANTHROPIC_API_URL, TOOL_CALLING_SUPPORTED_ANTHROPIC_MODELS, TOOL_CALLING_UNSUPPORTED_ANTHROPIC_MODELS", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput", "from langflow.schema.dotdict import dotdict", "from langchain_anthropic.chat_models import ChatAnthropic", "import anthropic", "from anthropic import BadRequestError", "from langchain_anthropic.chat_models import ChatAnthropic"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, value=4096, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.'), DropdownInput(name='model_name', display_name='Model Name', options=ANTHROPIC_MODELS, refresh_button=True, value=ANTHROPIC_MODELS[0], combobox=True), SecretStrInput(name='api_key', display_name='Anthropic API Key', info='Your Anthropic API key.', value=None, required=True, real_time_refresh=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, info='Run inference with this temperature. Must by in the closed interval [0.0, 1.0].', range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True), MessageTextInput(name='base_url', display_name='Anthropic API URL', info=\"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.\", value=DEFAULT_ANTHROPIC_API_URL, real_time_refresh=True, advanced=True), BoolInput(name='tool_model_enabled', display_name='Enable Tool Models', info='Select if you want to use models that can work with tools. If yes, only those models will be shown.', advanced=False, value=False, real_time_refresh=True), MessageTextInput(name='prefill', display_name='Prefill', info=\"Prefill text to guide the model's response.\", advanced=True)]", "outputs": "", "display_name": "Anthropic", "name": "AnthropicModel", "description": "Generate text using Anthropic Chat&Completion LLMs with prefill support.", "icon": "Anthropic"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/astradb_graph.py", "section": "class::AstraDBGraphVectorStoreComponent", "content": "import os\nimport orjson\nfrom astrapy.admin import parse_api_endpoint\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langchain_astradb import AstraDBGraphVectorStore\nfrom langchain_astradb.utils.astradb import SetupMode\n\nclass AstraDBGraphVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Astra DB Graph\"\n    description: str = \"Implementation of Graph Vector Store using Astra DB\"\n    icon = \"AstraDB\"\n    name = \"AstraDBGraph\"\n\n    inputs = [\n        SecretStrInput(name='token',\n        display_name='Astra DB Application Token',\n        info='Authentication token for accessing Astra DB.',\n        value='ASTRA_DB_APPLICATION_TOKEN',\n        required=True,\n        advanced=os.getenv('ASTRA_ENHANCED',\n        'false').lower() == 'true'),\n        SecretStrInput(name='api_endpoint',\n        display_name='Database' if os.getenv('ASTRA_ENHANCED',\n        'false').lower() == 'true' else 'API Endpoint',\n        info='API endpoint URL for the Astra DB service.',\n        value='ASTRA_DB_API_ENDPOINT',\n        required=True),\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        info='The name of the collection within Astra DB where the vectors will be stored.',\n        required=True),\n        StrInput(name='metadata_incoming_links_key',\n        display_name='Metadata incoming links key',\n        info='Metadata key used for incoming links.',\n        advanced=True),\n        *LCVectorStoreComponent.inputs,\n        StrInput(name='keyspace',\n        display_name='Keyspace',\n        info='Optional keyspace within Astra DB to use for the collection.',\n        advanced=True),\n        HandleInput(name='embedding_model',\n        display_name='Embedding Model',\n        input_types=['Embeddings'],\n        info='Allows an embedding model configuration.'),\n        DropdownInput(name='metric',\n        display_name='Metric',\n        info='Optional distance metric for vector comparisons in the vector store.',\n        options=['cosine',\n        'dot_product',\n        'euclidean'],\n        value='cosine',\n        advanced=True),\n        IntInput(name='batch_size',\n        display_name='Batch Size',\n        info='Optional number of data to process in a single batch.',\n        advanced=True),\n        IntInput(name='bulk_insert_batch_concurrency',\n        display_name='Bulk Insert Batch Concurrency',\n        info='Optional concurrency level for bulk insert operations.',\n        advanced=True),\n        IntInput(name='bulk_insert_overwrite_concurrency',\n        display_name='Bulk Insert Overwrite Concurrency',\n        info='Optional concurrency level for bulk insert operations that overwrite existing data.',\n        advanced=True),\n        IntInput(name='bulk_delete_concurrency',\n        display_name='Bulk Delete Concurrency',\n        info='Optional concurrency level for bulk delete operations.',\n        advanced=True),\n        DropdownInput(name='setup_mode',\n        display_name='Setup Mode',\n        info=\"Configuration mode for setting up the vector store,\n        with options like 'Sync',\n        or 'Off'.\",\n        options=['Sync',\n        'Off'],\n        advanced=True,\n        value='Sync'),\n        BoolInput(name='pre_delete_collection',\n        display_name='Pre Delete Collection',\n        info='Boolean flag to determine whether to delete the collection before creating a new one.',\n        advanced=True,\n        value=False),\n        StrInput(name='metadata_indexing_include',\n        display_name='Metadata Indexing Include',\n        info='Optional list of metadata fields to include in the indexing.',\n        advanced=True,\n        list=True),\n        StrInput(name='metadata_indexing_exclude',\n        display_name='Metadata Indexing Exclude',\n        info='Optional list of metadata fields to exclude from the indexing.',\n        advanced=True,\n        list=True),\n        StrInput(name='collection_indexing_policy',\n        display_name='Collection Indexing Policy',\n        info='Optional JSON string for the \"indexing\" field of the collection. See https://docs.datastax.com/en/astra-db-serverless/api-reference/collections.html#the-indexing-option',\n        advanced=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=4),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        info='Search type to use',\n        options=['Similarity',\n        'Similarity with score threshold',\n        'MMR (Max Marginal Relevance)',\n        'Graph Traversal',\n        'MMR (Max Marginal Relevance) Graph Traversal'],\n        value='MMR (Max Marginal Relevance) Graph Traversal',\n        advanced=True),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n        value=0,\n        advanced=True),\n        DictInput(name='search_filter',\n        display_name='Search Metadata Filter',\n        info='Optional dictionary of filters to apply to the search query.',\n        advanced=True,\n        is_list=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self, vector_store):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_retriever_kwargs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "AstraDBGraphVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self, vector_store)", "def get_retriever_kwargs(self)"], "imports": ["import os", "import orjson", "from astrapy.admin import parse_api_endpoint", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers import docs_to_data", "from langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langchain_astradb import AstraDBGraphVectorStore", "from langchain_astradb.utils.astradb import SetupMode"], "inputs": "[SecretStrInput(name='token', display_name='Astra DB Application Token', info='Authentication token for accessing Astra DB.', value='ASTRA_DB_APPLICATION_TOKEN', required=True, advanced=os.getenv('ASTRA_ENHANCED', 'false').lower() == 'true'), SecretStrInput(name='api_endpoint', display_name='Database' if os.getenv('ASTRA_ENHANCED', 'false').lower() == 'true' else 'API Endpoint', info='API endpoint URL for the Astra DB service.', value='ASTRA_DB_API_ENDPOINT', required=True), StrInput(name='collection_name', display_name='Collection Name', info='The name of the collection within Astra DB where the vectors will be stored.', required=True), StrInput(name='metadata_incoming_links_key', display_name='Metadata incoming links key', info='Metadata key used for incoming links.', advanced=True), *LCVectorStoreComponent.inputs, StrInput(name='keyspace', display_name='Keyspace', info='Optional keyspace within Astra DB to use for the collection.', advanced=True), HandleInput(name='embedding_model', display_name='Embedding Model', input_types=['Embeddings'], info='Allows an embedding model configuration.'), DropdownInput(name='metric', display_name='Metric', info='Optional distance metric for vector comparisons in the vector store.', options=['cosine', 'dot_product', 'euclidean'], value='cosine', advanced=True), IntInput(name='batch_size', display_name='Batch Size', info='Optional number of data to process in a single batch.', advanced=True), IntInput(name='bulk_insert_batch_concurrency', display_name='Bulk Insert Batch Concurrency', info='Optional concurrency level for bulk insert operations.', advanced=True), IntInput(name='bulk_insert_overwrite_concurrency', display_name='Bulk Insert Overwrite Concurrency', info='Optional concurrency level for bulk insert operations that overwrite existing data.', advanced=True), IntInput(name='bulk_delete_concurrency', display_name='Bulk Delete Concurrency', info='Optional concurrency level for bulk delete operations.', advanced=True), DropdownInput(name='setup_mode', display_name='Setup Mode', info=\"Configuration mode for setting up the vector store, with options like 'Sync', or 'Off'.\", options=['Sync', 'Off'], advanced=True, value='Sync'), BoolInput(name='pre_delete_collection', display_name='Pre Delete Collection', info='Boolean flag to determine whether to delete the collection before creating a new one.', advanced=True, value=False), StrInput(name='metadata_indexing_include', display_name='Metadata Indexing Include', info='Optional list of metadata fields to include in the indexing.', advanced=True, list=True), StrInput(name='metadata_indexing_exclude', display_name='Metadata Indexing Exclude', info='Optional list of metadata fields to exclude from the indexing.', advanced=True, list=True), StrInput(name='collection_indexing_policy', display_name='Collection Indexing Policy', info='Optional JSON string for the \"indexing\" field of the collection. See https://docs.datastax.com/en/astra-db-serverless/api-reference/collections.html#the-indexing-option', advanced=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=4), DropdownInput(name='search_type', display_name='Search Type', info='Search type to use', options=['Similarity', 'Similarity with score threshold', 'MMR (Max Marginal Relevance)', 'Graph Traversal', 'MMR (Max Marginal Relevance) Graph Traversal'], value='MMR (Max Marginal Relevance) Graph Traversal', advanced=True), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\", value=0, advanced=True), DictInput(name='search_filter', display_name='Search Metadata Filter', info='Optional dictionary of filters to apply to the search query.', advanced=True, is_list=True)]", "outputs": "", "display_name": "Astra DB Graph", "name": "AstraDBGraph", "description": "Implementation of Graph Vector Store using Astra DB", "icon": "AstraDB"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/google_serper_api_core.py", "section": "class::GoogleSerperAPICore", "content": "from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langflow.custom import Component\nfrom langflow.io import IntInput, MultilineInput, Output, SecretStrInput\nfrom langflow.schema import DataFrame\nfrom langflow.schema.message import Message\n\nclass GoogleSerperAPICore(Component):\n    display_name: str = \"Google Serper API\"\n    description: str = \"Call the Serper.dev Google Search API.\"\n    icon = \"Serper\"\n\n    inputs = [\n        SecretStrInput(name='serper_api_key',\n        display_name='Serper API Key',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input',\n        tool_mode=True),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Results',\n        name='results',\n        type_=DataFrame,\n        method='search_serper')\n    ]\n\n    def search_serper(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def text_search_serper(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "GoogleSerperAPICore", "base_classes": ["Component"], "public_methods": ["def search_serper(self)", "def text_search_serper(self)", "def build(self)"], "imports": ["from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper", "from langflow.custom import Component", "from langflow.io import IntInput, MultilineInput, Output, SecretStrInput", "from langflow.schema import DataFrame", "from langflow.schema.message import Message"], "inputs": "[SecretStrInput(name='serper_api_key', display_name='Serper API Key', required=True), MultilineInput(name='input_value', display_name='Input', tool_mode=True), IntInput(name='k', display_name='Number of results', value=4, required=True)]", "outputs": "[Output(display_name='Results', name='results', type_=DataFrame, method='search_serper')]", "display_name": "Google Serper API", "name": "", "description": "Call the Serper.dev Google Search API.", "icon": "Serper"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/language_semantic.py", "section": "class::SemanticTextSplitterComponent", "content": "from langchain.docstore.document import Document\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langflow.base.textsplitters.model import LCTextSplitterComponent\nfrom langflow.io import DropdownInput, FloatInput, HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\n\nclass SemanticTextSplitterComponent(LCTextSplitterComponent):\n    \"\"\"\n    Split text into semantically meaningful chunks using semantic similarity.\n    \"\"\"\n\n    display_name: str = \"Semantic Text Splitter\"\n    description: str = \"Split text into semantically meaningful chunks using semantic similarity.\"\n    icon = \"LangChain\"\n    name = \"SemanticTextSplitter\"\n\n    inputs = [\n        HandleInput(name='data_inputs',\n        display_name='Data Inputs',\n        info='List of Data objects containing text and metadata to split.',\n        input_types=['Data'],\n        is_list=True,\n        required=True),\n        HandleInput(name='embeddings',\n        display_name='Embeddings',\n        info='Embeddings model to use for semantic similarity. Required.',\n        input_types=['Embeddings'],\n        is_list=False,\n        required=True),\n        DropdownInput(name='breakpoint_threshold_type',\n        display_name='Breakpoint Threshold Type',\n        info=\"Method to determine breakpoints. Options: 'percentile',\n        'standard_deviation',\n        'interquartile'. Defaults to 'percentile'.\",\n        value='percentile',\n        options=['percentile',\n        'standard_deviation',\n        'interquartile']),\n        FloatInput(name='breakpoint_threshold_amount',\n        display_name='Breakpoint Threshold Amount',\n        info='Numerical amount for the breakpoint threshold.',\n        value=0.5),\n        IntInput(name='number_of_chunks',\n        display_name='Number of Chunks',\n        info='Number of chunks to split the text into.',\n        value=5),\n        MessageTextInput(name='sentence_split_regex',\n        display_name='Sentence Split Regex',\n        info='Regular expression to split sentences. Optional.',\n        value='',\n        advanced=True),\n        IntInput(name='buffer_size',\n        display_name='Buffer Size',\n        info='Size of the buffer.',\n        value=0,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Chunks',\n        name='chunks',\n        method='split_text')\n    ]\n\n    def split_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SemanticTextSplitterComponent", "base_classes": ["LCTextSplitterComponent"], "public_methods": ["def split_text(self)"], "imports": ["from langchain.docstore.document import Document", "from langchain_experimental.text_splitter import SemanticChunker", "from langflow.base.textsplitters.model import LCTextSplitterComponent", "from langflow.io import DropdownInput, FloatInput, HandleInput, IntInput, MessageTextInput, Output", "from langflow.schema import Data"], "inputs": "[HandleInput(name='data_inputs', display_name='Data Inputs', info='List of Data objects containing text and metadata to split.', input_types=['Data'], is_list=True, required=True), HandleInput(name='embeddings', display_name='Embeddings', info='Embeddings model to use for semantic similarity. Required.', input_types=['Embeddings'], is_list=False, required=True), DropdownInput(name='breakpoint_threshold_type', display_name='Breakpoint Threshold Type', info=\"Method to determine breakpoints. Options: 'percentile', 'standard_deviation', 'interquartile'. Defaults to 'percentile'.\", value='percentile', options=['percentile', 'standard_deviation', 'interquartile']), FloatInput(name='breakpoint_threshold_amount', display_name='Breakpoint Threshold Amount', info='Numerical amount for the breakpoint threshold.', value=0.5), IntInput(name='number_of_chunks', display_name='Number of Chunks', info='Number of chunks to split the text into.', value=5), MessageTextInput(name='sentence_split_regex', display_name='Sentence Split Regex', info='Regular expression to split sentences. Optional.', value='', advanced=True), IntInput(name='buffer_size', display_name='Buffer Size', info='Size of the buffer.', value=0, advanced=True)]", "outputs": "[Output(display_name='Chunks', name='chunks', method='split_text')]", "display_name": "Semantic Text Splitter", "name": "SemanticTextSplitter", "description": "Split text into semantically meaningful chunks using semantic similarity.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/message_to_data.py", "section": "class::MessageToDataComponent", "content": "from loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import MessageInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass MessageToDataComponent(Component):\n    display_name: str = \"Message to Data\"\n    description: str = \"Convert a Message object to a Data object\"\n    icon = \"message-square-share\"\n    name = \"MessagetoData\"\n\n    inputs = [\n        MessageInput(name='message',\n        display_name='Message',\n        info='The Message object to convert to a Data object')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='convert_message_to_data')\n    ]\n\n    def convert_message_to_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "MessageToDataComponent", "base_classes": ["Component"], "public_methods": ["def convert_message_to_data(self)"], "imports": ["from loguru import logger", "from langflow.custom import Component", "from langflow.io import MessageInput, Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[MessageInput(name='message', display_name='Message', info='The Message object to convert to a Data object')]", "outputs": "[Output(display_name='Data', name='data', method='convert_message_to_data')]", "display_name": "Message to Data", "name": "MessagetoData", "description": "Convert a Message object to a Data object", "icon": "message-square-share"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/xai.py", "section": "class::XAIModelComponent", "content": "import requests\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\nfrom openai import BadRequestError\n\nclass XAIModelComponent(LCModelComponent):\n    display_name: str = \"xAI\"\n    description: str = \"Generates text using xAI models like Grok.\"\n    icon = \"xAI\"\n    name = \"xAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.',\n        range_spec=RangeSpec(min=0,\n        max=128000)),\n        DictInput(name='model_kwargs',\n        display_name='Model Kwargs',\n        advanced=True,\n        info='Additional keyword arguments to pass to the model.'),\n        BoolInput(name='json_mode',\n        display_name='JSON Mode',\n        advanced=True,\n        info='If True,\n        it will output JSON regardless of passing a schema.'),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        advanced=False,\n        options=XAI_DEFAULT_MODELS,\n        value=XAI_DEFAULT_MODELS[0],\n        refresh_button=True,\n        combobox=True,\n        info='The xAI model to use'),\n        MessageTextInput(name='base_url',\n        display_name='xAI API Base',\n        advanced=True,\n        info='The base URL of the xAI API. Defaults to https://api.x.ai/v1',\n        value='https://api.x.ai/v1'),\n        SecretStrInput(name='api_key',\n        display_name='xAI API Key',\n        info='The xAI API Key to use for the model.',\n        advanced=False,\n        value='XAI_API_KEY',\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        advanced=True),\n        IntInput(name='seed',\n        display_name='Seed',\n        info='The seed controls the reproducibility of the job.',\n        advanced=True,\n        value=1)\n    ]\n\n    def get_models(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "XAIModelComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def get_models(self)", "def update_build_config(self, build_config, field_value, field_name)", "def build_model(self)"], "imports": ["import requests", "from langchain_openai import ChatOpenAI", "from pydantic.v1 import SecretStr", "from typing_extensions import override", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput", "from openai import BadRequestError"], "inputs": "[*LCModelComponent._base_inputs, IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate. Set to 0 for unlimited tokens.', range_spec=RangeSpec(min=0, max=128000)), DictInput(name='model_kwargs', display_name='Model Kwargs', advanced=True, info='Additional keyword arguments to pass to the model.'), BoolInput(name='json_mode', display_name='JSON Mode', advanced=True, info='If True, it will output JSON regardless of passing a schema.'), DropdownInput(name='model_name', display_name='Model Name', advanced=False, options=XAI_DEFAULT_MODELS, value=XAI_DEFAULT_MODELS[0], refresh_button=True, combobox=True, info='The xAI model to use'), MessageTextInput(name='base_url', display_name='xAI API Base', advanced=True, info='The base URL of the xAI API. Defaults to https://api.x.ai/v1', value='https://api.x.ai/v1'), SecretStrInput(name='api_key', display_name='xAI API Key', info='The xAI API Key to use for the model.', advanced=False, value='XAI_API_KEY', required=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01), advanced=True), IntInput(name='seed', display_name='Seed', info='The seed controls the reproducibility of the job.', advanced=True, value=1)]", "outputs": "", "display_name": "xAI", "name": "xAIModel", "description": "Generates text using xAI models like Grok.", "icon": "xAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/hcd.py", "section": "class::HCDVectorStoreComponent", "content": "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers import docs_to_data\nfrom langflow.inputs import DictInput, FloatInput\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom langchain_astradb import AstraDBVectorStore\nfrom langchain_astradb.utils.astradb import SetupMode\nfrom astrapy.authentication import UsernamePasswordTokenProvider\nfrom astrapy.constants import Environment\nfrom astrapy.info import VectorServiceOptions\n\nclass HCDVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Hyper-Converged Database\"\n    description: str = \"Implementation of Vector Store using Hyper-Converged Database (HCD) with search capabilities\"\n    icon = \"HCD\"\n    name = \"HCD\"\n\n    inputs = [\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        info='The name of the collection within HCD where the vectors will be stored.',\n        required=True),\n        StrInput(name='username',\n        display_name='HCD Username',\n        info='Authentication username for accessing HCD.',\n        value='hcd-superuser',\n        required=True),\n        SecretStrInput(name='password',\n        display_name='HCD Password',\n        info='Authentication password for accessing HCD.',\n        value='HCD_PASSWORD',\n        required=True),\n        SecretStrInput(name='api_endpoint',\n        display_name='HCD API Endpoint',\n        info='API endpoint URL for the HCD service.',\n        value='HCD_API_ENDPOINT',\n        required=True),\n        *LCVectorStoreComponent.inputs,\n        StrInput(name='namespace',\n        display_name='Namespace',\n        info='Optional namespace within HCD to use for the collection.',\n        value='default_namespace',\n        advanced=True),\n        MultilineInput(name='ca_certificate',\n        display_name='CA Certificate',\n        info='Optional CA certificate for TLS connections to HCD.',\n        advanced=True),\n        DropdownInput(name='metric',\n        display_name='Metric',\n        info='Optional distance metric for vector comparisons in the vector store.',\n        options=['cosine',\n        'dot_product',\n        'euclidean'],\n        advanced=True),\n        IntInput(name='batch_size',\n        display_name='Batch Size',\n        info='Optional number of data to process in a single batch.',\n        advanced=True),\n        IntInput(name='bulk_insert_batch_concurrency',\n        display_name='Bulk Insert Batch Concurrency',\n        info='Optional concurrency level for bulk insert operations.',\n        advanced=True),\n        IntInput(name='bulk_insert_overwrite_concurrency',\n        display_name='Bulk Insert Overwrite Concurrency',\n        info='Optional concurrency level for bulk insert operations that overwrite existing data.',\n        advanced=True),\n        IntInput(name='bulk_delete_concurrency',\n        display_name='Bulk Delete Concurrency',\n        info='Optional concurrency level for bulk delete operations.',\n        advanced=True),\n        DropdownInput(name='setup_mode',\n        display_name='Setup Mode',\n        info=\"Configuration mode for setting up the vector store,\n        with options like 'Sync',\n        'Async',\n        or 'Off'.\",\n        options=['Sync',\n        'Async',\n        'Off'],\n        advanced=True,\n        value='Sync'),\n        BoolInput(name='pre_delete_collection',\n        display_name='Pre Delete Collection',\n        info='Boolean flag to determine whether to delete the collection before creating a new one.',\n        advanced=True),\n        StrInput(name='metadata_indexing_include',\n        display_name='Metadata Indexing Include',\n        info='Optional list of metadata fields to include in the indexing.',\n        advanced=True),\n        HandleInput(name='embedding',\n        display_name='Embedding or Astra Vectorize',\n        input_types=['Embeddings',\n        'dict'],\n        info='Allows either an embedding model or an Astra Vectorize configuration.'),\n        StrInput(name='metadata_indexing_exclude',\n        display_name='Metadata Indexing Exclude',\n        info='Optional list of metadata fields to exclude from the indexing.',\n        advanced=True),\n        StrInput(name='collection_indexing_policy',\n        display_name='Collection Indexing Policy',\n        info='Optional dictionary defining the indexing policy for the collection.',\n        advanced=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=4),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        info='Search type to use',\n        options=['Similarity',\n        'Similarity with score threshold',\n        'MMR (Max Marginal Relevance)'],\n        value='Similarity',\n        advanced=True),\n        FloatInput(name='search_score_threshold',\n        display_name='Search Score Threshold',\n        info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n        value=0,\n        advanced=True),\n        DictInput(name='search_filter',\n        display_name='Search Metadata Filter',\n        info='Optional dictionary of filters to apply to the search query.',\n        advanced=True,\n        is_list=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_retriever_kwargs(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "HCDVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)", "def get_retriever_kwargs(self)"], "imports": ["from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers import docs_to_data", "from langflow.inputs import DictInput, FloatInput", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from langchain_astradb import AstraDBVectorStore", "from langchain_astradb.utils.astradb import SetupMode", "from astrapy.authentication import UsernamePasswordTokenProvider", "from astrapy.constants import Environment", "from astrapy.info import VectorServiceOptions"], "inputs": "[StrInput(name='collection_name', display_name='Collection Name', info='The name of the collection within HCD where the vectors will be stored.', required=True), StrInput(name='username', display_name='HCD Username', info='Authentication username for accessing HCD.', value='hcd-superuser', required=True), SecretStrInput(name='password', display_name='HCD Password', info='Authentication password for accessing HCD.', value='HCD_PASSWORD', required=True), SecretStrInput(name='api_endpoint', display_name='HCD API Endpoint', info='API endpoint URL for the HCD service.', value='HCD_API_ENDPOINT', required=True), *LCVectorStoreComponent.inputs, StrInput(name='namespace', display_name='Namespace', info='Optional namespace within HCD to use for the collection.', value='default_namespace', advanced=True), MultilineInput(name='ca_certificate', display_name='CA Certificate', info='Optional CA certificate for TLS connections to HCD.', advanced=True), DropdownInput(name='metric', display_name='Metric', info='Optional distance metric for vector comparisons in the vector store.', options=['cosine', 'dot_product', 'euclidean'], advanced=True), IntInput(name='batch_size', display_name='Batch Size', info='Optional number of data to process in a single batch.', advanced=True), IntInput(name='bulk_insert_batch_concurrency', display_name='Bulk Insert Batch Concurrency', info='Optional concurrency level for bulk insert operations.', advanced=True), IntInput(name='bulk_insert_overwrite_concurrency', display_name='Bulk Insert Overwrite Concurrency', info='Optional concurrency level for bulk insert operations that overwrite existing data.', advanced=True), IntInput(name='bulk_delete_concurrency', display_name='Bulk Delete Concurrency', info='Optional concurrency level for bulk delete operations.', advanced=True), DropdownInput(name='setup_mode', display_name='Setup Mode', info=\"Configuration mode for setting up the vector store, with options like 'Sync', 'Async', or 'Off'.\", options=['Sync', 'Async', 'Off'], advanced=True, value='Sync'), BoolInput(name='pre_delete_collection', display_name='Pre Delete Collection', info='Boolean flag to determine whether to delete the collection before creating a new one.', advanced=True), StrInput(name='metadata_indexing_include', display_name='Metadata Indexing Include', info='Optional list of metadata fields to include in the indexing.', advanced=True), HandleInput(name='embedding', display_name='Embedding or Astra Vectorize', input_types=['Embeddings', 'dict'], info='Allows either an embedding model or an Astra Vectorize configuration.'), StrInput(name='metadata_indexing_exclude', display_name='Metadata Indexing Exclude', info='Optional list of metadata fields to exclude from the indexing.', advanced=True), StrInput(name='collection_indexing_policy', display_name='Collection Indexing Policy', info='Optional dictionary defining the indexing policy for the collection.', advanced=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=4), DropdownInput(name='search_type', display_name='Search Type', info='Search type to use', options=['Similarity', 'Similarity with score threshold', 'MMR (Max Marginal Relevance)'], value='Similarity', advanced=True), FloatInput(name='search_score_threshold', display_name='Search Score Threshold', info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\", value=0, advanced=True), DictInput(name='search_filter', display_name='Search Metadata Filter', info='Optional dictionary of filters to apply to the search query.', advanced=True, is_list=True)]", "outputs": "", "display_name": "Hyper-Converged Database", "name": "HCD", "description": "Implementation of Vector Store using Hyper-Converged Database (HCD) with search capabilities", "icon": "HCD"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wikidata_api.py", "section": "class::WikidataSearchSchema", "content": "from typing import Any\nimport httpx\nfrom langchain_core.tools import StructuredTool, ToolException\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema import Data\n\nclass WikidataSearchSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "WikidataSearchSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from typing import Any", "import httpx", "from langchain_core.tools import StructuredTool, ToolException", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wikidata_api.py", "section": "class::WikidataAPIWrapper", "content": "from typing import Any\nimport httpx\nfrom langchain_core.tools import StructuredTool, ToolException\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema import Data\n\nclass WikidataAPIWrapper(BaseModel):\n    \"\"\"\n    Wrapper around Wikidata API.\n    \"\"\"\n\n\n    def results(self, query):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run(self, query):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WikidataAPIWrapper", "base_classes": ["BaseModel"], "public_methods": ["def results(self, query)", "def run(self, query)"], "imports": ["from typing import Any", "import httpx", "from langchain_core.tools import StructuredTool, ToolException", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wikidata_api.py", "section": "class::WikidataAPIComponent", "content": "from typing import Any\nimport httpx\nfrom langchain_core.tools import StructuredTool, ToolException\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema import Data\n\nclass WikidataAPIComponent(LCToolComponent):\n    display_name: str = \"Wikidata API [Deprecated]\"\n    description: str = \"Performs a search using the Wikidata API.\"\n    icon = \"Wikipedia\"\n    name = \"WikidataAPI\"\n\n    inputs = [\n        MultilineInput(name='query',\n        display_name='Query',\n        info='The text query for similarity search on Wikidata.',\n        required=True)\n    ]\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WikidataAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def build_tool(self)", "def run_model(self)"], "imports": ["from typing import Any", "import httpx", "from langchain_core.tools import StructuredTool, ToolException", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import MultilineInput", "from langflow.schema import Data"], "inputs": "[MultilineInput(name='query', display_name='Query', info='The text query for similarity search on Wikidata.', required=True)]", "outputs": "", "display_name": "Wikidata API [Deprecated]", "name": "WikidataAPI", "description": "Performs a search using the Wikidata API.", "icon": "Wikipedia"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/character.py", "section": "class::CharacterTextSplitterComponent", "content": "from typing import Any\nfrom langchain_text_splitters import CharacterTextSplitter, TextSplitter\nfrom langflow.base.textsplitters.model import LCTextSplitterComponent\nfrom langflow.inputs import DataInput, IntInput, MessageTextInput\nfrom langflow.utils.util import unescape_string\n\nclass CharacterTextSplitterComponent(LCTextSplitterComponent):\n    display_name: str = \"CharacterTextSplitter\"\n    description: str = \"Split text by number of characters.\"\n    icon = \"LangChain\"\n    name = \"CharacterTextSplitter\"\n\n    inputs = [\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        info='The maximum length of each chunk.',\n        value=1000),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='The amount of overlap between chunks.',\n        value=200),\n        DataInput(name='data_input',\n        display_name='Input',\n        info='The texts to split.',\n        input_types=['Document',\n        'Data'],\n        required=True),\n        MessageTextInput(name='separator',\n        display_name='Separator',\n        info='The characters to split on.\\nIf left empty defaults to \"\\\\n\\\\n\".')\n    ]\n\n    def get_data_input(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_text_splitter(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CharacterTextSplitterComponent", "base_classes": ["LCTextSplitterComponent"], "public_methods": ["def get_data_input(self)", "def build_text_splitter(self)"], "imports": ["from typing import Any", "from langchain_text_splitters import CharacterTextSplitter, TextSplitter", "from langflow.base.textsplitters.model import LCTextSplitterComponent", "from langflow.inputs import DataInput, IntInput, MessageTextInput", "from langflow.utils.util import unescape_string"], "inputs": "[IntInput(name='chunk_size', display_name='Chunk Size', info='The maximum length of each chunk.', value=1000), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='The amount of overlap between chunks.', value=200), DataInput(name='data_input', display_name='Input', info='The texts to split.', input_types=['Document', 'Data'], required=True), MessageTextInput(name='separator', display_name='Separator', info='The characters to split on.\\nIf left empty defaults to \"\\\\n\\\\n\".')]", "outputs": "", "display_name": "CharacterTextSplitter", "name": "CharacterTextSplitter", "description": "Split text by number of characters.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/filter_data_values.py", "section": "class::DataFilterComponent", "content": "from typing import Any\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data\n\nclass DataFilterComponent(Component):\n    display_name: str = \"Filter Values\"\n    description: str = \"Filter a list of data items based on a specified key, filter value, and comparison operator. Check advanced options to select match comparision.\"\n    icon = \"filter\"\n    name = \"FilterDataValues\"\n\n    inputs = [\n        DataInput(name='input_data',\n        display_name='Input Data',\n        info='The list of data items to filter.',\n        is_list=True),\n        MessageTextInput(name='filter_key',\n        display_name='Filter Key',\n        info=\"The key to filter on (e.g.,\n        'route').\",\n        value='route',\n        input_types=['Data']),\n        MessageTextInput(name='filter_value',\n        display_name='Filter Value',\n        info=\"The value to filter by (e.g.,\n        'CMIP').\",\n        value='CMIP',\n        input_types=['Data']),\n        DropdownInput(name='operator',\n        display_name='Comparison Operator',\n        options=['equals',\n        'not equals',\n        'contains',\n        'starts with',\n        'ends with'],\n        info='The operator to apply for comparing the values.',\n        value='equals',\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Filtered Data',\n        name='filtered_data',\n        method='filter_data')\n    ]\n\n    def compare_values(self, item_value, filter_value, operator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def filter_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DataFilterComponent", "base_classes": ["Component"], "public_methods": ["def compare_values(self, item_value, filter_value, operator)", "def filter_data(self)"], "imports": ["from typing import Any", "from langflow.custom import Component", "from langflow.io import DataInput, DropdownInput, MessageTextInput, Output", "from langflow.schema import Data"], "inputs": "[DataInput(name='input_data', display_name='Input Data', info='The list of data items to filter.', is_list=True), MessageTextInput(name='filter_key', display_name='Filter Key', info=\"The key to filter on (e.g., 'route').\", value='route', input_types=['Data']), MessageTextInput(name='filter_value', display_name='Filter Value', info=\"The value to filter by (e.g., 'CMIP').\", value='CMIP', input_types=['Data']), DropdownInput(name='operator', display_name='Comparison Operator', options=['equals', 'not equals', 'contains', 'starts with', 'ends with'], info='The operator to apply for comparing the values.', value='equals', advanced=True)]", "outputs": "[Output(display_name='Filtered Data', name='filtered_data', method='filter_data')]", "display_name": "Filter Values", "name": "FilterDataValues", "description": "Filter a list of data items based on a specified key, filter value, and comparison operator. Check advanced options to select match comparision.", "icon": "filter"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/watsonx.py", "section": "class::WatsonxAIComponent", "content": "import json\nfrom typing import Any\nimport requests\nfrom langchain_ibm import ChatWatsonx\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.schema.dotdict import dotdict\n\nclass WatsonxAIComponent(LCModelComponent):\n    display_name: str = \"IBM watsonx.ai\"\n    description: str = \"Generate text using IBM watsonx.ai foundation models.\"\n    icon = \"WatsonxAI\"\n    name = \"IBMwatsonxModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(name='url',\n        display_name='watsonx API Endpoint',\n        info='The base URL of the API.',\n        value=None,\n        options=['https://us-south.ml.cloud.ibm.com',\n        'https://eu-de.ml.cloud.ibm.com',\n        'https://eu-gb.ml.cloud.ibm.com',\n        'https://au-syd.ml.cloud.ibm.com',\n        'https://jp-tok.ml.cloud.ibm.com',\n        'https://ca-tor.ml.cloud.ibm.com'],\n        real_time_refresh=True),\n        StrInput(name='project_id',\n        display_name='watsonx Project ID',\n        required=True,\n        info='The project ID or deployment space ID that is associated with the foundation model.'),\n        SecretStrInput(name='api_key',\n        display_name='API Key',\n        info='The API Key to use for the model.',\n        required=True),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        options=[],\n        value=None,\n        dynamic=True,\n        required=True),\n        IntInput(name='max_tokens',\n        display_name='Max Tokens',\n        advanced=True,\n        info='The maximum number of tokens to generate.',\n        range_spec=RangeSpec(min=1,\n        max=4096),\n        value=1000),\n        StrInput(name='stop_sequence',\n        display_name='Stop Sequence',\n        advanced=True,\n        info='Sequence where generation should stop.',\n        field_type='str'),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        info='Controls randomness,\n        higher values increase diversity.',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        advanced=True),\n        SliderInput(name='top_p',\n        display_name='Top P',\n        info='The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller,\n        more top-weighted nucleus.',\n        value=0.9,\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True),\n        SliderInput(name='frequency_penalty',\n        display_name='Frequency Penalty',\n        info='Penalty for frequency of token usage.',\n        value=0.5,\n        range_spec=RangeSpec(min=-2.0,\n        max=2.0,\n        step=0.01),\n        advanced=True),\n        SliderInput(name='presence_penalty',\n        display_name='Presence Penalty',\n        info='Penalty for token presence in prior text.',\n        value=0.3,\n        range_spec=RangeSpec(min=-2.0,\n        max=2.0,\n        step=0.01),\n        advanced=True),\n        IntInput(name='seed',\n        display_name='Random Seed',\n        advanced=True,\n        info='The random seed for the model.',\n        value=8),\n        BoolInput(name='logprobs',\n        display_name='Log Probabilities',\n        advanced=True,\n        info='Whether to return log probabilities of the output tokens.',\n        value=True),\n        IntInput(name='top_logprobs',\n        display_name='Top Log Probabilities',\n        advanced=True,\n        info='Number of most likely tokens to return at each position.',\n        value=3,\n        range_spec=RangeSpec(min=1,\n        max=20)),\n        StrInput(name='logit_bias',\n        display_name='Logit Bias',\n        advanced=True,\n        info='JSON string of token IDs to bias or suppress (e.g.,\n        {\"1003\": -100,\n        \"1004\": 100}).',\n        field_type='str')\n    ]\n\n    def fetch_models(base_url):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WatsonxAIComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def fetch_models(base_url)", "def update_build_config(self, build_config, field_value, field_name)", "def build_model(self)"], "imports": ["import json", "from typing import Any", "import requests", "from langchain_ibm import ChatWatsonx", "from loguru import logger", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.inputs import BoolInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput", "from langflow.schema.dotdict import dotdict"], "inputs": "[*LCModelComponent._base_inputs, DropdownInput(name='url', display_name='watsonx API Endpoint', info='The base URL of the API.', value=None, options=['https://us-south.ml.cloud.ibm.com', 'https://eu-de.ml.cloud.ibm.com', 'https://eu-gb.ml.cloud.ibm.com', 'https://au-syd.ml.cloud.ibm.com', 'https://jp-tok.ml.cloud.ibm.com', 'https://ca-tor.ml.cloud.ibm.com'], real_time_refresh=True), StrInput(name='project_id', display_name='watsonx Project ID', required=True, info='The project ID or deployment space ID that is associated with the foundation model.'), SecretStrInput(name='api_key', display_name='API Key', info='The API Key to use for the model.', required=True), DropdownInput(name='model_name', display_name='Model Name', options=[], value=None, dynamic=True, required=True), IntInput(name='max_tokens', display_name='Max Tokens', advanced=True, info='The maximum number of tokens to generate.', range_spec=RangeSpec(min=1, max=4096), value=1000), StrInput(name='stop_sequence', display_name='Stop Sequence', advanced=True, info='Sequence where generation should stop.', field_type='str'), SliderInput(name='temperature', display_name='Temperature', info='Controls randomness, higher values increase diversity.', value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01), advanced=True), SliderInput(name='top_p', display_name='Top P', info='The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus.', value=0.9, range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True), SliderInput(name='frequency_penalty', display_name='Frequency Penalty', info='Penalty for frequency of token usage.', value=0.5, range_spec=RangeSpec(min=-2.0, max=2.0, step=0.01), advanced=True), SliderInput(name='presence_penalty', display_name='Presence Penalty', info='Penalty for token presence in prior text.', value=0.3, range_spec=RangeSpec(min=-2.0, max=2.0, step=0.01), advanced=True), IntInput(name='seed', display_name='Random Seed', advanced=True, info='The random seed for the model.', value=8), BoolInput(name='logprobs', display_name='Log Probabilities', advanced=True, info='Whether to return log probabilities of the output tokens.', value=True), IntInput(name='top_logprobs', display_name='Top Log Probabilities', advanced=True, info='Number of most likely tokens to return at each position.', value=3, range_spec=RangeSpec(min=1, max=20)), StrInput(name='logit_bias', display_name='Logit Bias', advanced=True, info='JSON string of token IDs to bias or suppress (e.g., {\"1003\": -100, \"1004\": 100}).', field_type='str')]", "outputs": "", "display_name": "IBM watsonx.ai", "name": "IBMwatsonxModel", "description": "Generate text using IBM watsonx.ai foundation models.", "icon": "WatsonxAI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/vectara_self_query.py", "section": "class::VectaraSelfQueryRetriverComponent", "content": "import json\nfrom typing import cast\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_core.vectorstores import VectorStore\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Retriever\nfrom langflow.field_typing.constants import LanguageModel\n\nclass VectaraSelfQueryRetriverComponent(CustomComponent):\n    \"\"\"\n    A custom component for implementing Vectara Self Query Retriever using a vector store.\n    \"\"\"\n\n    display_name: str = \"Vectara Self Query Retriever for Vectara Vector Store\"\n    description: str = \"Implementation of Vectara Self Query Retriever\"\n    icon = \"Vectara\"\n    name = \"VectaraSelfQueryRetriver\"\n\n    def build(self, vectorstore, document_content_description, llm, metadata_field_info):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "VectaraSelfQueryRetriverComponent", "base_classes": ["CustomComponent"], "public_methods": ["def build(self, vectorstore, document_content_description, llm, metadata_field_info)"], "imports": ["import json", "from typing import cast", "from langchain.chains.query_constructor.base import AttributeInfo", "from langchain.retrievers.self_query.base import SelfQueryRetriever", "from langchain_core.vectorstores import VectorStore", "from langflow.custom import CustomComponent", "from langflow.field_typing import Retriever", "from langflow.field_typing.constants import LanguageModel"], "inputs": "", "outputs": "", "display_name": "Vectara Self Query Retriever for Vectara Vector Store", "name": "VectaraSelfQueryRetriver", "description": "Implementation of Vectara Self Query Retriever", "icon": "Vectara"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/search.py", "section": "class::SearchComponent", "content": "from typing import Any\nfrom langchain_community.utilities.searchapi import SearchApiAPIWrapper\nfrom langflow.custom import Component\nfrom langflow.inputs import DictInput, DropdownInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\nclass SearchComponent(Component):\n    display_name: str = \"Search API\"\n    description: str = \"Call the searchapi.io API with result limiting\"\n    icon = \"SearchAPI\"\n\n    inputs = [\n        DropdownInput(name='engine',\n        display_name='Engine',\n        value='google',\n        options=['google',\n        'bing',\n        'duckduckgo']),\n        SecretStrInput(name='api_key',\n        display_name='SearchAPI API Key',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input',\n        tool_mode=True),\n        DictInput(name='search_params',\n        display_name='Search parameters',\n        advanced=True,\n        is_list=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=5,\n        advanced=True),\n        IntInput(name='max_snippet_length',\n        display_name='Max Snippet Length',\n        value=100,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Text',\n        name='text',\n        method='fetch_content_text'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SearchComponent", "base_classes": ["Component"], "public_methods": ["def run_model(self)", "def fetch_content(self)", "def fetch_content_text(self)", "def as_dataframe(self)"], "imports": ["from typing import Any", "from langchain_community.utilities.searchapi import SearchApiAPIWrapper", "from langflow.custom import Component", "from langflow.inputs import DictInput, DropdownInput, IntInput, MultilineInput, SecretStrInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame", "from langflow.schema.message import Message"], "inputs": "[DropdownInput(name='engine', display_name='Engine', value='google', options=['google', 'bing', 'duckduckgo']), SecretStrInput(name='api_key', display_name='SearchAPI API Key', required=True), MultilineInput(name='input_value', display_name='Input', tool_mode=True), DictInput(name='search_params', display_name='Search parameters', advanced=True, is_list=True), IntInput(name='max_results', display_name='Max Results', value=5, advanced=True), IntInput(name='max_snippet_length', display_name='Max Snippet Length', value=100, advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Text', name='text', method='fetch_content_text'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Search API", "name": "", "description": "Call the searchapi.io API with result limiting", "icon": "SearchAPI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/sql.py", "section": "class::SQLAgentComponent", "content": "from langchain.agents import AgentExecutor\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langchain_community.agent_toolkits.sql.base import create_sql_agent\nfrom langchain_community.utilities import SQLDatabase\nfrom langflow.base.agents.agent import LCAgentComponent\nfrom langflow.inputs import HandleInput, MessageTextInput\n\nclass SQLAgentComponent(LCAgentComponent):\n    display_name: str = \"SQLAgent\"\n    description: str = \"Construct an SQL agent from an LLM and tools.\"\n    icon = \"LangChain\"\n    name = \"SQLAgent\"\n\n    inputs = [\n        *LCAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        MessageTextInput(name='database_uri',\n        display_name='Database URI',\n        required=True),\n        HandleInput(name='extra_tools',\n        display_name='Extra Tools',\n        input_types=['Tool'],\n        is_list=True,\n        advanced=True)\n    ]\n\n    def build_agent(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SQLAgentComponent", "base_classes": ["LCAgentComponent"], "public_methods": ["def build_agent(self)"], "imports": ["from langchain.agents import AgentExecutor", "from langchain_community.agent_toolkits import SQLDatabaseToolkit", "from langchain_community.agent_toolkits.sql.base import create_sql_agent", "from langchain_community.utilities import SQLDatabase", "from langflow.base.agents.agent import LCAgentComponent", "from langflow.inputs import HandleInput, MessageTextInput"], "inputs": "[*LCAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), MessageTextInput(name='database_uri', display_name='Database URI', required=True), HandleInput(name='extra_tools', display_name='Extra Tools', input_types=['Tool'], is_list=True, advanced=True)]", "outputs": "", "display_name": "SQLAgent", "name": "SQLAgent", "description": "Construct an SQL agent from an LLM and tools.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/processing/data_operations.py", "section": "class::DataOperationsComponent", "content": "import ast\nfrom typing import TYPE_CHECKING, Any\nfrom langflow.custom import Component\nfrom langflow.inputs import DictInput, DropdownInput, MessageTextInput, SortableListInput\nfrom langflow.io import DataInput, Output\nfrom langflow.logging import logger\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.utils.component_utils import set_current_fields, set_field_display\nfrom collections.abc import Callable\n\nclass DataOperationsComponent(Component):\n    display_name: str = \"Data Operations\"\n    description: str = \"Perform various operations on a Data object.\"\n    icon = \"file-json\"\n    name = \"DataOperations\"\n\n    inputs = [\n        DataInput(name='data',\n        display_name='Data',\n        info='Data object to filter.',\n        required=True,\n        is_list=True),\n        SortableListInput(name='operations',\n        display_name='Operations',\n        placeholder='Select Operation',\n        info='List of operations to perform on the data.',\n        options=[{'name': 'Select Keys',\n        'icon': 'lasso-select'},\n        {'name': 'Literal Eval',\n        'icon': 'braces'},\n        {'name': 'Combine',\n        'icon': 'merge'},\n        {'name': 'Filter Values',\n        'icon': 'filter'},\n        {'name': 'Append or Update',\n        'icon': 'circle-plus'},\n        {'name': 'Remove Keys',\n        'icon': 'eraser'},\n        {'name': 'Rename Keys',\n        'icon': 'pencil-line'}],\n        real_time_refresh=True,\n        limit=1),\n        MessageTextInput(name='select_keys_input',\n        display_name='Select Keys',\n        info='List of keys to select from the data.',\n        show=False,\n        is_list=True),\n        MessageTextInput(name='filter_key',\n        display_name='Filter Key',\n        info='Key to filter by.',\n        is_list=True,\n        show=False),\n        DropdownInput(name='operator',\n        display_name='Comparison Operator',\n        options=['equals',\n        'not equals',\n        'contains',\n        'starts with',\n        'ends with'],\n        info='The operator to apply for comparing the values.',\n        value='equals',\n        advanced=False,\n        show=False),\n        DictInput(name='filter_values',\n        display_name='Filter Values',\n        info='List of values to filter by.',\n        show=False,\n        is_list=True),\n        DictInput(name='append_update_data',\n        display_name='Append or Update',\n        info='Data to Append or Updatethe existing data with.',\n        show=False,\n        value={'key': 'value'},\n        is_list=True),\n        MessageTextInput(name='remove_keys_input',\n        display_name='Remove Keys',\n        info='List of keys to remove from the data.',\n        show=False,\n        is_list=True),\n        DictInput(name='rename_keys_input',\n        display_name='Rename Keys',\n        info='List of keys to rename in the data.',\n        show=False,\n        is_list=True,\n        value={'old_key': 'new_key'})\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data_output',\n        method='as_data')\n    ]\n\n    def get_data_dict(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def get_normalized_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def data_is_list(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def validate_single_data(self, operation):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def operation_exception(self, operations):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def select_keys(self, evaluate):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def remove_keys(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def rename_keys(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def recursive_eval(self, data):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def evaluate_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def combine_data(self, evaluate):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def compare_values(self, item_value, filter_value, operator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def filter_data(self, input_data, filter_key, filter_value, operator):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def multi_filter_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def append_update(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DataOperationsComponent", "base_classes": ["Component"], "public_methods": ["def get_data_dict(self)", "def get_normalized_data(self)", "def data_is_list(self)", "def validate_single_data(self, operation)", "def operation_exception(self, operations)", "def select_keys(self, evaluate)", "def remove_keys(self)", "def rename_keys(self)", "def recursive_eval(self, data)", "def evaluate_data(self)", "def combine_data(self, evaluate)", "def compare_values(self, item_value, filter_value, operator)", "def filter_data(self, input_data, filter_key, filter_value, operator)", "def multi_filter_data(self)", "def append_update(self)", "def update_build_config(self, build_config, field_value, field_name)", "def as_data(self)"], "imports": ["import ast", "from typing import TYPE_CHECKING, Any", "from langflow.custom import Component", "from langflow.inputs import DictInput, DropdownInput, MessageTextInput, SortableListInput", "from langflow.io import DataInput, Output", "from langflow.logging import logger", "from langflow.schema import Data", "from langflow.schema.dotdict import dotdict", "from langflow.utils.component_utils import set_current_fields, set_field_display", "from collections.abc import Callable"], "inputs": "[DataInput(name='data', display_name='Data', info='Data object to filter.', required=True, is_list=True), SortableListInput(name='operations', display_name='Operations', placeholder='Select Operation', info='List of operations to perform on the data.', options=[{'name': 'Select Keys', 'icon': 'lasso-select'}, {'name': 'Literal Eval', 'icon': 'braces'}, {'name': 'Combine', 'icon': 'merge'}, {'name': 'Filter Values', 'icon': 'filter'}, {'name': 'Append or Update', 'icon': 'circle-plus'}, {'name': 'Remove Keys', 'icon': 'eraser'}, {'name': 'Rename Keys', 'icon': 'pencil-line'}], real_time_refresh=True, limit=1), MessageTextInput(name='select_keys_input', display_name='Select Keys', info='List of keys to select from the data.', show=False, is_list=True), MessageTextInput(name='filter_key', display_name='Filter Key', info='Key to filter by.', is_list=True, show=False), DropdownInput(name='operator', display_name='Comparison Operator', options=['equals', 'not equals', 'contains', 'starts with', 'ends with'], info='The operator to apply for comparing the values.', value='equals', advanced=False, show=False), DictInput(name='filter_values', display_name='Filter Values', info='List of values to filter by.', show=False, is_list=True), DictInput(name='append_update_data', display_name='Append or Update', info='Data to Append or Updatethe existing data with.', show=False, value={'key': 'value'}, is_list=True), MessageTextInput(name='remove_keys_input', display_name='Remove Keys', info='List of keys to remove from the data.', show=False, is_list=True), DictInput(name='rename_keys_input', display_name='Rename Keys', info='List of keys to rename in the data.', show=False, is_list=True, value={'old_key': 'new_key'})]", "outputs": "[Output(display_name='Data', name='data_output', method='as_data')]", "display_name": "Data Operations", "name": "DataOperations", "description": "Perform various operations on a Data object.", "icon": "file-json"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/cohere.py", "section": "class::CohereComponent", "content": "from langchain_cohere import ChatCohere\nfrom pydantic.v1 import SecretStr\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import SecretStrInput, SliderInput\n\nclass CohereComponent(LCModelComponent):\n    display_name: str = \"Cohere\"\n    description: str = \"Generate text using Cohere LLMs.\"\n    icon = \"Cohere\"\n    name = \"CohereModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name='cohere_api_key',\n        display_name='Cohere API Key',\n        info='The Cohere API Key to use for the Cohere model.',\n        advanced=False,\n        value='COHERE_API_KEY',\n        required=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.75,\n        range_spec=RangeSpec(min=0,\n        max=2,\n        step=0.01),\n        info='Controls randomness. Lower values are more deterministic,\n        higher values are more creative.',\n        advanced=True)\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CohereComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)"], "imports": ["from langchain_cohere import ChatCohere", "from pydantic.v1 import SecretStr", "from langflow.base.models.model import LCModelComponent", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import SecretStrInput, SliderInput"], "inputs": "[*LCModelComponent._base_inputs, SecretStrInput(name='cohere_api_key', display_name='Cohere API Key', info='The Cohere API Key to use for the Cohere model.', advanced=False, value='COHERE_API_KEY', required=True), SliderInput(name='temperature', display_name='Temperature', value=0.75, range_spec=RangeSpec(min=0, max=2, step=0.01), info='Controls randomness. Lower values are more deterministic, higher values are more creative.', advanced=True)]", "outputs": "", "display_name": "Cohere", "name": "CohereModel", "description": "Generate text using Cohere LLMs.", "icon": "Cohere"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/chroma.py", "section": "class::ChromaVectorStoreComponent", "content": "from copy import deepcopy\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom typing_extensions import override\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, StrInput\nfrom langflow.schema import Data, DataFrame\nfrom chromadb import Client\nfrom langchain_chroma import Chroma\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    Chroma Vector Store with search capabilities.\n    \"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search capabilities\"\n    icon = \"Chroma\"\n    name = \"Chroma\"\n\n    inputs = [\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        value='langflow'),\n        StrInput(name='persist_directory',\n        display_name='Persist Directory'),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        StrInput(name='chroma_server_cors_allow_origins',\n        display_name='Server CORS Allow Origins',\n        advanced=True),\n        StrInput(name='chroma_server_host',\n        display_name='Server Host',\n        advanced=True),\n        IntInput(name='chroma_server_http_port',\n        display_name='Server HTTP Port',\n        advanced=True),\n        IntInput(name='chroma_server_grpc_port',\n        display_name='Server gRPC Port',\n        advanced=True),\n        BoolInput(name='chroma_server_ssl_enabled',\n        display_name='Server SSL Enabled',\n        advanced=True),\n        BoolInput(name='allow_duplicates',\n        display_name='Allow Duplicates',\n        advanced=True,\n        info='If false,\n        will not add documents that are already in the Vector Store.'),\n        DropdownInput(name='search_type',\n        display_name='Search Type',\n        options=['Similarity',\n        'MMR'],\n        value='Similarity',\n        advanced=True),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        advanced=True,\n        value=10),\n        IntInput(name='limit',\n        display_name='Limit',\n        advanced=True,\n        info='Limit the number of records to compare when Allow Duplicates is False.')\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ChromaVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)"], "imports": ["from copy import deepcopy", "from chromadb.config import Settings", "from langchain_chroma import Chroma", "from typing_extensions import override", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.base.vectorstores.utils import chroma_collection_to_data", "from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, StrInput", "from langflow.schema import Data, DataFrame", "from chromadb import Client", "from langchain_chroma import Chroma"], "inputs": "[StrInput(name='collection_name', display_name='Collection Name', value='langflow'), StrInput(name='persist_directory', display_name='Persist Directory'), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), StrInput(name='chroma_server_cors_allow_origins', display_name='Server CORS Allow Origins', advanced=True), StrInput(name='chroma_server_host', display_name='Server Host', advanced=True), IntInput(name='chroma_server_http_port', display_name='Server HTTP Port', advanced=True), IntInput(name='chroma_server_grpc_port', display_name='Server gRPC Port', advanced=True), BoolInput(name='chroma_server_ssl_enabled', display_name='Server SSL Enabled', advanced=True), BoolInput(name='allow_duplicates', display_name='Allow Duplicates', advanced=True, info='If false, will not add documents that are already in the Vector Store.'), DropdownInput(name='search_type', display_name='Search Type', options=['Similarity', 'MMR'], value='Similarity', advanced=True), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', advanced=True, value=10), IntInput(name='limit', display_name='Limit', advanced=True, info='Limit the number of records to compare when Allow Duplicates is False.')]", "outputs": "", "display_name": "Chroma DB", "name": "Chroma", "description": "Chroma Vector Store with search capabilities", "icon": "Chroma"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/python_repl.py", "section": "class::PythonREPLToolComponent", "content": "import importlib\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom langchain_experimental.utilities import PythonREPL\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import StrInput\nfrom langflow.schema import Data\n\nclass PythonREPLToolComponent(LCToolComponent):\n    display_name: str = \"Python REPL [DEPRECATED]\"\n    description: str = \"A tool for running Python code in a REPL environment.\"\n    icon = \"Python\"\n    name = \"PythonREPLTool\"\n\n    inputs = [\n        StrInput(name='name',\n        display_name='Tool Name',\n        info='The name of the tool.',\n        value='python_repl'),\n        StrInput(name='description',\n        display_name='Tool Description',\n        info='A description of the tool.',\n        value='A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value,\n        you should print it out with `print(...)`.'),\n        StrInput(name='global_imports',\n        display_name='Global Imports',\n        info=\"A comma-separated list of modules to import globally,\n        e.g. 'math,numpy'.\",\n        value='math'),\n        StrInput(name='code',\n        display_name='Python Code',\n        info='The Python code to execute.',\n        value=\"print('Hello,\n        World!')\")\n    ]\n\n    def get_globals(self, global_imports):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "PythonREPLToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def get_globals(self, global_imports)", "def build_tool(self)", "def run_model(self)"], "imports": ["import importlib", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from langchain_experimental.utilities import PythonREPL", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='name', display_name='Tool Name', info='The name of the tool.', value='python_repl'), StrInput(name='description', display_name='Tool Description', info='A description of the tool.', value='A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.'), StrInput(name='global_imports', display_name='Global Imports', info=\"A comma-separated list of modules to import globally, e.g. 'math,numpy'.\", value='math'), StrInput(name='code', display_name='Python Code', info='The Python code to execute.', value=\"print('Hello, World!')\")]", "outputs": "", "display_name": "Python REPL [DEPRECATED]", "name": "PythonREPLTool", "description": "A tool for running Python code in a REPL environment.", "icon": "Python"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/python_repl.py", "section": "class::PythonREPLSchema", "content": "import importlib\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom langchain_experimental.utilities import PythonREPL\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import StrInput\nfrom langflow.schema import Data\n\nclass PythonREPLSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "PythonREPLSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import importlib", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from langchain_experimental.utilities import PythonREPL", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/recursive_character.py", "section": "class::RecursiveCharacterTextSplitterComponent", "content": "from typing import Any\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\nfrom langflow.base.textsplitters.model import LCTextSplitterComponent\nfrom langflow.inputs.inputs import DataInput, IntInput, MessageTextInput\nfrom langflow.utils.util import unescape_string\n\nclass RecursiveCharacterTextSplitterComponent(LCTextSplitterComponent):\n    display_name: str = \"Recursive Character Text Splitter\"\n    description: str = \"Split text trying to keep all related text together.\"\n    icon = \"LangChain\"\n    name = \"RecursiveCharacterTextSplitter\"\n\n    inputs = [\n        IntInput(name='chunk_size',\n        display_name='Chunk Size',\n        info='The maximum length of each chunk.',\n        value=1000),\n        IntInput(name='chunk_overlap',\n        display_name='Chunk Overlap',\n        info='The amount of overlap between chunks.',\n        value=200),\n        DataInput(name='data_input',\n        display_name='Input',\n        info='The texts to split.',\n        input_types=['Document',\n        'Data'],\n        required=True),\n        MessageTextInput(name='separators',\n        display_name='Separators',\n        info='The characters to split on.\\nIf left empty defaults to [\"\\\\n\\\\n\",\n        \"\\\\n\",\n        \" \",\n        \"\"].',\n        is_list=True)\n    ]\n\n    def get_data_input(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_text_splitter(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "RecursiveCharacterTextSplitterComponent", "base_classes": ["LCTextSplitterComponent"], "public_methods": ["def get_data_input(self)", "def build_text_splitter(self)"], "imports": ["from typing import Any", "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter", "from langflow.base.textsplitters.model import LCTextSplitterComponent", "from langflow.inputs.inputs import DataInput, IntInput, MessageTextInput", "from langflow.utils.util import unescape_string"], "inputs": "[IntInput(name='chunk_size', display_name='Chunk Size', info='The maximum length of each chunk.', value=1000), IntInput(name='chunk_overlap', display_name='Chunk Overlap', info='The amount of overlap between chunks.', value=200), DataInput(name='data_input', display_name='Input', info='The texts to split.', input_types=['Document', 'Data'], required=True), MessageTextInput(name='separators', display_name='Separators', info='The characters to split on.\\nIf left empty defaults to [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"].', is_list=True)]", "outputs": "", "display_name": "Recursive Character Text Splitter", "name": "RecursiveCharacterTextSplitter", "description": "Split text trying to keep all related text together.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/models/ollama.py", "section": "class::ChatOllamaComponent", "content": "import asyncio\nfrom typing import Any\nfrom urllib.parse import urljoin\nimport httpx\nfrom langchain_ollama import ChatOllama\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.ollama_constants import OLLAMA_TOOL_MODELS_BASE, URL_LIST\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SliderInput\nfrom langflow.logging import logger\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name: str = \"Ollama\"\n    description: str = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    inputs = [\n        MessageTextInput(name='base_url',\n        display_name='Base URL',\n        info='Endpoint of the Ollama API.',\n        value=''),\n        DropdownInput(name='model_name',\n        display_name='Model Name',\n        options=[],\n        info='Refer to https://ollama.com/library for more models.',\n        refresh_button=True,\n        real_time_refresh=True),\n        SliderInput(name='temperature',\n        display_name='Temperature',\n        value=0.1,\n        range_spec=RangeSpec(min=0,\n        max=1,\n        step=0.01),\n        advanced=True),\n        MessageTextInput(name='format',\n        display_name='Format',\n        info='Specify the format of the output (e.g.,\n        json).',\n        advanced=True),\n        DictInput(name='metadata',\n        display_name='Metadata',\n        info='Metadata to add to the run trace.',\n        advanced=True),\n        DropdownInput(name='mirostat',\n        display_name='Mirostat',\n        options=['Disabled',\n        'Mirostat',\n        'Mirostat 2.0'],\n        info='Enable/disable Mirostat sampling for controlling perplexity.',\n        value='Disabled',\n        advanced=True,\n        real_time_refresh=True),\n        FloatInput(name='mirostat_eta',\n        display_name='Mirostat Eta',\n        info='Learning rate for Mirostat algorithm. (Default: 0.1)',\n        advanced=True),\n        FloatInput(name='mirostat_tau',\n        display_name='Mirostat Tau',\n        info='Controls the balance between coherence and diversity of the output. (Default: 5.0)',\n        advanced=True),\n        IntInput(name='num_ctx',\n        display_name='Context Window Size',\n        info='Size of the context window for generating tokens. (Default: 2048)',\n        advanced=True),\n        IntInput(name='num_gpu',\n        display_name='Number of GPUs',\n        info='Number of GPUs to use for computation. (Default: 1 on macOS,\n        0 to disable)',\n        advanced=True),\n        IntInput(name='num_thread',\n        display_name='Number of Threads',\n        info='Number of threads to use during computation. (Default: detected for optimal performance)',\n        advanced=True),\n        IntInput(name='repeat_last_n',\n        display_name='Repeat Last N',\n        info='How far back the model looks to prevent repetition. (Default: 64,\n        0 = disabled,\n        -1 = num_ctx)',\n        advanced=True),\n        FloatInput(name='repeat_penalty',\n        display_name='Repeat Penalty',\n        info='Penalty for repetitions in generated text. (Default: 1.1)',\n        advanced=True),\n        FloatInput(name='tfs_z',\n        display_name='TFS Z',\n        info='Tail free sampling value. (Default: 1)',\n        advanced=True),\n        IntInput(name='timeout',\n        display_name='Timeout',\n        info='Timeout for the request stream.',\n        advanced=True),\n        IntInput(name='top_k',\n        display_name='Top K',\n        info='Limits token selection to top K. (Default: 40)',\n        advanced=True),\n        FloatInput(name='top_p',\n        display_name='Top P',\n        info='Works together with top-k. (Default: 0.9)',\n        advanced=True),\n        BoolInput(name='verbose',\n        display_name='Verbose',\n        info='Whether to print out response text.',\n        advanced=True),\n        MessageTextInput(name='tags',\n        display_name='Tags',\n        info='Comma-separated list of tags to add to the run trace.',\n        advanced=True),\n        MessageTextInput(name='stop_tokens',\n        display_name='Stop Tokens',\n        info='Comma-separated list of tokens to signal the model to stop generating text.',\n        advanced=True),\n        MessageTextInput(name='system',\n        display_name='System',\n        info='System to use for generating text.',\n        advanced=True),\n        BoolInput(name='tool_model_enabled',\n        display_name='Tool Model Enabled',\n        info='Whether to enable tool calling in the model.',\n        value=False,\n        real_time_refresh=True),\n        MessageTextInput(name='template',\n        display_name='Template',\n        info='Template to use for generating text.',\n        advanced=True),\n        *LCModelComponent._base_inputs\n    ]\n\n    def build_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def supports_tool_calling(self, model):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ChatOllamaComponent", "base_classes": ["LCModelComponent"], "public_methods": ["def build_model(self)", "def supports_tool_calling(self, model)"], "imports": ["import asyncio", "from typing import Any", "from urllib.parse import urljoin", "import httpx", "from langchain_ollama import ChatOllama", "from langflow.base.models.model import LCModelComponent", "from langflow.base.models.ollama_constants import OLLAMA_TOOL_MODELS_BASE, URL_LIST", "from langflow.field_typing import LanguageModel", "from langflow.field_typing.range_spec import RangeSpec", "from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SliderInput", "from langflow.logging import logger"], "inputs": "[MessageTextInput(name='base_url', display_name='Base URL', info='Endpoint of the Ollama API.', value=''), DropdownInput(name='model_name', display_name='Model Name', options=[], info='Refer to https://ollama.com/library for more models.', refresh_button=True, real_time_refresh=True), SliderInput(name='temperature', display_name='Temperature', value=0.1, range_spec=RangeSpec(min=0, max=1, step=0.01), advanced=True), MessageTextInput(name='format', display_name='Format', info='Specify the format of the output (e.g., json).', advanced=True), DictInput(name='metadata', display_name='Metadata', info='Metadata to add to the run trace.', advanced=True), DropdownInput(name='mirostat', display_name='Mirostat', options=['Disabled', 'Mirostat', 'Mirostat 2.0'], info='Enable/disable Mirostat sampling for controlling perplexity.', value='Disabled', advanced=True, real_time_refresh=True), FloatInput(name='mirostat_eta', display_name='Mirostat Eta', info='Learning rate for Mirostat algorithm. (Default: 0.1)', advanced=True), FloatInput(name='mirostat_tau', display_name='Mirostat Tau', info='Controls the balance between coherence and diversity of the output. (Default: 5.0)', advanced=True), IntInput(name='num_ctx', display_name='Context Window Size', info='Size of the context window for generating tokens. (Default: 2048)', advanced=True), IntInput(name='num_gpu', display_name='Number of GPUs', info='Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)', advanced=True), IntInput(name='num_thread', display_name='Number of Threads', info='Number of threads to use during computation. (Default: detected for optimal performance)', advanced=True), IntInput(name='repeat_last_n', display_name='Repeat Last N', info='How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)', advanced=True), FloatInput(name='repeat_penalty', display_name='Repeat Penalty', info='Penalty for repetitions in generated text. (Default: 1.1)', advanced=True), FloatInput(name='tfs_z', display_name='TFS Z', info='Tail free sampling value. (Default: 1)', advanced=True), IntInput(name='timeout', display_name='Timeout', info='Timeout for the request stream.', advanced=True), IntInput(name='top_k', display_name='Top K', info='Limits token selection to top K. (Default: 40)', advanced=True), FloatInput(name='top_p', display_name='Top P', info='Works together with top-k. (Default: 0.9)', advanced=True), BoolInput(name='verbose', display_name='Verbose', info='Whether to print out response text.', advanced=True), MessageTextInput(name='tags', display_name='Tags', info='Comma-separated list of tags to add to the run trace.', advanced=True), MessageTextInput(name='stop_tokens', display_name='Stop Tokens', info='Comma-separated list of tokens to signal the model to stop generating text.', advanced=True), MessageTextInput(name='system', display_name='System', info='System to use for generating text.', advanced=True), BoolInput(name='tool_model_enabled', display_name='Tool Model Enabled', info='Whether to enable tool calling in the model.', value=False, real_time_refresh=True), MessageTextInput(name='template', display_name='Template', info='Template to use for generating text.', advanced=True), *LCModelComponent._base_inputs]", "outputs": "", "display_name": "Ollama", "name": "OllamaModel", "description": "Generate text using Ollama Local LLMs.", "icon": "Ollama"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/upstash.py", "section": "class::UpstashVectorStoreComponent", "content": "from langchain_community.vectorstores import UpstashVectorStore\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, MultilineInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass UpstashVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Upstash\"\n    description: str = \"Upstash Vector Store with search capabilities\"\n    icon = \"Upstash\"\n    name = \"Upstash\"\n\n    inputs = [\n        StrInput(name='index_url',\n        display_name='Index URL',\n        info='The URL of the Upstash index.',\n        required=True),\n        SecretStrInput(name='index_token',\n        display_name='Index Token',\n        info='The token for the Upstash index.',\n        required=True),\n        StrInput(name='text_key',\n        display_name='Text Key',\n        info='The key in the record to use as text.',\n        value='text',\n        advanced=True),\n        StrInput(name='namespace',\n        display_name='Namespace',\n        info='Leave empty for default namespace.'),\n        *LCVectorStoreComponent.inputs,\n        MultilineInput(name='metadata_filter',\n        display_name='Metadata Filter',\n        info='Filters documents by metadata. Look at the documentation for more information.'),\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings'],\n        info=\"To use Upstash's embeddings,\n        don't provide an embedding.\"),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "UpstashVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from langchain_community.vectorstores import UpstashVectorStore", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import HandleInput, IntInput, MultilineInput, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[StrInput(name='index_url', display_name='Index URL', info='The URL of the Upstash index.', required=True), SecretStrInput(name='index_token', display_name='Index Token', info='The token for the Upstash index.', required=True), StrInput(name='text_key', display_name='Text Key', info='The key in the record to use as text.', value='text', advanced=True), StrInput(name='namespace', display_name='Namespace', info='Leave empty for default namespace.'), *LCVectorStoreComponent.inputs, MultilineInput(name='metadata_filter', display_name='Metadata Filter', info='Filters documents by metadata. Look at the documentation for more information.'), HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings'], info=\"To use Upstash's embeddings, don't provide an embedding.\"), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Upstash", "name": "Upstash", "description": "Upstash Vector Store with search capabilities", "icon": "Upstash"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wikidata.py", "section": "class::WikidataComponent", "content": "import httpx\nfrom httpx import HTTPError\nfrom langchain_core.tools import ToolException\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass WikidataComponent(Component):\n    display_name: str = \"Wikidata\"\n    description: str = \"Performs a search using the Wikidata API.\"\n    icon = \"Wikipedia\"\n\n    inputs = [\n        MultilineInput(name='query',\n        display_name='Query',\n        info='The text query for similarity search on Wikidata.',\n        required=True,\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Message',\n        name='text',\n        method='fetch_content_text')\n    ]\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WikidataComponent", "base_classes": ["Component"], "public_methods": ["def fetch_content(self)", "def fetch_content_text(self)"], "imports": ["import httpx", "from httpx import HTTPError", "from langchain_core.tools import ToolException", "from langflow.custom import Component", "from langflow.helpers.data import data_to_text", "from langflow.io import MultilineInput, Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[MultilineInput(name='query', display_name='Query', info='The text query for similarity search on Wikidata.', required=True, tool_mode=True)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Message', name='text', method='fetch_content_text')]", "display_name": "Wikidata", "name": "", "description": "Performs a search using the Wikidata API.", "icon": "Wikipedia"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/fake_embeddings.py", "section": "class::FakeEmbeddingsComponent", "content": "from langchain_community.embeddings import FakeEmbeddings\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import IntInput\n\nclass FakeEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"Fake Embeddings\"\n    description: str = \"Generate fake embeddings, useful for initial testing and connecting components.\"\n    icon = \"LangChain\"\n    name = \"LangChainFakeEmbeddings\"\n\n    inputs = [\n        IntInput(name='dimensions',\n        display_name='Dimensions',\n        info='The number of dimensions the resulting output embeddings should have.',\n        value=5)\n    ]\n\n    def build_embeddings(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "FakeEmbeddingsComponent", "base_classes": ["LCEmbeddingsModel"], "public_methods": ["def build_embeddings(self)"], "imports": ["from langchain_community.embeddings import FakeEmbeddings", "from langflow.base.embeddings.model import LCEmbeddingsModel", "from langflow.field_typing import Embeddings", "from langflow.io import IntInput"], "inputs": "[IntInput(name='dimensions', display_name='Dimensions', info='The number of dimensions the resulting output embeddings should have.', value=5)]", "outputs": "", "display_name": "Fake Embeddings", "name": "LangChainFakeEmbeddings", "description": "Generate fake embeddings, useful for initial testing and connecting components.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/vectorstores/couchbase.py", "section": "class::CouchbaseVectorStoreComponent", "content": "from datetime import timedelta\nfrom langchain_community.vectorstores import CouchbaseVectorStore\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema import Data\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.options import ClusterOptions\n\nclass CouchbaseVectorStoreComponent(LCVectorStoreComponent):\n    display_name: str = \"Couchbase\"\n    description: str = \"Couchbase Vector Store with search capabilities\"\n    icon = \"Couchbase\"\n    name = \"Couchbase\"\n\n    inputs = [\n        SecretStrInput(name='couchbase_connection_string',\n        display_name='Couchbase Cluster connection string',\n        required=True),\n        StrInput(name='couchbase_username',\n        display_name='Couchbase username',\n        required=True),\n        SecretStrInput(name='couchbase_password',\n        display_name='Couchbase password',\n        required=True),\n        StrInput(name='bucket_name',\n        display_name='Bucket Name',\n        required=True),\n        StrInput(name='scope_name',\n        display_name='Scope Name',\n        required=True),\n        StrInput(name='collection_name',\n        display_name='Collection Name',\n        required=True),\n        StrInput(name='index_name',\n        display_name='Index Name',\n        required=True),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name='embedding',\n        display_name='Embedding',\n        input_types=['Embeddings']),\n        IntInput(name='number_of_results',\n        display_name='Number of Results',\n        info='Number of results to return.',\n        value=4,\n        advanced=True)\n    ]\n\n    def build_vector_store(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def search_documents(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CouchbaseVectorStoreComponent", "base_classes": ["LCVectorStoreComponent"], "public_methods": ["def build_vector_store(self)", "def search_documents(self)"], "imports": ["from datetime import timedelta", "from langchain_community.vectorstores import CouchbaseVectorStore", "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store", "from langflow.helpers.data import docs_to_data", "from langflow.io import HandleInput, IntInput, SecretStrInput, StrInput", "from langflow.schema import Data", "from couchbase.auth import PasswordAuthenticator", "from couchbase.cluster import Cluster", "from couchbase.options import ClusterOptions"], "inputs": "[SecretStrInput(name='couchbase_connection_string', display_name='Couchbase Cluster connection string', required=True), StrInput(name='couchbase_username', display_name='Couchbase username', required=True), SecretStrInput(name='couchbase_password', display_name='Couchbase password', required=True), StrInput(name='bucket_name', display_name='Bucket Name', required=True), StrInput(name='scope_name', display_name='Scope Name', required=True), StrInput(name='collection_name', display_name='Collection Name', required=True), StrInput(name='index_name', display_name='Index Name', required=True), *LCVectorStoreComponent.inputs, HandleInput(name='embedding', display_name='Embedding', input_types=['Embeddings']), IntInput(name='number_of_results', display_name='Number of Results', info='Number of results to return.', value=4, advanced=True)]", "outputs": "", "display_name": "Couchbase", "name": "Couchbase", "description": "Couchbase Vector Store with search capabilities", "icon": "Couchbase"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/yahoo_finance.py", "section": "class::YahooFinanceMethod", "content": "import ast\nimport pprint\nfrom enum import Enum\nimport yfinance as yf\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput\nfrom langflow.schema import Data\n\nclass YahooFinanceMethod(Enum):\n", "metadata": {"parser": "python_component", "class_name": "YahooFinanceMethod", "base_classes": ["Enum"], "public_methods": [], "imports": ["import ast", "import pprint", "from enum import Enum", "import yfinance as yf", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/yahoo_finance.py", "section": "class::YahooFinanceSchema", "content": "import ast\nimport pprint\nfrom enum import Enum\nimport yfinance as yf\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput\nfrom langflow.schema import Data\n\nclass YahooFinanceSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "YahooFinanceSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["import ast", "import pprint", "from enum import Enum", "import yfinance as yf", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/yahoo_finance.py", "section": "class::YfinanceToolComponent", "content": "import ast\nimport pprint\nfrom enum import Enum\nimport yfinance as yf\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DropdownInput, IntInput, MessageTextInput\nfrom langflow.schema import Data\n\nclass YfinanceToolComponent(LCToolComponent):\n    display_name: str = \"Yahoo Finance [DEPRECATED]\"\n    description: str = \"Uses [yfinance](https://pypi.org/project/yfinance/) (unofficial package) to access financial data and market information from Yahoo Finance.\"\n    icon = \"trending-up\"\n    name = \"YahooFinanceTool\"\n\n    inputs = [\n        MessageTextInput(name='symbol',\n        display_name='Stock Symbol',\n        info='The stock symbol to retrieve data for (e.g.,\n        AAPL,\n        GOOG).'),\n        DropdownInput(name='method',\n        display_name='Data Method',\n        info='The type of data to retrieve.',\n        options=list(YahooFinanceMethod),\n        value='get_news'),\n        IntInput(name='num_news',\n        display_name='Number of News',\n        info='The number of news articles to retrieve (only applicable for get_news).',\n        value=5)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "YfinanceToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["import ast", "import pprint", "from enum import Enum", "import yfinance as yf", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DropdownInput, IntInput, MessageTextInput", "from langflow.schema import Data"], "inputs": "[MessageTextInput(name='symbol', display_name='Stock Symbol', info='The stock symbol to retrieve data for (e.g., AAPL, GOOG).'), DropdownInput(name='method', display_name='Data Method', info='The type of data to retrieve.', options=list(YahooFinanceMethod), value='get_news'), IntInput(name='num_news', display_name='Number of News', info='The number of news articles to retrieve (only applicable for get_news).', value=5)]", "outputs": "", "display_name": "Yahoo Finance [DEPRECATED]", "name": "YahooFinanceTool", "description": "Uses [yfinance](https://pypi.org/project/yfinance/) (unofficial package) to access financial data and market information from Yahoo Finance.", "icon": "trending-up"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/json_agent.py", "section": "class::JsonAgentComponent", "content": "from pathlib import Path\nimport yaml\nfrom langchain.agents import AgentExecutor\nfrom langchain_community.agent_toolkits import create_json_agent\nfrom langchain_community.agent_toolkits.json.toolkit import JsonToolkit\nfrom langchain_community.tools.json.tool import JsonSpec\nfrom langflow.base.agents.agent import LCAgentComponent\nfrom langflow.inputs import FileInput, HandleInput\n\nclass JsonAgentComponent(LCAgentComponent):\n    display_name: str = \"JsonAgent\"\n    description: str = \"Construct a json agent from an LLM and tools.\"\n    name = \"JsonAgent\"\n\n    inputs = [\n        *LCAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True),\n        FileInput(name='path',\n        display_name='File Path',\n        file_types=['json',\n        'yaml',\n        'yml'],\n        required=True)\n    ]\n\n    def build_agent(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "JsonAgentComponent", "base_classes": ["LCAgentComponent"], "public_methods": ["def build_agent(self)"], "imports": ["from pathlib import Path", "import yaml", "from langchain.agents import AgentExecutor", "from langchain_community.agent_toolkits import create_json_agent", "from langchain_community.agent_toolkits.json.toolkit import JsonToolkit", "from langchain_community.tools.json.tool import JsonSpec", "from langflow.base.agents.agent import LCAgentComponent", "from langflow.inputs import FileInput, HandleInput"], "inputs": "[*LCAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True), FileInput(name='path', display_name='File Path', file_types=['json', 'yaml', 'yml'], required=True)]", "outputs": "", "display_name": "JsonAgent", "name": "JsonAgent", "description": "Construct a json agent from an LLM and tools.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_search_tool.py", "section": "class::TavilySearchDepth", "content": "from enum import Enum\nimport httpx\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass TavilySearchDepth(Enum):\n", "metadata": {"parser": "python_component", "class_name": "TavilySearchDepth", "base_classes": ["Enum"], "public_methods": [], "imports": ["from enum import Enum", "import httpx", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_search_tool.py", "section": "class::TavilySearchTopic", "content": "from enum import Enum\nimport httpx\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass TavilySearchTopic(Enum):\n", "metadata": {"parser": "python_component", "class_name": "TavilySearchTopic", "base_classes": ["Enum"], "public_methods": [], "imports": ["from enum import Enum", "import httpx", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_search_tool.py", "section": "class::TavilySearchTimeRange", "content": "from enum import Enum\nimport httpx\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass TavilySearchTimeRange(Enum):\n", "metadata": {"parser": "python_component", "class_name": "TavilySearchTimeRange", "base_classes": ["Enum"], "public_methods": [], "imports": ["from enum import Enum", "import httpx", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_search_tool.py", "section": "class::TavilySearchSchema", "content": "from enum import Enum\nimport httpx\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass TavilySearchSchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "TavilySearchSchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from enum import Enum", "import httpx", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/tavily_search_tool.py", "section": "class::TavilySearchToolComponent", "content": "from enum import Enum\nimport httpx\nfrom langchain.tools import StructuredTool\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass TavilySearchToolComponent(LCToolComponent):\n    display_name: str = \"Tavily Search API\"\n    description: str = \"**Tavily Search API** is a search engine optimized for LLMs and RAG,         aimed at efficient, quick, and persistent search results. It can be used independently or as an agent tool.\n\nNote: Check 'Advanced' for all options.\n\"\n    icon = \"TavilyIcon\"\n    name = \"TavilyAISearch\"\n\n    inputs = [\n        SecretStrInput(name='api_key',\n        display_name='Tavily API Key',\n        required=True,\n        info='Your Tavily API Key.'),\n        MessageTextInput(name='query',\n        display_name='Search Query',\n        info='The search query you want to execute with Tavily.'),\n        DropdownInput(name='search_depth',\n        display_name='Search Depth',\n        info='The depth of the search.',\n        options=list(TavilySearchDepth),\n        value=TavilySearchDepth.ADVANCED,\n        advanced=True),\n        IntInput(name='chunks_per_source',\n        display_name='Chunks Per Source',\n        info='The number of content chunks to retrieve from each source (1-3). Only works with advanced search.',\n        value=MAX_CHUNKS_PER_SOURCE,\n        advanced=True),\n        DropdownInput(name='topic',\n        display_name='Search Topic',\n        info='The category of the search.',\n        options=list(TavilySearchTopic),\n        value=TavilySearchTopic.GENERAL,\n        advanced=True),\n        IntInput(name='days',\n        display_name='Days',\n        info='Number of days back from current date to include. Only available with news topic.',\n        value=7,\n        advanced=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        info='The maximum number of search results to return.',\n        value=5,\n        advanced=True),\n        BoolInput(name='include_answer',\n        display_name='Include Answer',\n        info='Include a short answer to original query.',\n        value=True,\n        advanced=True),\n        DropdownInput(name='time_range',\n        display_name='Time Range',\n        info='The time range back from the current date to filter results.',\n        options=list(TavilySearchTimeRange),\n        value=None,\n        advanced=True),\n        BoolInput(name='include_images',\n        display_name='Include Images',\n        info='Include a list of query-related images in the response.',\n        value=True,\n        advanced=True),\n        MessageTextInput(name='include_domains',\n        display_name='Include Domains',\n        info='Comma-separated list of domains to include in the search results.',\n        advanced=True),\n        MessageTextInput(name='exclude_domains',\n        display_name='Exclude Domains',\n        info='Comma-separated list of domains to exclude from the search results.',\n        advanced=True),\n        BoolInput(name='include_raw_content',\n        display_name='Include Raw Content',\n        info='Include the cleaned and parsed HTML content of each search result.',\n        value=False,\n        advanced=True)\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "TavilySearchToolComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def run_model(self)", "def build_tool(self)"], "imports": ["from enum import Enum", "import httpx", "from langchain.tools import StructuredTool", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='api_key', display_name='Tavily API Key', required=True, info='Your Tavily API Key.'), MessageTextInput(name='query', display_name='Search Query', info='The search query you want to execute with Tavily.'), DropdownInput(name='search_depth', display_name='Search Depth', info='The depth of the search.', options=list(TavilySearchDepth), value=TavilySearchDepth.ADVANCED, advanced=True), IntInput(name='chunks_per_source', display_name='Chunks Per Source', info='The number of content chunks to retrieve from each source (1-3). Only works with advanced search.', value=MAX_CHUNKS_PER_SOURCE, advanced=True), DropdownInput(name='topic', display_name='Search Topic', info='The category of the search.', options=list(TavilySearchTopic), value=TavilySearchTopic.GENERAL, advanced=True), IntInput(name='days', display_name='Days', info='Number of days back from current date to include. Only available with news topic.', value=7, advanced=True), IntInput(name='max_results', display_name='Max Results', info='The maximum number of search results to return.', value=5, advanced=True), BoolInput(name='include_answer', display_name='Include Answer', info='Include a short answer to original query.', value=True, advanced=True), DropdownInput(name='time_range', display_name='Time Range', info='The time range back from the current date to filter results.', options=list(TavilySearchTimeRange), value=None, advanced=True), BoolInput(name='include_images', display_name='Include Images', info='Include a list of query-related images in the response.', value=True, advanced=True), MessageTextInput(name='include_domains', display_name='Include Domains', info='Comma-separated list of domains to include in the search results.', advanced=True), MessageTextInput(name='exclude_domains', display_name='Exclude Domains', info='Comma-separated list of domains to exclude from the search results.', advanced=True), BoolInput(name='include_raw_content', display_name='Include Raw Content', info='Include the cleaned and parsed HTML content of each search result.', value=False, advanced=True)]", "outputs": "", "display_name": "Tavily Search API", "name": "TavilyAISearch", "description": "**Tavily Search API** is a search engine optimized for LLMs and RAG,         aimed at efficient, quick, and persistent search results. It can be used independently or as an agent tool.\n\nNote: Check 'Advanced' for all options.\n", "icon": "TavilyIcon"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/spider.py", "section": "class::SpiderTool", "content": "from spider.spider import Spider\nfrom langflow.base.langchain_utilities.spider_constants import MODES\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, DropdownInput, IntInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass SpiderTool(Component):\n    display_name: str = \"Spider Web Crawler & Scraper\"\n    description: str = \"Spider API for web crawling and scraping.\"\n\n    inputs = [\n        SecretStrInput(name='spider_api_key',\n        display_name='Spider API Key',\n        required=True,\n        password=True,\n        info='The Spider API Key,\n        get it from https://spider.cloud'),\n        StrInput(name='url',\n        display_name='URL',\n        required=True,\n        info='The URL to scrape or crawl'),\n        DropdownInput(name='mode',\n        display_name='Mode',\n        required=True,\n        options=MODES,\n        value=MODES[0],\n        info='The mode of operation: scrape or crawl'),\n        IntInput(name='limit',\n        display_name='Limit',\n        info='The maximum amount of pages allowed to crawl per website. Set to 0 to crawl all pages.',\n        advanced=True),\n        IntInput(name='depth',\n        display_name='Depth',\n        info='The crawl limit for maximum depth. If 0,\n        no limit will be applied.',\n        advanced=True),\n        StrInput(name='blacklist',\n        display_name='Blacklist',\n        info='Blacklist paths that you do not want to crawl. Use Regex patterns.',\n        advanced=True),\n        StrInput(name='whitelist',\n        display_name='Whitelist',\n        info='Whitelist paths that you want to crawl,\n        ignoring all other routes. Use Regex patterns.',\n        advanced=True),\n        BoolInput(name='readability',\n        display_name='Use Readability',\n        info='Use readability to pre-process the content for reading.',\n        advanced=True),\n        IntInput(name='request_timeout',\n        display_name='Request Timeout',\n        info='Timeout for the request in seconds.',\n        advanced=True),\n        BoolInput(name='metadata',\n        display_name='Metadata',\n        info='Include metadata in the response.',\n        advanced=True),\n        DictInput(name='params',\n        display_name='Additional Parameters',\n        info='Additional parameters to pass to the API. If provided,\n        other inputs will be ignored.')\n    ]\n\n    outputs = [\n        Output(display_name='Markdown',\n        name='content',\n        method='crawl')\n    ]\n\n    def crawl(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SpiderTool", "base_classes": ["Component"], "public_methods": ["def crawl(self)"], "imports": ["from spider.spider import Spider", "from langflow.base.langchain_utilities.spider_constants import MODES", "from langflow.custom import Component", "from langflow.io import BoolInput, DictInput, DropdownInput, IntInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='spider_api_key', display_name='Spider API Key', required=True, password=True, info='The Spider API Key, get it from https://spider.cloud'), StrInput(name='url', display_name='URL', required=True, info='The URL to scrape or crawl'), DropdownInput(name='mode', display_name='Mode', required=True, options=MODES, value=MODES[0], info='The mode of operation: scrape or crawl'), IntInput(name='limit', display_name='Limit', info='The maximum amount of pages allowed to crawl per website. Set to 0 to crawl all pages.', advanced=True), IntInput(name='depth', display_name='Depth', info='The crawl limit for maximum depth. If 0, no limit will be applied.', advanced=True), StrInput(name='blacklist', display_name='Blacklist', info='Blacklist paths that you do not want to crawl. Use Regex patterns.', advanced=True), StrInput(name='whitelist', display_name='Whitelist', info='Whitelist paths that you want to crawl, ignoring all other routes. Use Regex patterns.', advanced=True), BoolInput(name='readability', display_name='Use Readability', info='Use readability to pre-process the content for reading.', advanced=True), IntInput(name='request_timeout', display_name='Request Timeout', info='Timeout for the request in seconds.', advanced=True), BoolInput(name='metadata', display_name='Metadata', info='Include metadata in the response.', advanced=True), DictInput(name='params', display_name='Additional Parameters', info='Additional parameters to pass to the API. If provided, other inputs will be ignored.')]", "outputs": "[Output(display_name='Markdown', name='content', method='crawl')]", "display_name": "Spider Web Crawler & Scraper", "name": "", "description": "Spider API for web crawling and scraping.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/spider.py", "section": "class::SpiderToolError", "content": "from spider.spider import Spider\nfrom langflow.base.langchain_utilities.spider_constants import MODES\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, DropdownInput, IntInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\nclass SpiderToolError(Exception):\n    \"\"\"\n    SpiderTool error.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "SpiderToolError", "base_classes": ["Exception"], "public_methods": [], "imports": ["from spider.spider import Spider", "from langflow.base.langchain_utilities.spider_constants import MODES", "from langflow.custom import Component", "from langflow.io import BoolInput, DictInput, DropdownInput, IntInput, Output, SecretStrInput, StrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/search_api.py", "section": "class::SearchAPIComponent", "content": "from typing import Any\nfrom langchain.tools import StructuredTool\nfrom langchain_community.utilities.searchapi import SearchApiAPIWrapper\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DictInput, IntInput, MessageTextInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass SearchAPIComponent(LCToolComponent):\n    display_name: str = \"Search API [DEPRECATED]\"\n    description: str = \"Call the searchapi.io API with result limiting\"\n    icon = \"SearchAPI\"\n    name = \"SearchAPI\"\n\n    inputs = [\n        MessageTextInput(name='engine',\n        display_name='Engine',\n        value='google'),\n        SecretStrInput(name='api_key',\n        display_name='SearchAPI API Key',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input'),\n        DictInput(name='search_params',\n        display_name='Search parameters',\n        advanced=True,\n        is_list=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=5,\n        advanced=True),\n        IntInput(name='max_snippet_length',\n        display_name='Max Snippet Length',\n        value=100,\n        advanced=True)\n    ]\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SearchAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def build_tool(self)", "def run_model(self)"], "imports": ["from typing import Any", "from langchain.tools import StructuredTool", "from langchain_community.utilities.searchapi import SearchApiAPIWrapper", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DictInput, IntInput, MessageTextInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "[MessageTextInput(name='engine', display_name='Engine', value='google'), SecretStrInput(name='api_key', display_name='SearchAPI API Key', required=True), MultilineInput(name='input_value', display_name='Input'), DictInput(name='search_params', display_name='Search parameters', advanced=True, is_list=True), IntInput(name='max_results', display_name='Max Results', value=5, advanced=True), IntInput(name='max_snippet_length', display_name='Max Snippet Length', value=100, advanced=True)]", "outputs": "", "display_name": "Search API [DEPRECATED]", "name": "SearchAPI", "description": "Call the searchapi.io API with result limiting", "icon": "SearchAPI"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/search_api.py", "section": "class::SearchAPISchema", "content": "from typing import Any\nfrom langchain.tools import StructuredTool\nfrom langchain_community.utilities.searchapi import SearchApiAPIWrapper\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DictInput, IntInput, MessageTextInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass SearchAPISchema(BaseModel):\n", "metadata": {"parser": "python_component", "class_name": "SearchAPISchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from typing import Any", "from langchain.tools import StructuredTool", "from langchain_community.utilities.searchapi import SearchApiAPIWrapper", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DictInput, IntInput, MessageTextInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/json_document_builder.py", "section": "class::JSONDocumentBuilder", "content": "from langchain_core.documents import Document\nfrom langflow.custom import CustomComponent\nfrom langflow.services.database.models.base import orjson_dumps\n\nclass JSONDocumentBuilder(CustomComponent):\n    display_name: str = \"JSON Document Builder\"\n    description: str = \"Build a Document containing a JSON object using a key and another Document page content.\"\n    name = \"JSONDocumentBuilder\"\n\n    def build(self, key, document):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "JSONDocumentBuilder", "base_classes": ["CustomComponent"], "public_methods": ["def build(self, key, document)"], "imports": ["from langchain_core.documents import Document", "from langflow.custom import CustomComponent", "from langflow.services.database.models.base import orjson_dumps"], "inputs": "", "outputs": "", "display_name": "JSON Document Builder", "name": "JSONDocumentBuilder", "description": "Build a Document containing a JSON object using a key and another Document page content.", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/wikipedia.py", "section": "class::WikipediaComponent", "content": "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\nfrom langflow.custom import Component\nfrom langflow.inputs import BoolInput, IntInput, MessageTextInput, MultilineInput\nfrom langflow.io import Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\nclass WikipediaComponent(Component):\n    display_name: str = \"Wikipedia\"\n    description: str = \"Call Wikipedia API.\"\n    icon = \"Wikipedia\"\n\n    inputs = [\n        MultilineInput(name='input_value',\n        display_name='Input',\n        tool_mode=True),\n        MessageTextInput(name='lang',\n        display_name='Language',\n        value='en'),\n        IntInput(name='k',\n        display_name='Number of results',\n        value=4,\n        required=True),\n        BoolInput(name='load_all_available_meta',\n        display_name='Load all available meta',\n        value=False,\n        advanced=True),\n        IntInput(name='doc_content_chars_max',\n        display_name='Document content characters max',\n        value=4000,\n        advanced=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='DataFrame',\n        name='dataframe',\n        method='as_dataframe')\n    ]\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def as_dataframe(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "WikipediaComponent", "base_classes": ["Component"], "public_methods": ["def fetch_content(self)", "def fetch_content_text(self)", "def as_dataframe(self)"], "imports": ["from langchain_community.utilities.wikipedia import WikipediaAPIWrapper", "from langflow.custom import Component", "from langflow.inputs import BoolInput, IntInput, MessageTextInput, MultilineInput", "from langflow.io import Output", "from langflow.schema import Data, DataFrame", "from langflow.schema.message import Message"], "inputs": "[MultilineInput(name='input_value', display_name='Input', tool_mode=True), MessageTextInput(name='lang', display_name='Language', value='en'), IntInput(name='k', display_name='Number of results', value=4, required=True), BoolInput(name='load_all_available_meta', display_name='Load all available meta', value=False, advanced=True), IntInput(name='doc_content_chars_max', display_name='Document content characters max', value=4000, advanced=True)]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='DataFrame', name='dataframe', method='as_dataframe')]", "display_name": "Wikipedia", "name": "", "description": "Call Wikipedia API.", "icon": "Wikipedia"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/tool_calling.py", "section": "class::ToolCallingAgentComponent", "content": "from langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MessageTextInput\nfrom langflow.inputs.inputs import DataInput, HandleInput\nfrom langflow.schema import Data\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"An agent designed to utilize various tools seamlessly within workflows.\"\n    icon = \"LangChain\"\n    name = \"ToolCallingAgent\"\n\n    inputs = [\n        *LCToolsAgentComponent._base_inputs,\n        HandleInput(name='llm',\n        display_name='Language Model',\n        input_types=['LanguageModel'],\n        required=True,\n        info='Language model that the agent utilizes to perform tasks effectively.'),\n        MessageTextInput(name='system_prompt',\n        display_name='System Prompt',\n        info=\"System prompt to guide the agent's behavior.\",\n        value='You are a helpful assistant that can use tools to answer questions and perform tasks.'),\n        DataInput(name='chat_history',\n        display_name='Chat Memory',\n        is_list=True,\n        advanced=True,\n        info='This input stores the chat history,\n        allowing the agent to remember previous conversations.')\n    ]\n\n    def get_chat_history_data(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def create_agent_runnable(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "ToolCallingAgentComponent", "base_classes": ["LCToolsAgentComponent"], "public_methods": ["def get_chat_history_data(self)", "def create_agent_runnable(self)"], "imports": ["from langchain.agents import create_tool_calling_agent", "from langchain_core.prompts import ChatPromptTemplate", "from langflow.base.agents.agent import LCToolsAgentComponent", "from langflow.inputs import MessageTextInput", "from langflow.inputs.inputs import DataInput, HandleInput", "from langflow.schema import Data"], "inputs": "[*LCToolsAgentComponent._base_inputs, HandleInput(name='llm', display_name='Language Model', input_types=['LanguageModel'], required=True, info='Language model that the agent utilizes to perform tasks effectively.'), MessageTextInput(name='system_prompt', display_name='System Prompt', info=\"System prompt to guide the agent's behavior.\", value='You are a helpful assistant that can use tools to answer questions and perform tasks.'), DataInput(name='chat_history', display_name='Chat Memory', is_list=True, advanced=True, info='This input stores the chat history, allowing the agent to remember previous conversations.')]", "outputs": "", "display_name": "Tool Calling Agent", "name": "ToolCallingAgent", "description": "An agent designed to utilize various tools seamlessly within workflows.", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/duck_duck_go_search_run.py", "section": "class::DuckDuckGoSearchComponent", "content": "from langchain_community.tools import DuckDuckGoSearchRun\nfrom langflow.custom import Component\nfrom langflow.inputs import IntInput, MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\nclass DuckDuckGoSearchComponent(Component):\n    \"\"\"\n    Component for performing web searches using DuckDuckGo.\n    \"\"\"\n\n    display_name: str = \"DuckDuckGo Search\"\n    description: str = \"Search the web using DuckDuckGo with customizable result limits\"\n    icon = \"DuckDuckGo\"\n\n    inputs = [\n        MessageTextInput(name='input_value',\n        display_name='Search Query',\n        required=True,\n        info='The search query to execute with DuckDuckGo',\n        tool_mode=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=5,\n        required=False,\n        advanced=True,\n        info='Maximum number of search results to return'),\n        IntInput(name='max_snippet_length',\n        display_name='Max Snippet Length',\n        value=100,\n        required=False,\n        advanced=True,\n        info='Maximum length of each result snippet')\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='data',\n        method='fetch_content'),\n        Output(display_name='Text',\n        name='text',\n        method='fetch_content_text')\n    ]\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def fetch_content_text(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "DuckDuckGoSearchComponent", "base_classes": ["Component"], "public_methods": ["def run_model(self)", "def fetch_content(self)", "def fetch_content_text(self)"], "imports": ["from langchain_community.tools import DuckDuckGoSearchRun", "from langflow.custom import Component", "from langflow.inputs import IntInput, MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data", "from langflow.schema.message import Message"], "inputs": "[MessageTextInput(name='input_value', display_name='Search Query', required=True, info='The search query to execute with DuckDuckGo', tool_mode=True), IntInput(name='max_results', display_name='Max Results', value=5, required=False, advanced=True, info='Maximum number of search results to return'), IntInput(name='max_snippet_length', display_name='Max Snippet Length', value=100, required=False, advanced=True, info='Maximum length of each result snippet')]", "outputs": "[Output(display_name='Data', name='data', method='fetch_content'), Output(display_name='Text', name='text', method='fetch_content_text')]", "display_name": "DuckDuckGo Search", "name": "", "description": "Search the web using DuckDuckGo with customizable result limits", "icon": "DuckDuckGo"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/langchain_utilities/langchain_hub.py", "section": "class::LangChainHubPromptComponent", "content": "import re\nfrom langchain_core.prompts import HumanMessagePromptTemplate\nfrom langflow.custom import Component\nfrom langflow.inputs import DefaultPromptField, SecretStrInput, StrInput\nfrom langflow.io import Output\nfrom langflow.schema.message import Message\nimport langchain.hub\n\nclass LangChainHubPromptComponent(Component):\n    display_name: str = \"Prompt Hub\"\n    description: str = \"Prompt Component that uses LangChain Hub prompts\"\n    icon = \"LangChain\"\n    name = \"LangChain Hub Prompt\"\n\n    inputs = [\n        SecretStrInput(name='langchain_api_key',\n        display_name='Your LangChain API Key',\n        info='The LangChain API Key to use.',\n        required=True),\n        StrInput(name='langchain_hub_prompt',\n        display_name='LangChain Hub Prompt',\n        info=\"The LangChain Hub prompt to use,\n        i.e.,\n        'efriis/my-first-prompt'\",\n        refresh_button=True,\n        required=True)\n    ]\n\n    outputs = [\n        Output(display_name='Build Prompt',\n        name='prompt',\n        method='build_prompt')\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "LangChainHubPromptComponent", "base_classes": ["Component"], "public_methods": ["def update_build_config(self, build_config, field_value, field_name)"], "imports": ["import re", "from langchain_core.prompts import HumanMessagePromptTemplate", "from langflow.custom import Component", "from langflow.inputs import DefaultPromptField, SecretStrInput, StrInput", "from langflow.io import Output", "from langflow.schema.message import Message", "import langchain.hub"], "inputs": "[SecretStrInput(name='langchain_api_key', display_name='Your LangChain API Key', info='The LangChain API Key to use.', required=True), StrInput(name='langchain_hub_prompt', display_name='LangChain Hub Prompt', info=\"The LangChain Hub prompt to use, i.e., 'efriis/my-first-prompt'\", refresh_button=True, required=True)]", "outputs": "[Output(display_name='Build Prompt', name='prompt', method='build_prompt')]", "display_name": "Prompt Hub", "name": "LangChain Hub Prompt", "description": "Prompt Component that uses LangChain Hub prompts", "icon": "LangChain"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/calculator_core.py", "section": "class::CalculatorComponent", "content": "import ast\nimport operator\nfrom collections.abc import Callable\nfrom langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import Output\nfrom langflow.schema import Data\n\nclass CalculatorComponent(Component):\n    display_name: str = \"Calculator\"\n    description: str = \"Perform basic arithmetic operations on a given expression.\"\n    icon = \"calculator\"\n\n    inputs = [\n        MessageTextInput(name='expression',\n        display_name='Expression',\n        info=\"The arithmetic expression to evaluate (e.g.,\n        '4*4*(33/22)+12-20').\",\n        tool_mode=True)\n    ]\n\n    outputs = [\n        Output(display_name='Data',\n        name='result',\n        type_=Data,\n        method='evaluate_expression')\n    ]\n\n    def evaluate_expression(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def build(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "CalculatorComponent", "base_classes": ["Component"], "public_methods": ["def evaluate_expression(self)", "def build(self)"], "imports": ["import ast", "import operator", "from collections.abc import Callable", "from langflow.custom import Component", "from langflow.inputs import MessageTextInput", "from langflow.io import Output", "from langflow.schema import Data"], "inputs": "[MessageTextInput(name='expression', display_name='Expression', info=\"The arithmetic expression to evaluate (e.g., '4*4*(33/22)+12-20').\", tool_mode=True)]", "outputs": "[Output(display_name='Data', name='result', type_=Data, method='evaluate_expression')]", "display_name": "Calculator", "name": "", "description": "Perform basic arithmetic operations on a given expression.", "icon": "calculator"}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/serp_api.py", "section": "class::SerpAPISchema", "content": "from typing import Any\nfrom langchain.tools import StructuredTool\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass SerpAPISchema(BaseModel):\n    \"\"\"\n    Schema for SerpAPI search parameters.\n    \"\"\"\n\n", "metadata": {"parser": "python_component", "class_name": "SerpAPISchema", "base_classes": ["BaseModel"], "public_methods": [], "imports": ["from typing import Any", "from langchain.tools import StructuredTool", "from langchain_community.utilities.serpapi import SerpAPIWrapper", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "", "outputs": "", "display_name": "", "name": "", "description": "", "icon": ""}, "content_type": "class", "vector": null}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/src/backend/base/langflow/components/tools/serp_api.py", "section": "class::SerpAPIComponent", "content": "from typing import Any\nfrom langchain.tools import StructuredTool\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\nfrom langchain_core.tools import ToolException\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\nclass SerpAPIComponent(LCToolComponent):\n    display_name: str = \"Serp Search API [DEPRECATED]\"\n    description: str = \"Call Serp Search API with result limiting\"\n    icon = \"SerpSearch\"\n    name = \"SerpAPI\"\n\n    inputs = [\n        SecretStrInput(name='serpapi_api_key',\n        display_name='SerpAPI API Key',\n        required=True),\n        MultilineInput(name='input_value',\n        display_name='Input'),\n        DictInput(name='search_params',\n        display_name='Parameters',\n        advanced=True,\n        is_list=True),\n        IntInput(name='max_results',\n        display_name='Max Results',\n        value=5,\n        advanced=True),\n        IntInput(name='max_snippet_length',\n        display_name='Max Snippet Length',\n        value=100,\n        advanced=True)\n    ]\n\n    def build_tool(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n\n    def run_model(self):\n        \"\"\"Method implementation...\"\"\"\n        pass\n", "metadata": {"parser": "python_component", "class_name": "SerpAPIComponent", "base_classes": ["LCToolComponent"], "public_methods": ["def build_tool(self)", "def run_model(self)"], "imports": ["from typing import Any", "from langchain.tools import StructuredTool", "from langchain_community.utilities.serpapi import SerpAPIWrapper", "from langchain_core.tools import ToolException", "from loguru import logger", "from pydantic import BaseModel, Field", "from langflow.base.langchain_utilities.model import LCToolComponent", "from langflow.field_typing import Tool", "from langflow.inputs import DictInput, IntInput, MultilineInput, SecretStrInput", "from langflow.schema import Data"], "inputs": "[SecretStrInput(name='serpapi_api_key', display_name='SerpAPI API Key', required=True), MultilineInput(name='input_value', display_name='Input'), DictInput(name='search_params', display_name='Parameters', advanced=True, is_list=True), IntInput(name='max_results', display_name='Max Results', value=5, advanced=True), IntInput(name='max_snippet_length', display_name='Max Snippet Length', value=100, advanced=True)]", "outputs": "", "display_name": "Serp Search API [DEPRECATED]", "name": "SerpAPI", "description": "Call Serp Search API with result limiting", "icon": "SerpSearch"}, "content_type": "class", "vector": null}
