{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "title: Create custom Python components\nslug: /components-custom-components", "content": "Custom components extend Langflow's functionality through Python classes that inherit from `Component`. This enables integration of new features, data manipulation, external services, and specialized tools.\nIn Langflow's node-based environment, each node is a \"component\" that performs discrete functions. Custom components are Python classes which define:\n**Inputs** — Data or parameters your component requires.\n**Outputs** — Data your component provides to downstream nodes.\n**Logic** — How you process inputs to produce outputs.\nThe benefits of creating custom components include unlimited extensibility, reusability, automatic UI field generation based on inputs, and type-safe connections between nodes.\nCreate custom components for performing specialized tasks, calling APIs, or adding advanced logic.\nCustom components in Langflow are built upon:\nThe Python class that inherits from `Component`.\nClass-level attributes that identify and describe the component.\nInput and output lists that determine data flow.\nInternal variables for logging and advanced logic.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Class-level attributes", "content": "Define these attributes to control a custom component's appearance and behavior:\n**display_name**: A user-friendly label in the node header.\n**description**: A brief summary shown in tooltips.\n**icon**: A visual identifier from Langflow's icon library.\n**name**: A unique internal identifier.\n**documentation**: An optional link to external docs.\n### Structure of a custom component\nA **Langflow custom component** goes beyond a simple class with inputs and outputs. It includes an internal structure with optional lifecycle steps, output generation, front-end interaction, and logic organization.\nA basic component:\nInherits from `langflow.custom.Component`.\nDeclares metadata like `display_name`, `description`, `icon`, and more.\nDefines `inputs` and `outputs` lists.\nImplements methods matching output specifications.\nA minimal custom component skeleton contains the following:\n### Internal Lifecycle and Execution Flow\nLangflow's engine manages:\n**Instantiation**:  A component is created and internal structures are initialized.\n**Assigning Inputs**: Values from the UI or connections are assigned to component fields.\n**Validation and Setup**: Optional hooks like `_pre_run_setup`.\n**Outputs Generation**: `run()` or `build_results()` triggers output methods.\n**Optional Hooks**:\n`initialize_data` or `_pre_run_setup` can run setup logic before the component's main execution.\n`__call__`, `run()`, or `_run()` can be overridden to customize how the component is called or to define custom execution logic.\n### Inputs and outputs\nCustom component inputs are defined with properties like:\n`name`, `display_name`\nOptional: `info`, `value`, `advanced`, `is_list`, `tool_mode`, `real_time_refresh`\nFor example:\n`StrInput`: simple text input.\n`DropdownInput`: selectable options.\n`HandleInput`: specialized connections.\nCustom component `Output` properties define:\n`name`, `display_name`, `method`\nOptional: `info`\nFor more information, see [Custom component inputs and outputs](/components-custom-components#custom-component-inputs-and-outputs).\n### Associated Methods\nEach output is linked to a method:\nThe output method name must match the method name.\nThe method typically returns objects like Message, Data, or DataFrame.\nThe method can use inputs with `self.<input_name>`.\nFor example:\n### Components with multiple outputs\nA component can define multiple outputs.\nEach output can have a different corresponding method.\nFor example:\n### Common internal patterns\n`_pre_run_setup()`\nTo initialize a custom component with counters set:\nOverride `run` or `_run`\nYou can override `async def _run(self): ...` to define custom execution logic, although the default behavior from the base class usually covers most cases.\nStore data in `self.ctx`\nUse `self.ctx` as a shared storage for data or counters across the component's execution flow:", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Directory structure requirements", "content": "By default, Langflow looks for custom components in the `langflow/components` directory.\nIf you're creating custom components in a different location using the [LANGFLOW_COMPONENTS_PATH](/environment-variables#LANGFLOW_COMPONENTS_PATH) environment variable, components must be organized in a specific directory structure to be properly loaded and displayed in the UI:\nComponents must be placed inside **category folders**, not directly in the base directory.\nThe category folder name determines where the component appears in the UI menu.\nFor example, to add a component to the **Helpers** menu, place it in a `helpers` subfolder:\nYou can have **multiple category folders** to organize components into different menus:\nThis folder structure is required for Langflow to properly discover and load your custom components. Components placed directly in the base directory will not be loaded.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Custom component inputs and outputs", "content": "Inputs and outputs define how data flows through the component, how it appears in the UI, and how connections to other components are validated.\n### Inputs\nInputs are defined in a class-level `inputs` list. When Langflow loads the component, it uses this list to render fields and handles in the UI. Users or other components provide values or connections to fill these inputs.\nAn input is usually an instance of a class from `langflow.io` (such as `StrInput`, `DataInput`, or `MessageTextInput`). The most common constructor parameters are:\n**`name`**: The internal variable name, accessed via `self.<name>`.\n**`display_name`**: The label shown to users in the UI.\n**`info`** *(optional)*: A tooltip or short description.\n**`value`** *(optional)*: The default value.\n**`advanced`** *(optional)*: If `True`, moves the field into the \"Advanced\" section.\n**`required`** *(optional)*: If `True`, forces the user to provide a value.\n**`is_list`** *(optional)*: If `True`, allows multiple values.\n**`input_types`** *(optional)*: Restricts allowed connection types (e.g., `[\"Data\"]`, `[\"LanguageModel\"]`).\nHere are the most commonly used input classes and their typical usage.\n**Text Inputs**: For simple text entries.\n**`StrInput`** creates a single-line text field.\n**`MultilineInput`** creates a multi-line text area.\n**Numeric and Boolean Inputs**: Ensures users can only enter valid numeric or boolean data.\n**`BoolInput`**, **`IntInput`**, and **`FloatInput`** provide fields for boolean, integer, and float values, ensuring type consistency.\n**Dropdowns**: For selecting from predefined options, useful for modes or levels.\n**`DropdownInput`**\n**Secrets**: A specialized input for sensitive data, ensuring input is hidden in the UI.\n**`SecretStrInput`** for API keys and passwords.\n**Specialized Data Inputs**: Ensures type-checking and color-coded connections in the UI.\n**`DataInput`** expects a `Data` object (typically with `.data` and optional `.text`).\n**`MessageInput`** expects a `Message` object, used in chat or agent-based flows.\n**`MessageTextInput`** simplifies access to the `.text` field of a `Message`.\n**Handle-Based Inputs**: Used to connect outputs of specific types, ensuring correct pipeline connections.\n**`HandleInput`**\n**File Uploads**: Allows users to upload files directly through the UI or receive file paths from other components.\n**`FileInput`**\n**Lists**: Set `is_list=True` to accept multiple values, ideal for batch or grouped operations.\nThis example defines three inputs: a text field (`StrInput`), a boolean toggle (`BoolInput`), and a dropdown selection (`DropdownInput`).\n### Outputs\nOutputs are defined in a class-level `outputs` list. When Langflow renders a component, each output becomes a connector point in the UI. When you connect something to an output, Langflow automatically calls the corresponding method and passes the returned object to the next component.\nAn output is usually an instance of `Output` from `langflow.io`, with common parameters:\n**`name`**: The internal variable name.\n**`display_name`**: The label shown in the UI.\n**`method`**: The name of the method called to produce the output.\n**`info`** *(optional)*: Help text shown on hover.\nThe method must exist in the class, and it is recommended to annotate its return type for better type checking.\nYou can also set a `self.status` message inside the method to show progress or logs.\n**Common Return Types**:\n**`Message`**: Structured chat messages.\n**`Data`**: Flexible object with `.data` and optional `.text`.\n**`DataFrame`**: Pandas-based tables (`langflow.schema.DataFrame`).\n**Primitive types**: `str`, `int`, `bool` (not recommended if you need type/color consistency).\nIn this example, the `DataToDataFrame` component defines its output using the outputs list. The `df_out` output is linked to the `build_df` method, so when connected in the UI, Langflow calls this method and passes its returned DataFrame to the next node. This demonstrates how each output maps to a method that generates the actual output data.\n### Tool mode\nYou can configure a Custom Component to work as a **Tool** by setting the parameter `tool_mode=True`. This allows the component to be used in Langflow's Tool Mode workflows, such as by Agent components.\nLangflow currently supports the following input types for Tool Mode:\n`DataInput`\n`DataFrameInput`\n`PromptInput`\n`MessageTextInput`\n`MultilineInput`\n`DropdownInput`", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Typed annotations", "content": "In Langflow, **typed annotations** allow Langflow to visually guide users and maintain flow consistency.\nTyped annotations provide:\n**Color-coding**: Outputs like `-> Data` or `-> Message` get distinct colors.\n**Validation**: Langflow blocks incompatible connections automatically.\n**Readability**: Developers can quickly understand data flow.\n**Development tools**: Better code suggestions and error checking in your code editor.\n### Common Return Types\n**`Message`**\nFor chat-style outputs.\nIn the UI, connects only to Message-compatible inputs.\n**`Data`**\nFor structured data like dicts or partial texts.\nIn the UI, connects only with DataInput.\n**`DataFrame`**\nFor tabular data\nIn the UI, connects only to DataFrameInput.\n**Primitive Types (`str`, `int`, `bool`)**\nReturning primitives is allowed but wrapping in Data or Message is recommended for better UI consistency.\n### Tips for typed annotations\nWhen using typed annotations, consider the following best practices:\n**Always Annotate Outputs**: Specify return types like `-> Data`, `-> Message`, or `-> DataFrame` to enable proper UI color-coding and validation.\n**Wrap Raw Data**: Use `Data`, `Message`, or `DataFrame` wrappers instead of returning plain structures.\n**Use Primitives Carefully**: Direct `str` or `int` returns are fine for simple flows, but wrapping improves flexibility.\n**Annotate Helpers Too**: Even if internal, typing improves maintainability and clarity.\n**Handle Edge Cases**: Prefer returning structured `Data` with error fields when needed.\n**Stay Consistent**: Use the same types across your components to make flows predictable and easier to build.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Enable dynamic fields", "content": "In **Langflow**, dynamic fields allow inputs to change or appear based on user interactions. You can make an input dynamic by setting `dynamic=True`.\nOptionally, setting `real_time_refresh=True` triggers the `update_build_config` method to adjust the input's visibility or properties in real time, creating a contextual UI that only displays relevant fields based on the user's choices.\nIn this example, the operator field triggers updates via `real_time_refresh=True`.\nThe `regex_pattern` field is initially hidden and controlled via `dynamic=True`.\n### Implement `update_build_config`\nWhen a field with `real_time_refresh=True` is modified, Langflow calls the `update_build_config` method, passing the updated field name, value, and the component's configuration to dynamically adjust the visibility or properties of other fields based on user input.\nThis example will show or hide the `regex_pattern` field when the user selects a different operator.\n### Additional Dynamic Field Controls\nYou can also modify other properties within `update_build_config`, such as:\n`required`: Set `build_config[\"some_field\"][\"required\"] = True/False`\n`advanced`: Set `build_config[\"some_field\"][\"advanced\"] = True`\n`options`: Modify dynamic dropdown options.\n### Tips for Managing Dynamic Fields\nWhen working with dynamic fields, consider the following best practices to ensure a smooth user experience:\n**Minimize field changes**: Hide only fields that are truly irrelevant to avoid confusing users.\n**Test behavior**: Ensure that adding or removing fields doesn't accidentally erase user input.\n**Preserve data**: Use `build_config[\"some_field\"][\"show\"] = False` to hide fields without losing their values.\n**Clarify logic**: Add `info` notes to explain why fields appear or disappear based on conditions.\n**Keep it manageable**: If the dynamic logic becomes too complex, consider breaking it into smaller components, unless it serves a clear purpose in a single node.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Error handling and logging", "content": "In Langflow, robust error handling ensures that your components behave predictably, even when unexpected situations occur, such as invalid inputs, external API failures, or internal logic errors.\n### Error handling techniques\n**Raise Exceptions**:\nIf a critical error occurs, you can raise standard Python exceptions such as `ValueError`, or specialized exceptions like `ToolException`. Langflow will automatically catch these and display appropriate error messages in the UI, helping users quickly identify what went wrong.\n**Return Structured Error Data**:\nInstead of stopping a flow abruptly, you can return a Data object containing an \"error\" field. This approach allows the flow to continue operating and enables downstream components to detect and handle the error gracefully.\n### Improve debugging and flow management\n**Use `self.status`**:\nEach component has a status field where you can store short messages about the execution result—such as success summaries, partial progress, or error notifications. These appear directly in the UI, making troubleshooting easier for users.\n**Stop specific outputs with `self.stop(...)`**:\nYou can halt individual output paths when certain conditions fail, without affecting the entire component. This is especially useful when working with components that have multiple output branches.\n**Log events**:\nYou can log key execution details inside components. Logs are displayed in the \"Logs\" or \"Events\" section of the component's detail view and can be accessed later through the flow's debug panel or exported files, providing a clear trace of the component's behavior for easier debugging.\n### Tips for error handling and logging\nTo build more reliable components, consider the following best practices:\n**Validate inputs early**: Catch missing or invalid inputs at the start to prevent broken logic.\n**Summarize with `self.status`**: Use short success or error summaries to help users understand results quickly.\n**Keep logs concise**: Focus on meaningful messages to avoid cluttering the UI.\n**Return structured errors**: When appropriate, return `Data(data={\"error\": ...})` instead of raising exceptions to allow downstream handling.\n**Stop outputs selectively**: Only halt specific outputs with `self.stop(...)` if necessary, to preserve correct flow behavior elsewhere.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-custom-components.md", "section": "Contribute custom components to Langflow", "content": "See [How to Contribute](/contributing-components) to contribute your custom component to Langflow.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-bundles.md", "section": "title: Bundles\nslug: /components-bundle-components", "content": "**Bundles** are third-party components grouped by provider.\nFor more information on bundled components, see the component provider's documentation.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-io.md", "section": "title: Inputs and outputs\nslug: /components-io", "content": "import Icon from \"@site/src/components/icon\";\nInput and output components in Langflow\nInput and output components define where data enters and exits your flow.\nBoth components accept user input and return a `Message` object, but serve different purposes.\nThe **Text Input** component accepts a text string input and returns a `Message` object containing only the input text. The output does not appear in the **Playground**.\nThe **Chat Input** component accepts multiple input types including text, files, and metadata, and returns a `Message` object containing the text along with sender information, session ID, and file attachments.\nThe **Chat Input** component provides an interactive chat interface in the **Playground**.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-io.md", "section": "Chat Input", "content": "This component collects user input as `Text` strings from the chat and wraps it in a [Message](/concepts-objects#message-object) object that includes the input text, sender information, session ID, file attachments, and styling properties.\nIt can optionally store the message in a chat history.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Text | The Message to be passed as input. |\n| should_store_message | Store Messages | Store the message in the history. |\n| sender | Sender Type | The type of sender. |\n| sender_name | Sender Name | The name of the sender. |\n| session_id | Session ID | The session ID of the chat. If empty, the current session ID parameter is used. |\n| files | Files | The files to be sent with the message. |\n| background_color | Background Color | The background color of the icon. |\n| chat_icon | Icon | The icon of the message. |\n| text_color | Text Color | The text color of the name. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| message | Message | The resulting chat message object with all specified properties. |\n### Message method\nThe `ChatInput` class provides an asynchronous method to create and store a `Message` object based on the input parameters.\nThe `Message` object is created in the `message_response` method of the ChatInput class using the `Message.create()` factory method.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-io.md", "section": "Text Input", "content": "The **Text Input** component accepts a text string input and returns a `Message` object containing only the input text.\nThe output does not appear in the **Playground**.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Text | The text/content to be passed as output. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| text | Text | The resulting text message. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-io.md", "section": "Chat Output", "content": "The **Chat Output** component creates a [Message](/concepts-objects#message-object) object that includes the input text, sender information, session ID, and styling properties.\nThe component accepts the following input types.\n[Data](/concepts-objects#data-object)\n[DataFrame](/concepts-objects#dataframe-object)\n[Message](/concepts-objects#message-object)\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Text | The message to be passed as output. |\n| should_store_message | Store Messages | The flag to store the message in the history. |\n| sender | Sender Type | The type of sender. |\n| sender_name | Sender Name | The name of the sender. |\n| session_id | Session ID | The session ID of the chat. If empty, the current session ID parameter is used. |\n| data_template | Data Template | The template to convert Data to Text. If the option is left empty, it is dynamically set to the Data's text key. |\n| background_color | Background Color | The background color of the icon. |\n| chat_icon | Icon | The icon of the message. |\n| text_color | Text Color | The text color of the name. |\n| clean_data | Basic Clean Data | When enabled, `DataFrame` inputs are cleaned when converted to text. Cleaning removes empty rows, empty lines in cells, and multiple newlines. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| message | Message | The resulting chat message object with all specified properties. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-io.md", "section": "Text Output", "content": "The **Text Output** takes a single input of text and returns a [Message](/concepts-objects#message-object) object containing that text.\nThe output does not appear in the **Playground**.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Text | The text to be passed as output. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| text | Text | The resulting text message. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-io.md", "section": "Chat components example flow", "content": "To use the **Chat Input** and **Chat Output** components in a flow, connect them to components that accept or send the [Message](/concepts-objects#message-object) type.\nFor this example, connect a **Chat Input** component to an **OpenAI** model component's **Input** port, and then connect the **OpenAI** model component's **Message** port to the **Chat Output** component.\nIn the **OpenAI** model component, in the **OpenAI API Key** field, add your **OpenAI API key**.\nThe flow looks like this:\n![Chat input and output components connected to an OpenAI model](/img/component-chat-io.png)\nTo send a message to your flow, open the **Playground**, and then enter a message.\nThe **OpenAI** model component responds.\nOptionally, in the **OpenAI** model component, enter a **System Message** to control the model's response.\nIn the Langflow UI, click your flow name, and then click **Logs**.\nThe **Logs** pane opens.\nHere, you can inspect your component logs.\n![Logs pane](/img/logs.png)\nYour first message was sent by the **Chat Input** component to the **OpenAI** model component.\nClick **Outputs** to view the sent message:\nYour second message was sent by the **OpenAI** model component to the **Chat Output** component.\nThis is the raw text output of the model's response.\nThe **Chat Output** component accepts this text as input and presents it as a formatted message.\nClick **Outputs** to view the sent message:\n:::tip\nOptionally, to view the outputs of each component in the flow, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\n:::\n### Send chat messages with the API\nThe **Chat Input** component is often the entry point for passing messages to the Langflow API.\nTo send the same example messages programmatically to your Langflow server, do the following:\nTo get your Langflow endpoint, click **Publish**, and then click **API access**.\nCopy the command from the **cURL** tab, and then paste it in your terminal.\nIt looks similar to this:\nModify `input_value` so it contains the question, `What's the recommended way to install Docker on Mac M1?`.\nNote the `output_type` and `input_type` parameters that are passed with the message. The `chat` type provides additional configuration options, and the messages appear in the **Playground**. The `text` type returns only text strings, and does not appear in the **Playground**.\nAdd a custom `session_id` to the message's `data` object.\nThe custom `session_id` value starts a new chat session between your client and the Langflow server, and can be useful in keeping conversations and AI context separate.\nSend the POST request.\nYour request is answered.\nNavigate to the **Playground**.\nA new chat session called `docker-question-on-m1` has appeared, using your unique `session_id`.\nTo modify additional parameters with **Tweaks** for your **Chat Input** and **Chat Output** components, click **Publish**, and then click **API access**.\nClick **Tweaks** to modify parameters in the component's `data` object.\nFor example, disabling storing messages from the **Chat Input** component adds a **Tweak** to your command:\nTo confirm your command is using the tweak, navigate to the **Logs** pane and view the request from the **Chat Input** component.\nThe value for `should_store_message` is `false`.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "title: Processing\nslug: /components-processing", "content": "import Icon from \"@site/src/components/icon\";\nProcessing components process and transform data within a flow.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Use a processing component in a flow", "content": "The **Split Text** processing component in this flow splits the incoming [Data](/concepts-objects) into chunks to be embedded into the vector store component.\nThe component offers control over chunk size, overlap, and separator, which affect context and granularity in vector store retrieval results.\n![](/img/vector-store-document-ingestion.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Combine data", "content": ":::important\nPrior to Langflow version 1.1.3, this component was named **Merge Data**.\n:::\nThis component combines multiple data sources into a single unified [Data](/concepts-objects#data-object) object.\nThe component iterates through the input list of data objects, merging them into a single data object. If the input list is empty, it returns an empty data object. If there's only one input data object, it returns that object unchanged. The merging process uses the addition operator to combine data objects.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | A list of data objects to be merged. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| merged_data | Merged Data | A single [Data](/concepts-objects#data-object) object containing the combined information from all input data objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "DataFrame operations", "content": "This component performs operations on [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) rows and columns.\nTo use this component in a flow, connect a component that outputs [DataFrame](/concepts-objects#dataframe-object) to the **DataFrame Operations** component.\nThis example fetches JSON data from an API. The **Lambda filter** component extracts and flattens the results into a tabular DataFrame. The **DataFrame Operations** component can then work with the retrieved data.\n![Dataframe operations with flattened dataframe](/img/component-dataframe-operations.png)\nThe **API Request** component retrieves data with only `source` and `result` fields.\nFor this example, the desired data is nested within the `result` field.\nConnect a **Lambda Filter** to the API request component, and a **Language model** to the **Lambda Filter**. This example connects a **Groq** model component.\nIn the **Groq** model component, add your **Groq** API key.\nTo filter the data, in the **Lambda filter** component, in the **Instructions** field, use natural language to describe how the data should be filtered.\nFor this example, enter:\n:::tip\nAvoid punctuation in the **Instructions** field, as it can cause errors.\n:::\n5. To run the flow, in the **Lambda Filter** component, click <Icon name=\"Play\" aria-label=\"Play icon\" />.\n6. To inspect the filtered data, in the **Lambda Filter** component, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\nThe result is a structured DataFrame.\nAdd the **DataFrame Operations** component, and a **Chat Output** component to the flow.\nIn the **DataFrame Operations** component, in the **Operation** field, select **Filter**.\nTo apply a filter, in the **Column Name** field, enter a column to filter on. This example filters by `name`.\nClick **Playground**, and then click **Run Flow**.\nThe flow extracts the values from the `name` column.\n### Operations\nThis component can perform the following operations on Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n| Operation | Description | Required Inputs |\n| ----------- | ------------- | ----------------- |\n| Add Column | Adds a new column with a constant value | new_column_name, new_column_value |\n| Drop Column | Removes a specified column | column_name |\n| Filter | Filters rows based on column value | column_name, filter_value |\n| Head | Returns first n rows | num_rows |\n| Rename Column | Renames an existing column | column_name, new_column_name |\n| Replace Value | Replaces values in a column | column_name, replace_value, replacement_value |\n| Select Columns | Selects specific columns | columns_to_select |\n| Sort | Sorts DataFrame by column | column_name, ascending |\n| Tail | Returns last n rows | num_rows |\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| df | DataFrame | The input DataFrame to operate on. |\n| operation | Operation | The DataFrame operation to perform. Options include Add Column, Drop Column, Filter, Head, Rename Column, Replace Value, Select Columns, Sort, and Tail. |\n| column_name | Column Name | The column name to use for the operation. |\n| filter_value | Filter Value | The value to filter rows by. |\n| ascending | Sort Ascending | Whether to sort in ascending order. |\n| new_column_name | New Column Name | The new column name when renaming or adding a column. |\n| new_column_value | New Column Value | The value to populate the new column with. |\n| columns_to_select | Columns to Select | A list of column names to select. |\n| num_rows | Number of Rows | The number of rows to return for head/tail operations. The default is 5. |\n| replace_value | Value to Replace | The value to replace in the column. |\n| replacement_value | Replacement Value | The value to replace with. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| output | DataFrame | The resulting DataFrame after the operation. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Data to DataFrame", "content": "This component converts one or multiple [Data](/concepts-objects#data-object) objects into a [DataFrame](/concepts-objects#dataframe-object). Each Data object corresponds to one row in the resulting DataFrame. Fields from the `.data` attribute become columns, and the `.text` field (if present) is placed in a 'text' column.\nTo use this component in a flow, connect a component that outputs [Data](/concepts-objects#data-object) to the **Data to Dataframe** component's input.\nThis example connects a **Webhook** component to convert `text` and `data` into a DataFrame.\nTo view the flow's output, connect a **Chat Output** component to the **Data to Dataframe** component.\n![A webhook and data to dataframe](/img/component-data-to-dataframe.png)\nSend a POST request to the **Webhook** containing your JSON data.\nReplace `YOUR_FLOW_ID` with your flow ID.\nThis example uses the default Langflow server address.\nIn the **Playground**, view the output of your flow.\nThe **Data to DataFrame** component converts the webhook request into a `DataFrame`, with `text` and `data` fields as columns.\nSend another employee data object.\nIn the **Playground**, this request is also converted to `DataFrame`.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data_list | Data or Data List | One or multiple Data objects to transform into a DataFrame. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| dataframe | DataFrame | A DataFrame built from each Data object's fields plus a text column. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Filter data", "content": ":::important\nThis component is in **Beta** as of Langflow version 1.1.3, and is not yet fully supported.\n:::\nThis component filters a [Data](/concepts-objects#data-object) object based on a list of keys.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | The Data object to filter. |\n| filter_criteria | Filter Criteria | A list of keys to filter by. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| filtered_data | Filtered Data | A new Data object containing only the key-value pairs that match the filter criteria. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Filter values", "content": ":::important\nThis component is in **Beta** as of Langflow version 1.1.3, and is not yet fully supported.\n:::\nThe Filter values component filters a list of data items based on a specified key, filter value, and comparison operator.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_data | Input data | The list of data items to filter. |\n| filter_key | Filter Key | The key to filter on. |\n| filter_value | Filter Value | The value to filter by. |\n| operator | Comparison Operator | The operator to apply for comparing the values. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| filtered_data | Filtered data | The resulting list of filtered data items. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Lambda filter", "content": "This component uses an LLM to generate a Lambda function for filtering or transforming structured data.\nTo use the **Lambda filter** component, you must connect it to a [Language Model](/components-models#language-model) component, which the component uses to generate a function based on the natural language instructions in the **Instructions** field.\nThis example gets JSON data from the `https://jsonplaceholder.typicode.com/users` API endpoint.\nThe **Instructions** field in the **Lambda filter** component specifies the task `extract emails`.\nThe connected LLM creates a filter based on the instructions, and successfully extracts a list of email addresses from the JSON data.\n![](/img/component-lambda-filter.png)\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | The structured data to filter or transform using a Lambda function. |\n| llm | Language Model | The connection port for a [Model](/components-models) component. |\n| filter_instruction | Instructions | The natural language instructions for how to filter or transform the data using a Lambda function, such as `Filter the data to only include items where the 'status' is 'active'`. |\n| sample_size | Sample Size | For large datasets, the number of characters to sample from the dataset head and tail. |\n| max_size | Max Size | The number of characters for the data to be considered \"large\", which triggers sampling by the `sample_size` value. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| filtered_data | Filtered Data | The filtered or transformed [Data object](/concepts-objects#data-object). |\n| dataframe | DataFrame | The filtered data as a [DataFrame](/concepts-objects#dataframe-object). |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "LLM router", "content": "This component routes requests to the most appropriate LLM based on OpenRouter model specifications.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| models | Language Models | A list of LLMs to route between. |\n| input_value | Input | The input message to be routed. |\n| judge_llm | Judge LLM | The LLM that evaluates and selects the most appropriate model. |\n| optimization | Optimization | The optimization preference between quality, speed, cost, or balanced. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| output | Output | The response from the selected model. |\n| selected_model | Selected Model | The name of the chosen model. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Message to data", "content": "This component converts [Message](/concepts-objects#message-object) objects to [Data](/concepts-objects#data-object) objects.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| message | Message | The Message object to convert to a Data object. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | The converted Data object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Parser", "content": "This component formats `DataFrame` or `Data` objects into text using templates, with an option to convert inputs directly to strings using `stringify`.\nTo use this component, create variables for values in the `template` the same way you would in a [Prompt](/components-prompts) component. For `DataFrames`, use column names, for example `Name: {Name}`. For `Data` objects, use `{text}`.\nTo use the **Parser** component with a **Structured Output** component, do the following:\nConnect a **Structured Output** component's **DataFrame** output to the **Parser** component's **DataFrame** input.\nConnect the **File** component to the **Structured Output** component's **Message** input.\nConnect the **OpenAI** model component's **Language Model** output to the **Structured Output** component's **Language Model** input.\nThe flow looks like this:\n![A parser component connected to OpenAI and structured output](/img/component-parser.png)\nIn the **Structured Output** component, click **Open Table**.\nThis opens a pane for structuring your table.\nThe table contains the rows **Name**, **Description**, **Type**, and **Multiple**.\nCreate a table that maps to the data you're loading from the **File** loader.\nFor example, to create a table for employees, you might have the rows `id`, `name`, and `email`, all of type `string`.\nIn the **Template** field of the **Parser** component, enter a template for parsing the **Structured Output** component's DataFrame output into structured text.\nCreate variables for values in the `template` the same way you would in a [Prompt](/components-prompts) component.\nFor example, to present a table of employees in Markdown:\nTo run the flow, in the **Parser** component, click <Icon name=\"Play\" aria-label=\"Play icon\" />.\nTo view your parsed text, in the **Parser** component, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\nOptionally, connect a **Chat Output** component, and open the **Playground** to see the output.\nFor an additional example of using the **Parser** component to format a DataFrame from a **Structured Output** component, see the **Market Research** template flow.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| mode | Mode | The tab selection between \"Parser\" and \"Stringify\" modes. \"Stringify\" converts input to a string instead of using a template. |\n| pattern | Template | The template for formatting using variables in curly brackets. For DataFrames, use column names, such as `Name: {Name}`. For Data objects, use `{text}`. |\n| input_data | Data or DataFrame | The input to parse. Accepts either a DataFrame or Data object. |\n| sep | Separator | The string used to separate rows or items. The default is a newline. |\n| clean_data | Clean Data | When stringify is enabled, this option cleans data by removing empty rows and lines. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| parsed_text | Parsed Text | The resulting formatted text as a [Message](/concepts-objects#message-object) object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Regex extractor", "content": "This component extracts patterns from text using regular expressions. It can be used to find and extract specific patterns or information from text data.\nTo use this component in a flow:\nConnect the **Regex Extractor** to a **URL** component and a **Chat Output** component.\n![Regex extractor connected to url component](/img/component-url-regex.png)\nIn the **Regex Extractor** tool, enter a pattern to extract text from the **URL** component's raw output.\nThis example extracts the first paragraph from the \"In the News\" section of `https://en.wikipedia.org/wiki/Main_Page`:\nResult:", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Save to File", "content": "This component saves [DataFrames, Data, or Messages](/concepts-objects) to various file formats.\nTo use this component in a flow, connect a component that outputs [DataFrames, Data, or Messages](/concepts-objects) to the **Save to File** component's input.\nThe following example connects a **Webhook** component to two **Save to File** components to demonstrate the different outputs.\n![Two Save-to File components connected to a webhook](/img/component-save-to-file.png)\nIn the **Save to File** component's **Input Type** field, select the expected input type.\nThis example expects **Data** from the **Webhook**.\nIn the **File Format** field, select the file type for your saved file.\nThis example uses `.md` in one **Save to File** component, and `.xlsx` in another.\nIn the **File Path** field, enter the path for your saved file.\nThis example uses `./output/employees.xlsx` and `./output/employees.md` to save the files in a directory relative to where Langflow is running.\nThe component accepts both relative and absolute paths, and creates any necessary directories if they don't exist.\n:::tip\nIf you enter a format in the `file_path` that is not accepted, the component appends the proper format to the file.\nFor example, if the selected `file_format` is `csv`, and you enter `file_path` as `./output/test.txt`, the file is saved as `./output/test.txt.csv` so the file is not corrupted.\n:::\nSend a POST request to the **Webhook** containing your JSON data.\nReplace `YOUR_FLOW_ID` with your flow ID.\nThis example uses the default Langflow server address.\nIn your local filesystem, open the `outputs` directory.\nYou should see two files created from the data you've sent: one in `.xlsx` for structured spreadsheets, and one in Markdown.\n### File input format options\nFor `DataFrame` and `Data` inputs, the component can create:\n`csv`\n`excel`\n`json`\n`markdown`\n`pdf`\nFor `Message` inputs, the component can create:\n`txt`\n`json`\n`markdown`\n`pdf`\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_text | Input Text | The text to analyze and extract patterns from. |\n| pattern | Regex Pattern | The regular expression pattern to match in the text. |\n| input_type | Input Type | The type of input to save. |\n| df | DataFrame | The DataFrame to save. |\n| data | Data | The Data object to save. |\n| message | Message | The Message to save. |\n| file_format | File Format | The file format to save the input in. |\n| file_path | File Path | The full file path including filename and extension. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | A list of extracted matches as Data objects. |\n| text | Message | The extracted matches formatted as a Message object. |\n| confirmation | Confirmation | The confirmation message after saving the file. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Split text", "content": "This component splits text into chunks based on specified criteria. It's ideal for chunking data to be tokenized and embedded into vector databases.\nThe **Split Text** component outputs **Chunks** or **DataFrame**.\nThe **Chunks** output returns a list of individual text chunks.\nThe **DataFrame** output returns a structured data format, with additional `text` and `metadata` columns applied.\nTo use this component in a flow, connect a component that outputs [Data or DataFrame](/concepts-objects) to the **Split Text** component's **Data** port.\nThis example uses the **URL** component, which is fetching JSON placeholder data.\n![Split text component and chroma-db](/img/component-split-text.png)\nIn the **Split Text** component, define your data splitting parameters.\nThis example splits incoming JSON data at the separator `},`, so each chunk contains one JSON object.\nThe order of precedence is **Separator**, then **Chunk Size**, and then **Chunk Overlap**.\nIf any segment after separator splitting is longer than `chunk_size`, it is split again to fit within `chunk_size`.\nAfter `chunk_size`, **Chunk Overlap** is applied between chunks to maintain context.\nConnect a **Chat Output** component to the **Split Text** component's **DataFrame** output to view its output.\nClick **Playground**, and then click **Run Flow**.\nThe output contains a table of JSON objects split at `},`.\nClear the **Separator** field, and then run the flow again.\nInstead of JSON objects, the output contains 50-character lines of text with 10 characters of overlap.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data_inputs | Input Documents | The data to split. The component accepts [Data](/concepts-objects#data-object) or [DataFrame](/concepts-objects#dataframe-object) objects. |\n| chunk_overlap | Chunk Overlap | The number of characters to overlap between chunks. Default: `200`. |\n| chunk_size | Chunk Size | The maximum number of characters in each chunk. Default: `1000`. |\n| separator | Separator | The character to split on. Default: `newline`. |\n| text_key | Text Key | The key to use for the text column. Default: `text`. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| chunks | Chunks | A list of split text chunks as [Data](/concepts-objects#data-object) objects. |\n| dataframe | DataFrame | A list of split text chunks as [DataFrame](/concepts-objects#dataframe-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Update data", "content": "This component dynamically updates or appends data with specified fields.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| old_data | Data | The records to update. |\n| number_of_fields | Number of Fields | The number of fields to add. The maximum is 15. |\n| text_key | Text Key | The key for text content. |\n| text_key_validator | Text Key Validator | Validates the text key presence. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | The updated Data objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-processing.md", "section": "Legacy components", "content": "**Legacy** components are available for use but are no longer supported.\n### Alter metadata\nThis component modifies metadata of input objects. It can add new metadata, update existing metadata, and remove specified metadata fields. The component works with both [Message](/concepts-objects#message-object) and [Data](/concepts-objects#data-object) objects, and can also create a new Data object from user-provided text.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Input | Objects to which Metadata should be added |\n| text_in | User Text | Text input; the value is contained in the 'text' attribute of the [Data](/concepts-objects#data-object) object. Empty text entries are ignored. |\n| metadata | Metadata | Metadata to add to each object |\n| remove_fields | Fields to Remove | Metadata fields to remove |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | List of Input objects, each with added metadata |\n### Combine text\n:::important\nThis component is in **Legacy**, which means it is no longer in active development.\nInstead, use the [Combine data](#combine-data) component.\n:::\nThis component concatenates two text sources into a single text chunk using a specified delimiter.\nTo use this component in a flow, connect two components that output [Messages](/concepts-objects#message-object) to the **Combine Text** component's **First Text** and **Second Text** inputs.\nThis example uses two **Text Input** components.\n![Combine text component](/img/component-combine-text.png)\nIn the **Combine Text** component, in the **Text** fields of both **Text Input** components, enter some text to combine.\nIn the **Combine Text** component, enter an optional **Delimiter** value.\nThe delimiter character separates the combined texts.\nThis example uses `\\n\\n **end first text** \\n\\n **start second text** \\n\\n` to label the texts and create newlines between them.\nConnect a **Chat Output** component to view the text combination.\nClick **Playground**, and then click **Run Flow**.\nThe combined text appears in the **Playground**.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| first_text | First Text | The first text input to concatenate. |\n| second_text | Second Text | The second text input to concatenate. |\n| delimiter | Delimiter | A string used to separate the two text inputs. The default is a space. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| message | Message | A Message object containing the combined text. |\n### Create data\n:::important\nThis component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.1.3.\n:::\nThis component dynamically creates a [Data](/concepts-objects#data-object) object with a specified number of fields.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| number_of_fields | Number of Fields | The number of fields to be added to the record. |\n| text_key | Text Key | Key that identifies the field to be used as the text content. |\n| text_key_validator | Text Key Validator | If enabled, checks if the given `Text Key` is present in the given `Data`. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | A [Data](/concepts-objects#data-object) object created with the specified fields and text key. |\n### JSON cleaner\nThe JSON cleaner component cleans JSON strings to ensure they are fully compliant with the JSON specification.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| json_str | JSON String | The JSON string to be cleaned. This can be a raw, potentially malformed JSON string produced by language models or other sources that may not fully comply with JSON specifications. |\n| remove_control_chars | Remove Control Characters | If set to True, this option removes control characters (ASCII characters 0-31 and 127) from the JSON string. This can help eliminate invisible characters that might cause parsing issues or make the JSON invalid. |\n| normalize_unicode | Normalize Unicode | When enabled, this option normalizes Unicode characters in the JSON string to their canonical composition form (NFC). This ensures consistent representation of Unicode characters across different systems and prevents potential issues with character encoding. |\n| validate_json | Validate JSON | If set to True, this option attempts to parse the JSON string to ensure it is well-formed before applying the final repair operation. It raises a ValueError if the JSON is invalid, allowing for early detection of major structural issues in the JSON. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| output | Cleaned JSON String | The resulting cleaned, repaired, and validated JSON string that fully complies with the JSON specification. |\n### Parse DataFrame\n:::important\nThis component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.3.\nInstead, use the [Parser](#parser) component.\n:::\nThis component converts DataFrames into plain text using templates.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| df | DataFrame | The DataFrame to convert to text rows. |\n| template | Template | Template for formatting (use `{column_name}` placeholders). |\n| sep | Separator | String to join rows in output. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| text | Text | All rows combined into single text. |\n### Parse JSON\n:::important\nThis component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.1.3.\n:::\nThis component converts and extracts JSON fields using JQ queries.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_value | Input | Data object to filter ([Message](/concepts-objects#message-object) or [Data](/concepts-objects#data-object)). |\n| query | JQ Query | JQ Query to filter the data |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| filtered_data | Filtered Data | Filtered data as list of [Data](/concepts-objects#data-object) objects. |\n### Select data\n:::important\nThis component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.1.3.\n:::\nThis component selects a single [Data](/concepts-objects#data-object) item from a list.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data_list | Data List | List of data to select from |\n| data_index | Data Index | Index of the data to select |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| selected_data | Selected Data | The selected [Data](/concepts-objects#data-object) object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-prompts.md", "section": "title: Prompts\nslug: /components-prompts", "content": "Prompt components in Langflow\nA prompt is a structured input to a language model that instructs the model how to handle user inputs and variables.\nPrompt components create prompt templates with custom fields and dynamic variables for providing your model structured, repeatable prompts.\nPrompts are a combination of natural language and variables created with curly braces.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-prompts.md", "section": "Use a prompt component in a flow", "content": "An example of modifying a prompt can be found in the [Quickstart](/get-started-quickstart#run-the-chatbot-with-retrieved-context), where a basic chatbot flow is extended to include a full vector RAG pipeline.\n![](/img/quickstart-add-document-ingestion.png)\nThe default prompt in the **Prompt** component is `Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.`\nThis prompt creates a \"personality\" for your LLM's chat interactions, but it doesn't include variables that you may find useful when templating prompts.\nTo modify the prompt template, in the **Prompt** component, click the **Template** field. For example, the `{context}` variable gives the LLM model access to embedded vector data to return better answers.\nWhen variables are added to a prompt template, new fields are automatically created in the component. These fields can be connected to receive text input from other components to automate prompting, or to output instructions to other components. An example of prompts controlling agents behavior is available in the [sequential tasks agent starter flow](/sequential-agent).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| template | Template | Create a prompt template with dynamic variables. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| prompt | Prompt Message | The built prompt message returned by the `build_prompt` method. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-prompts.md", "section": "Langchain Hub Prompt Template", "content": ":::important\nThis component is available in the **Components** menu under **Bundles**.\n:::\nThis component fetches prompts from the [Langchain Hub](https://docs.smith.langchain.com/old/category/prompt-hub).\nWhen a prompt is loaded, the component generates input fields for custom variables. For example, the default prompt \"efriis/my-first-prompt\" generates fields for `profession` and `question`.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| langchain_api_key | Your LangChain API Key | The LangChain API Key to use. |\n| langchain_hub_prompt | LangChain Hub Prompt | The LangChain Hub prompt to use. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| prompt | Build Prompt | The built prompt message returned by the `build_prompt` method. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "title: Memories\nslug: /components-memories", "content": "Memory components in Langflow\nMemory components store and retrieve chat messages by `session_id`.\nThey are distinct from vector store components, because they are built specifically for storing and retrieving chat messages from external databases.\nMemory components provide access to their respective external databases **as memory**. This allows Large Language Models (LLMs) or [agents](/components-agents) to access external memory for persistence and context retention.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "Use a memory component in a flow", "content": "This example flow stores and retrieves chat history from an **Astra DB Chat Memory** component with **Store Message** and **Message history** components.\nThe **Store Message** helper component stores chat memories as [Data](/concepts-objects) objects, and the **Message History** helper component retrieves chat messages as [Data](/concepts-objects) objects or strings.\n![Sample Flow storing Message history in AstraDB](/img/astra_db_chat_memory_rounded.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "AstraDBChatMemory Component", "content": "This component creates an `AstraDBChatMessageHistory` instance, which stores and retrieves chat messages using Astra DB, a cloud-native database service.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| collection_name | String | The name of the Astra DB collection for storing messages. Required. |\n| token | SecretString | The authentication token for Astra DB access. Required. |\n| api_endpoint | SecretString | The API endpoint URL for the Astra DB service. Required. |\n| namespace | String | The optional namespace within Astra DB for the collection. |\n| session_id | MessageText | The chat session ID. Uses the current session ID if not provided. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| message_history | BaseChatMessageHistory | An instance of AstraDBChatMessageHistory for the session. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "CassandraChatMemory Component", "content": "This component creates a `CassandraChatMessageHistory` instance, enabling storage and retrieval of chat messages using Apache Cassandra or DataStax Astra DB.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| database_ref | MessageText | The contact points for the Cassandra database or Astra DB database ID. Required. |\n| username | MessageText | The username for Cassandra. Leave empty for Astra DB. |\n| token | SecretString | The password for Cassandra or the token for Astra DB. Required. |\n| keyspace | MessageText | The keyspace in Cassandra or namespace in Astra DB. Required. |\n| table_name | MessageText | The name of the table or collection for storing messages. Required. |\n| session_id | MessageText | The unique identifier for the chat session. Optional. |\n| cluster_kwargs | Dictionary | Additional keyword arguments for the Cassandra cluster configuration. Optional. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| message_history | BaseChatMessageHistory | An instance of CassandraChatMessageHistory for the session. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "Mem0 Chat Memory", "content": "The Mem0 Chat Memory component retrieves and stores chat messages using Mem0 memory storage.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| mem0_config | Mem0 Configuration | The configuration dictionary for initializing the Mem0 memory instance. |\n| ingest_message | Message to Ingest | The message content to be ingested into Mem0 memory. |\n| existing_memory | Existing Memory Instance | An optional existing Mem0 memory instance. |\n| user_id | User ID | The identifier for the user associated with the messages. |\n| search_query | Search Query | The input text for searching related memories in Mem0. |\n| mem0_api_key | Mem0 API Key | The API key for the Mem0 platform. Leave empty to use the local version. |\n| metadata | Metadata | The additional metadata to associate with the ingested message. |\n| openai_api_key | OpenAI API Key | The API key for OpenAI. Required when using OpenAI embeddings without a provided configuration. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| memory | Mem0 Memory | The resulting Mem0 Memory object after ingesting data. |\n| search_results | Search Results | The search results from querying Mem0 memory. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "Redis Chat Memory", "content": "This component retrieves and stores chat messages from Redis.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| host | hostname | The IP address or hostname. |\n| port | port | The Redis Port Number. |\n| database | database | The Redis database. |\n| username | Username | The Redis username. |\n| password | Password | The password for the username. |\n| key_prefix | Key prefix | The key prefix. |\n| session_id | Session ID | The session ID for the message. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| memory | Memory | The Redis chat message history object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-memories.md", "section": "Legacy components", "content": "**Legacy** components are available for use but are no longer supported.\n### ZepChatMemory Component\nThis component creates a `ZepChatMessageHistory` instance, enabling storage and retrieval of chat messages using Zep, a memory server for Large Language Models (LLMs).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| url | MessageText | The URL of the Zep instance. Required. |\n| api_key | SecretString | The API Key for authentication with the Zep instance. |\n| api_base_path | Dropdown | The API version to use. Options include api/v1 or api/v2. |\n| session_id | MessageText | The unique identifier for the chat session. Optional. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| message_history | BaseChatMessageHistory | An instance of ZepChatMessageHistory for the session. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "title: Tools\nslug: /components-tools", "content": "import Icon from \"@site/src/components/icon\";\nTool components in Langflow\nTools are typically connected to agent components at the **Tools** port. Agents use LLMs as a reasoning engine to decide which of the connected tool components to use to solve a problem.\nTools in agentic functions are, essentially, functions that the agent can call to perform tasks or access external resources.\nA function is wrapped as a `Tool` object, with a common interface the agent understands.\nAgents become aware of tools through tool registration, where the agent is provided a list of available tools, typically at agent initialization. The `Tool` object's description tells the agent what the tool can do.\nThe agent then uses a connected LLM to reason through the problem to decide which tool is best for the job.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Use a tool in a flow", "content": "Tools are typically connected to agent components at the **Tools** port.\nThe [simple agent starter project](/starter-projects-simple-agent) uses URL and Calculator tools connected to an [agent component](/components-agents#agent-component) to answer a user's questions. The OpenAI LLM acts as a brain for the agent to decide which tool to use.\n![Simple agent starter flow](/img/starter-flow-simple-agent.png)\nTo make a component into a tool that an agent can use, enable **Tool mode** in the component. Enabling **Tool mode** modifies a component input to accept calls from an agent.\nIf the component you want to connect to an agent doesn't have a **Tool mode** option, you can modify the component's inputs to become a tool.\nFor an example, see [Make any component a tool](/agents-tool-calling-agent-component#make-any-component-a-tool).", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "arXiv", "content": "This component searches and retrieves papers from [arXiv.org](https://arXiv.org).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| search_query | String | The search query for arXiv papers. For example, `quantum computing`. |\n| search_type | String | The field to search in. |\n| max_results | Integer | The maximum number of results to return. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| papers | List[Data] | A list of retrieved arXiv papers. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Astra DB tool", "content": "This component allows agents to query data from Astra DB collections.\nTo use this tool in a flow, connect it to an **Agent** component.\nThe flow looks like this:\n![Astra DB JSON tool connected to an Agent](/img/component-astra-db-json-tool.png)\nThe **Tool Name** and **Tool Description** fields are required for the Agent to decide when to use the tool.\n**Tool Name** cannot contain spaces.\nThe values for **Collection Name**, **Astra DB Application Token**, and **Astra DB API Endpoint** are found in your Astra DB deployment. For more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html).\nIn this example, an **OpenAI** embeddings component is connected to use the Astra DB tool component's **Semantic Search** capability.\nTo use **Semantic Search**, you must have an embedding model or Astra DB Vectorize enabled.\nIf you try to run the flow without an embedding model, you will get an error.\nOpen the **Playground** and ask a question about your data.\nThe Agent uses the **Astra DB Tool** to return information about your collection.\n### Define Astra DB tool parameters\nThe **Tool Parameters** configuration pane allows you to define parameters for [filter conditions](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters) for the component's **Find** command.\nThese filters become available as parameters that the LLM can use when calling the tool, with a better understanding of each parameter provided by the **Description** field.\nTo define a parameter for your query, in the **Tool Parameters** pane, click <Icon name=\"Plus\" aria-label=\"Add\"/>.\nComplete the fields based on your data. For example, with this filter, the LLM can filter by unique `customer_id` values.\nName: `customer_id`\nAttribute Name: Leave empty if the attribute matches the field name in the database.\nDescription: `\"The unique identifier of the customer to filter by\"`.\nIs Metadata: `False` unless the value stored in the metadata field.\nIs Mandatory: `True` to require this filter.\nIs Timestamp: `False` since the value is an ID, not a timestamp.\nOperator: `$eq` to look for an exact match.\nIf you want to apply filters regardless of the LLM's input, use the **Static Filters** option, which is available in the component's **Controls** pane.\n| Parameter | Description |\n| ----------- | ------------- |\n| Name | The name of the parameter that is exposed to the LLM. It can be the same as the underlying field name or a more descriptive label. The LLM uses this name, along with the description, to infer what value to provide during execution. |\n| Attribute Name | When the parameter name shown to the LLM differs from the actual field or property in the database, use this setting to map the user-facing name to the correct attribute. For example, to apply a range filter to the timestamp field, define two separate parameters, such as `start_date` and `end_date`, that both reference the same timestamp attribute. |\n| Description | Provides instructions to the LLM on how the parameter should be used. Clear and specific guidance helps the LLM provide valid input. For example, if a field such as `specialty` is stored in lowercase, the description should indicate that the input must be lowercase. |\n| Is Metadata | When loading data using LangChain or Langflow, additional attributes may be stored under a metadata object. If the target attribute is stored this way, enable this option. It adjusts the query by generating a filter in the format: `{\"metadata.<attribute_name>\": \"<value>\"}` |\n| Is Timestamp | For date or time-based filters, enable this option to automatically convert values to the timestamp format that the Astrapy client expects. This ensures compatibility with the underlying API without requiring manual formatting. |\n| Operator | Defines the filtering logic applied to the attribute. You can use any valid [Data API filter operator](https://docs.datastax.com/en/astra-db-serverless/api-reference/filter-operator-collections.html). For example, to filter a time range on the timestamp attribute, use two parameters: one with the `$gt` operator for \"greater than\", and another with the `$lt` operator for \"less than\". |\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Tool Name | String | The name used to reference the tool in the agent's prompt. |\n| Tool Description | String | A brief description of the tool. This helps the model decide when to use it. |\n| Collection Name | String | The name of the Astra DB collection to query. |\n| Token | SecretString | The authentication token for accessing Astra DB. |\n| API Endpoint | String | The Astra DB API endpoint. |\n| Projection Fields | String | The attributes to return, separated by commas. The default is `*`. |\n| Tool Parameters | Dict | Parameters the model needs to fill to execute the tool. For required parameters, use an exclamation mark, for example `!customer_id`. |\n| Static Filters | Dict | Attribute-value pairs used to filter query results. |\n| Limit | String | The number of documents to return. |\n**Outputs**\nThe **Data** output is used when directly querying Astra DB, while the **Tool** output is used when integrating with agents.\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Data | List[Data] | A list of [Data](/concepts-objects) objects containing the query results from Astra DB. Each `Data` object contains the document fields specified by the projection attributes. Limited by the `number_of_results` parameter. |\n| Tool | StructuredTool | A LangChain `StructuredTool` object that can be used in agent workflows. Contains the tool name, description, argument schema based on tool parameters, and the query function. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Astra DB CQL Tool", "content": "The `Astra DB CQL Tool` allows agents to query data from CQL tables in Astra DB.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Tool Name | String | The name used to reference the tool in the agent's prompt. |\n| Tool Description | String | A brief description of the tool to guide the model in using it. |\n| Keyspace | String | The name of the keyspace. |\n| Table Name | String | The name of the Astra DB CQL table to query. |\n| Token | SecretString | The authentication token for Astra DB. |\n| API Endpoint | String | The Astra DB API endpoint. |\n| Projection Fields | String | The attributes to return, separated by commas. Default: \"*\". |\n| Partition Keys | Dict | Required parameters that the model must fill to query the tool. |\n| Clustering Keys | Dict | Optional parameters the model can fill to refine the query. Required parameters should be marked with an exclamation mark, for example, `!customer_id`. |\n| Static Filters | Dict | Attribute-value pairs used to filter query results. |\n| Limit | String | The number of records to return. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Data | List[Data] | A list of [Data](/concepts-objects) objects containing the query results from the Astra DB CQL table. Each Data object contains the document fields specified by the projection fields. Limited by the `number_of_results` parameter. |\n| Tool | StructuredTool | A LangChain StructuredTool object that can be used in agent workflows. Contains the tool name, description, argument schema based on partition and clustering keys, and the query function. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Bing Search API", "content": "This component allows you to call the Bing Search API.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| bing_subscription_key | SecretString | A Bing API subscription key. |\n| input_value | String | The search query input. |\n| bing_search_url | String | A custom Bing Search URL. |\n| k | Integer | The number of search results to return. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| results | List[Data] | A list of search results. |\n| tool | Tool | A Bing Search tool for use in LangChain. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Combinatorial Reasoner", "content": "This component runs Icosa's Combinatorial Reasoning (CR) pipeline on an input to create an optimized prompt with embedded reasons. For more information, see [Icosa computing](https://www.icosacomputing.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| prompt | String | The input to run CR on. |\n| openai_api_key | SecretString | An OpenAI API key for authentication. |\n| username | String | A username for Icosa API authentication. |\n| password | SecretString | A password for Icosa API authentication. |\n| model_name | String | The OpenAI LLM to use for reason generation. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| optimized_prompt | Message | A message object containing the optimized prompt. |\n| reasons | List[String] | A list of the selected reasons that are embedded in the optimized prompt. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "DuckDuckGo search", "content": "This component performs web searches using the [DuckDuckGo](https://www.duckduckgo.com) search engine with result-limiting capabilities.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| input_value | String | The search query to execute with DuckDuckGo. |\n| max_results | Integer | The maximum number of search results to return. Default: 5. |\n| max_snippet_length | Integer | The maximum length of each result snippet. Default: 100. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| data | List[Data] | A list of search results as Data objects containing snippets and full content. |\n| text | String | The search results formatted as a single text string. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Exa Search", "content": "This component provides an [Exa Search](https://exa.ai/) toolkit for search and content retrieval.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| metaphor_api_key | SecretString | An API key for Exa Search. |\n| use_autoprompt | Boolean | Whether to use the autoprompt feature. Default: true. |\n| search_num_results | Integer | The number of results to return for search. Default: 5. |\n| similar_num_results | Integer | The number of similar results to return. Default: 5. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| tools | List[Tool] | A list of search tools provided by the toolkit. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Glean Search API", "content": "This component allows you to call the Glean Search API.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| glean_api_url | String | The URL of the Glean API. |\n| glean_access_token | SecretString | An access token for Glean API authentication. |\n| query | String | The search query input. |\n| page_size | Integer | The number of results per page. Default: 10. |\n| request_options | Dict | Additional options for the API request. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| results | List[Data] | A list of search results. |\n| tool | Tool | A Glean Search tool for use in LangChain. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Google Serper API", "content": "This component allows you to call the Serper.dev Google Search API.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| serper_api_key | SecretString | An API key for Serper.dev authentication. |\n| input_value | String | The search query input. |\n| k | Integer | The number of search results to return. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| results | List[Data] | A list of search results. |\n| tool | Tool | A Google Serper search tool for use in LangChain. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "MCP connection", "content": "The **MCP connection** component connects to a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server and exposes the MCP server's tools as tools for Langflow agents.\nIn addition to being an MCP client that can leverage MCP servers, the **MCP connection** component's [SSE mode](#mcp-sse-mode) allows you to connect your flow to the Langflow MCP server at the `/api/v1/mcp/sse` API endpoint, exposing all flows within your [project](/concepts-overview#projects) as tools within a flow.\nTo use the **MCP connection** component with an agent component, follow these steps:\nAdd the **MCP connection** component to your workflow.\nIn the **MCP connection** component, in the **MCP Command** field, enter the command to start your MCP server. For example, to start a [Fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) server, the command is:\n`uvx` is included with `uv` in the Langflow package.\n To use `npx` server commands, you must first install an LTS release of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n For an example of starting `npx` MCP servers, see [Connect an Astra DB MCP server to Langflow](/mcp-component-astra).\nTo include environment variables with your server command, add them to the **Env** field like this:\n:::important\n Langflow passes environment variables from the `.env` file to MCP, but not global variables declared in the UI.\n To add a value for an environment variable as a global variable, add it to Langflow's `.env` file at startup.\n For more information, see [global variables](/configuration-global-variables).\n :::\nClick <Icon name=\"RefreshCw\" aria-label=\"Refresh\"/> to get the server's list of **Tools**.\nIn the **Tool** field, select the server tool you want the component to use.\nThe available fields change based on the selected tool.\nFor information on the parameters, see the MCP server's documentation.\nIn the **MCP connection** component, enable **Tool mode**.\nConnect the **MCP connection** component's **Toolset** port to an **Agent** component's **Tools** port.\nThe flow looks similar to this:\n ![MCP connection component](/img/component-mcp-stdio.png)\nOpen the **Playground**.\nAsk the agent to summarize recent tech news. The agent calls the MCP server function `fetch` and returns the summary.\nThis confirms the MCP server is connected, and its tools are being used in Langflow.\nFor more information, see [MCP server](/mcp-server).\n### MCP Server-Sent Events (SSE) mode {#mcp-sse-mode}\n:::important\nIf you're using **Langflow for Desktop**, the default address is `http://127.0.0.1:7868/`.\n:::\nThe MCP component's SSE mode connects your flow to the Langflow MCP server through the component.\nThis allows you to use all flows within your [project](/concepts-overview#projects) as tools within a flow.\nIn the **MCP connection** component, select **SSE**.\nA default address appears in the **MCP SSE URL** field.\nIn the **MCP SSE URL** field, modify the default address to point at the SSE endpoint of the Langflow server you're currently running.\nThe default value is `http://localhost:7860/api/v1/mcp/sse`.\nIn the **MCP connection** component, click <Icon name=\"RefreshCw\" aria-label=\"Refresh\"/> to retrieve the server's list of **Tools**.\nClick the **Tools** field.\nAll of your flows are listed as tools.\nEnable **Tool Mode**, and then connect the **MCP connection** component to an agent component's tool port.\nThe flow looks like this:\n![MCP component with SSE mode enabled](/img/component-mcp-sse-mode.png)\nOpen the **Playground** and chat with your tool.\nThe agent chooses the correct tool based on your query.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| command | String | The MCP command. Default: `uvx mcp-sse-shim@latest`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| tools | List[Tool] | A list of tools exposed by the MCP server. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Wikidata", "content": "This component performs a search using the Wikidata API.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| query | String | The text query for similarity search on Wikidata. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| data | List[Data] | The search results from Wikidata API as a list of Data objects. |\n| text | Message | The search results formatted as a text message. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Legacy components", "content": "Legacy components are available for use but are no longer supported.\n### Calculator Tool\nThis component allows you to evaluate basic arithmetic expressions. It supports addition, subtraction, multiplication, division, and exponentiation.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| expression | String | The arithmetic expression to evaluate. For example, `4*4*(33/22)+12-20`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| result | Tool | A calculator tool for use in LangChain. |\n### Google Search API\nThis component allows you to call the Google Search API.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| google_api_key | SecretString | A Google API key for authentication. |\n| google_cse_id | SecretString | A Google Custom Search Engine ID. |\n| input_value | String | The search query input. |\n| k | Integer | The number of search results to return. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| results | List[Data] | A list of search results. |\n| tool | Tool | A Google Search tool for use in LangChain. |\n### Python Code Structured Tool\nThis component creates a structured tool from Python code using a dataclass.\nThe component dynamically updates its configuration based on the provided Python code, allowing for custom function arguments and descriptions.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| tool_code | String | The Python code for the tool's dataclass. |\n| tool_name | String | The name of the tool. |\n| tool_description | String | The description of the tool. |\n| return_direct | Boolean | Whether to return the function output directly. |\n| tool_function | String | The selected function for the tool. |\n| global_variables | Dict | Global variables or data for the tool. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| result_tool | Tool | A structured tool created from the Python code. |\n### Python REPL Tool\nThis component creates a Python REPL (Read-Eval-Print Loop) tool for executing Python code.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| name | String | The name of the tool. Default: `python_repl`. |\n| description | String | A description of the tool's functionality. |\n| global_imports | List[String] | A list of modules to import globally. Default: `math`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| tool | Tool | A Python REPL tool for use in LangChain. |\n### Retriever Tool\nThis component creates a tool for interacting with a retriever in LangChain.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| retriever | BaseRetriever | The retriever to interact with. |\n| name | String | The name of the tool. |\n| description | String | A description of the tool's functionality. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| tool | Tool | A retriever tool for use in LangChain. |\n### Search API\nThis component calls the `searchapi.io` API. It can be used to search the web for information.\nFor more information, see the [SearchAPI documentation](https://www.searchapi.io/docs/google).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| engine | String | The search engine to use. Default: `google`. |\n| api_key | SecretString | The API key for authenticating with SearchAPI. |\n| input_value | String | The search query or input for the API call. |\n| search_params | Dict | Additional parameters for customizing the search. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| data | List[Data] | A list of Data objects containing search results. |\n| tool | Tool | A Tool object for use in LangChain workflows. |\n### SearXNG Search Tool\nThis component creates a tool for searching using SearXNG, a metasearch engine.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| url | String | The URL of the SearXNG instance. |\n| max_results | Integer | The maximum number of results to return. |\n| categories | List[String] | The categories to search in. |\n| language | String | The language for the search results. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| result_tool | Tool | A SearXNG search tool for use in LangChain. |\n### Wikipedia API\nThis component creates a tool for searching and retrieving information from Wikipedia.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| input_value | String | The search query input. |\n| lang | String | The language code for Wikipedia. Default: `en`. |\n| k | Integer | The number of results to return. |\n| load_all_available_meta | Boolean | Whether to load all available metadata. |\n| doc_content_chars_max | Integer | The maximum number of characters for document content. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| results | List[Data] | A list of Wikipedia search results. |\n| tool | Tool | A Wikipedia search tool for use in LangChain. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-tools.md", "section": "Deprecated components", "content": "Deprecated components have been replaced by newer alternatives and should not be used in new projects.\n### MCP Tools (stdio)\n:::important\nThis component is deprecated as of Langflow version 1.3.\nInstead, use the [MCP connection component](/components-tools#mcp-connection)\n:::\n### MCP Tools (SSE)\n:::important\nThis component is deprecated as of Langflow version 1.3.\nInstead, use the [MCP connection component](/components-tools#mcp-connection)\n:::", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-agents.md", "section": "title: Agents\nslug: /components-agents", "content": "Agent components in Langflow\nAgent components define the behavior and capabilities of AI agents in your flow.\nAgents use LLMs as a reasoning engine to decide which of the connected tool components to use to solve a problem.\nTools in agentic functions are essentially functions that the agent can call to perform tasks or access external resources.\nA function is wrapped as a `Tool` object with a common interface the agent understands.\nAgents become aware of tools through tool registration where the agent is provided a list of available tools typically at agent initialization. The `Tool` object's description tells the agent what the tool can do.\nThe agent then uses a connected LLM to reason through the problem to decide which tool is best for the job.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-agents.md", "section": "Use an agent in a flow", "content": "The [simple agent starter project](/starter-projects-simple-agent) uses an [agent component](#agent-component) connected to URL and Calculator tools to answer a user's questions. The OpenAI LLM acts as a brain for the agent to decide which tool to use. Tools are connected to agent components at the **Tools** port.\n![Simple agent starter flow](/img/starter-flow-simple-agent.png)\nFor a multi-agent example see, [Create a problem-solving agent](/agents-tool-calling-agent-component).", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-agents.md", "section": "Agent component {#agent-component}", "content": "This component creates an agent that can use tools to answer questions and perform tasks based on given instructions.\nThe component includes an LLM model integration, a system message prompt, and a **Tools** port to connect tools to extend its capabilities.\nFor more information on this component, see the [tool calling agent documentation](/agents-tool-calling-agent-component).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent_llm | Dropdown | The provider of the language model that the agent uses to generate responses. Options include OpenAI and other providers or Custom. |\n| system_prompt | String | The system prompt provides initial instructions and context to guide the agent's behavior. |\n| tools | List | The list of tools available for the agent to use. |\n| input_value | String | The input task or question for the agent to process. |\n| add_current_date_tool | Boolean | When true this adds a tool to the agent that returns the current date. |\n| memory | Memory | An optional memory configuration for maintaining conversation history. |\n| max_iterations | Integer | The maximum number of iterations the agent can perform. |\n| handle_parsing_errors | Boolean | This determines whether to handle parsing errors during agent execution. |\n| verbose | Boolean | This enables verbose output for detailed logging. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| response | Message | The agent's response to the given input task. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-agents.md", "section": "Legacy components", "content": "**Legacy** components are available for use but are no longer supported.\n### JSON Agent\nThis component creates a JSON agent from a JSON or YAML file and an LLM.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use for the agent. |\n| path | File | The path to the JSON or YAML file. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The JSON agent instance. |\n### Vector Store Agent\nThis component creates a Vector Store Agent using LangChain.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use for the agent. |\n| vectorstore | VectorStoreInfo | The vector store information for the agent to use. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The Vector Store Agent instance. |\n### Vector Store Router Agent\nThis component creates a Vector Store Router Agent using LangChain.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use for the agent. |\n| vectorstores | List[VectorStoreInfo] | The list of vector store information for the agent to route between. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The Vector Store Router Agent instance. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-agents.md", "section": "Moved components", "content": "The following components are available under **Bundles**.\n### CrewAI Agent\nThis component represents an Agent of CrewAI allowing for the creation of specialized AI agents with defined roles goals and capabilities within a crew.\nFor more information, see the [CrewAI documentation](https://docs.crewai.com/core-concepts/Agents/).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| role | Role | The role of the agent. |\n| goal | Goal | The objective of the agent. |\n| backstory | Backstory | The backstory of the agent. |\n| tools | Tools | The tools at the agent's disposal. |\n| llm | Language Model | The language model that runs the agent. |\n| memory | Memory | This determines whether the agent should have memory or not. |\n| verbose | Verbose | This enables verbose output. |\n| allow_delegation | Allow Delegation | This determines whether the agent is allowed to delegate tasks to other agents. |\n| allow_code_execution | Allow Code Execution | This determines whether the agent is allowed to execute code. |\n| kwargs | kwargs | Additional keyword arguments for the agent. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| output | Agent | The constructed CrewAI Agent object. |\n### Hierarchical Crew\nThis component represents a group of agents managing how they should collaborate and the tasks they should perform in a hierarchical structure. This component allows for the creation of a crew with a manager overseeing the task execution.\nFor more information, see the [CrewAI documentation](https://docs.crewai.com/how-to/Hierarchical/).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| agents | Agents | The list of Agent objects representing the crew members. |\n| tasks | Tasks | The list of HierarchicalTask objects representing the tasks to be executed. |\n| manager_llm | Manager LLM | The language model for the manager agent. |\n| manager_agent | Manager Agent | The specific agent to act as the manager. |\n| verbose | Verbose | This enables verbose output for detailed logging. |\n| memory | Memory | The memory configuration for the crew. |\n| use_cache | Use Cache | This enables caching of results. |\n| max_rpm | Max RPM | This sets the maximum requests per minute. |\n| share_crew | Share Crew | This determines if the crew information is shared among agents. |\n| function_calling_llm | Function Calling LLM | The language model for function calling. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| crew | Crew | The constructed Crew object with hierarchical task execution. |\n### CSV Agent\nThis component creates a CSV agent from a CSV file and LLM.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use for the agent. |\n| path | File | The path to the CSV file. |\n| agent_type | String | The type of agent to create. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The CSV agent instance. |\n### OpenAI Tools Agent\nThis component creates an OpenAI Tools Agent.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use. |\n| tools | List of Tools | The tools to give the agent access to. |\n| system_prompt | String | The system prompt to provide context to the agent. |\n| input_value | String | The user's input to the agent. |\n| memory | Memory | The memory for the agent to use for context persistence. |\n| max_iterations | Integer | The maximum number of iterations to allow the agent to execute. |\n| verbose | Boolean | This determines whether to print out the agent's intermediate steps. |\n| handle_parsing_errors | Boolean | This determines whether to handle parsing errors in the agent. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The OpenAI Tools agent instance. |\n| output | String | The output from executing the agent on the input. |\n### OpenAPI Agent\nThis component creates an agent for interacting with OpenAPI services.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use. |\n| openapi_spec | String | The OpenAPI specification for the service. |\n| base_url | String | The base URL for the API. |\n| headers | Dict | The optional headers for API requests. |\n| agent_executor_kwargs | Dict | The optional parameters for the agent executor. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The OpenAPI agent instance. |\n### Sequential Crew\nThis component represents a group of agents with tasks that are executed sequentially. This component allows for the creation of a crew that performs tasks in a specific order.\nFor more information, see the [CrewAI documentation](https://docs.crewai.com/how-to/Sequential/).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| tasks | Tasks | The list of SequentialTask objects representing the tasks to be executed. |\n| verbose | Verbose | This enables verbose output for detailed logging. |\n| memory | Memory | The memory configuration for the crew. |\n| use_cache | Use Cache | This enables caching of results. |\n| max_rpm | Max RPM | This sets the maximum requests per minute. |\n| share_crew | Share Crew | This determines if the crew information is shared among agents. |\n| function_calling_llm | Function Calling LLM | The language model for function calling. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| crew | Crew | The constructed Crew object with sequential task execution. |\n### Sequential task agent\nThis component creates a CrewAI Task and its associated Agent allowing for the definition of sequential tasks with specific agent roles and capabilities.\nFor more information, see the [CrewAI documentation](https://docs.crewai.com/how-to/Sequential/).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| role | Role | The role of the agent. |\n| goal | Goal | The objective of the agent. |\n| backstory | Backstory | The backstory of the agent. |\n| tools | Tools | The tools at the agent's disposal. |\n| llm | Language Model | The language model that runs the agent. |\n| memory | Memory | This determines whether the agent should have memory or not. |\n| verbose | Verbose | This enables verbose output. |\n| allow_delegation | Allow Delegation | This determines whether the agent is allowed to delegate tasks to other agents. |\n| allow_code_execution | Allow Code Execution | This determines whether the agent is allowed to execute code. |\n| agent_kwargs | Agent kwargs | The additional kwargs for the agent. |\n| task_description | Task Description | The descriptive text detailing the task's purpose and execution. |\n| expected_output | Expected Task Output | The clear definition of the expected task outcome. |\n| async_execution | Async Execution | The boolean flag indicating asynchronous task execution. |\n| previous_task | Previous Task | The previous task in the sequence for chaining. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| task_output | Sequential Task | The list of SequentialTask objects representing the created tasks. |\n### SQL Agent\nThis component creates an agent for interacting with SQL databases.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use. |\n| database | Database | The SQL database connection. |\n| top_k | Integer | The number of results to return from a SELECT query. |\n| use_tools | Boolean | This determines whether to use tools for query execution. |\n| return_intermediate_steps | Boolean | This determines whether to return the agent's intermediate steps. |\n| max_iterations | Integer | The maximum number of iterations to run the agent. |\n| max_execution_time | Integer | The maximum execution time in seconds. |\n| early_stopping_method | String | The method to use for early stopping. |\n| verbose | Boolean | This determines whether to print the agent's thoughts. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The SQL agent instance. |\n### Tool Calling Agent\nThis component creates an agent for structured tool calling with various language models.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use. |\n| tools | List[Tool] | The list of tools available to the agent. |\n| system_message | String | The system message to use for the agent. |\n| return_intermediate_steps | Boolean | This determines whether to return the agent's intermediate steps. |\n| max_iterations | Integer | The maximum number of iterations to run the agent. |\n| max_execution_time | Integer | The maximum execution time in seconds. |\n| early_stopping_method | String | The method to use for early stopping. |\n| verbose | Boolean | This determines whether to print the agent's thoughts. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The tool calling agent instance. |\n### XML Agent\nThis component creates an XML Agent using LangChain.\nThe agent uses XML formatting for tool instructions to the Language Model.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use for the agent. |\n| user_prompt | String | The custom prompt template for the agent with XML formatting instructions. |\n| tools | List[Tool] | The list of tools available to the agent. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| agent | AgentExecutor | The XML Agent instance. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "title: Embeddings\nslug: /components-embedding-models", "content": "import Icon from \"@site/src/components/icon\";\nEmbeddings models in Langflow\nEmbeddings models convert text into numerical vectors. These embeddings capture the semantic meaning of the input text, and allow LLMs to understand context.\nRefer to your specific component's documentation for more information on parameters.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Use an embeddings model component in a flow", "content": "In this example of a document ingestion pipeline, the **OpenAI** embeddings model is connected to a vector database. The component converts the text chunks into vectors and stores them in the vector database. The vectorized data can be used to inform AI workloads like chatbots, similarity searches, and agents.\nThis embeddings component uses an OpenAI API key for authentication. Refer to your specific embeddings component's documentation for more information on authentication.\n![URL component in a data ingestion pipeline](/img/url-component.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "AI/ML", "content": "This component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model_name | String | The name of the AI/ML embedding model to use. |\n| aiml_api_key | SecretString | The API key required for authenticating with the AI/ML service. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance of `AIMLEmbeddingsImpl` for generating embeddings. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Amazon Bedrock Embeddings", "content": "This component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| credentials_profile_name | String | The name of the AWS credentials profile in `~/.aws/credentials` or `~/.aws/config`, which has access keys or role information. |\n| model_id | String | The ID of the model to call, such as `amazon.titan-embed-text-v1`. This is equivalent to the `modelId` property in the `list-foundation-models` API. |\n| endpoint_url | String | The URL to set a specific service endpoint other than the default AWS endpoint. |\n| region_name | String | The AWS region to use, such as `us-west-2`. Falls back to the `AWS_DEFAULT_REGION` environment variable or region specified in `~/.aws/config` if not provided. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using Amazon Bedrock. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Astra DB vectorize", "content": ":::important\nThis component is deprecated as of Langflow version 1.1.2.\nInstead, use the [Astra DB vector store component](/components-vector-stores#astra-db-vector-store).\n:::\nConnect this component to the **Embeddings** port of the [Astra DB vector store component](/components-vector-stores#astra-db-vector-store) to generate embeddings.\nThis component requires that your Astra DB database has a collection that uses a vectorize embedding provider integration.\nFor more information and instructions, see [Embedding Generation](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| provider | Embedding Provider | The embedding provider to use. |\n| model_name | Model Name | The embedding model to use. |\n| authentication | Authentication | The name of the API key in Astra that stores your [vectorize embedding provider credentials](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#embedding-provider-authentication). (Not required if using an [Astra-hosted embedding provider](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#supported-embedding-providers).) |\n| provider_api_key | Provider API Key | As an alternative to `authentication`, directly provide your embedding provider credentials. |\n| model_parameters | Model Parameters | Additional model parameters. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using Astra vectorize. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Azure OpenAI Embeddings", "content": "This component generates embeddings using Azure OpenAI models.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Model | String | The name of the model to use. Default: `text-embedding-3-small`. |\n| Azure Endpoint | String | Your Azure endpoint, including the resource, such as `https://example-resource.azure.openai.com/`. |\n| Deployment Name | String | The name of the deployment. |\n| API Version | String | The API version to use, with options including various dates. |\n| API Key | String | The API key required to access the Azure OpenAI service. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using Azure OpenAI. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Cloudflare Workers AI Embeddings", "content": "This component generates embeddings using [Cloudflare Workers AI models](https://developers.cloudflare.com/workers-ai/).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| account_id | Cloudflare account ID | [Find your Cloudflare account ID](https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages). |\n| api_token | Cloudflare API token | [Create an API token](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/). |\n| model_name | Model Name | [List of supported models](https://developers.cloudflare.com/workers-ai/models/#text-embeddings). |\n| strip_new_lines | Strip New Lines | Whether to strip new lines from the input text. |\n| batch_size | Batch Size | The number of texts to embed in each batch. |\n| api_base_url | Cloudflare API base URL | The base URL for the Cloudflare API. |\n| headers | Headers | Additional request headers. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embeddings | An instance for generating embeddings using Cloudflare Workers. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Cohere Embeddings", "content": "This component is used to load embedding models from [Cohere](https://cohere.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| cohere_api_key | String | The API key required to authenticate with the Cohere service. |\n| model | String | The language model used for embedding text documents and performing queries. Default: `embed-english-v2.0`. |\n| truncate | Boolean | Whether to truncate the input text to fit within the model's constraints. Default: `False`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using Cohere. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Embedding similarity", "content": "This component computes selected forms of similarity between two embedding vectors.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embedding_vectors | Embedding Vectors | A list containing exactly two data objects with embedding vectors to compare. |\n| similarity_metric | Similarity Metric | Select the similarity metric to use. Options: \"Cosine Similarity\", \"Euclidean Distance\", \"Manhattan Distance\". |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| similarity_data | Similarity Data | A data object containing the computed similarity score and additional information. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Google generative AI embeddings", "content": "This component connects to Google's generative AI embedding service using the GoogleGenerativeAIEmbeddings class from the `langchain-google-genai` package.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| api_key | API Key | The secret API key for accessing Google's generative AI service. Required. |\n| model_name | Model Name | The name of the embedding model to use. Default: \"models/text-embedding-004\". |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embeddings | The built GoogleGenerativeAIEmbeddings object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Hugging Face Embeddings", "content": ":::note\nThis component is deprecated as of Langflow version 1.0.18.\nInstead, use the [Hugging Face Embeddings Inference component](#hugging-face-embeddings-inference).\n:::\nThis component loads embedding models from HuggingFace.\nUse this component to generate embeddings using locally downloaded Hugging Face models. Ensure you have sufficient computational resources to run the models.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| Cache Folder | Cache Folder | The folder path to cache HuggingFace models. |\n| Encode Kwargs | Encoding Arguments | Additional arguments for the encoding process. |\n| Model Kwargs | Model Arguments | Additional arguments for the model. |\n| Model Name | Model Name | The name of the HuggingFace model to use. |\n| Multi Process | Multi-Process | Whether to use multiple processes. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embeddings | The generated embeddings. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Hugging Face embeddings inference", "content": "This component generates embeddings using [Hugging Face Inference API models](https://huggingface.co/) and requires a [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) to authenticate. Local inference models do not require an API key.\nUse this component to create embeddings with Hugging Face's hosted models, or to connect to your own locally hosted models.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| API Key | API Key | The API key for accessing the Hugging Face Inference API. |\n| API URL | API URL | The URL of the Hugging Face Inference API. |\n| Model Name | Model Name | The name of the model to use for embeddings. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embeddings | The generated embeddings. |\n### Connect the Hugging Face component to a local embeddings model\nTo run an embeddings inference locally, see the [HuggingFace documentation](https://huggingface.co/docs/text-embeddings-inference/local_cpu).\nTo connect the local Hugging Face model to the **Hugging Face embeddings inference** component and use it in a flow, follow these steps:\nCreate a [Vector store RAG flow](/starter-projects-vector-store-rag).\nThere are two embeddings models in this flow that you can replace with **Hugging Face** embeddings inference components.\nReplace both **OpenAI** embeddings model components with **Hugging Face** model components.\nConnect both **Hugging Face** components to the **Embeddings** ports of the **Astra DB vector store** components.\nIn the **Hugging Face** components, set the **Inference Endpoint** field to the URL of your local inference model. **The **API Key** field is not required for local inference.**\nRun the flow. The local inference models generate embeddings for the input text.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "IBM watsonx embeddings", "content": "This component generates text using [IBM watsonx.ai](https://www.ibm.com/watsonx) foundation models.\nTo use **IBM watsonx.ai** embeddings components, replace an embeddings component with the IBM watsonx.ai component in a flow.\nAn example document processing flow looks like the following:\n![IBM watsonx embeddings model loading a chroma-db with split text](/img/component-watsonx-embeddings-chroma.png)\nThis flow loads a PDF file from local storage and splits the text into chunks.\nThe **IBM watsonx** embeddings component converts the text chunks into embeddings, which are then stored in a Chroma DB vector store.\nThe values for **API endpoint**, **Project ID**, **API key**, and **Model Name** are found in your IBM watsonx.ai deployment.\nFor more information, see the [Langchain documentation](https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/).\n### Default models\nThe component supports several default models with the following vector dimensions:\n`sentence-transformers/all-minilm-l12-v2`: 384-dimensional embeddings\n`ibm/slate-125m-english-rtrvr-v2`: 768-dimensional embeddings\n`ibm/slate-30m-english-rtrvr-v2`: 768-dimensional embeddings\n`intfloat/multilingual-e5-large`: 1024-dimensional embeddings\nThe component automatically fetches and updates the list of available models from your watsonx.ai instance when you provide your API endpoint and credentials.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| url | watsonx API Endpoint | The base URL of the API. |\n| project_id | watsonx project id | The project ID for your watsonx.ai instance. |\n| api_key | API Key | The API Key to use for the model. |\n| model_name | Model Name | The name of the embedding model to use. |\n| truncate_input_tokens | Truncate Input Tokens | The maximum number of tokens to process. Default: `200`. |\n| input_text | Include the original text in the output | Determines if the original text is included in the output. Default: `True`. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embeddings | An instance for generating embeddings using watsonx.ai. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "LM Studio Embeddings", "content": "This component generates embeddings using [LM Studio](https://lmstudio.ai/docs) models.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| model | Model | The LM Studio model to use for generating embeddings. |\n| base_url | LM Studio Base URL | The base URL for the LM Studio API. |\n| api_key | LM Studio API Key | The API key for authentication with LM Studio. |\n| temperature | Model Temperature | The temperature setting for the model. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embeddings | The generated embeddings. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "MistralAI", "content": "This component generates embeddings using [MistralAI](https://docs.mistral.ai/) models.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | String | The MistralAI model to use. Default: \"mistral-embed\". |\n| mistral_api_key | SecretString | The API key for authenticating with MistralAI. |\n| max_concurrent_requests | Integer | The maximum number of concurrent API requests. Default: 64. |\n| max_retries | Integer | The maximum number of retry attempts for failed requests. Default: 5. |\n| timeout | Integer | The request timeout in seconds. Default: 120. |\n| endpoint | String | The custom API endpoint URL. Default: `https://api.mistral.ai/v1/`). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | A MistralAIEmbeddings instance for generating embeddings. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "NVIDIA", "content": "This component generates embeddings using [NVIDIA models](https://docs.nvidia.com).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | String | The NVIDIA model to use for embeddings, such as `nvidia/nv-embed-v1`. |\n| base_url | String | The base URL for the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |\n| nvidia_api_key | SecretString | The API key for authenticating with NVIDIA's service. |\n| temperature | Float | The model temperature for embedding generation. Default: `0.1`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | A NVIDIAEmbeddings instance for generating embeddings. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Ollama embeddings", "content": "This component generates embeddings using [Ollama models](https://ollama.com/).\nFor a list of Ollama embeddings models, see the [Ollama documentation](https://ollama.com/search?c=embedding).\nTo use this component in a flow, connect Langflow to your locally running Ollama server and select an embeddings model.\nIn the Ollama component, in the **Ollama Base URL** field, enter the address for your locally running Ollama server.\nThis value is set as the `OLLAMA_HOST` environment variable in Ollama. The default base URL is `http://127.0.0.1:11434`.\nTo refresh the server's list of models, click <Icon name=\"RefreshCw\" aria-label=\"Refresh\"/>.\nIn the **Ollama Model** field, select an embeddings model. This example uses `all-minilm:latest`.\nConnect the **Ollama** embeddings component to a flow.\nFor example, this flow connects a local Ollama server running a `all-minilm:latest` embeddings model to a [Chroma DB](/components-vector-stores#chroma-db) vector store to generate embeddings for split text.\n![Ollama embeddings connected to Chroma DB](/img/component-ollama-embeddings-chromadb.png)\nFor more information, see the [Ollama documentation](https://ollama.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Ollama Model | String | The name of the Ollama model to use. Default: `llama2`. |\n| Ollama Base URL | String | The base URL of the Ollama API. Default: `http://localhost:11434`. |\n| Model Temperature | Float | The temperature parameter for the model. Adjusts the randomness in the generated embeddings. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using Ollama. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "OpenAI Embeddings", "content": "This component is used to load embedding models from [OpenAI](https://openai.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| OpenAI API Key | String | The API key to use for accessing the OpenAI API. |\n| Default Headers | Dict | The default headers for the HTTP requests. |\n| Default Query | NestedDict | The default query parameters for the HTTP requests. |\n| Allowed Special | List | The special tokens allowed for processing. Default: `[]`. |\n| Disallowed Special | List | The special tokens disallowed for processing. Default: `[\"all\"]`. |\n| Chunk Size | Integer | The chunk size for processing. Default: `1000`. |\n| Client | Any | The HTTP client for making requests. |\n| Deployment | String | The deployment name for the model. Default: `text-embedding-3-small`. |\n| Embedding Context Length | Integer | The length of embedding context. Default: `8191`. |\n| Max Retries | Integer | The maximum number of retries for failed requests. Default: `6`. |\n| Model | String | The name of the model to use. Default: `text-embedding-3-small`. |\n| Model Kwargs | NestedDict | Additional keyword arguments for the model. |\n| OpenAI API Base | String | The base URL of the OpenAI API. |\n| OpenAI API Type | String | The type of the OpenAI API. |\n| OpenAI API Version | String | The version of the OpenAI API. |\n| OpenAI Organization | String | The organization associated with the API key. |\n| OpenAI Proxy | String | The proxy server for the requests. |\n| Request Timeout | Float | The timeout for the HTTP requests. |\n| Show Progress Bar | Boolean | Whether to show a progress bar for processing. Default: `False`. |\n| Skip Empty | Boolean | Whether to skip empty inputs. Default: `False`. |\n| TikToken Enable | Boolean | Whether to enable TikToken. Default: `True`. |\n| TikToken Model Name | String | The name of the TikToken model. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using OpenAI. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "Text embedder", "content": "This component generates embeddings for a given message using a specified embedding model.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embedding_model | Embedding Model | The embedding model to use for generating embeddings. |\n| message | Message | The message for which to generate embeddings. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embeddings | Embedding Data | A data object containing the original text and its embedding vector. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-embedding-models.md", "section": "VertexAI Embeddings", "content": "This component is a wrapper around [Google Vertex AI](https://cloud.google.com/vertex-ai) [Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| credentials | Credentials | The default custom credentials to use. |\n| location | String | The default location to use when making API calls. Default: `us-central1`. |\n| max_output_tokens | Integer | The token limit determines the maximum amount of text output from one prompt. Default: `128`. |\n| model_name | String | The name of the Vertex AI large language model. Default: `text-bison`. |\n| project | String | The default GCP project to use when making Vertex API calls. |\n| request_parallelism | Integer | The amount of parallelism allowed for requests issued to VertexAI models. Default: `5`. |\n| temperature | Float | Tunes the degree of randomness in text generations. Should be a non-negative value. Default: `0`. |\n| top_k | Integer | How the model selects tokens for output. The next token is selected from the top `k` tokens. Default: `40`. |\n| top_p | Float | Tokens are selected from the most probable to least until the sum of their probabilities exceeds the top `p` value. Default: `0.95`. |\n| tuned_model_name | String | The name of a tuned model. If provided, `model_name` is ignored. |\n| verbose | Boolean | This parameter controls the level of detail in the output. When set to `True`, it prints internal states of the chain to help debug. Default: `False`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| embeddings | Embeddings | An instance for generating embeddings using VertexAI. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "title: Logic\nslug: /components-logic", "content": "Logic components in Langflow\nLogic components provide functionalities for routing, conditional processing, and flow management.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Use a logic component in a flow", "content": "This flow creates a summarizing \"for each\" loop with the [Loop](/components-logic#loop) component.\nThe component iterates over a list of [Data](/concepts-objects#data-object) objects until it's completed, and then the **Done** loop aggregates the results.\nThe **File** component loads text files from your local machine, and then the **Parser** component parses them into a list of structured `Data` objects.\nThe **Loop** component passes each `Data` object to a **Prompt** to be summarized.\nWhen the **Loop** component runs out of `Data`, the **Done** loop activates, which counts the number of pages and summarizes their tone with another **Prompt**.\nThis is represented in Langflow by connecting the Parser component's **Data List** output to the Loop component's `Data` loop input.\n![Sample Flow looping summarizer](/img/loop-text-summarizer.png)\nThe output is similar to this:", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Conditional router (If-Else component)", "content": "This component routes messages by comparing two strings.\nIt evaluates a condition by comparing two text inputs using the specified operator and routes the message to `true_result` or `false_result`.\nThe operator looks for single strings based on your defined [operator behavior](#operator-behavior), but it can also search for multiple words by regex matching.\nTo use the **Conditional router** component to check incoming messages with regex matching, do the following:\nConnect the **If-Else** component's **Text Input** port to a **Chat Input** component.\nIn the If-Else component, enter the following values.\nIn the **Match Text** field, enter `.*(urgent|warning|caution).*`. The component looks for these values. The regex match is case sensitive, so to look for all permutations of `warning`, enter `warning|Warning|WARNING`.\nIn the **Operator** field, enter `regex`. The component looks for the strings `urgent`, `warning`, and `caution`. For more operators, see [Operator behavior](#operator-behavior).\nIn the **Message** field, enter `New Message Detected`. This field is optional. The message is sent to both the **True** and **False** ports.\nThe component is now set up to send a `New Message Detected` message out of its **True** port if it matches any of the strings.\nIf no strings are detected, it sends a message out of the **False** port.\nCreate two identical flows to process the messages. Connect an **Open AI** component, a **Prompt**, and a **Chat Output** component together.\nConnect one chain to the **If-Else** component's **True** port, and one chain to the **False** port.\nThe flow looks like this:\n![A conditional router connected to two OpenAI components](/img/component-conditional-router.png)\nAdd your **OpenAI API key** to both **OpenAI** components.\nIn both **Prompt** components, enter the behavior you want each route to take.\nWhen a match is found:\nWhen a match is not found:\nOpen the **Playground**.\nSend the flow some messages. Your messages route differently based on the if-else component's evaluation.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| input_text | String | The primary text input for the operation. |\n| match_text | String | The text to compare against. |\n| operator | Dropdown | The operator used to compare texts. Options include equals, not equals, contains, starts with, ends with, and regex. The default is equals. |\n| case_sensitive | Boolean | When set to true, the comparison is case sensitive. This setting does not apply to regex comparison. The default is false. |\n| message | Message | The message to pass through either route. |\n| max_iterations | Integer | The maximum number of iterations allowed for the conditional router. The default is 10. |\n| default_route | Dropdown | The route to take when max iterations are reached. Options include true_result or false_result. The default is false_result. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| true_result | Message | The output produced when the condition is true. |\n| false_result | Message | The output produced when the condition is false. |\n### Operator Behavior\nThe **If-else** component includes a comparison operator to compare the values in `input_text` and `match_text`.\nAll options respect the `case_sensitive` setting except **regex**.\n**equals**: Exact match comparison.\n**not equals**: Inverse of exact match.\n**contains**: Checks if match_text is found within input_text.\n**starts with**: Checks if input_text begins with match_text.\n**ends with**: Checks if input_text ends with match_text.\n**regex**: Performs regular expression matching. It is always case sensitive and ignores the case_sensitive setting.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Listen", "content": "This component listens for a notification and retrieves its associated state.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| name | String | The name of the notification to listen for. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| output | Data | The state associated with the notification. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Loop", "content": ":::tip\nFor another **Loop** component example, see the **Research Translation Loop** template.\n:::\nThis component iterates over a list of [Data](/concepts-objects#data-object) objects, outputting one item at a time and aggregating results from loop inputs.\nIn this example, the **Loop** component iterates over a CSV file through the **Item** port until there are no rows left to process. Then, the **Loop** component performs the actions connected to the **Done** port, which in this case is loading the structured data into **Chroma DB**.\nThink of it this way: the **Item** port forms the \"main\" loop that repeats until a \"complete\" condition is reached.\nThe **Loop** component accepts **Data** from the **Load CSV** component, and outputs the data from the **Item** port.\nEach CSV row is converted to a **Message** and processed into structured data with the **Structured Output** component.\nThe dotted line connected from the **Structured Output** component's **Looping** port tells you where the loop begins again.\nThe **Loop** component repeatedly extracts rows by **Text Key** until there are no more rows to extract.\nOnce all items are processed, the action connected to the **Done** port is performed.\nIn this example, the data is loaded into **Chroma DB**.\n![Loop CSV parser](/img/component-loop-csv.png)\nFollow along with this step-by-step video guide for creating this flow and adding agentic RAG: [Mastering the Loop Component & Agentic RAG in Langflow](https://www.youtube.com/watch?v=9Wx7WODSKTo).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| data | Data/List | The initial list of Data objects to process. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| item | Data | The current item being processed from the data list. |\n| done | Data | The aggregated results after all items are processed. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Notify", "content": "This component generates a notification for the Listen component to use.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| name | String | The name of the notification. |\n| data | Data | The data to store in the notification. |\n| append | Boolean | When set to true, the record is added to the existing notification. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| output | Data | The data stored in the notification. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Pass", "content": "This component forwards the input message, unchanged.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| input_message | Input Message | The message to forward. |\n| ignored_message | Ignored Message | A second message that is ignored. Used as a workaround for continuity. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| output_message | Output Message | The forwarded message from the input. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Run flow", "content": "This component allows you to run any flow stored in your Langflow database without opening the flow editor.\nThe Run Flow component can also be used as a tool when connected to an [Agent](/components-agents). The `name` and `description` metadata that the Agent uses to register the tool are created automatically.\nWhen you select a flow, the component fetches the flow's graph structure and uses it to generate the inputs and outputs for the Run Flow component.\nTo use the Run Flow component as a tool, do the following:\nAdd the **Run Flow** component to the [Simple Agent](/starter-projects-simple-agent) flow.\nIn the **Flow Name** menu, select the sub-flow you want to run.\nThe appearance of the **Run Flow** component changes to reflect the inputs and outputs of the selected flow.\nOn the **Run Flow** component, enable **Tool Mode**.\nConnect the **Run Flow** component to the **Toolset** input of the Agent.\nYour flow should now look like this:\n![Run Flow component](/img/component-run-flow.png)\nRun the flow. The Agent uses the Run Flow component as a tool to run the selected sub-flow.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| flow_name_selected | Dropdown | The name of the flow to run. |\n| flow_tweak_data | Dict | Dictionary of tweaks to customize the flow's behavior. |\n| dynamic inputs | Various | Additional inputs that are generated based on the selected flow. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| run_outputs | A `List` of types `Data`, `Message,` or `DataFrame` | All outputs are generated from running the flow. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Legacy components", "content": "**Legacy** components are available for use but are no longer supported.\n### Data Conditional Router\n:::important\nThis component is in **Legacy**, which means it is no longer in active development as of Langflow version 1.3.\n:::\nThis component routes `Data` objects based on a condition applied to a specified key, including boolean validation. It can process either a single Data object or a list of Data objects.\nThis component is particularly useful in workflows that require conditional routing of complex data structures, enabling dynamic decision-making based on data content.\nInputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| data_input | Data | The Data object or list of Data objects to process. This input can handle both single items and lists. |\n| key_name | String | The name of the key in the Data object to check. |\n| operator | Dropdown | The operator to apply. Options: \"equals\", \"not equals\", \"contains\", \"starts with\", \"ends with\", \"boolean validator\". Default: \"equals\". |\n| compare_value | String | The value to compare against. Not shown/used when operator is \"boolean validator\". |\nOutputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| true_output | Data/List | Output when the condition is met. |\n| false_output | Data/List | Output when the condition is not met. |\nOperator behavior\n**equals**: Exact match comparison between the key's value and compare_value.\n**not equals**: Inverse of exact match.\n**contains**: Checks if compare_value is found within the key's value.\n**starts with**: Checks if the key's value begins with compare_value.\n**ends with**: Checks if the key's value ends with compare_value.\n**boolean validator**: Treats the key's value as a boolean. The following values are considered true:\nBoolean `true`.\nStrings: \"true\", \"1\", \"yes\", \"y\", \"on\" (case-insensitive).\nAny other value is converted using Python's `bool()` function.\nList processing\nThe following actions occur when processing a list of Data objects:\nEach object in the list is evaluated individually\nObjects meeting the condition go to true_output\nObjects not meeting the condition go to false_output\nIf all objects go to one output, the other output is empty", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-logic.md", "section": "Deprecated components", "content": "Deprecated components have been replaced by newer alternatives and should not be used in new projects.\n### Flow as tool {#flow-as-tool}\n:::important\nThis component is deprecated as of Langflow version 1.1.2.\nInstead, use the [Run flow component](/components-logic#run-flow)\n:::\nThis component constructs a tool from a function that runs a loaded flow.\nInputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| flow_name | Dropdown | The name of the flow to run. |\n| tool_name | String | The name of the tool. |\n| tool_description | String | The description of the tool. |\n| return_direct | Boolean | If true, returns the result directly from the tool. |\nOutputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| api_build_tool | Tool | The constructed tool from the flow. |\n### Sub flow\n:::important\nThis component is deprecated as of Langflow version 1.1.2.\nInstead, use the [Run flow component](/components-logic#run-flow)\n:::\nThis `SubFlowComponent` generates a component from a flow with all of its inputs and outputs.\nThis component can integrate entire flows as components within a larger workflow. It dynamically generates inputs based on the selected flow and executes the flow with provided parameters.\nInputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| flow_name | Dropdown | The name of the flow to run. |\nOutputs\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| flow_outputs | List[Data] | The outputs generated from the flow. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "title: Helpers\nslug: /components-helpers", "content": "import Icon from \"@site/src/components/icon\";\nHelper components in Langflow\nHelper components provide utility functions to help manage data, tasks, and other components in your flow.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Use a helper component in a flow", "content": "Chat memory in Langflow is stored either in local Langflow tables with `LCBufferMemory`, or connected to an external database.\nThe **Store Message** helper component stores chat memories as [Data](/concepts-objects) objects, and the **Message History** helper component retrieves chat messages as data objects or strings.\nThis example flow stores and retrieves chat history from an [AstraDBChatMemory](/components-memories#astradbchatmemory-component) component with **Store Message** and **Chat Memory** components.\n![Sample Flow storing Chat Memory in AstraDB](/img/astra_db_chat_memory_rounded.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Batch Run", "content": "The **Batch Run** component runs a language model over **each row** of a [DataFrame](/concepts-objects#dataframe-object) text column and returns a new DataFrame with the original text and an LLM response.\nThe response contains the following columns:\n`text_input`: The original text from the input DataFrame.\n`model_response`: The model's response for each input.\n`batch_index`: The processing order, with a `0`-based index.\n`metadata` (optional): Additional information about the processing.\nThese columns, when connected to a **Parser** component, can be used as variables within curly braces.\nTo use the Batch Run component with a **Parser** component, do the following:\nConnect a **Model** component to the **Batch Run** component's **Language model** port.\nConnect a component that outputs DataFrame, like **File** component, to the **Batch Run** component's **DataFrame** input.\nConnect the **Batch Run** component's **Batch Results** output to a **Parser** component's **DataFrame** input.\nThe flow looks like this:\n![A batch run component connected to OpenAI and a Parser](/img/component-batch-run.png)\nIn the **Column Name** field of the **Batch Run** component, enter a column name based on the data you're loading from the **File** loader. For example, to process a column of `name`, enter `name`.\nOptionally, in the **System Message** field of the **Batch Run** component, enter a **System Message** to instruct the connected LLM on how to process your file. For example, `Create a business card for each name.`\nIn the **Template** field of the **Parser** component, enter a template for using the **Batch Run** component's new DataFrame columns.\nTo use all three columns from the **Batch Run** component, include them like this:\nTo run the flow, in the **Parser** component, click <Icon name=\"Play\" aria-label=\"Play icon\" />.\nTo view your created DataFrame, in the **Parser** component, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\nOptionally, connect a **Chat Output** component, and open the **Playground** to see the output.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | HandleInput | Connect the 'Language Model' output from your LLM component here. Required. |\n| system_message | MultilineInput | A multi-line system instruction for all rows in the DataFrame. |\n| df | DataFrameInput | The DataFrame whose column is treated as text messages, as specified by 'column_name'. Required. |\n| column_name | MessageTextInput | The name of the DataFrame column to treat as text messages. If empty, all columns are formatted in TOML. |\n| output_column_name | MessageTextInput | Name of the column where the model's response is stored. Default=`model_response`. |\n| enable_metadata | BoolInput | If True, add metadata to the output DataFrame. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| batch_results | DataFrame | A DataFrame with all original columns plus the model's response column. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Current date", "content": "The Current Date component returns the current date and time in a selected timezone. This component provides a flexible way to obtain timezone-specific date and time information within a Langflow pipeline.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| timezone | String | The timezone for the current date and time. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| current_date | String | The resulting current date and time in the selected timezone. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "ID Generator", "content": "This component generates a unique ID.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| unique_id | String | The generated unique ID. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| id | String | The generated unique ID. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Message history", "content": ":::info\nPrior to Langflow 1.1, this component was known as the Chat Memory component.\n:::\nThis component retrieves chat messages from Langflow tables or external memory.\nIn this example, the **Message Store** component stores the complete chat history in a local Langflow table, which the **Message History** component retrieves as context for the LLM to answer each question.\n![Message store and history components](/img/component-message-history-message-store.png)\nFor more information on configuring memory in Langflow, see [Memory](/memory).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| memory | Memory | Retrieve messages from an external memory. If empty, the Langflow tables are used. |\n| sender | String | Filter by sender type. |\n| sender_name | String | Filter by sender name. |\n| n_messages | Integer | The number of messages to retrieve. |\n| session_id | String | The session ID of the chat. If empty, the current session ID parameter is used. |\n| order | String | The order of the messages. |\n| template | String | The template to use for formatting the data. It can contain the keys `{text}`, `{sender}` or any other key in the message data. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| messages | Data | The retrieved messages as Data objects. |\n| messages_text | Message | The retrieved messages formatted as text. |\n| dataframe | DataFrame | A DataFrame containing the message data. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Message store", "content": "This component stores chat messages or text in Langflow tables or external memory.\nIn this example, the **Message Store** component stores the complete chat history in a local Langflow table, which the **Message History** component retrieves as context for the LLM to answer each question.\n![Message store and history components](/img/component-message-history-message-store.png)\nFor more information on configuring memory in Langflow, see [Memory](/memory).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| message | String | The chat message to be stored. (Required) |\n| memory | Memory | The external memory to store the message. If empty, the Langflow tables are used. |\n| sender | String | The sender of the message. Can be Machine or User. If empty, the current sender parameter is used. |\n| sender_name | String | The name of the sender. Can be AI or User. If empty, the current sender parameter is used. |\n| session_id | String | The session ID of the chat. If empty, the current session ID parameter is used. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| stored_messages | List[Data] | The list of stored messages after the current message has been added. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Structured output", "content": "This component transforms LLM responses into structured data formats.\nIn this example from the **Financial Support Parser** template, the **Structured Output** component transforms unstructured financial reports into structured data.\n![Structured output example](/img/component-structured-output.png)\nThe connected LLM model is prompted by the **Structured Output** component's `Format Instructions` parameter to extract structured output from the unstructured text. `Format Instructions` is utilized as the system prompt for the **Structured Output** component.\nIn the **Structured Output** component, click the **Open table** button to view the `Output Schema` table.\nThe `Output Schema` parameter defines the structure and data types for the model's output using a table with the following fields:\n**Name**: The name of the output field.\n**Description**: The purpose of the output field.\n**Type**: The data type of the output field. The available types are `str`, `int`, `float`, `bool`, `list`, or `dict`. The default is `text`.\n**Multiple**: This feature is deprecated. Currently, it is set to `True` by default if you expect multiple values for a single field. For example, a `list` of `features` is set to `True` to contain multiple values, such as `[\"waterproof\", \"durable\", \"lightweight\"]`. Default: `True`.\nThe **Parse DataFrame** component parses the structured output into a template for orderly presentation in chat output. The template receives the values from the `output_schema` table with curly braces.\nFor example, the template `EBITDA: {EBITDA}  ,  Net Income: {NET_INCOME} , GROSS_PROFIT: {GROSS_PROFIT}` presents the extracted values in the **Playground** as `EBITDA: 900 million , Net Income: 500 million , GROSS_PROFIT: 1.2 billion`.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| llm | LanguageModel | The language model to use to generate the structured output. |\n| input_value | String | The input message to the language model. |\n| system_prompt | String | The instructions to the language model for formatting the output. |\n| schema_name | String | The name for the output data schema. |\n| output_schema | Table | The structure and data types for the model's output. |\n| multiple | Boolean | [Deprecated] Always set to `True`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| structured_output | Data | The structured output is a Data object based on the defined schema. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-helpers.md", "section": "Legacy components", "content": "Legacy components are available for use but are no longer supported.\n### Create List\nThis component dynamically creates a record with a specified number of fields.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| n_fields | Integer | The number of fields to be added to the record. |\n| text_key | String | The key used as text. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| list | List | The dynamically created list with the specified number of fields. |\n### Output Parser\nThis component transforms the output of a language model into a specified format. It supports CSV format parsing, which converts LLM responses into comma-separated lists using Langchain's `CommaSeparatedListOutputParser`.\n:::note\nThis component only provides formatting instructions and parsing functionality. It does not include a prompt. You'll need to connect it to a separate Prompt component to create the actual prompt template for the LLM to use.\n:::\nBoth the **Output Parser** and **Structured Output** components format LLM responses, but they have different use cases.\nThe **Output Parser** is simpler and focused on converting responses into comma-separated lists. Use this when you just need a list of items, for example `[\"item1\", \"item2\", \"item3\"]`.\nThe **Structured Output** is more complex and flexible, and allows you to define custom schemas with multiple fields of different types. Use this when you need to extract structured data with specific fields and types.\nTo use this component:\nCreate a Prompt component and connect the Output Parser's `format_instructions` output to it. This ensures the LLM knows how to format its response.\nWrite your actual prompt text in the Prompt component, including the `{format_instructions}` variable.\nFor example, in your Prompt component, the template might look like:\nConnect the `output_parser` output to your LLM model.\nThe output parser converts this into a Python list: `[\"apple\", \"banana\", \"orange\"]`.\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| parser_type | String | The parser type. Currently supports \"CSV\". |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| format_instructions | String | Pass to a prompt template to include formatting instructions for LLM responses. |\n| output_parser | Parser | The constructed output parser that can be used to parse LLM responses. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "title: Data\nslug: /components-data", "content": "import Icon from \"@site/src/components/icon\";\nData components in Langflow\nData components load data from a source into your flow.\nThey may perform some processing or type checking, like converting raw HTML data into text, or ensuring your loaded file is of an acceptable type.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "Use a data component in a flow", "content": "The **URL** data component loads content from a list of URLs.\nIn the component's **URLs** field, enter the URL you want to load. To add multiple URL fields, click <Icon name=\"Plus\" aria-label=\"Add\"/>.\nAlternatively, connect a component that outputs the `Message` type, like the **Chat Input** component, to supply your URLs from a component.\nIn this example of a document ingestion pipeline, the URL component outputs raw HTML to a text splitter, which splits the raw content into chunks for a vector database to ingest.\n![URL component in a data ingestion pipeline](/img/url-component.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "API Request", "content": "This component makes HTTP requests using URLs or cURL commands.\nTo use this component in a flow, connect the **Data** output to a component that accepts the input.\nFor example, connect the **API Request** component to a **Chat Output** component.\n![API request into a chat output component](/img/component-api-request-chat-output.png)\nIn the API component's **URLs** field, enter the endpoint for your request.\nThis example uses `https://dummy-json.mock.beeceptor.com/posts`, which is a list of technology blog posts.\nIn the **Method** field, enter the type of request.\nThis example uses GET to retrieve a list of blog posts.\nThe component also supports POST, PATCH, PUT, and DELETE.\nOptionally, enable the **Use cURL** button to create a field for pasting curl requests.\nThe equivalent call in this example is `curl -v https://dummy-json.mock.beeceptor.com/posts`.\nClick **Playground**, and then click **Run Flow**.\nYour request returns a list of blog posts in the `result` field.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| urls | URLs | Enter one or more URLs, separated by commas. |\n| curl | cURL | Paste a curl command to populate the dictionary fields for headers and body. |\n| method | Method | The HTTP method to use. |\n| use_curl | Use cURL | Enable cURL mode to populate fields from a cURL command. |\n| query_params | Query Parameters | The query parameters to append to the URL. |\n| body | Body | The body to send with the request as a dictionary (for `POST`, `PATCH`, `PUT`). |\n| headers | Headers | The headers to send with the request as a dictionary. |\n| timeout | Timeout | The timeout to use for the request. |\n| follow_redirects | Follow Redirects | Whether to follow http redirects. |\n| save_to_file | Save to File | Save the API response to a temporary file. |\n| include_httpx_metadata | Include HTTPx Metadata | Include properties such as `headers`, `status_code`, `response_headers`, and `redirection_history` in the output. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | The result of the API requests. Returns a Data object containing source URL and results. |\n| dataframe | DataFrame | Converts the API response data into a tabular DataFrame format. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "Directory", "content": "This component recursively loads files from a directory, with options for file types, depth, and concurrency.\n**Inputs**\n| Input | Type | Description |\n| ------- | ------ | ------------- |\n| path | MessageTextInput | The path to the directory to load files from. |\n| types | MessageTextInput | The file types to load (leave empty to load all types). |\n| depth | IntInput | The depth to search for files. |\n| max_concurrency | IntInput | The maximum concurrency for loading files. |\n| load_hidden | BoolInput | If true, hidden files are loaded. |\n| recursive | BoolInput | If true, the search is recursive. |\n| silent_errors | BoolInput | If true, errors do not raise an exception. |\n| use_multithreading | BoolInput | If true, multithreading is used. |\n**Outputs**\n| Output | Type | Description |\n| -------- | ------ | ------------- |\n| data | List[Data] | The loaded file data from the directory. |\n| dataframe | DataFrame | The loaded file data in tabular DataFrame format. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "File", "content": "This component loads and parses files of various supported formats and converts the content into a [Data](/concepts-objects) object. It supports multiple file types and provides options for parallel processing and error handling.\nTo load a document, follow these steps:\nClick the **Select files** button.\nSelect a local file or a file loaded with [File management](/concepts-file-management), and then click **Select file**.\nThe loaded file name appears in the component.\nThe default maximum supported file size is 100 MB.\nTo modify this value, see [--max-file-size-upload](/environment-variables#LANGFLOW_MAX_FILE_SIZE_UPLOAD).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| path | Files | The path to files to load. Supports individual files or bundled archives. |\n| file_path | Server File Path | A Data object with a `file_path` property pointing to the server file or a Message object with a path to the file. Supersedes 'Path' but supports the same file types. |\n| separator | Separator | The separator to use between multiple outputs in Message format. |\n| silent_errors | Silent Errors | If true, errors do not raise an exception. |\n| delete_server_file_after_processing | Delete Server File After Processing | If true, the Server File Path is deleted after processing. |\n| ignore_unsupported_extensions | Ignore Unsupported Extensions | If true, files with unsupported extensions are not processed. |\n| ignore_unspecified_files | Ignore Unspecified Files | If true, `Data` with no `file_path` property is ignored. |\n| use_multithreading | [Deprecated] Use Multithreading | Set 'Processing Concurrency' greater than `1` to enable multithreading. This option is deprecated. |\n| concurrency_multithreading | Processing Concurrency | When multiple files are being processed, the number of files to process concurrently. Default is 1. Values greater than 1 enable parallel processing for 2 or more files. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | The parsed content of the file as a [Data](/concepts-objects) object. |\n| dataframe | DataFrame | The file content as a [DataFrame](/concepts-objects#dataframe-object) object. |\n| message | Message | The file content as a [Message](/concepts-objects#message-object) object. |\n### Supported File Types\nText files:\n`.txt` - Text files\n`.md`, `.mdx` - Markdown files\n`.csv` - CSV files\n`.json` - JSON files\n`.yaml`, `.yml` - YAML files\n`.xml` - XML files\n`.html`, `.htm` - HTML files\n`.pdf` - PDF files\n`.docx` - Word documents\n`.py` - Python files\n`.sh` - Shell scripts\n`.sql` - SQL files\n`.js` - JavaScript files\n`.ts`, `.tsx` - TypeScript files\nArchive formats (for bundling multiple files):\n`.zip` - ZIP archives\n`.tar` - TAR archives\n`.tgz` - Gzipped TAR archives\n`.bz2` - Bzip2 compressed files\n`.gz` - Gzip compressed files", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "SQL Query", "content": "This component executes SQL queries on a specified database.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| query | Query | The SQL query to execute. |\n| database_url | Database URL | The URL of the database. |\n| include_columns | Include Columns | Include columns in the result. |\n| passthrough | Passthrough | If an error occurs, return the query instead of raising an exception. |\n| add_error | Add Error | Add the error to the result. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| result | Result | The result of the SQL query execution. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "URL", "content": "This component fetches content from one or more URLs, processes the content, and returns it in various formats. It supports output in plain text or raw HTML.\nIn the component's **URLs** field, enter the URL you want to load. To add multiple URL fields, click <Icon name=\"Plus\" aria-label=\"Add\"/>.\nTo use this component in a flow, connect the **DataFrame** output to a component that accepts the input.\nFor example, connect the **URL** component to a **Chat Output** component.\n![URL request into a chat output component](/img/component-url.png)\nIn the URL component's **URLs** field, enter the URL for your request.\nThis example uses `langflow.org`.\nOptionally, in the **Max Depth** field, enter how many pages away from the initial URL you want to crawl.\nSelect `1` to crawl only the page specified in the **URLs** field.\nSelect `2` to crawl all pages linked from that page.\nThe component crawls by link traversal, not by URL path depth.\nClick **Playground**, and then click **Run Flow**.\nThe text contents of the URL are returned to the Playground as a structured DataFrame.\nIn the **URL** component, change the output port to **Message**, and then run the flow again.\nThe text contents of the URL are returned as unstructured raw text, which you can extract patterns from with the **Regex Extractor** tool.\nConnect the **URL** component to a **Regex Extractor** and **Chat Output**.\n![Regex extractor connected to url component](/img/component-url-regex.png)\nIn the **Regex Extractor** tool, enter a pattern to extract text from the **URL** component's raw output.\nThis example extracts the first paragraph from the \"In the News\" section of `https://en.wikipedia.org/wiki/Main_Page`.\nResult:\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| urls | URLs | Click the '+' button to enter one or more URLs to crawl recursively. |\n| max_depth | Max Depth | Controls how many 'clicks' away from the initial page the crawler will go. |\n| prevent_outside | Prevent Outside | If enabled, only crawls URLs within the same domain as the root URL. |\n| use_async | Use Async | If enabled, uses asynchronous loading which can be significantly faster but might use more system resources. |\n| format | Output Format | Output Format. Use `Text` to extract the text from the HTML or `HTML` for the raw HTML content. |\n| timeout | Timeout | Timeout for the request in seconds. |\n| headers | Headers | The headers to send with the request. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| data | Data | A list of [Data](/concepts-objects) objects containing fetched content and metadata. |\n| text | Message | The fetched content as formatted text. |\n| dataframe | DataFrame | The content formatted as a [DataFrame](/concepts-objects#dataframe-object) object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "Webhook", "content": "This component defines a webhook trigger that runs a flow when it receives an HTTP POST request.\nIf the input is not valid JSON, the component wraps it in a `payload` object so that it can be processed and still trigger the flow. The component does not require an API key.\nWhen a **Webhook** component is added to the workspace, a new **Webhook cURL** tab becomes available in the **API** pane that contains an HTTP POST request for triggering the webhook component. For example:\nTo test the webhook component:\nAdd a **Webhook** component to the flow.\nConnect the **Webhook** component's **Data** output to the **Data** input of a [Parser](/components-processing#parser) component.\nConnect the **Parser** component's **Parsed Text** output to the **Text** input of a [Chat Output](/components-io#chat-output) component.\nIn the **Parser** component, under **Mode**, select **Stringify**.\nThis mode passes the webhook's data as a string for the **Chat Output** component to print.\nTo send a POST request, copy the code from the **Webhook cURL** tab in the **API** pane and paste it into a terminal.\nSend the POST request.\nOpen the **Playground**.\nYour JSON data is posted to the **Chat Output** component, which indicates that the webhook component is correctly triggering the flow.\n**Inputs**\n| Name | Display Name | Description |\n| ------ | -------------- | ------------- |\n| data | Payload | Receives a payload from external systems through HTTP POST requests. |\n| curl | cURL | The cURL command template for making requests to this webhook. |\n| endpoint | Endpoint | The endpoint URL where this webhook receives requests. |\n**Outputs**\n| Name | Display Name | Description |\n| ------ | -------------- | ------------- |\n| output_data | Data | Outputs processed data from the webhook input, and returns an empty [Data](/concepts-objects) object if no input is provided. If the input is not valid JSON, the component wraps it in a `payload` object. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-data.md", "section": "Legacy components", "content": "Legacy components are available for use but are no longer supported.\n### Gmail Loader\nThis component loads emails from Gmail using provided credentials and filters.\nFor more information about creating a service account JSON, see [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account).\n**Inputs**\n| Input | Type | Description |\n| ------- | ------ | ------------- |\n| json_string | SecretStrInput | A JSON string containing OAuth 2.0 access token information for service account access. |\n| label_ids | MessageTextInput | A comma-separated list of label IDs to filter emails. |\n| max_results | MessageTextInput | The maximum number of emails to load. |\n**Outputs**\n| Output | Type | Description |\n| -------- | ------ | ------------- |\n| data | Data | The loaded email data. |\n### Google Drive Loader\nThis component loads documents from Google Drive using provided credentials and a single document ID.\nFor more information about creating a service account JSON, see [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account).\n**Inputs**\n| Input | Type | Description |\n| ------- | ------ | ------------- |\n| json_string | SecretStrInput | A JSON string containing OAuth 2.0 access token information for service account access. |\n| document_id | MessageTextInput | A single Google Drive document ID. |\n**Outputs**\n| Output | Type | Description |\n| -------- | ------ | ------------- |\n| docs | Data | The loaded document data. |\n### Google Drive Search\nThis component searches Google Drive files using provided credentials and query parameters.\nFor more information about creating a service account JSON, see [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account).\n**Inputs**\n| Input | Type | Description |\n| ------- | ------ | ------------- |\n| token_string | SecretStrInput | A JSON string containing OAuth 2.0 access token information for service account access. |\n| query_item | DropdownInput | The field to query. |\n| valid_operator | DropdownInput | The operator to use in the query. |\n| search_term | MessageTextInput | The value to search for in the specified query item. |\n| query_string | MessageTextInput | The query string used for searching. |\n**Outputs**\n| Output | Type | Description |\n| -------- | ------ | ------------- |\n| doc_urls | List[str] | The URLs of the found documents. |\n| doc_ids | List[str] | The IDs of the found documents. |\n| doc_titles | List[str] | The titles of the found documents. |\n| Data | Data | The document titles and URLs in a structured format. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "title: Vector stores\nslug: /components-vector-stores", "content": "import Icon from \"@site/src/components/icon\";\nVector store components in Langflow\nVector databases store vector data, which backs AI workloads like chatbots and Retrieval Augmented Generation.\nVector database components establish connections to existing vector databases or create in-memory vector stores for storing and retrieving vector data.\nVector database components are distinct from [memory components](/components-memories), which are built specifically for storing and retrieving chat messages from external databases.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Use a vector store component in a flow", "content": "This example uses the **Astra DB vector store** component. Your vector store component's parameters and authentication may be different, but the document ingestion workflow is the same. A document is loaded from a local machine and chunked. The Astra DB vector store generates embeddings with the connected [model](/components-models) component, and stores them in the connected Astra DB database.\nThis vector data can then be retrieved for workloads like Retrieval Augmented Generation.\n![](/img/vector-store-retrieval.png)\nThe user's chat input is embedded and compared to the vectors embedded during document ingestion for a similarity search.\nThe results are output from the vector database component as a [Data](/concepts-objects) object and parsed into text.\nThis text fills the `{context}` variable in the **Prompt** component, which informs the **Open AI model** component's responses.\nAlternatively, connect the vector database component's **Retriever** port to a [retriever tool](components-tools#retriever-tool), and then to an [agent](/components-agents) component. This enables the agent to use your vector database as a tool and make decisions based on the available data.\n![](/img/vector-store-agent-retrieval-tool.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Astra DB Vector Store", "content": "This component implements a Vector Store using Astra DB with search capabilities.\nFor more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| token | Astra DB Application Token | The authentication token for accessing Astra DB. |\n| environment | Environment | The environment for the Astra DB API Endpoint. For example, `dev` or `prod`. |\n| database_name | Database | The database name for the Astra DB instance. |\n| api_endpoint | Astra DB API Endpoint | The API endpoint for the Astra DB instance. This supersedes the database selection. |\n| collection_name | Collection | The name of the collection within Astra DB where the vectors are stored. |\n| keyspace | Keyspace | An optional keyspace within Astra DB to use for the collection. |\n| embedding_choice | Embedding Model or Astra Vectorize | Choose an embedding model or use Astra vectorize. |\n| embedding_model | Embedding Model | Specify the embedding model. Not required for Astra vectorize collections. |\n| number_of_results | Number of Search Results | The number of search results to return. Default:`4`. |\n| search_type | Search Type | The search type to use. The options are `Similarity`, `Similarity with score threshold`, and `MMR (Max Marginal Relevance)`. |\n| search_score_threshold | Search Score Threshold | The minimum similarity score threshold for search results when using the `Similarity with score threshold` option. |\n| advanced_search_filter | Search Metadata Filter | An optional dictionary of filters to apply to the search query. |\n| autodetect_collection | Autodetect Collection | A boolean flag to determine whether to autodetect the collection. |\n| content_field | Content Field | A field to use as the text content field for the vector store. |\n| deletion_field | Deletion Based On Field | When provided, documents in the target collection with metadata field values matching the input metadata field value are deleted before new data is loaded. |\n| ignore_invalid_documents | Ignore Invalid Documents | A boolean flag to determine whether to ignore invalid documents at runtime. |\n| astradb_vectorstore_kwargs | AstraDBVectorStore Parameters | An optional dictionary of additional parameters for the AstraDBVectorStore. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The Astra DB vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |\n### Generate embeddings\nThe **Astra DB Vector Store** component offers two methods for generating embeddings.\n**Embedding Model**: Use your own embedding model by connecting an [Embeddings](/components-embedding-models) component in Langflow.\n**Astra Vectorize**: Use Astra DB's built-in embedding generation service. When creating a new collection, choose the embeddings provider and models, including NVIDIA's `NV-Embed-QA` model hosted by Datastax.\n:::important\nThe embedding model selection is made when creating a new collection and cannot be changed later.\n:::\nFor an example of using the **Astra DB Vector Store** component with an embedding model, see the [Vector Store RAG starter project](/starter-projects-vector-store-rag).\nFor more information, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html).\n### Hybrid search\nThe **Astra DB** component includes **hybrid search**, which is enabled by default.\nThe component fields related to hybrid search are **Search Query**, **Lexical Terms**, and **Reranker**.\n**Search Query** finds results by vector similarity.\n**Lexical Terms** is a comma-separated string of keywords, like `features, data, attributes, characteristics`.\n**Reranker** is the re-ranker model used in the hybrid search.\nThe re-ranker model is `nvidia/llama-3.2-nv.reranker`.\n[Hybrid search](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html) performs a vector similarity search and a lexical search, compares the results of both searches, and then returns the most relevant results overall.\n:::important\nTo use hybrid search, your collection must be created with vector, lexical, and rerank capabilities enabled. These capabilities are enabled by default when you create a collection in a database in the AWS us-east-2 region.\nFor more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid).\n:::\nTo use **Hybrid search** in the **Astra DB** component, do the following:\nClick **New Flow** > **RAG** > **Hybrid Search RAG**.\nIn the **OpenAI** model component, add your **OpenAI API key**.\nIn the **Astra DB** vector store component, add your **Astra DB Application Token**.\nIn the **Database** field, select your database.\nIn the **Collection** field, select or create a collection with hybrid search capabilities enabled.\nIn the **Playground**, enter a question about your data, such as `What are the features of my data?`\nYour query is sent to two components: an **OpenAI** model component and the **Astra DB** vector database component.\nThe **OpenAI** component contains a prompt for creating the lexical query from your input:\nTo view the keywords and questions the **OpenAI** component generates from your collection, in the **OpenAI** component, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\nTo view the [DataFrame](/concepts-objects#dataframe-object) generated from the **OpenAI** component's response, in the **Structured Output** component, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\nThe DataFrame is passed to a **Parser** component, which parses the contents of the **Keywords** column into a string.\nThis string of comma-separated words is passed to the **Lexical Terms** port of the **Astra DB** component.\n Note that the **Search Query** port of the Astra DB port is connected to the **Chat Input** component from step 6.\n This **Search Query** is vectorized, and both the **Search Query** and **Lexical Terms** content are sent to the reranker at the `find_and_rerank` endpoint.\nThe reranker compares the vector search results against the string of terms from the lexical search.\n The highest-ranked results of your hybrid search are returned to the **Playground**.\nFor more information, see the [DataStax documentation](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html).", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "AstraDB Graph vector store", "content": "This component implements a Vector Store using AstraDB with graph capabilities.\nFor more information, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/tutorials/graph-rag.html).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| collection_name | Collection Name | The name of the collection within AstraDB where the vectors are stored. Required. |\n| token | Astra DB Application Token | Authentication token for accessing AstraDB. Required. |\n| api_endpoint | API Endpoint | API endpoint URL for the AstraDB service. Required. |\n| search_input | Search Input | Query string for similarity search. |\n| ingest_data | Ingest Data | Data to be ingested into the vector store. |\n| namespace | Namespace | Optional namespace within AstraDB to use for the collection. |\n| embedding | Embedding Model | Embedding model to use. |\n| metric | Metric | Distance metric for vector comparisons. The options are \"cosine\", \"euclidean\", \"dot_product\". |\n| setup_mode | Setup Mode | Configuration mode for setting up the vector store. The options are \"Sync\", \"Async\", \"Off\". |\n| pre_delete_collection | Pre Delete Collection | Boolean flag to determine whether to delete the collection before creating a new one. |\n| number_of_results | Number of Results | Number of results to return in similarity search. Default: 4. |\n| search_type | Search Type | Search type to use. The options are \"Similarity\", \"Graph Traversal\", \"Hybrid\". |\n| traversal_depth | Traversal Depth | Maximum depth for graph traversal searches. Default: 1. |\n| search_score_threshold | Search Score Threshold | Minimum similarity score threshold for search results. |\n| search_filter | Search Metadata Filter | Optional dictionary of filters to apply to the search query. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The Graph RAG vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Cassandra", "content": "This component creates a Cassandra Vector Store with search capabilities.\nFor more information, see the [Cassandra documentation](https://cassandra.apache.org/doc/latest/cassandra/vector-search/overview.html).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| database_ref | String | Contact points for the database or AstraDB database ID. |\n| username | String | Username for the database (leave empty for AstraDB). |\n| token | SecretString | User password for the database or AstraDB token. |\n| keyspace | String | Table Keyspace or AstraDB namespace. |\n| table_name | String | Name of the table or AstraDB collection. |\n| ttl_seconds | Integer | Time-to-live for added texts. |\n| batch_size | Integer | Number of data to process in a single batch. |\n| setup_mode | String | Configuration mode for setting up the Cassandra table. |\n| cluster_kwargs | Dict | Additional keyword arguments for the Cassandra cluster. |\n| search_query | String | Query for similarity search. |\n| ingest_data | Data | Data to be ingested into the vector store. |\n| embedding | Embeddings | Embedding function to use. |\n| number_of_results | Integer | Number of results to return in search. |\n| search_type | String | Type of search to perform. |\n| search_score_threshold | Float | Minimum similarity score for search results. |\n| search_filter | Dict | Metadata filters for search query. |\n| body_search | String | Document textual search terms. |\n| enable_body_search | Boolean | Flag to enable body search. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | Cassandra | The Cassandra vector store instance configured with the specified parameters. |\n| search_results | List[Data] | The results of the similarity search as a list of `Data` objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Cassandra Graph Vector Store", "content": "This component implements a Cassandra Graph Vector Store with search capabilities.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| database_ref | Contact Points / Astra Database ID | The contact points for the database or AstraDB database ID. Required. |\n| username | Username | The username for the database. Leave this field empty for AstraDB. |\n| token | Password / AstraDB Token | The user password for the database or AstraDB token. Required. |\n| keyspace | Keyspace | The table Keyspace or AstraDB namespace. Required. |\n| table_name | Table Name | The name of the table or AstraDB collection where vectors are stored. Required. |\n| setup_mode | Setup Mode | The configuration mode for setting up the Cassandra table. The options are \"Sync\" or \"Off\". Default: \"Sync\". |\n| cluster_kwargs | Cluster arguments | An optional dictionary of additional keyword arguments for the Cassandra cluster. |\n| search_query | Search Query | The query string for similarity search. |\n| ingest_data | Ingest Data | The list of data to be ingested into the vector store. |\n| embedding | Embedding | The embedding model to use. |\n| number_of_results | Number of Results | The number of results to return in similarity search. Default: 4. |\n| search_type | Search Type | The search type to use. The options are \"Traversal\", \"MMR traversal\", \"Similarity\", \"Similarity with score threshold\", or \"MMR (Max Marginal Relevance)\". Default: \"Traversal\". |\n| depth | Depth of traversal | The maximum depth of edges to traverse. Used for \"Traversal\" or \"MMR traversal\" search types. Default: 1. |\n| search_score_threshold | Search Score Threshold | The minimum similarity score threshold for search results. Used for \"Similarity with score threshold\" search types. |\n| search_filter | Search Metadata Filter | An optional dictionary of filters to apply to the search query. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The Cassandra Graph vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Chroma DB", "content": "This component creates a Chroma Vector Store with search capabilities.\nThe Chroma DB component creates an ephemeral vector database for experimentation and vector storage.\nTo use this component in a flow, connect it to a component that outputs **Data** or **DataFrame**.\nThis example splits text from a [URL](/components-data#url) component, and computes embeddings with the connected **OpenAI Embeddings** component. Chroma DB computes embeddings by default, but you can connect your own embeddings model, as seen in this example.\n![ChromaDB receiving split text](/img/component-chroma-db.png)\nIn the **Chroma DB** component, in the **Collection** field, enter a name for your embeddings collection.\nOptionally, to persist the Chroma database, in the **Persist** field, enter a directory to store the `chroma.sqlite3` file.\nThis example uses `./chroma-db` to create a directory relative to where Langflow is running.\nTo load data and embeddings into your Chroma database, in the **Chroma DB** component, click <Icon name=\"Play\" aria-label=\"Play icon\" />.\n:::tip\nWhen loading duplicate documents, enable the **Allow Duplicates** option in Chroma DB if you want to store multiple copies of the same content, or disable it to automatically deduplicate your data.\n:::\nTo view the split data, in the **Split Text** component, click <Icon name=\"TextSearch\" aria-label=\"Inspect icon\" />.\nTo query your loaded data, open the **Playground** and query your database.\nYour input is converted to vector data and compared to the stored vectors in a vector similarity search.\nFor more information, see the [Chroma documentation](https://docs.trychroma.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| collection_name | String | The name of the Chroma collection. Default: \"langflow\". |\n| persist_directory | String | The directory to persist the Chroma database. |\n| search_query | String | The query to search for in the vector store. |\n| ingest_data | Data | The data to ingest into the vector store (list of `Data` objects). |\n| embedding | Embeddings | The embedding function to use for the vector store. |\n| chroma_server_cors_allow_origins | String | The CORS allow origins for the Chroma server. |\n| chroma_server_host | String | The host for the Chroma server. |\n| chroma_server_http_port | Integer | The HTTP port for the Chroma server. |\n| chroma_server_grpc_port | Integer | The gRPC port for the Chroma server. |\n| chroma_server_ssl_enabled | Boolean | Enable SSL for the Chroma server. |\n| allow_duplicates | Boolean | Allow duplicate documents in the vector store. |\n| search_type | String | The type of search to perform: \"Similarity\" or \"MMR\". |\n| number_of_results | Integer | The number of results to return from the search. Default: `10`. |\n| limit | Integer | The limit of the number of records to compare when `Allow Duplicates` is `False`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | Chroma | The Chroma vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Clickhouse", "content": "This component implements a Clickhouse Vector Store with search capabilities.\nFor more information, see the [Clickhouse Documentation](https://clickhouse.com/docs/en/intro).\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| host | hostname | The Clickhouse server hostname. Required. Default: \"localhost\". |\n| port | port | The Clickhouse server port. Required. Default: 8123. |\n| database | database | The Clickhouse database name. Required. |\n| table | Table name | The Clickhouse table name. Required. |\n| username | The ClickHouse user name. | Username for authentication. Required. |\n| password | The password for username. | Password for authentication. Required. |\n| index_type | index_type | Type of the index. The options are \"annoy\" and \"vector_similarity\". Default: \"annoy\". |\n| metric | metric | Metric to compute distance. The options are \"angular\", \"euclidean\", \"manhattan\", \"hamming\", \"dot\". Default: \"angular\". |\n| secure | Use https/TLS | Overrides inferred values from the interface or port arguments. Default: false. |\n| index_param | Param of the index | Index parameters. Default: \"'L2Distance',100\". |\n| index_query_params | index query params | Additional index query parameters. |\n| search_query | Search Query | The query string for similarity search. |\n| ingest_data | Ingest Data | The data to be ingested into the vector store. |\n| embedding | Embedding | The embedding model to use. |\n| number_of_results | Number of Results | The number of results to return in similarity search. Default: 4. |\n| score_threshold | Score threshold | The threshold for similarity scores. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The Clickhouse vector store. |\n| search_results | Search Results | The results of the similarity search as a list of Data objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Couchbase", "content": "This component creates a Couchbase Vector Store with search capabilities.\nFor more information, see the [Couchbase documentation](https://docs.couchbase.com/home/index.html).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| couchbase_connection_string | SecretString | Couchbase Cluster connection string. Required. |\n| couchbase_username | String | Couchbase username. Required. |\n| couchbase_password | SecretString | Couchbase password. Required. |\n| bucket_name | String | Name of the Couchbase bucket. Required. |\n| scope_name | String | Name of the Couchbase scope. Required. |\n| collection_name | String | Name of the Couchbase collection. Required. |\n| index_name | String | Name of the Couchbase index. Required. |\n| search_query | String | The query to search for in the vector store. |\n| ingest_data | Data | The list of data to ingest into the vector store. |\n| embedding | Embeddings | The embedding function to use for the vector store. |\n| number_of_results | Integer | Number of results to return from the search. Default: 4. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | CouchbaseVectorStore | A Couchbase vector store instance configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Local DB", "content": "The **Local DB** component is Langflow's enhanced version of Chroma DB.\nThe component adds a user-friendly interface with two modes (Ingest and Retrieve), automatic collection management, and built-in persistence in Langflow's cache directory.\nLocal DB includes **Ingest** and **Retrieve** modes.\nThe **Ingest** mode works similarly to [ChromaDB](#chroma-db), and persists your database to the Langflow cache directory. The Langflow cache directory location is specified in `LANGFLOW_CONFIG_DIR`. For more information, see [Environment variables](/environment-variables).\nThe **Retrieve** mode can query your **Chroma DB** collections.\n![Local DB retrieving vectors](/img/component-local-db.png)\nFor more information, see the [Chroma documentation](https://docs.trychroma.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| collection_name | String | The name of the Chroma collection. Default: \"langflow\". |\n| persist_directory | String | Custom base directory to save the vector store. Collections are stored under `{directory}/vector_stores/{collection_name}`. If not specified, it will use your system's cache folder. |\n| existing_collections | String | Select a previously created collection to search through its stored data. |\n| embedding | Embeddings | The embedding function to use for the vector store. |\n| allow_duplicates | Boolean | If false, will not add documents that are already in the Vector Store. |\n| search_type | String | Type of search to perform: \"Similarity\" or \"MMR\". |\n| ingest_data | Data/DataFrame | Data to store. It is embedded and indexed for semantic search. |\n| search_query | String | Enter text to search for similar content in the selected collection. |\n| number_of_results | Integer | Number of results to return. Default: 10. |\n| limit | Integer | Limit the number of records to compare when Allow Duplicates is False. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | Chroma | A local Chroma vector store instance configured with the specified parameters. |\n| search_results | List[Data](/concepts-objects#data-object) | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Elasticsearch", "content": "This component creates an Elasticsearch Vector Store with search capabilities.\nFor more information, see the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| es_url | String | Elasticsearch server URL. |\n| es_user | String | Username for Elasticsearch authentication. |\n| es_password | SecretString | Password for Elasticsearch authentication. |\n| index_name | String | Name of the Elasticsearch index. |\n| strategy | String | Strategy for vector search. The options are \"approximate_k_nearest_neighbors\" or \"script_scoring\". |\n| distance_strategy | String | Strategy for distance calculation. The options are \"COSINE\", \"EUCLIDEAN_DISTANCE\", or \"DOT_PRODUCT\". |\n| search_query | String | Query for similarity search. |\n| ingest_data | Data | Data to be ingested into the vector store. |\n| embedding | Embeddings | Embedding function to use. |\n| number_of_results | Integer | Number of results to return in search. Default: `4`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | ElasticsearchStore | The Elasticsearch vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "FAISS", "content": "This component creates a FAISS Vector Store with search capabilities.\nFor more information, see the [FAISS documentation](https://faiss.ai/index.html).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| index_name | String | The name of the FAISS index. Default: \"langflow_index\". |\n| persist_directory | String | Path to save the FAISS index. It is relative to where Langflow is running. |\n| search_query | String | The query to search for in the vector store. |\n| ingest_data | Data | The list of data to ingest into the vector store. |\n| allow_dangerous_deserialization | Boolean | Set to True to allow loading pickle files from untrusted sources. Default: True. |\n| embedding | Embeddings | The embedding function to use for the vector store. |\n| number_of_results | Integer | Number of results to return from the search. Default: 4. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The FAISS vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Graph RAG", "content": "This component performs Graph RAG (Retrieval Augmented Generation) traversal in a vector store, enabling graph-based document retrieval.\nFor more information, see the [Graph RAG documentation](https://datastax.github.io/graph-rag/).\nFor an example flow, see the **Graph RAG** template.\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| embedding_model | Embedding Model | Specify the embedding model. This is not required for collections embedded with [Astra vectorize](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). |\n| vector_store | Vector Store Connection | Connection to the vector store. |\n| edge_definition | Edge Definition | Edge definition for the graph traversal. For more information, see the [GraphRAG documentation](https://datastax.github.io/graph-rag/reference/graph_retriever/edges/). |\n| strategy | Traversal Strategies | The strategy to use for graph traversal. Strategy options are dynamically loaded from available strategies. |\n| search_query | Search Query | The query to search for in the vector store. |\n| graphrag_strategy_kwargs | Strategy Parameters | Optional dictionary of additional parameters for the retrieval strategy. For more information, see the [strategy documentation](https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| search_results | List[Data] | Results of the graph-based document retrieval as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Hyper-Converged Database (HCD)", "content": "This component implements a Vector Store using HCD.\nTo use the HCD vector store, add your deployment's collection name, username, password, and HCD Data API endpoint.\nThe endpoint must be formatted like `http[s]://**DOMAIN_NAME** or **IP_ADDRESS**[:port]`, for example, `http://192.0.2.250:8181`.\nReplace **DOMAIN_NAME** or **IP_ADDRESS** with the domain name or IP address of your HCD Data API connection.\nTo use the HCD vector store for embeddings ingestion, connect it to an embeddings model and a file loader:\n![HCD vector store embeddings ingestion](/img/component-hcd-example-flow.png)\n**Inputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| collection_name | Collection Name | The name of the collection within HCD where the vectors will be stored. Required. |\n| username | HCD Username | Authentication username for accessing HCD. Default is \"hcd-superuser\". Required. |\n| password | HCD Password | Authentication password for accessing HCD. Required. |\n| api_endpoint | HCD API Endpoint | API endpoint URL for the HCD service. Required. |\n| search_input | Search Input | Query string for similarity search. |\n| ingest_data | Ingest Data | Data to be ingested into the vector store. |\n| namespace | Namespace | Optional namespace within HCD to use for the collection. Default is \"default_namespace\". |\n| ca_certificate | CA Certificate | Optional CA certificate for TLS connections to HCD. |\n| metric | Metric | Optional distance metric for vector comparisons. Options are \"cosine\", \"dot_product\", \"euclidean\". |\n| batch_size | Batch Size | Optional number of data to process in a single batch. |\n| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Optional concurrency level for bulk insert operations. |\n| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Optional concurrency level for bulk insert operations that overwrite existing data. |\n| bulk_delete_concurrency | Bulk Delete Concurrency | Optional concurrency level for bulk delete operations. |\n| setup_mode | Setup Mode | Configuration mode for setting up the vector store. Options are \"Sync\", \"Async\", \"Off\". Default is \"Sync\". |\n| pre_delete_collection | Pre Delete Collection | Boolean flag to determine whether to delete the collection before creating a new one. |\n| metadata_indexing_include | Metadata Indexing Include | Optional list of metadata fields to include in the indexing. |\n| embedding | Embedding or Astra Vectorize | Allows either an embedding model or an Astra Vectorize configuration. |\n| metadata_indexing_exclude | Metadata Indexing Exclude | Optional list of metadata fields to exclude from the indexing. |\n| collection_indexing_policy | Collection Indexing Policy | Optional dictionary defining the indexing policy for the collection. |\n| number_of_results | Number of Results | Number of results to return in similarity search. Default is 4. |\n| search_type | Search Type | Search type to use. Options are \"Similarity\", \"Similarity with score threshold\", \"MMR (Max Marginal Relevance)\". Default is \"Similarity\". |\n| search_score_threshold | Search Score Threshold | Minimum similarity score threshold for search results. Default is 0. |\n| search_filter | Search Metadata Filter | Optional dictionary of filters to apply to the search query. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | HyperConvergedDatabaseVectorStore | The HCD vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Milvus", "content": "This component creates a Milvus Vector Store with search capabilities.\nFor more information, see the [Milvus documentation](https://milvus.io/docs).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| collection_name | String | Name of the Milvus collection. |\n| collection_description | String | Description of the Milvus collection. |\n| uri | String | Connection URI for Milvus. |\n| password | SecretString | Password for Milvus. |\n| username | SecretString | Username for Milvus. |\n| batch_size | Integer | Number of data to process in a single batch. |\n| search_query | String | Query for similarity search. |\n| ingest_data | Data | Data to be ingested into the vector store. |\n| embedding | Embeddings | Embedding function to use. |\n| number_of_results | Integer | Number of results to return in search. |\n| search_type | String | Type of search to perform. |\n| search_score_threshold | Float | Minimum similarity score for search results. |\n| search_filter | Dict | Metadata filters for search query. |\n| setup_mode | String | Configuration mode for setting up the vector store. |\n| vector_dimensions | Integer | Number of dimensions of the vectors. |\n| pre_delete_collection | Boolean | Whether to delete the collection before creating a new one. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | Milvus | A Milvus vector store instance configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "MongoDB Atlas", "content": "This component creates a MongoDB Atlas Vector Store with search capabilities.\nFor more information, see the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| mongodb_atlas_cluster_uri | SecretString | The connection URI for your MongoDB Atlas cluster. Required. |\n| enable_mtls | Boolean | Enable mutual TLS authentication. Default: false. |\n| mongodb_atlas_client_cert | SecretString | Client certificate combined with private key for mTLS authentication. Required if mTLS is enabled. |\n| db_name | String | The name of the database to use. Required. |\n| collection_name | String | The name of the collection to use. Required. |\n| index_name | String | The name of the Atlas Search index, it should be a Vector Search. Required. |\n| insert_mode | String | How to insert new documents into the collection. The options are \"append\" or \"overwrite\". Default: \"append\". |\n| embedding | Embeddings | The embedding model to use. |\n| number_of_results | Integer | Number of results to return in similarity search. Default: 4. |\n| index_field | String | The field to index. Default: \"embedding\". |\n| filter_field | String | The field to filter the index. |\n| number_dimensions | Integer | Embedding context length. Default: 1536. |\n| similarity | String | The method used to measure similarity between vectors. The options are \"cosine\", \"euclidean\", or \"dotProduct\". Default: \"cosine\". |\n| quantization | String | Quantization reduces memory costs by converting 32-bit floats to smaller data types. The options are \"scalar\" or \"binary\". |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | MongoDBAtlasVectorSearch | The MongoDB Atlas vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Opensearch", "content": "This component creates an Opensearch vector store with search capabilities\nFor more information, see [Opensearch documentation](https://opensearch.org/platform/search/vector-database.html).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| opensearch_url | String | URL for OpenSearch cluster, such as `https://192.168.1.1:9200`. |\n| index_name | String | The index name where the vectors are stored in OpenSearch cluster. |\n| search_input | String | Enter a search query. Leave empty to retrieve all documents or if hybrid search is being used. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| embedding | Embeddings | The embedding function to use. |\n| search_type | String | The options are \"similarity\", \"similarity_score_threshold\", \"mmr\". |\n| number_of_results | Integer | The number of results to return in search. |\n| search_score_threshold | Float | The minimum similarity score threshold for search results. |\n| username | String | The username for the opensource cluster. |\n| password | SecretString | The password for the opensource cluster. |\n| use_ssl | Boolean | Use SSL. |\n| verify_certs | Boolean | Verify certificates. |\n| hybrid_search_query | String | Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | OpenSearchVectorSearch | OpenSearch vector store instance |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "PGVector", "content": "This component creates a PGVector Vector Store with search capabilities.\nFor more information, see the [PGVector documentation](https://github.com/pgvector/pgvector).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| pg_server_url | SecretString | The PostgreSQL server connection string. |\n| collection_name | String | The table name for the vector store. |\n| search_query | String | The query for similarity search. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| embedding | Embeddings | The embedding function to use. |\n| number_of_results | Integer | The number of results to return in search. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The PGVector vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Pinecone", "content": "This component creates a Pinecone Vector Store with search capabilities.\nFor more information, see the [Pinecone documentation](https://docs.pinecone.io/home).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| index_name | String | The name of the Pinecone index. |\n| namespace | String | The namespace for the index. |\n| distance_strategy | String | The strategy for calculating distance between vectors. |\n| pinecone_api_key | SecretString | The API key for Pinecone. |\n| text_key | String | The key in the record to use as text. |\n| search_query | String | The query for similarity search. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| embedding | Embeddings | The embedding function to use. |\n| number_of_results | Integer | The number of results to return in search. |\n**Outputs**\n| Name | Display Name | Info |\n| ------ | -------------- | ------ |\n| vector_store | Vector Store | The Pinecone vector store instance configured with the specified parameters. |\n| search_results | Search Results | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Qdrant", "content": "This component creates a Qdrant Vector Store with search capabilities.\nFor more information, see the [Qdrant documentation](https://qdrant.tech/documentation/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| collection_name | String | The name of the Qdrant collection. |\n| host | String | The Qdrant server host. |\n| port | Integer | The Qdrant server port. |\n| grpc_port | Integer | The Qdrant gRPC port. |\n| api_key | SecretString | The API key for Qdrant. |\n| prefix | String | The prefix for Qdrant. |\n| timeout | Integer | The timeout for Qdrant operations. |\n| path | String | The path for Qdrant. |\n| url | String | The URL for Qdrant. |\n| distance_func | String | The distance function for vector similarity. |\n| content_payload_key | String | The content payload key. |\n| metadata_payload_key | String | The metadata payload key. |\n| search_query | String | The query for similarity search. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| embedding | Embeddings | The embedding function to use. |\n| number_of_results | Integer | The number of results to return in search. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | Qdrant | A Qdrant vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Redis", "content": "This component creates a Redis Vector Store with search capabilities.\nFor more information, see the [Redis documentation](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| redis_server_url | SecretString | The Redis server connection string. |\n| redis_index_name | String | The name of the Redis index. |\n| code | String | The custom code for Redis (advanced). |\n| schema | String | The schema for Redis index. |\n| search_query | String | The query for similarity search. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| number_of_results | Integer | The number of results to return in search. |\n| embedding | Embeddings | The embedding function to use. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | Redis | Redis vector store instance |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Supabase", "content": "This component creates a connection to a Supabase Vector Store with search capabilities.\nFor more information, see the [Supabase documentation](https://supabase.com/docs/guides/ai).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| supabase_url | String | The URL of the Supabase instance. |\n| supabase_service_key | SecretString | The service key for Supabase authentication. |\n| table_name | String | The name of the table in Supabase. |\n| query_name | String | The name of the query to use. |\n| search_query | String | The query for similarity search. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| embedding | Embeddings | The embedding function to use. |\n| number_of_results | Integer | The number of results to return in search. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | SupabaseVectorStore | A Supabase vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Upstash", "content": "This component creates an Upstash Vector Store with search capabilities.\nFor more information, see the [Upstash documentation](https://upstash.com/docs/introduction).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| index_url | String | The URL of the Upstash index. |\n| index_token | SecretString | The token for the Upstash index. |\n| text_key | String | The key in the record to use as text. |\n| namespace | String | The namespace for the index. |\n| search_query | String | The query for similarity search. |\n| metadata_filter | String | Filter documents by metadata. |\n| ingest_data | Data | The data to be ingested into the vector store. |\n| embedding | Embeddings | The embedding function to use. |\n| number_of_results | Integer | The number of results to return in search. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | UpstashVectorStore | An Upstash vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Vectara", "content": "This component creates a Vectara Vector Store with search capabilities.\nFor more information, see the [Vectara documentation](https://docs.vectara.com/docs/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vectara_customer_id | String | The Vectara customer ID. |\n| vectara_corpus_id | String | The Vectara corpus ID. |\n| vectara_api_key | SecretString | The Vectara API key. |\n| embedding | Embeddings | The embedding function to use (optional). |\n| ingest_data | List[Document/Data] | The data to be ingested into the vector store. |\n| search_query | String | The query for similarity search. |\n| number_of_results | Integer | The number of results to return in search. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | VectaraVectorStore | Vectara vector store instance. |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Vectara Search", "content": "This component searches a Vectara Vector Store for documents based on the provided input.\nFor more information, see the [Vectara documentation](https://docs.vectara.com/docs/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| search_type | String | The type of search, such as \"Similarity\" or \"MMR\". |\n| input_value | String | The search query. |\n| vectara_customer_id | String | The Vectara customer ID. |\n| vectara_corpus_id | String | The Vectara corpus ID. |\n| vectara_api_key | SecretString | The Vectara API key. |\n| files_url | List[String] | Optional URLs for file initialization. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Weaviate", "content": "This component facilitates a Weaviate Vector Store setup, optimizing text and document indexing and retrieval.\nFor more information, see the [Weaviate Documentation](https://weaviate.io/developers/weaviate).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| weaviate_url | String | The default instance URL. |\n| search_by_text | Boolean | Indicates whether to search by text. |\n| api_key | SecretString | The optional API key for authentication. |\n| index_name | String | The optional index name. |\n| text_key | String | The default text extraction key. |\n| input | Document | The document or record. |\n| embedding | Embeddings | The embedding model used. |\n| attributes | List[String] | Optional additional attributes. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| vector_store | WeaviateVectorStore | The Weaviate vector store instance. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-vector-stores.md", "section": "Weaviate Search", "content": "This component searches a Weaviate Vector Store for documents similar to the input.\nFor more information, see the [Weaviate Documentation](https://weaviate.io/developers/weaviate).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| search_type | String | The type of search, such as \"Similarity\" or \"MMR\" |\n| input_value | String | The search query. |\n| weaviate_url | String | The default instance URL. |\n| search_by_text | Boolean | A boolean value that indicates whether to search by text. |\n| api_key | SecretString | The optional API key for authentication. |\n| index_name | String | The optional index name. |\n| text_key | String | The default text extraction key. |\n| embedding | Embeddings | The embeddings model used. |\n| attributes | List[String] | Optional additional attributes. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| search_results | List[Data] | The results of the similarity search as a list of [Data](/concepts-objects#data-object) objects. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "title: Models\nslug: /components-models", "content": "import Icon from \"@site/src/components/icon\";\nModel components in Langflow\nModel components generate text using large language models.\nRefer to your specific component's documentation for more information on parameters.", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Use a model component in a flow", "content": "Model components receive inputs and prompts for generating text, and the generated text is sent to an output component.\nThe model output can also be sent to the **Language Model** port and on to a **Parse Data** component, where the output can be parsed into structured [Data](/concepts-objects) objects.\nThis example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/starter-projects-basic-prompting).\n![](/img/starter-flow-basic-prompting.png)", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "AIML", "content": "This component creates a ChatOpenAI model instance using the AIML API.\nFor more information, see [AIML documentation](https://docs.aimlapi.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000. |\n| model_kwargs | Dictionary | Additional keyword arguments for the model. |\n| model_name | String | The name of the AIML model to use. Options are predefined in `AIML_CHAT_MODELS`. |\n| aiml_api_base | String | The base URL of the AIML API. Defaults to `https://api.aimlapi.com`. |\n| api_key | SecretString | The AIML API Key to use for the model. |\n| temperature | Float | Controls randomness in the output. Default: `0.1`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatOpenAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Amazon Bedrock", "content": "This component generates text using Amazon Bedrock LLMs.\nFor more information, see [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model_id | String | The ID of the Amazon Bedrock model to use. Options include various models. |\n| aws_access_key | SecretString | AWS Access Key for authentication. |\n| aws_secret_key | SecretString | AWS Secret Key for authentication. |\n| aws_session_token | SecretString | The session key for your AWS account. |\n| credentials_profile_name | String | Name of the AWS credentials profile to use. |\n| region_name | String | AWS region name. Default: `us-east-1`. |\n| model_kwargs | Dictionary | Additional keyword arguments for the model. |\n| endpoint_url | String | Custom endpoint URL for the Bedrock service. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatBedrock configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Anthropic", "content": "This component allows the generation of text using Anthropic Chat and Language models.\nFor more information, see the [Anthropic documentation](https://docs.anthropic.com/en/docs/welcome).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens. Default: `4096`. |\n| model | String | The name of the Anthropic model to use. Options include various Claude 3 models. |\n| anthropic_api_key | SecretString | Your Anthropic API key for authentication. |\n| temperature | Float | Controls randomness in the output. Default: `0.1`. |\n| anthropic_api_url | String | Endpoint of the Anthropic API. Defaults to `https://api.anthropic.com` if not specified (advanced). |\n| prefill | String | Prefill text to guide the model's response (advanced). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatAnthropic configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Azure OpenAI", "content": "This component generates text using Azure OpenAI LLM.\nFor more information, see the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Model Name | String | Specifies the name of the Azure OpenAI model to be used for text generation. |\n| Azure Endpoint | String | Your Azure endpoint, including the resource. |\n| Deployment Name | String | Specifies the name of the deployment. |\n| API Version | String | Specifies the version of the Azure OpenAI API to be used. |\n| API Key | SecretString | Your Azure OpenAI API key. |\n| Temperature | Float | Specifies the sampling temperature. Defaults to `0.7`. |\n| Max Tokens | Integer | Specifies the maximum number of tokens to generate. Defaults to `1000`. |\n| Input Value | String | Specifies the input text for text generation. |\n| Stream | Boolean | Specifies whether to stream the response from the model. Defaults to `False`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of AzureOpenAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Cohere", "content": "This component generates text using Cohere's language models.\nFor more information, see the [Cohere documentation](https://cohere.ai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Cohere API Key | SecretString | Your Cohere API key. |\n| Max Tokens | Integer | Specifies the maximum number of tokens to generate. Defaults to `256`. |\n| Temperature | Float | Specifies the sampling temperature. Defaults to `0.75`. |\n| Input Value | String | Specifies the input text for text generation. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of the Cohere model configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "DeepSeek", "content": "This component generates text using DeepSeek's language models.\nFor more information, see the [DeepSeek documentation](https://api-docs.deepseek.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |\n| model_kwargs | Dictionary | Additional keyword arguments for the model. |\n| json_mode | Boolean | If `True`, outputs JSON regardless of passing a schema. |\n| model_name | String | The DeepSeek model to use. Default: `deepseek-chat`. |\n| api_base | String | Base URL for API requests. Default: `https://api.deepseek.com`. |\n| api_key | SecretString | Your DeepSeek API key for authentication. |\n| temperature | Float | Controls randomness in responses. Range: `[0.0, 2.0]`. Default: `1.0`. |\n| seed | Integer | Number initialized for random number generation. Use the same seed integer for more reproducible results, and use a different seed number for more random results. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatOpenAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Google Generative AI", "content": "This component generates text using Google's Generative AI models.\nFor more information, see the [Google Generative AI documentation](https://cloud.google.com/vertex-ai/docs/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Google API Key | SecretString | Your Google API key to use for the Google Generative AI. |\n| Model | String | The name of the model to use, such as `\"gemini-pro\"`. |\n| Max Output Tokens | Integer | The maximum number of tokens to generate. |\n| Temperature | Float | Run inference with this temperature. |\n| Top K | Integer | Consider the set of top K most probable tokens. |\n| Top P | Float | The maximum cumulative probability of tokens to consider when sampling. |\n| N | Integer | Number of chat completions to generate for each prompt. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatGoogleGenerativeAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Groq", "content": "This component generates text using Groq's language models.\nTo use this component in a flow, connect it as a **Model** in a flow like the [Basic prompting flow](/starter-projects-basic-prompting), or select it as the **Model Provider** if you're using an **Agent** component.\n![Groq component in a basic prompting flow](/img/component-groq.png)\nIn the **Groq API Key** field, paste your Groq API key.\nThe Groq model component automatically retrieves a list of the latest models.\nTo refresh your list of models, click <Icon name=\"RefreshCw\" aria-label=\"Refresh\"/>.\nIn the **Model** field, select the model you want to use for your LLM.\nThis example uses [llama-3.1-8b-instant](https://console.groq.com/docs/model/llama-3.1-8b-instant), which Groq recommends for real-time conversational interfaces.\nIn the **Prompt** component, enter:\nClick **Playground** and ask your Groq LLM a question.\nThe responses include a list of sources.\nFor more information, see the [Groq documentation](https://groq.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| groq_api_key | SecretString | API key for the Groq API. |\n| groq_api_base | String | Base URL path for API requests. Default: `https://api.groq.com`. |\n| max_tokens | Integer | The maximum number of tokens to generate. |\n| temperature | Float | Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.1`. |\n| n | Integer | Number of chat completions to generate for each prompt. |\n| model_name | String | The name of the Groq model to use. Options are dynamically fetched from the Groq API. |\n| tool_mode_enabled | Bool | If enabled, the component only displays models that work with tools. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatGroq configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Hugging Face API", "content": "This component sends requests to the Hugging Face API to generate text using the model specified in the **Model ID** field.\nThe Hugging Face API is a hosted inference API for models hosted on Hugging Face, and requires a [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) to authenticate.\nIn this example based on the [Basic prompting flow](/starter-projects-basic-prompting), the **Hugging Face API** model component replaces the **Open AI** model. By selecting different hosted models, you can see how different models return different results.\nCreate a [Basic prompting flow](/starter-projects-basic-prompting).\nReplace the **OpenAI** model component with a **Hugging Face API** model component.\nIn the **Hugging Face API** component, add your Hugging Face API token to the **API Token** field.\nOpen the **Playground** and ask a question to the model, and see how it responds.\nTry different models, and see how they perform differently.\nFor more information, see the [Hugging Face documentation](https://huggingface.co/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model_id | String | The model ID from Hugging Face Hub. For example, \"gpt2\", \"facebook/bart-large\". |\n| huggingfacehub_api_token | SecretString | Your Hugging Face API token for authentication. |\n| temperature | Float | Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| max_new_tokens | Integer | Maximum number of tokens to generate. Default: 512. |\n| top_p | Float | Nucleus sampling parameter. Range: [0.0, 1.0]. Default: 0.95. |\n| top_k | Integer | Top-k sampling parameter. Default: 50. |\n| model_kwargs | Dictionary | Additional keyword arguments to pass to the model. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of HuggingFaceHub configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "IBM watsonx.ai", "content": "This component generates text using [IBM watsonx.ai](https://www.ibm.com/watsonx) foundation models.\nTo use **IBM watsonx.ai** model components, replace a model component with the IBM watsonx.ai component in a flow.\nAn example flow looks like the following:\n![IBM watsonx model component in a basic prompting flow](/img/component-watsonx-model.png)\nThe values for **API endpoint**, **Project ID**, **API key**, and **Model Name** are found in your IBM watsonx.ai deployment.\nFor more information, see the [Langchain documentation](https://python.langchain.com/docs/integrations/chat/ibm_watsonx/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| url | String | The base URL of the watsonx API. |\n| project_id | String | Your watsonx Project ID. |\n| api_key | SecretString | Your IBM watsonx API Key. |\n| model_name | String | The name of the watsonx model to use. Options are dynamically fetched from the API. |\n| max_tokens | Integer | The maximum number of tokens to generate. Default: `1000`. |\n| stop_sequence | String | The sequence where generation should stop. |\n| temperature | Float | Controls randomness in the output. Default: `0.1`. |\n| top_p | Float | Controls nucleus sampling, which limits the model to tokens whose probability is below the `top_p` value. Range: Default: `0.9`. |\n| frequency_penalty | Float | Controls frequency penalty. A positive value decreases the probability of repeating tokens, and a negative value increases the probability. Range: Default: `0.5`. |\n| presence_penalty | Float | Controls presence penalty. A positive value increases the likelihood of new topics being introduced. Default: `0.3`. |\n| seed | Integer | A random seed for the model. Default: `8`. |\n| logprobs | Boolean | Whether to return log probabilities of output tokens or not. Default: `True`. |\n| top_logprobs | Integer | The number of most likely tokens to return at each position. Default: `3`. |\n| logit_bias | String | A JSON string of token IDs to bias or suppress. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of [ChatWatsonx](https://python.langchain.com/docs/integrations/chat/ibm_watsonx/) configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Language model", "content": "This component generates text using either OpenAI or Anthropic language models.\nUse this component as a drop-in replacement for LLM models to switch between different model providers and models.\nInstead of swapping out model components when you want to try a different provider, like switching between OpenAI and Anthropic components, change the provider dropdown in this single component. This makes it easier to experiment with and compare different models while keeping the rest of your flow intact.\nFor more information, see the [OpenAI documentation](https://platform.openai.com/docs) and [Anthropic documentation](https://docs.anthropic.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| provider | String | The model provider to use. Options: \"OpenAI\", \"Anthropic\". Default: \"OpenAI\". |\n| model_name | String | The name of the model to use. Options depend on the selected provider. |\n| api_key | SecretString | The API Key for authentication with the selected provider. |\n| input_value | String | The input text to send to the model. |\n| system_message | String | A system message that helps set the behavior of the assistant (advanced). |\n| stream | Boolean | Whether to stream the response. Default: `False` (advanced). |\n| temperature | Float | Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1` (advanced). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatOpenAI or ChatAnthropic configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "LMStudio", "content": "This component generates text using LM Studio's local language models.\nFor more information, see [LM Studio documentation](https://lmstudio.ai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| base_url | String | The URL where LM Studio is running. Default: `\"http://localhost:1234\"`. |\n| max_tokens | Integer | Maximum number of tokens to generate in the response. Default: `512`. |\n| temperature | Float | Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.7`. |\n| top_p | Float | Controls diversity via nucleus sampling. Range: `[0.0, 1.0]`. Default: `1.0`. |\n| stop | List[String] | List of strings that stop generation when encountered. |\n| stream | Boolean | Whether to stream the response. Default: `False`. |\n| presence_penalty | Float | Penalizes repeated tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |\n| frequency_penalty | Float | Penalizes frequent tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of LMStudio configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Maritalk", "content": "This component generates text using Maritalk LLMs.\nFor more information, see [Maritalk documentation](https://www.maritalk.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to `0` for unlimited tokens. Default: `512`. |\n| model_name | String | The name of the Maritalk model to use. Options: `sabia-2-small`, `sabia-2-medium`. Default: `sabia-2-small`. |\n| api_key | SecretString | The Maritalk API Key to use for authentication. |\n| temperature | Float | Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.5`. |\n| endpoint_url | String | The Maritalk API endpoint. Default: `https://api.maritalk.com`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatMaritalk configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Mistral", "content": "This component generates text using MistralAI LLMs.\nFor more information, see [Mistral AI documentation](https://docs.mistral.ai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens (advanced). |\n| model_name | String | The name of the Mistral AI model to use. Options include `open-mixtral-8x7b`, `open-mixtral-8x22b`, `mistral-small-latest`, `mistral-medium-latest`, `mistral-large-latest`, and `codestral-latest`. Default: `codestral-latest`. |\n| mistral_api_base | String | The base URL of the Mistral API. Defaults to `https://api.mistral.ai/v1` (advanced). |\n| api_key | SecretString | The Mistral API Key to use for authentication. |\n| temperature | Float | Controls randomness in the output. Default: 0.5. |\n| max_retries | Integer | Maximum number of retries for API calls. Default: 5 (advanced). |\n| timeout | Integer | Timeout for API calls in seconds. Default: 60 (advanced). |\n| max_concurrent_requests | Integer | Maximum number of concurrent API requests. Default: 3 (advanced). |\n| top_p | Float | Nucleus sampling parameter. Default: 1 (advanced). |\n| random_seed | Integer | Seed for random number generation. Default: 1 (advanced). |\n| safe_mode | Boolean | Enables safe mode for content generation (advanced). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatMistralAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Novita AI", "content": "This component generates text using Novita AI's language models.\nFor more information, see [Novita AI documentation](https://novita.ai/docs/model-api/reference/llm/llm.html?utm_source=github_langflow&utm_medium=github_readme&utm_campaign=link).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| api_key | SecretString | Your Novita AI API Key. |\n| model | String | The id of the Novita AI model to use. |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| top_p | Float | Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |\n| frequency_penalty | Float | Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |\n| presence_penalty | Float | Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of Novita AI model configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "NVIDIA", "content": "This component generates text using NVIDIA LLMs.\nFor more information, see [NVIDIA AI documentation](https://developer.nvidia.com/generative-ai).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to `0` for unlimited tokens (advanced). |\n| model_name | String | The name of the NVIDIA model to use. Default: `mistralai/mixtral-8x7b-instruct-v0.1`. |\n| base_url | String | The base URL of the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |\n| nvidia_api_key | SecretString | The NVIDIA API Key for authentication. |\n| temperature | Float | Controls randomness in the output. Default: `0.1`. |\n| seed | Integer | The seed controls the reproducibility of the job (advanced). Default: `1`. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatNVIDIA configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Ollama", "content": "This component generates text using Ollama's language models.\nTo use this component in a flow, connect Langflow to your locally running Ollama server and select a model.\nIn the Ollama component, in the **Base URL** field, enter the address for your locally running Ollama server.\nThis value is set as the `OLLAMA_HOST` environment variable in Ollama.\nThe default base URL is `http://127.0.0.1:11434`.\nTo refresh the server's list of models, click <Icon name=\"RefreshCw\" aria-label=\"Refresh\"/>.\nIn the **Model Name** field, select a model. This example uses `llama3.2:latest`.\nConnect the **Ollama** model component to a flow. For example, this flow connects a local Ollama server running a Llama 3.2 model as the custom model for an [Agent](/components-agents) component.\n![Ollama model as Agent custom model](/img/component-ollama-model.png)\nFor more information, see the [Ollama documentation](https://ollama.com/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| Base URL | String | Endpoint of the Ollama API. |\n| Model Name | String | The model name to use. |\n| Temperature | Float | Controls the creativity of model responses. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of an Ollama model configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "OpenAI", "content": "This component generates text using OpenAI's language models.\nFor more information, see [OpenAI documentation](https://beta.openai.com/docs/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| api_key | SecretString | Your OpenAI API Key. |\n| model | String | The name of the OpenAI model to use. Options include \"gpt-3.5-turbo\" and \"gpt-4\". |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| top_p | Float | Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |\n| frequency_penalty | Float | Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |\n| presence_penalty | Float | Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of OpenAI model configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "OpenRouter", "content": "This component generates text using OpenRouter's unified API for multiple AI models from different providers.\nFor more information, see [OpenRouter documentation](https://openrouter.ai/docs).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| api_key | SecretString | Your OpenRouter API key for authentication. |\n| site_url | String | Your site URL for OpenRouter rankings (advanced). |\n| app_name | String | Your app name for OpenRouter rankings (advanced). |\n| provider | String | The AI model provider to use. |\n| model_name | String | The specific model to use for chat completion. |\n| temperature | Float | Controls randomness in the output. Range: [0.0, 2.0]. Default: 0.7. |\n| max_tokens | Integer | The maximum number of tokens to generate (advanced). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatOpenAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Perplexity", "content": "This component generates text using Perplexity's language models.\nFor more information, see [Perplexity documentation](https://perplexity.ai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model_name | String | The name of the Perplexity model to use. Options include various Llama 3.1 models. |\n| max_output_tokens | Integer | The maximum number of tokens to generate. |\n| api_key | SecretString | The Perplexity API Key for authentication. |\n| temperature | Float | Controls randomness in the output. Default: 0.75. |\n| top_p | Float | The maximum cumulative probability of tokens to consider when sampling (advanced). |\n| n | Integer | Number of chat completions to generate for each prompt (advanced). |\n| top_k | Integer | Number of top tokens to consider for top-k sampling. Must be positive (advanced). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatPerplexity configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "Qianfan", "content": "This component generates text using Qianfan's language models.\nFor more information, see [Qianfan documentation](https://github.com/baidubce/bce-qianfan-sdk).", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "SambaNova", "content": "This component generates text using SambaNova LLMs.\nFor more information, see [Sambanova Cloud documentation](https://cloud.sambanova.ai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| sambanova_url | String | Base URL path for API requests. Default: `https://api.sambanova.ai/v1/chat/completions`. |\n| sambanova_api_key | SecretString | Your SambaNova API Key. |\n| model_name | String | The name of the Sambanova model to use. Options include various Llama models. |\n| max_tokens | Integer | The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.07. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of SambaNova model configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "VertexAI", "content": "This component generates text using Vertex AI LLMs.\nFor more information, see [Google Vertex AI documentation](https://cloud.google.com/vertex-ai).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| credentials | File | JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. |\n| model_name | String | The name of the Vertex AI model to use. Default: \"gemini-1.5-pro\". |\n| project | String | The project ID (advanced). |\n| location | String | The location for the Vertex AI API. Default: \"us-central1\" (advanced). |\n| max_output_tokens | Integer | The maximum number of tokens to generate (advanced). |\n| max_retries | Integer | Maximum number of retries for API calls. Default: 1 (advanced). |\n| temperature | Float | Controls randomness in the output. Default: 0.0. |\n| top_k | Integer | The number of highest probability vocabulary tokens to keep for top-k-filtering (advanced). |\n| top_p | Float | The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Default: 0.95 (advanced). |\n| verbose | Boolean | Whether to print verbose output. Default: False (advanced). |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatVertexAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
{"source": "/Users/pedropacheco/Projects/dev/langflow.current/docs/docs/Components/components-models.md", "section": "xAI", "content": "This component generates text using xAI models like [Grok](https://x.ai/grok).\nFor more information, see the [xAI documentation](https://x.ai/).\n**Inputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| max_tokens | Integer | Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |\n| model_kwargs | Dictionary | Additional keyword arguments for the model. |\n| json_mode | Boolean | If `True`, outputs JSON regardless of passing a schema. |\n| model_name | String | The xAI model to use. Default: `grok-2-latest`. |\n| base_url | String | Base URL for API requests. Default: `https://api.x.ai/v1`. |\n| api_key | SecretString | Your xAI API key for authentication. |\n| temperature | Float | Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.1`. |\n| seed | Integer | Controls reproducibility of the job. |\n**Outputs**\n| Name | Type | Description |\n| ------ | ------ | ------------- |\n| model | LanguageModel | An instance of ChatOpenAI configured with the specified parameters. |", "metadata": {"parser": "langflow_docs_markdown", "level": "h2"}, "content_type": "heading"}
